{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0505 12:12:16.262557 139954278307648 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-05-05 12:12:17 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - logger已啟動\n",
      "I0505 12:12:17.141977 139954278307648 train_util.py:102] logger已啟動\n"
     ]
    }
   ],
   "source": [
    "from utils import config\n",
    "from utils.seq2seq import data\n",
    "\n",
    "from utils.seq2seq.batcher import *\n",
    "from utils.seq2seq.train_util import *\n",
    "from utils.seq2seq.rl_util import *\n",
    "from utils.seq2seq.initialize import loadCheckpoint, save_model\n",
    "from utils.seq2seq.write_result import *\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from translate.seq2seq_beam import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from utils.seq2seq.rl_util import *\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--key_attention', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--intra_encoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--intra_decoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--copy', type=bool, default=True, help = 'True/False') # for transformer\n",
    "\n",
    "parser.add_argument('--model_type', type=str, default='seq2seq', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--train_rl', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_FOP_keywords', \n",
    "                    help = 'POS_FOP_keywords / DEP_FOP_keywords / TextRank_keywords')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--rand_unif_init_mag', type=float, default=0.02)\n",
    "parser.add_argument('--trunc_norm_init_std', type=float, default=0.001)\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument('--gound_truth_prob', type=float, default=0.1)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=1000)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=50)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=8)\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=16)\n",
    "parser.add_argument('--batch_size', type=int, default=4)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=300)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default='0630000', help='0002000')\n",
    "parser.add_argument('--word_emb_type', type=str, default='word2Vec', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=True, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 12:13:25 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - train : 439076, test : 48787\n",
      "I0505 12:13:25.375670 139954278307648 batcher.py:178] train : 439076, test : 48787\n",
      "2020-05-05 12:13:25 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - train batches : 109769, test batches : 12196\n",
      "I0505 12:13:25.763247 139954278307648 batcher.py:192] train batches : 109769, test batches : 12196\n"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "train_batches = len(iter(train_loader))\n",
    "test_batches = len(iter(validate_loader))\n",
    "save_steps = int(train_batches/1000)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0505 12:13:26.309538 139954278307648 utils_any2vec.py:341] loading projection weights from ../Train-Data/Mix6_mainCat/Embedding/word2Vec/word2Vec.300d.txt\n",
      "I0505 12:13:37.929805 139954278307648 utils_any2vec.py:405] loaded (49050, 300) matrix from ../Train-Data/Mix6_mainCat/Embedding/word2Vec/word2Vec.300d.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0630000.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 12:13:43 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Loaded model at model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0630000.tar\n",
      "I0505 12:13:43.591931 139954278307648 initialize.py:212] Loaded model at model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0630000.tar\n",
      "2020-05-05 12:13:43 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Loaded model step = 630000, loss = 2.13, r_loss = 0.00 \n",
      "I0505 12:13:43.593542 139954278307648 initialize.py:213] Loaded model step = 630000, loss = 2.13, r_loss = 0.00 \n"
     ]
    }
   ],
   "source": [
    "from seq2seq import Model\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "load_step = None\n",
    "model = Model(pre_train_emb=config.pre_train_emb, \n",
    "              word_emb_type = config.word_emb_type, \n",
    "              vocab = vocab)\n",
    "\n",
    "model = model.cuda()\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=config.lr)   \n",
    "# optimizer = T.optim.Adagrad(model.parameters(),lr=config.lr, initial_accumulator_value=0.1)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(logger, load_model_path, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model, config, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    " \n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input \n",
    "#         print(enc_key_batch.shape)\n",
    "#         print(enc_key_batch[0])\n",
    "#         print(enc_key_batch[0])\n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        'Decoder data'\n",
    "        dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(batch, config, batch_first = True) # Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(START))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "            use_gound_truth = get_cuda((T.rand(len(enc_out)) > config.gound_truth_prob)).long()  # Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "            x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t  # Select decoder input based on use_ground_truth probabilities\n",
    "            x_t = model.embeds(x_t)  \n",
    "            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                      ct_e, extra_zeros,\n",
    "                                                                                      enc_batch_extend_vocab,\n",
    "                                                                                      sum_temporal_srcs, prev_s, enc_key_batch, enc_key_mask)\n",
    "            target = target_batch[:, t]\n",
    "            log_probs = T.log(final_dist + config.eps)\n",
    "            step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=PAD)\n",
    "            step_losses.append(step_loss)\n",
    "            x_t = T.multinomial(final_dist,1).squeeze()  # Sample words from final distribution which can be used as input in next time step\n",
    "\n",
    "            is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t.detach() + (is_oov) * UNKNOWN_TOKEN  # Replace OOVs with [UNK] token\n",
    "\n",
    "        losses = T.sum(T.stack(step_losses, 1), 1)  # unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens  # Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)  # Average batch loss\n",
    "        return mle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "#     batch = next(iter(validate_loader))\n",
    "    val_num = len(iter(validate_loader))\n",
    "    for idx, batch in enumerate(validate_loader):\n",
    "        loss = train_one(model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "        if idx>= val_num/10: break\n",
    "    model.train()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "@torch.autograd.no_grad()\n",
    "def decode(writer, logger, step, config, model, batch, mode):\n",
    "    # 動態取batch\n",
    "    if mode == 'test':\n",
    "#         num = len(iter(batch))\n",
    "#         select_batch = None\n",
    "#         rand_b_id = randint(0,num-1)\n",
    "# #         logger.info('test_batch : ' + str(num)+ ' ' + str(rand_b_id))\n",
    "#         for idx, b in enumerate(batch):\n",
    "#             if idx == rand_b_id:\n",
    "#                 select_batch = b\n",
    "#                 break\n",
    "        select_batch = next(iter(batch))\n",
    "        batch = select_batch\n",
    "        if type(batch) == torch.utils.data.dataloader.DataLoader:\n",
    "            batch = next(iter(batch))\n",
    "    'Encoder data'\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "    enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input   \n",
    "    enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "    'Feed encoder data to predict'\n",
    "    pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                           enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                           START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "    article_sents, decoded_sents, keywords_list, \\\n",
    "    ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "    rouge_1, rouge_2, rouge_l = write_rouge(writer, step, mode,article_sents, decoded_sents, \\\n",
    "                keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    write_bleu(writer, step, mode, article_sents, decoded_sents, \\\n",
    "               keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    write_group(writer, step, mode, article_sents, decoded_sents,\\\n",
    "                keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    return rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import time\n",
    "@torch.autograd.no_grad()\n",
    "def avg_acc(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(iter(dataloader))\n",
    "    avg_rouge_l = []\n",
    "    acc_st, acc_cost = 0, 0\n",
    "    avg_acc_cost = 0\n",
    "    for idx, batch in enumerate(dataloader): \n",
    "        if idx >= num/100: break\n",
    "        acc_st = time.time()\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "        'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                               enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                               START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, \\\n",
    "        ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "        rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents, \\\n",
    "                    keywords_list, ref_sents, long_seq_index, write = False)\n",
    "        avg_rouge_l.append(rouge_l)\n",
    "        acc_cost = time.time() - acc_st\n",
    "        avg_acc_cost += acc_cost\n",
    "\n",
    "\n",
    "    avg_rouge_l = sum(avg_rouge_l) / len(avg_rouge_l)\n",
    "    writer.add_scalars('scalar_avg/acc',  \n",
    "                   {'%sing_avg_acc'%(mode): avg_rouge_l\n",
    "                   }, epoch)\n",
    "#     avg_acc_cost = avg_acc_cost / len(avg_rouge_l)\n",
    "#     print('decode 1% batches %s data, cost time %s ms' % (mode, avg_acc_cost ))\n",
    "    return avg_rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL(model, config, batch, greedy):    \n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param article_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "        \n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        s_t = enc_hidden                                                                            #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(START))  # Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []                       # Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []       # Stores padding masks of generated samples\n",
    "        log_probs = []                                                                              #Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "        # Generate RL tokens and compute rl-log-loss\n",
    "        # ----------------------------------------------------------------------\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = model.embeds(x_t)\n",
    "            \n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                      ct_e, extra_zeros,\n",
    "                                                                                      enc_batch_extend_vocab,\n",
    "                                                                                      sum_temporal_srcs, prev_s, enc_key_batch, enc_key_mask)\n",
    "            \n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs) # 建立以參數probs為標準的類別分佈\n",
    "                # perform multinomial sampling\n",
    "                x_t = multi_dist.sample()  # 將下一個時間點的x_t，視為下一個action   \n",
    "                # 使用log_prob实施梯度方法 Policy Gradient，构造一个等价類別分佈的损失函数\n",
    "                log_prob = multi_dist.log_prob(x_t)  \n",
    "                log_probs.append(log_prob) #\n",
    "            else:\n",
    "                # perform greedy sampling distribution\n",
    "                _, x_t = T.max(probs, dim=1)  # 因greedy以機率最大進行取樣，視為其中一個action   \n",
    "            x_t = x_t.detach() # detach返回的 Variable 永远不会需要梯度\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (x_t == END) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
    "            x_t = (1-is_oov)*x_t + (is_oov)*UNKNOWN_TOKEN                                             #Replace OOVs with [UNK] token\n",
    "        # -----------------------------------End loop -----------------------------------\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1) # 在第1个维度上stack, 增加新的维度进行堆叠\n",
    "            log_probs = log_probs * decoder_padding_mask # 遮罩掉為[END] or [STOP]不計算損失           #Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1) # 計算每個sample words生成的總長度               #Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs, dim=1) / lens  # 計算平均的每個句子的log loss # (bs,1)        #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy() # 取出每個sample sentence 的word id list\n",
    "            S = output2words(id_list, vocab, batch.art_oovs[i]) #Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:          #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "        return decoded_strs, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_RL(model, config, batch):\n",
    "    # Self-Critical sequence training(SCST)\n",
    "    sample_sents, RL_log_probs = RL(model, config, batch, greedy=False)   # multinomial sampling\n",
    "    with T.autograd.no_grad():        \n",
    "        greedy_sents, _ = RL(model, config, batch, greedy=True)  # greedy sampling\n",
    "\n",
    "    sample_reward = reward_function(sample_sents, batch.original_abstract) # r(w^s):通过根据概率来随机sample词生成句子的reward值\n",
    "    baseline_reward = reward_function(greedy_sents, batch.original_abstract) # r(w^):测试阶段使用greedy decoding取概率最大的词来生成句子的reward值\n",
    "\n",
    "    batch_reward = T.mean(sample_reward).item()\n",
    "    #Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    rl_loss = -(sample_reward - baseline_reward) * RL_log_probs  # SCST梯度計算公式     \n",
    "    rl_loss = T.mean(rl_loss)  \n",
    "    '''\n",
    "    公式的意思就是：对于如果当前sample到的词比测试阶段生成的词好，那么在这次词的维度上，整个式子的值就是负的（因为后面那一项一定为负），\n",
    "    这样梯度就会上升，从而提高这个词的分数st；而对于其他词，后面那一项为正，梯度就会下降，从而降低其他词的分数\n",
    "    '''                 \n",
    "    return rl_loss, batch_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(dataloader)\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "    avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    avg_meteor = []\n",
    "    outFrame = None\n",
    "    avg_time = 0\n",
    "        \n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        start = time.time() \n",
    "#         'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "#         'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                                enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                                START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "        cost = (time.time() - start)\n",
    "        avg_time += cost        \n",
    "\n",
    "        \n",
    "        rouge_1, rouge_2, rouge_l, self_Bleu_1, self_Bleu_2, self_Bleu_3, self_Bleu_4, \\\n",
    "            Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        \n",
    "        if idx %1000 ==0 and idx >0 : print(idx)\n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "        avg_self_bleu1.extend(self_Bleu_1)\n",
    "        avg_self_bleu2.extend(self_Bleu_2)\n",
    "        avg_self_bleu3.extend(self_Bleu_3)\n",
    "        avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "        avg_bleu1.extend(Bleu_1)\n",
    "        avg_bleu2.extend(Bleu_2)\n",
    "        avg_bleu3.extend(Bleu_3)\n",
    "        avg_bleu4.extend(Bleu_4)\n",
    "        avg_meteor.extend(Meteor)\n",
    "        # ----------------------------------------------------    \n",
    "    avg_time = avg_time / (num * config.batch_size) \n",
    "    \n",
    "    avg_rouge_l, outFrame = total_output(mode, writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l, \\\n",
    "        avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4, \\\n",
    "        avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "    )\n",
    "    \n",
    "    return avg_rouge_l, outFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# loss_st, loss_cost = 0,0\n",
    "# decode_st, decode_cost = 0,0\n",
    "\n",
    "# write_train_para(writer, config)\n",
    "# logger.info('------Training START--------')\n",
    "# running_avg_loss, running_avg_rl_loss = 0, 0\n",
    "# sum_total_reward = 0\n",
    "# step = 0\n",
    "\n",
    "# try:\n",
    "#     for epoch in range(config.max_epochs):\n",
    "#         for batch in train_loader:\n",
    "#             step += 1\n",
    "#             loss_st = time.time()\n",
    "#             mle_loss = train_one(model, config, batch)\n",
    "#             if config.train_rl:\n",
    "#                 rl_loss, batch_reward = train_one_RL(model, config, batch)             \n",
    "        \n",
    "#                 if step%1000 == 0 :\n",
    "#                     writer.add_scalars('scalar/RL_Loss',  \n",
    "#                        {'rl_loss': rl_loss\n",
    "#                        }, step)\n",
    "#                     writer.add_scalars('scalar/Reward',  \n",
    "#                        {'batch_reward': batch_reward\n",
    "#                        }, step)\n",
    "# #                     logger.info('epoch %d: %d, RL_Loss = %f, batch_reward = %f'\n",
    "# #                                     % (epoch, step, rl_loss, batch_reward))\n",
    "#                 sum_total_reward += batch_reward\n",
    "#             else:\n",
    "#                 rl_loss = T.FloatTensor([0]).cuda()\n",
    "#             (config.mle_weight * mle_loss + config.rl_weight * rl_loss).backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "#             '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "#             if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "#     #             clip_grad_norm_(model.parameters(), 5.0)                      \n",
    "#                 optimizer.step() # 根据累计的梯度更新网络参数\n",
    "#                 optimizer.zero_grad() # 清空过往梯度 \n",
    "#             if step%1000 == 0 :\n",
    "#                 with T.autograd.no_grad():\n",
    "#                     train_batch_loss = mle_loss.item()\n",
    "#                     train_batch_rl_loss = rl_loss.item()\n",
    "#                     val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "#                     running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "#                     running_avg_rl_loss = calc_running_avg_loss(train_batch_rl_loss, running_avg_rl_loss)\n",
    "#                     running_avg_reward = sum_total_reward / step\n",
    "#                     if step % save_steps == 0:\n",
    "#                         logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "#                                     % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "#                     writer.add_scalars('scalar/Loss',  \n",
    "#                        {'train_batch_loss': train_batch_loss\n",
    "#                        }, step)\n",
    "#                     writer.add_scalars('scalar_avg/loss',  \n",
    "#                        {'train_avg_loss': running_avg_loss,\n",
    "#                         'test_avg_loss': val_avg_loss\n",
    "#                        }, step)\n",
    "#                     if running_avg_reward > 0:\n",
    "# #                         logger.info('epoch %d: %d, running_avg_reward = %f'\n",
    "# #                                 % (epoch, step, running_avg_reward))\n",
    "#                         writer.add_scalars('scalar_avg/Reward',  \n",
    "#                            {'running_avg_reward': running_avg_reward\n",
    "#                            }, step)\n",
    "#                     if running_avg_rl_loss != 0:\n",
    "# #                         logger.info('epoch %d: %d, running_avg_rl_loss = %f'\n",
    "# #                                 % (epoch, step, running_avg_rl_loss))\n",
    "#                         writer.add_scalars('scalar_avg/RL_Loss',  \n",
    "#                            {'running_avg_rl_loss': running_avg_rl_loss\n",
    "#                            }, step)\n",
    "#                     loss_cost = time.time() - loss_st\n",
    "#                     if step % save_steps == 0: logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "#                                 % (epoch, step, loss_cost))\n",
    "\n",
    "#             if step % save_steps == 0:\n",
    "#                 save_model(config, logger, model, optimizer, step, vocab, running_avg_loss, \\\n",
    "#                            r_loss=0, title = loggerName)\n",
    "#             if step%1000 == 0 and step > 0:\n",
    "#                 decode_st = time.time()\n",
    "#                 train_rouge_l_f = decode(writer, logger, step, config, model, batch, mode = 'train') # call batch by validate_loader\n",
    "#                 test_rouge_l_f = decode(writer, logger, step, config, model, validate_loader, mode = 'test') # call batch by validate_loader\n",
    "#                 decode_cost = time.time() - decode_st\n",
    "#                 if step%save_steps == 0: logger.info('epoch %d|step %d| decode cost = %f ms'% (epoch, step, decode_cost))\n",
    "\n",
    "#                 writer.add_scalars('scalar/Rouge-L',  \n",
    "#                    {'train_rouge_l_f': train_rouge_l_f,\n",
    "#                     'test_rouge_l_f': test_rouge_l_f\n",
    "#                    }, step)\n",
    "# #                 logger.info('epoch %d: %d, train_rouge_l_f = %f, test_rouge_l_f = %f'\n",
    "# #                                 % (epoch, step, train_rouge_l_f, test_rouge_l_f))\n",
    "# #         break\n",
    "#         logger.info('-------------------------------------------------------------')\n",
    "#         train_avg_acc = avg_acc(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "#         test_avg_acc = avg_acc(writer, logger, epoch, config, model, validate_loader, mode = 'test')                   \n",
    "#         logger.info('epoch %d|step %d| train_avg_acc = %f, test_avg_acc = %f' % (epoch, step, train_avg_acc, test_avg_acc))\n",
    "#         if running_avg_reward > 0:\n",
    "#             logger.info('epoch %d|step %d| running_avg_reward = %f'% (epoch, step, running_avg_reward))\n",
    "#         if running_avg_rl_loss != 0:\n",
    "#             logger.info('epoch %d|step %d| running_avg_rl_loss = %f'% (epoch, step, running_avg_rl_loss))\n",
    "#         logger.info('-------------------------------------------------------------')\n",
    "\n",
    "# except Excepation as e:\n",
    "#         print(e)\n",
    "# else:\n",
    "#     logger.info(u'------Training SUCCESS--------')  \n",
    "# finally:\n",
    "#     logger.info(u'------Training END--------')    \n",
    "#     train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "#     test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "#     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc))\n",
    "#     removeLogger(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 23:03:06 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -----------------------------------------------------------\n",
      "I0505 23:03:06.684964 139954278307648 <ipython-input-13-eca4ca5d6040>:17] -----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 00:00:12 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 10: train_avg_acc = 0.466037, test_avg_acc = 0.463693\n",
      "I0506 00:00:12.730406 139954278307648 <ipython-input-13-eca4ca5d6040>:19] epoch 10: train_avg_acc = 0.466037, test_avg_acc = 0.463693\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>keywords</th>\n",
       "      <th>reference</th>\n",
       "      <th>decoded</th>\n",
       "      <th>rouge_1</th>\n",
       "      <th>rouge_2</th>\n",
       "      <th>rouge_l</th>\n",
       "      <th>self_Bleu_1</th>\n",
       "      <th>self_Bleu_2</th>\n",
       "      <th>self_Bleu_3</th>\n",
       "      <th>self_Bleu_4</th>\n",
       "      <th>Bleu_1</th>\n",
       "      <th>Bleu_2</th>\n",
       "      <th>Bleu_3</th>\n",
       "      <th>Bleu_4</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>article_lens</th>\n",
       "      <th>ref_lens</th>\n",
       "      <th>overlap</th>\n",
       "      <th>too_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48780</th>\n",
       "      <td>only had little time use this this just genera...</td>\n",
       "      <td>['overview', 'just', 'most', 'more', 'truly', ...</td>\n",
       "      <td>nearly perfect big phone but is it worthy upgr...</td>\n",
       "      <td>and see much better difference like have same car</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>996</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48781</th>\n",
       "      <td>wifi work fine make this router work for home ...</td>\n",
       "      <td>['procedure', 'just', 'there', 'reasonably', '...</td>\n",
       "      <td>terrible tech support experience try to sell m...</td>\n",
       "      <td>wifi work fine make this router work for home ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>995</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48783</th>\n",
       "      <td>company like neuros . purchase one their earli...</td>\n",
       "      <td>['edge', 'not', 'reasonably', 'extremely', 've...</td>\n",
       "      <td>good company not good enough product</td>\n",
       "      <td>company like neuros purchase one of their earl...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.248244</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>7.031791e-155</td>\n",
       "      <td>5.416106e-204</td>\n",
       "      <td>1.250850e-231</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>994</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48782</th>\n",
       "      <td>over the past twenty plus year buy two these f...</td>\n",
       "      <td>['school', 'very', 'long', 'rapidly', 'very', ...</td>\n",
       "      <td>the absolutely best radar laser detector milit...</td>\n",
       "      <td>the best radar detector on the market</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.549451</td>\n",
       "      <td>0.495359</td>\n",
       "      <td>1.444796e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.495359</td>\n",
       "      <td>2.675243e-01</td>\n",
       "      <td>1.190116e-102</td>\n",
       "      <td>7.183446e-155</td>\n",
       "      <td>0.399525</td>\n",
       "      <td>994</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48776</th>\n",
       "      <td>love this gps . previously had magellan handhe...</td>\n",
       "      <td>['gps', 'here', 'still', 'sized']</td>\n",
       "      <td>great for both car and foot use</td>\n",
       "      <td>work great with etrex vista</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158120</td>\n",
       "      <td>0.134064</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.134064</td>\n",
       "      <td>4.471667e-155</td>\n",
       "      <td>3.506464e-204</td>\n",
       "      <td>8.166727e-232</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>993</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "48780  only had little time use this this just genera...   \n",
       "48781  wifi work fine make this router work for home ...   \n",
       "48783  company like neuros . purchase one their earli...   \n",
       "48782  over the past twenty plus year buy two these f...   \n",
       "48776  love this gps . previously had magellan handhe...   \n",
       "\n",
       "                                                keywords  \\\n",
       "48780  ['overview', 'just', 'most', 'more', 'truly', ...   \n",
       "48781  ['procedure', 'just', 'there', 'reasonably', '...   \n",
       "48783  ['edge', 'not', 'reasonably', 'extremely', 've...   \n",
       "48782  ['school', 'very', 'long', 'rapidly', 'very', ...   \n",
       "48776                  ['gps', 'here', 'still', 'sized']   \n",
       "\n",
       "                                               reference  \\\n",
       "48780  nearly perfect big phone but is it worthy upgr...   \n",
       "48781  terrible tech support experience try to sell m...   \n",
       "48783               good company not good enough product   \n",
       "48782  the absolutely best radar laser detector milit...   \n",
       "48776                    great for both car and foot use   \n",
       "\n",
       "                                                 decoded   rouge_1   rouge_2  \\\n",
       "48780  and see much better difference like have same car  0.000000  0.000000   \n",
       "48781  wifi work fine make this router work for home ...  0.000000  0.000000   \n",
       "48783  company like neuros purchase one of their earl...  0.285714  0.000000   \n",
       "48782              the best radar detector on the market  0.571429  0.153846   \n",
       "48776                        work great with etrex vista  0.166667  0.000000   \n",
       "\n",
       "        rouge_l  self_Bleu_1    self_Bleu_2    self_Bleu_3    self_Bleu_4  \\\n",
       "48780  0.000000     0.000000   0.000000e+00   0.000000e+00   0.000000e+00   \n",
       "48781  0.000000     0.000000   0.000000e+00   0.000000e+00   0.000000e+00   \n",
       "48783  0.248244     0.222222  2.225074e-308  2.225074e-308  2.225074e-308   \n",
       "48782  0.549451     0.495359   1.444796e-01   0.000000e+00   0.000000e+00   \n",
       "48776  0.158120     0.134064   0.000000e+00   0.000000e+00   0.000000e+00   \n",
       "\n",
       "         Bleu_1         Bleu_2         Bleu_3         Bleu_4    Meteor  \\\n",
       "48780  0.000000   0.000000e+00   0.000000e+00   0.000000e+00  0.000000   \n",
       "48781  0.000000   0.000000e+00   0.000000e+00   0.000000e+00  0.000000   \n",
       "48783  0.222222  7.031791e-155  5.416106e-204  1.250850e-231  0.158730   \n",
       "48782  0.495359   2.675243e-01  1.190116e-102  7.183446e-155  0.399525   \n",
       "48776  0.134064  4.471667e-155  3.506464e-204  8.166727e-232  0.073529   \n",
       "\n",
       "       article_lens  ref_lens  overlap  too_overlap  \n",
       "48780           996        12        9         True  \n",
       "48781           995        10        7        False  \n",
       "48783           994         6        5         True  \n",
       "48782           994         8        7         True  \n",
       "48776           993         7        7         True  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config.load_ckpt = '0780000'\n",
    "\n",
    "# logger = getLogger(loggerName)\n",
    "# writer = SummaryWriter(writerPath)\n",
    "\n",
    "# config.load_ckpt = '0630000'\n",
    "\n",
    "# load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "# config.batch_size = 8\n",
    "# train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "epoch = 10\n",
    "# print(load_model_path)\n",
    "# if os.path.exists(load_model_path):\n",
    "#     model, optimizer, load_step = loadCheckpoint(logger, load_model_path, model, optimizer)\n",
    "# model    \n",
    "train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "logger.info('-----------------------------------------------------------')\n",
    "test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc))\n",
    "\n",
    "# !ipython nbconvert --to script Pointer_generator.ipynb\n",
    "train_outFrame.head()\n",
    "test_outFrame.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
