{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0423 23:35:41.623326 140046402754368 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-04-23 23:35:42 - Pointer_generator_word2Vec - INFO: - logger已啟動\n",
      "I0423 23:35:42.471922 140046402754368 train_util.py:102] logger已啟動\n"
     ]
    }
   ],
   "source": [
    "from utils import config\n",
    "from utils.seq2seq import data\n",
    "\n",
    "from utils.seq2seq.batcher import *\n",
    "from utils.seq2seq.train_util import *\n",
    "from utils.seq2seq.rl_util import *\n",
    "from utils.seq2seq.initialize import loadCheckpoint, save_model\n",
    "from utils.seq2seq.write_result import *\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from translate.seq2seq_beam import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from utils.seq2seq.rl_util import *\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--key_attention', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_encoder', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_decoder', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--copy', type=bool, default=True, help = 'True/False') # for transformer\n",
    "\n",
    "parser.add_argument('--model_type', type=str, default='seq2seq', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--train_rl', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_FOP_keywords', \n",
    "                    help = 'POS_FOP_keywords / DEP_FOP_keywords / TextRank_keywords')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--rand_unif_init_mag', type=float, default=0.02)\n",
    "parser.add_argument('--trunc_norm_init_std', type=float, default=0.001)\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument('--gound_truth_prob', type=float, default=0.1)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=1000)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=50)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=8)\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=16)\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=300)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default='0760000', help='0002000')\n",
    "parser.add_argument('--word_emb_type', type=str, default='word2Vec', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=True, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-23 23:37:02 - Pointer_generator_word2Vec - INFO: - train : 504075, test : 56009\n",
      "I0423 23:37:02.296135 140046402754368 batcher.py:173] train : 504075, test : 56009\n",
      "2020-04-23 23:37:02 - Pointer_generator_word2Vec - INFO: - train batches : 63009, test batches : 7001\n",
      "I0423 23:37:02.796639 140046402754368 batcher.py:184] train batches : 63009, test batches : 7001\n"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab = getDataLoader(logger, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0423 23:37:03.328100 140046402754368 utils_any2vec.py:341] loading projection weights from ../Train-Data/Mix6_mainCat/Embedding/word2Vec/word2Vec.300d.txt\n",
      "I0423 23:37:14.659542 140046402754368 utils_any2vec.py:405] loaded (49050, 300) matrix from ../Train-Data/Mix6_mainCat/Embedding/word2Vec/word2Vec.300d.txt\n"
     ]
    }
   ],
   "source": [
    "from seq2seq import Model\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "load_step = None\n",
    "model = Model(pre_train_emb=config.pre_train_emb, \n",
    "              word_emb_type = config.word_emb_type, \n",
    "              vocab = vocab)\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=config.lr)   \n",
    "# optimizer = T.optim.Adagrad(model.parameters(),lr=config.lr, initial_accumulator_value=0.1)\n",
    "\n",
    "# load_model_path = config.save_model_path + '/%s/%s.tar' % (logger, config.load_ckpt)\n",
    "\n",
    "# if os.path.exists(load_model_path):\n",
    "#     model, optimizer, load_step = loadCheckpoint(loggerName, load_model_path, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model, config, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    " \n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        'Decoder data'\n",
    "        dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(batch, config, batch_first = True) # Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(START))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "            use_gound_truth = get_cuda((T.rand(len(enc_out)) > config.gound_truth_prob)).long()  # Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "            x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t  # Select decoder input based on use_ground_truth probabilities\n",
    "            x_t = model.embeds(x_t)  \n",
    "            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                      ct_e, extra_zeros,\n",
    "                                                                                      enc_batch_extend_vocab,\n",
    "                                                                                      sum_temporal_srcs, prev_s, enc_key_batch, enc_key_mask)\n",
    "            target = target_batch[:, t]\n",
    "            log_probs = T.log(final_dist + config.eps)\n",
    "            step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=PAD)\n",
    "            step_losses.append(step_loss)\n",
    "            x_t = T.multinomial(final_dist,1).squeeze()  # Sample words from final distribution which can be used as input in next time step\n",
    "\n",
    "            is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t.detach() + (is_oov) * UNKNOWN_TOKEN  # Replace OOVs with [UNK] token\n",
    "\n",
    "        losses = T.sum(T.stack(step_losses, 1), 1)  # unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens  # Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)  # Average batch loss\n",
    "        return mle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "#     batch = next(iter(validate_loader))\n",
    "    val_num = len(iter(validate_loader))\n",
    "    for idx, batch in enumerate(validate_loader):\n",
    "        loss = train_one(model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "        if idx>= val_num/10: break\n",
    "    model.train()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "@torch.autograd.no_grad()\n",
    "def decode(writer, logger, step, config, model, batch, mode):\n",
    "    # 動態取batch\n",
    "    if mode == 'test':\n",
    "#         num = len(iter(batch))\n",
    "#         select_batch = None\n",
    "#         rand_b_id = randint(0,num-1)\n",
    "#         logger.info('test_batch : ' + str(num)+ ' ' + str(rand_b_id))\n",
    "#         for idx, b in enumerate(batch):\n",
    "#             if idx == rand_b_id:\n",
    "#                 select_batch = b\n",
    "#                 break\n",
    "        select_batch = next(iter(batch))\n",
    "        batch = select_batch\n",
    "        if type(batch) == torch.utils.data.dataloader.DataLoader:\n",
    "            batch = next(iter(batch))\n",
    "    'Encoder data'\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "    enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "    enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "    'Feed encoder data to predict'\n",
    "    pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                           enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                           START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "    article_sents, decoded_sents, keywords_list, \\\n",
    "    ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "    rouge_1, rouge_2, rouge_l = write_rouge(writer, step, mode,article_sents, decoded_sents, \\\n",
    "                keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    write_bleu(writer, step, mode, article_sents, decoded_sents, \\\n",
    "               keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    write_group(writer, step, mode, article_sents, decoded_sents,\\\n",
    "                keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    return rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import time\n",
    "@torch.autograd.no_grad()\n",
    "def avg_acc(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(iter(dataloader))\n",
    "    avg_rouge_l = []\n",
    "    acc_st, acc_cost = 0, 0\n",
    "    avg_acc_cost = []\n",
    "    for idx, batch in enumerate(dataloader): \n",
    "        if idx >= num/100: break\n",
    "        acc_st = time.time()\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "        'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                               enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                               START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, \\\n",
    "        ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "        rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents, \\\n",
    "                    keywords_list, ref_sents, long_seq_index, write = False)\n",
    "        avg_rouge_l.append(rouge_l)\n",
    "        acc_cost = time.time() - acc_st\n",
    "        avg_acc_cost.append(acc_cost)\n",
    "\n",
    "\n",
    "    avg_rouge_l = sum(avg_rouge_l) / len(avg_rouge_l)\n",
    "    writer.add_scalars('scalar_avg/acc',  \n",
    "                   {'%sing_avg_acc'%(mode): avg_rouge_l\n",
    "                   }, epoch)\n",
    "    avg_acc_cost = sum(avg_acc_cost) / len(avg_acc_cost)\n",
    "#     avg_acc_cost = avg_acc_cost / len(avg_rouge_l)\n",
    "#     print('decode 1% batches %s data, cost time %s ms' % (mode, avg_acc_cost ))\n",
    "    return avg_rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL(model, config, batch, greedy):    \n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param article_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "        \n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        s_t = enc_hidden                                                                            #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(START))  # Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []                       # Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []       # Stores padding masks of generated samples\n",
    "        log_probs = []                                                                              #Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "        # Generate RL tokens and compute rl-log-loss\n",
    "        # ----------------------------------------------------------------------\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = model.embeds(x_t)\n",
    "            \n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                      ct_e, extra_zeros,\n",
    "                                                                                      enc_batch_extend_vocab,\n",
    "                                                                                      sum_temporal_srcs, prev_s, enc_key_batch, enc_key_mask)\n",
    "            \n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs) # 建立以參數probs為標準的類別分佈\n",
    "                # perform multinomial sampling\n",
    "                x_t = multi_dist.sample()  # 將下一個時間點的x_t，視為下一個action   \n",
    "                # 使用log_prob实施梯度方法 Policy Gradient，构造一个等价類別分佈的损失函数\n",
    "                log_prob = multi_dist.log_prob(x_t)  \n",
    "                log_probs.append(log_prob) #\n",
    "            else:\n",
    "                # perform greedy sampling distribution\n",
    "                _, x_t = T.max(probs, dim=1)  # 因greedy以機率最大進行取樣，視為其中一個action   \n",
    "            x_t = x_t.detach() # detach返回的 Variable 永远不会需要梯度\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (x_t == END) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
    "            x_t = (1-is_oov)*x_t + (is_oov)*UNKNOWN_TOKEN                                             #Replace OOVs with [UNK] token\n",
    "        # -----------------------------------End loop -----------------------------------\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1) # 在第1个维度上stack, 增加新的维度进行堆叠\n",
    "            log_probs = log_probs * decoder_padding_mask # 遮罩掉為[END] or [STOP]不計算損失           #Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1) # 計算每個sample words生成的總長度               #Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs, dim=1) / lens  # 計算平均的每個句子的log loss # (bs,1)        #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy() # 取出每個sample sentence 的word id list\n",
    "            S = output2words(id_list, vocab, batch.art_oovs[i]) #Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:          #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "        return decoded_strs, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_RL(model, config, batch):\n",
    "    # Self-Critical sequence training(SCST)\n",
    "    sample_sents, RL_log_probs = RL(model, config, batch, greedy=False)   # multinomial sampling\n",
    "    with T.autograd.no_grad():        \n",
    "        greedy_sents, _ = RL(model, config, batch, greedy=True)  # greedy sampling\n",
    "\n",
    "    sample_reward = reward_function(sample_sents, batch.original_abstract) # r(w^s):通过根据概率来随机sample词生成句子的reward值\n",
    "    baseline_reward = reward_function(greedy_sents, batch.original_abstract) # r(w^):测试阶段使用greedy decoding取概率最大的词来生成句子的reward值\n",
    "\n",
    "    batch_reward = T.mean(sample_reward).item()\n",
    "    #Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    rl_loss = -(sample_reward - baseline_reward) * RL_log_probs  # SCST梯度計算公式     \n",
    "    rl_loss = T.mean(rl_loss)  \n",
    "    '''\n",
    "    公式的意思就是：对于如果当前sample到的词比测试阶段生成的词好，那么在这次词的维度上，整个式子的值就是负的（因为后面那一项一定为负），\n",
    "    这样梯度就会上升，从而提高这个词的分数st；而对于其他词，后面那一项为正，梯度就会下降，从而降低其他词的分数\n",
    "    '''                 \n",
    "    return rl_loss, batch_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from utils.seq2seq.write_result import *\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(iter(dataloader))\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l,  = [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    outFrame = None\n",
    "    avg_time = 0\n",
    "    \n",
    "    rouge = Rouge()  \n",
    "    \n",
    "#     score = rouge.get_scores(decoded_sents, ref_sents, avg = False) \n",
    "#     score['rouge-l']['f']\n",
    "    \n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        start = time.time() \n",
    "#         'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "#         'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                                enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                                START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "        cost = (time.time() - start)\n",
    "        avg_time += cost        \n",
    "        # ----------------------------------------------------\n",
    "#         rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents, \\\n",
    "#                     keywords_list, ref_sents, long_seq_index, write = False)\n",
    "#         Bleu_1, Bleu_2, Bleu_3, Bleu_4 = write_bleu(writer, None, None, article_sents, decoded_sents, \\\n",
    "#             keywords_list, ref_sents, long_seq_index, write = False)\n",
    "#         if idx % 100 == 0: \n",
    "# #             logger.info('%s step :%s decode batch cost time : %s ms'%(mode, idx, cost / (config.batch_size)))        \n",
    "#             logger.info('%s step :%s rouge_1 :%s, rouge_2 :%s, rouge_l :%s'%(mode, idx, rouge_1, rouge_2, rouge_l))        \n",
    "# #             logger.info('%s step :%s Bleu_1 :%s, Bleu_2 :%s, Bleu_3 :%s, Bleu_4 :%s'%(mode, step, Bleu_1, Bleu_2, Bleu_3, Bleu_4))        \n",
    "#             logger.info('----------------------------------------------')\n",
    "        # ----------------------------------------------------\n",
    "#         .strip()\n",
    "        overlap = [len(set(article_sents[i].split(\" \")) & set(ref_sents[i].split(\" \"))) for i in range(len(article_sents))]\n",
    "        too_overlap = [overlap[i] > len(set(ref_sents[i].split(\" \")))-3 for i in range(len(article_sents))]\n",
    "        scores = rouge.get_scores(decoded_sents, ref_sents, avg = False)\n",
    "        rouge_1 = [score['rouge-1']['f'] for score in scores]\n",
    "        rouge_2 = [score['rouge-2']['f'] for score in scores]\n",
    "        rouge_l = [score['rouge-l']['f'] for score in scores]\n",
    "\n",
    "#         print([len(r.split(\" \")) for r in ref_sents])\n",
    "        batch_frame = {\n",
    "            'article':article_sents,\n",
    "            'keywords':keywords_list,\n",
    "            'reference':ref_sents,\n",
    "            'decoded':decoded_sents,\n",
    "            'ref_lens': [len(r.split(\" \")) for r in ref_sents],\n",
    "            'overlap': overlap,\n",
    "            'too_overlap': too_overlap,\n",
    "            'rouge_1':rouge_1,\n",
    "            'rouge_2':rouge_2,\n",
    "            'rouge_l':rouge_l\n",
    "#             'Bleu_1':Bleu_1,\n",
    "#             'Bleu_2':Bleu_2,\n",
    "#             'Bleu_3':Bleu_3,\n",
    "#             'Bleu_4':Bleu_4,\n",
    "        }\n",
    "        batch_frame = pd.DataFrame(batch_frame)\n",
    "        if idx %1000 == 0: print(idx)\n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        elif idx < 100 : outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)        \n",
    "#         avg_bleu1.append(Bleu_1)\n",
    "#         avg_bleu2.append(Bleu_2)\n",
    "#         avg_bleu3.append(Bleu_3)\n",
    "#         avg_bleu4.append(Bleu_4)\n",
    "        # ----------------------------------------------------\n",
    "#     print(avg_rouge_1)\n",
    "    avg_rouge_1 = sum(avg_rouge_1) / len(avg_rouge_1)\n",
    "    avg_rouge_2 = sum(avg_rouge_2) / len(avg_rouge_2)\n",
    "    avg_rouge_l = sum(avg_rouge_l) / len(avg_rouge_l)\n",
    "    writer.add_scalars('Rouge_avg/mode',  \n",
    "                    {'avg_rouge_1': avg_rouge_1,\n",
    "                    'avg_rouge_2': avg_rouge_2,\n",
    "                    'avg_rouge_l': avg_rouge_l\n",
    "                    }, epoch)\n",
    "    # --------------------------------------               \n",
    "#     avg_bleu1 = sum(avg_bleu1)/len(avg_bleu1)\n",
    "#     avg_bleu2 = sum(avg_bleu2)/len(avg_bleu2)\n",
    "#     avg_bleu3 = sum(avg_bleu3)/len(avg_bleu3)\n",
    "#     avg_bleu4 = sum(avg_bleu4)/len(avg_bleu4)\n",
    "    \n",
    "#     writer.add_scalars('BLEU_avg/mode',  \n",
    "#                     {\n",
    "#                     '%sing_avg_bleu1'%(mode): avg_bleu1,\n",
    "#                     '%sing_avg_bleu1'%(mode): avg_bleu2,\n",
    "#                     '%sing_avg_bleu1'%(mode): avg_bleu3,\n",
    "#                     '%sing_avg_bleu1'%(mode): avg_bleu4,                   \n",
    "#                     }, epoch)\n",
    "    # --------------------------------------      \n",
    "    outFrame.to_excel(writerPath + '/%s_output.xls'% mode)\n",
    "    avg_time = avg_time / (num * config.batch_size) \n",
    "    with open(writerPath + '/%s_res.txt'% mode, 'w', encoding='utf-8') as f:\n",
    "        f.write('Accuracy result:\\n')\n",
    "        f.write('##-- Rouge --##\\n')\n",
    "        f.write('%sing_avg_rouge_1: %s \\n'%(mode, avg_rouge_1))\n",
    "        f.write('%sing_avg_rouge_2: %s \\n'%(mode, avg_rouge_2))\n",
    "        f.write('%sing_avg_rouge_l: %s \\n'%(mode, avg_rouge_l))\n",
    "\n",
    "#         f.write('##-- BLEU --##\\n')\n",
    "#         f.write('%sing_avg_bleu1: %s \\n'%(mode, avg_bleu1))\n",
    "#         f.write('%sing_avg_bleu2: %s \\n'%(mode, avg_bleu2))\n",
    "#         f.write('%sing_avg_bleu3: %s \\n'%(mode, avg_bleu3))\n",
    "#         f.write('%sing_avg_bleu4: %s \\n'%(mode, avg_bleu4))\n",
    "\n",
    "        f.write('Execute Time: %s \\n' % avg_time)        \n",
    "    # --------------------------------------              \n",
    "    return avg_rouge_l, outFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# loss_st, loss_cost = 0,0\n",
    "# decode_st, decode_cost = 0,0\n",
    "\n",
    "# write_train_para(writer, config)\n",
    "# logger.info('------Training START--------')\n",
    "# running_avg_loss, running_avg_rl_loss = 0, 0\n",
    "# sum_total_reward = 0\n",
    "# step = 0\n",
    "\n",
    "# try:\n",
    "#     for epoch in range(config.max_epochs):\n",
    "#         for batch in train_loader:\n",
    "#             step += 1\n",
    "#             loss_st = time.time()\n",
    "#             mle_loss = train_one(model, config, batch)\n",
    "#             if config.train_rl:\n",
    "#                 rl_loss, batch_reward = train_one_RL(model, config, batch)             \n",
    "        \n",
    "#                 if step%1000 == 0 :\n",
    "#                     writer.add_scalars('scalar/RL_Loss',  \n",
    "#                        {'rl_loss': rl_loss\n",
    "#                        }, step)\n",
    "#                     writer.add_scalars('scalar/Reward',  \n",
    "#                        {'batch_reward': batch_reward\n",
    "#                        }, step)\n",
    "# #                     logger.info('epoch %d: %d, RL_Loss = %f, batch_reward = %f'\n",
    "# #                                     % (epoch, step, rl_loss, batch_reward))\n",
    "#                 sum_total_reward += batch_reward\n",
    "#             else:\n",
    "#                 rl_loss = T.FloatTensor([0]).cuda()\n",
    "#             (config.mle_weight * mle_loss + config.rl_weight * rl_loss).backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "#             '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "#             if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "#     #             clip_grad_norm_(model.parameters(), 5.0)                      \n",
    "#                 optimizer.step() # 根据累计的梯度更新网络参数\n",
    "#                 optimizer.zero_grad() # 清空过往梯度 \n",
    "#             if step%1000 == 0 :\n",
    "#                 with T.autograd.no_grad():\n",
    "#                     train_batch_loss = mle_loss.item()\n",
    "#                     train_batch_rl_loss = rl_loss.item()\n",
    "#                     val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "#                     running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "#                     running_avg_rl_loss = calc_running_avg_loss(train_batch_rl_loss, running_avg_rl_loss)\n",
    "#                     running_avg_reward = sum_total_reward / step\n",
    "# #                     logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "# #                                 % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "#                     writer.add_scalars('scalar/Loss',  \n",
    "#                        {'train_batch_loss': train_batch_loss\n",
    "#                        }, step)\n",
    "#                     writer.add_scalars('scalar_avg/loss',  \n",
    "#                        {'train_avg_loss': running_avg_loss,\n",
    "#                         'test_avg_loss': val_avg_loss\n",
    "#                        }, step)\n",
    "#                     if running_avg_reward > 0:\n",
    "# #                         logger.info('epoch %d: %d, running_avg_reward = %f'\n",
    "# #                                 % (epoch, step, running_avg_reward))\n",
    "#                         writer.add_scalars('scalar_avg/Reward',  \n",
    "#                            {'running_avg_reward': running_avg_reward\n",
    "#                            }, step)\n",
    "#                     if running_avg_rl_loss != 0:\n",
    "# #                         logger.info('epoch %d: %d, running_avg_rl_loss = %f'\n",
    "# #                                 % (epoch, step, running_avg_rl_loss))\n",
    "#                         writer.add_scalars('scalar_avg/RL_Loss',  \n",
    "#                            {'running_avg_rl_loss': running_avg_rl_loss\n",
    "#                            }, step)\n",
    "#                     loss_cost = time.time() - loss_st\n",
    "#                     if step%10000 == 0: logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "#                                 % (epoch, step, loss_cost))\n",
    "\n",
    "#             if step%10000 == 0:\n",
    "#                 save_model(config, logger, model, optimizer, step, vocab, running_avg_loss, \\\n",
    "#                            r_loss=0, title = loggerName)\n",
    "#             if step%1000 == 0 and step > 0:\n",
    "#                 decode_st = time.time()\n",
    "#                 train_rouge_l_f = decode(writer, logger, step, config, model, batch, mode = 'train') # call batch by validate_loader\n",
    "#                 test_rouge_l_f = decode(writer, logger, step, config, model, validate_loader, mode = 'test') # call batch by validate_loader\n",
    "#                 decode_cost = time.time() - decode_st\n",
    "#                 if step%10000 == 0: logger.info('epoch %d|step %d| decode cost = %f ms'% (epoch, step, decode_cost))\n",
    "\n",
    "#                 writer.add_scalars('scalar/Rouge-L',  \n",
    "#                    {'train_rouge_l_f': train_rouge_l_f,\n",
    "#                     'test_rouge_l_f': test_rouge_l_f\n",
    "#                    }, step)\n",
    "# #                 logger.info('epoch %d: %d, train_rouge_l_f = %f, test_rouge_l_f = %f'\n",
    "# #                                 % (epoch, step, train_rouge_l_f, test_rouge_l_f))\n",
    "# #         break\n",
    "#         logger.info('-------------------------------------------------------------')\n",
    "#         train_avg_acc = avg_acc(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "#         test_avg_acc = avg_acc(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "#         logger.info('epoch %d|step %d| training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "#                      % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                    \n",
    "#         logger.info('epoch %d|step %d| train_avg_acc = %f, test_avg_acc = %f' % (epoch, step, train_avg_acc, test_avg_acc))\n",
    "#         if running_avg_reward > 0:\n",
    "#             logger.info('epoch %d|step %d| running_avg_reward = %f'% (epoch, step, running_avg_reward))\n",
    "#         if running_avg_rl_loss != 0:\n",
    "#             logger.info('epoch %d|step %d| running_avg_rl_loss = %f'% (epoch, step, running_avg_rl_loss))\n",
    "#         logger.info('-------------------------------------------------------------')\n",
    "\n",
    "# except Excepation as e:\n",
    "#         print(e)\n",
    "# else:\n",
    "#     logger.info(u'------Training SUCCESS--------')  \n",
    "# finally:\n",
    "#     logger.info(u'------Training END--------')    \n",
    "# #     train_avg_acc = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "#     test_avg_acc = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "#     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc))\n",
    "#     removeLogger(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-23 23:38:40 - Pointer_generator_word2Vec - INFO: - train : 504075, test : 56009\n",
      "I0423 23:38:40.143437 140046402754368 batcher.py:173] train : 504075, test : 56009\n",
      "2020-04-23 23:38:40 - Pointer_generator_word2Vec - INFO: - train batches : 63009, test batches : 7001\n",
      "I0423 23:38:40.606800 140046402754368 batcher.py:184] train batches : 63009, test batches : 7001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/saved_models/Pointer_generator_word2Vec/0760000.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-23 23:38:41 - Pointer_generator_word2Vec - INFO: - Loaded model at model/saved_models/Pointer_generator_word2Vec/0760000.tar\n",
      "I0423 23:38:41.257909 140046402754368 initialize.py:211] Loaded model at model/saved_models/Pointer_generator_word2Vec/0760000.tar\n",
      "2020-04-23 23:38:41 - Pointer_generator_word2Vec - INFO: - Loaded model step = 760000, loss = 1.97, r_loss = 0.00 \n",
      "I0423 23:38:41.259596 140046402754368 initialize.py:212] Loaded model step = 760000, loss = 1.97, r_loss = 0.00 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 09:56:42 - Pointer_generator_word2Vec - INFO: - -----------------------------------------------------------\n",
      "I0424 09:56:42.728431 140046402754368 <ipython-input-13-6f30c3689283>:15] -----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 10:55:33 - Pointer_generator_word2Vec - INFO: - epoch 12: train_avg_acc = 0.503226, test_avg_acc = 0.453499\n",
      "I0424 10:55:33.942378 140046402754368 <ipython-input-13-6f30c3689283>:17] epoch 12: train_avg_acc = 0.503226, test_avg_acc = 0.453499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>keywords</th>\n",
       "      <th>reference</th>\n",
       "      <th>decoded</th>\n",
       "      <th>ref_lens</th>\n",
       "      <th>overlap</th>\n",
       "      <th>too_overlap</th>\n",
       "      <th>rouge_1</th>\n",
       "      <th>rouge_2</th>\n",
       "      <th>rouge_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does not work . will not hold charge . unfortu...</td>\n",
       "      <td>['piece', 'useless']</td>\n",
       "      <td>unfortunately wait too long to use it and reai...</td>\n",
       "      <td>it does not work it will not hold charge</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this food great for mbuna cichlid other algae ...</td>\n",
       "      <td>['color', 'great']</td>\n",
       "      <td>this food is great for mbuna cichlid or other ...</td>\n",
       "      <td>this food is great for mbuna cichlid or other ...</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this item had japanese instruction how assembl...</td>\n",
       "      <td>['instruction', 'japanese']</td>\n",
       "      <td>this item had japanese instruction on how to a...</td>\n",
       "      <td>this item had japanese instruction on how to a...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.942163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>terrible door last less than month and the doo...</td>\n",
       "      <td>['door', 'terrible']</td>\n",
       "      <td>terrible door last less than month and the door</td>\n",
       "      <td>terrible door last less than month and the door</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>work great white lite not quite bright thought...</td>\n",
       "      <td>['lite', 'quite', 'great']</td>\n",
       "      <td>work great white lite is not quite as bright as</td>\n",
       "      <td>work great white lite is not quite bright</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.934730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  does not work . will not hold charge . unfortu...   \n",
       "1  this food great for mbuna cichlid other algae ...   \n",
       "2  this item had japanese instruction how assembl...   \n",
       "3  terrible door last less than month and the doo...   \n",
       "4  work great white lite not quite bright thought...   \n",
       "\n",
       "                      keywords  \\\n",
       "0         ['piece', 'useless']   \n",
       "1           ['color', 'great']   \n",
       "2  ['instruction', 'japanese']   \n",
       "3         ['door', 'terrible']   \n",
       "4   ['lite', 'quite', 'great']   \n",
       "\n",
       "                                           reference  \\\n",
       "0  unfortunately wait too long to use it and reai...   \n",
       "1  this food is great for mbuna cichlid or other ...   \n",
       "2  this item had japanese instruction on how to a...   \n",
       "3    terrible door last less than month and the door   \n",
       "4    work great white lite is not quite as bright as   \n",
       "\n",
       "                                             decoded  ref_lens  overlap  \\\n",
       "0           it does not work it will not hold charge        14       10   \n",
       "1  this food is great for mbuna cichlid or other ...        12       10   \n",
       "2  this item had japanese instruction on how to a...        10        8   \n",
       "3    terrible door last less than month and the door         9        8   \n",
       "4          work great white lite is not quite bright        10        7   \n",
       "\n",
       "   too_overlap   rouge_1   rouge_2   rouge_l  \n",
       "0        False  0.095238  0.000000  0.079365  \n",
       "1         True  1.000000  1.000000  1.000000  \n",
       "2         True  0.947368  0.941176  0.942163  \n",
       "3         True  1.000000  1.000000  1.000000  \n",
       "4         True  0.941176  0.750000  0.934730  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config.load_ckpt = '0780000'\n",
    "\n",
    "# logger = getLogger(loggerName)\n",
    "# writer = SummaryWriter(writerPath)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "config.batch_size = 8\n",
    "train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "epoch = 12\n",
    "print(load_model_path)\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(logger, load_model_path, model, optimizer)\n",
    "# model    \n",
    "train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "logger.info('-----------------------------------------------------------')\n",
    "test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc))\n",
    "\n",
    "# !ipython nbconvert --to script Pointer_generator.ipynb\n",
    "train_outFrame.head()\n",
    "test_outFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_outFrame.to_excel(writerPath + '/%s_output.xls'% 'train')\n",
    "# test_outFrame.to_excel(writerPath + '/%s_output.xls'% 'test')\n",
    "# len(test_outFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_overlap = len(train_outFrame[train_outFrame['rouge_1']>0.9])\n",
    "# test_overlap = len(test_outFrame[test_outFrame['rouge_1']>0.9])\n",
    "# print('train_overlap : %s'%train_overlap)\n",
    "# print('test_overlap : %s'%test_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# # 非符號alpha word重疊數\n",
    "# train_outFrame['overlap'] = train_outFrame['article']\n",
    "# train_outFrame['too_overlap'] = False\n",
    "# with tqdm(total=len(train_outFrame)) as pbar:\n",
    "#     for i ,row in train_outFrame.iterrows():        \n",
    "#         pbar.update(1)\n",
    "#         pbar.set_description(\"row %s \" % (i))\n",
    "\n",
    "#         art_set = row['article'].split(' ')        \n",
    "#         ref_set = row['reference'].split(' ')\n",
    "#         overlap = set(art_set) & set(ref_set)\n",
    "#         train_outFrame.loc[i, 'overlap'] = len(overlap)\n",
    "#         if len(overlap)> len(ref_set)-3: \n",
    "#             train_outFrame.loc[i, 'too_overlap'] = True\n",
    "        \n",
    "# train_outFrame.head()    \n",
    "\n",
    "# # from tqdm import tqdm\n",
    "# # 非符號alpha word重疊數\n",
    "# test_outFrame['overlap'] = test_outFrame['article']\n",
    "# test_outFrame['too_overlap'] = False\n",
    "\n",
    "# with tqdm(total=len(test_outFrame)) as pbar:\n",
    "#     for i ,row in test_outFrame.iterrows():        \n",
    "#         pbar.update(1)\n",
    "#         pbar.set_description(\"row %s \" % (i))\n",
    "\n",
    "#         art_set = row['article'].split(' ')        \n",
    "#         ref_set = row['reference'].split(' ')\n",
    "#         overlap = set(art_set) & set(ref_set)\n",
    "#         test_outFrame.loc[i, 'overlap'] = len(overlap)\n",
    "#         if len(overlap)> len(ref_set)-3: \n",
    "#             test_outFrame.loc[i, 'too_overlap'] = True\n",
    "            \n",
    "# test_outFrame.head()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_outFrame.to_excel(writerPath + '/%s_output.xls'% 'train')\n",
    "# test_outFrame.to_excel(writerPath + '/%s_output.xls'% 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_outFrame.head()\n",
    "\n",
    "# train_overlap = len(train_outFrame[train_outFrame['too_overlap']==True])\n",
    "# test_overlap = len(test_outFrame[test_outFrame['too_overlap']==True])\n",
    "# print('train_overlap : %s'%train_overlap)\n",
    "# print('test_overlap : %s'%test_overlap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
