{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0514 12:11:22.940802 140532369233728 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-05-14 12:11:24 - Pointer_generator_word2Vec - INFO: - logger已啟動\n",
      "I0514 12:11:24.147618 140532369233728 train_util.py:106] logger已啟動\n"
     ]
    }
   ],
   "source": [
    "from utils import config\n",
    "from utils.seq2seq import data\n",
    "\n",
    "from utils.seq2seq.batcher import *\n",
    "from utils.seq2seq.train_util import *\n",
    "from utils.seq2seq.rl_util import *\n",
    "from utils.seq2seq.initialize import loadCheckpoint, save_model\n",
    "from utils.seq2seq.write_result import *\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from translate.seq2seq_beam import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from utils.seq2seq.rl_util import *\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--key_attention', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_encoder', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_decoder', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--copy', type=bool, default=True, help = 'True/False') # for transformer\n",
    "\n",
    "parser.add_argument('--model_type', type=str, default='seq2seq', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--train_rl', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_keys', \n",
    "                    help = 'POS_keys / DEP_keys / Noun_adj_keys / TextRank_keys')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--rand_unif_init_mag', type=float, default=0.02)\n",
    "parser.add_argument('--trunc_norm_init_std', type=float, default=0.001)\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument('--gound_truth_prob', type=float, default=0.1)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=1000)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=50)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=8)\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=16)\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=300)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default='0588000', help='0760000')\n",
    "parser.add_argument('--word_emb_type', type=str, default='word2Vec', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=True, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-14 12:12:40 - Pointer_generator_word2Vec - INFO: - train : 397524, test : 44170\n",
      "I0514 12:12:40.016286 140532369233728 batcher.py:184] train : 397524, test : 44170\n",
      "2020-05-14 12:12:40 - Pointer_generator_word2Vec - INFO: - train batches : 49690, test batches : 5521\n",
      "I0514 12:12:40.467759 140532369233728 batcher.py:198] train batches : 49690, test batches : 5521\n"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab = getDataLoader(logger, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0514 12:12:41.206679 140532369233728 utils_any2vec.py:341] loading projection weights from ../Train-Data/Mix6_mainCat/Embedding/word2Vec/word2Vec.300d.txt\n",
      "I0514 12:12:53.887259 140532369233728 utils_any2vec.py:405] loaded (49676, 300) matrix from ../Train-Data/Mix6_mainCat/Embedding/word2Vec/word2Vec.300d.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/saved_models/Pointer_generator_word2Vec/0588000.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-14 12:13:01 - Pointer_generator_word2Vec - INFO: - Loaded model at model/saved_models/Pointer_generator_word2Vec/0588000.tar\n",
      "I0514 12:13:01.831597 140532369233728 initialize.py:212] Loaded model at model/saved_models/Pointer_generator_word2Vec/0588000.tar\n",
      "2020-05-14 12:13:01 - Pointer_generator_word2Vec - INFO: - Loaded model step = 588000, loss = 2.28, r_loss = 0.00 \n",
      "I0514 12:13:01.832611 140532369233728 initialize.py:213] Loaded model step = 588000, loss = 2.28, r_loss = 0.00 \n"
     ]
    }
   ],
   "source": [
    "from seq2seq import Model\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "load_step = None\n",
    "model = Model(pre_train_emb=config.pre_train_emb, \n",
    "              word_emb_type = config.word_emb_type, \n",
    "              vocab = vocab)\n",
    "\n",
    "model = model.cuda()\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=config.lr)   \n",
    "# optimizer = T.optim.Adagrad(model.parameters(),lr=config.lr, initial_accumulator_value=0.1)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(logger, load_model_path, model, optimizer)\n",
    "    T.save(model, 'model/%s.pkl'%(loggerName))  # 保存整个网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model, config, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    " \n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        'Decoder data'\n",
    "        dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(batch, config, batch_first = True) # Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(START))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "            use_gound_truth = get_cuda((T.rand(len(enc_out)) > config.gound_truth_prob)).long()  # Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "            x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t  # Select decoder input based on use_ground_truth probabilities\n",
    "            x_t = model.embeds(x_t)  \n",
    "            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                      ct_e, extra_zeros,\n",
    "                                                                                      enc_batch_extend_vocab,\n",
    "                                                                                      sum_temporal_srcs, prev_s, enc_key_batch, enc_key_mask)\n",
    "            target = target_batch[:, t]\n",
    "            log_probs = T.log(final_dist + config.eps)\n",
    "            step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=PAD)\n",
    "            step_losses.append(step_loss)\n",
    "            x_t = T.multinomial(final_dist,1).squeeze()  # Sample words from final distribution which can be used as input in next time step\n",
    "\n",
    "            is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t.detach() + (is_oov) * UNKNOWN_TOKEN  # Replace OOVs with [UNK] token\n",
    "\n",
    "        losses = T.sum(T.stack(step_losses, 1), 1)  # unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens  # Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)  # Average batch loss\n",
    "        return mle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "#     batch = next(iter(validate_loader))\n",
    "    val_num = len(iter(validate_loader))\n",
    "    for idx, batch in enumerate(validate_loader):\n",
    "        loss = train_one(model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "        if idx>= val_num/10: break\n",
    "    model.train()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "@torch.autograd.no_grad()\n",
    "def decode(writer, logger, step, config, model, batch, mode):\n",
    "    # 動態取batch\n",
    "    if mode == 'test':\n",
    "#         num = len(iter(batch))\n",
    "#         select_batch = None\n",
    "#         rand_b_id = randint(0,num-1)\n",
    "#         logger.info('test_batch : ' + str(num)+ ' ' + str(rand_b_id))\n",
    "#         for idx, b in enumerate(batch):\n",
    "#             if idx == rand_b_id:\n",
    "#                 select_batch = b\n",
    "#                 break\n",
    "        select_batch = next(iter(batch))\n",
    "        batch = select_batch\n",
    "        if type(batch) == torch.utils.data.dataloader.DataLoader:\n",
    "            batch = next(iter(batch))\n",
    "    'Encoder data'\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "    enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "    enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "    'Feed encoder data to predict'\n",
    "    pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                           enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                           START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "    article_sents, decoded_sents, keywords_list, \\\n",
    "    ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "    rouge_1, rouge_2, rouge_l = write_rouge(writer, step, mode,article_sents, decoded_sents, \\\n",
    "                keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    write_bleu(writer, step, mode, article_sents, decoded_sents, \\\n",
    "               keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    write_group(writer, step, mode, article_sents, decoded_sents,\\\n",
    "                keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    return rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import time\n",
    "@torch.autograd.no_grad()\n",
    "def avg_acc(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(iter(dataloader))\n",
    "    avg_rouge_l = []\n",
    "    acc_st, acc_cost = 0, 0\n",
    "    avg_acc_cost = []\n",
    "    for idx, batch in enumerate(dataloader): \n",
    "        if idx >= num/100: break\n",
    "        acc_st = time.time()\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "        'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                               enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                               START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, \\\n",
    "        ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "        rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents, \\\n",
    "                    keywords_list, ref_sents, long_seq_index, write = False)\n",
    "        avg_rouge_l.append(rouge_l)\n",
    "        acc_cost = time.time() - acc_st\n",
    "        avg_acc_cost.append(acc_cost)\n",
    "\n",
    "\n",
    "    avg_rouge_l = sum(avg_rouge_l) / len(avg_rouge_l)\n",
    "    writer.add_scalars('scalar_avg/acc',  \n",
    "                   {'%sing_avg_acc'%(mode): avg_rouge_l\n",
    "                   }, epoch)\n",
    "    avg_acc_cost = sum(avg_acc_cost) / len(avg_acc_cost)\n",
    "#     avg_acc_cost = avg_acc_cost / len(avg_rouge_l)\n",
    "#     print('decode 1% batches %s data, cost time %s ms' % (mode, avg_acc_cost ))\n",
    "    return avg_rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL(model, config, batch, greedy):    \n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param article_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "        \n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        s_t = enc_hidden                                                                            #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(START))  # Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []                       # Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []       # Stores padding masks of generated samples\n",
    "        log_probs = []                                                                              #Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "        # Generate RL tokens and compute rl-log-loss\n",
    "        # ----------------------------------------------------------------------\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = model.embeds(x_t)\n",
    "            \n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                      ct_e, extra_zeros,\n",
    "                                                                                      enc_batch_extend_vocab,\n",
    "                                                                                      sum_temporal_srcs, prev_s, enc_key_batch, enc_key_mask)\n",
    "            \n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs) # 建立以參數probs為標準的類別分佈\n",
    "                # perform multinomial sampling\n",
    "                x_t = multi_dist.sample()  # 將下一個時間點的x_t，視為下一個action   \n",
    "                # 使用log_prob实施梯度方法 Policy Gradient，构造一个等价類別分佈的损失函数\n",
    "                log_prob = multi_dist.log_prob(x_t)  \n",
    "                log_probs.append(log_prob) #\n",
    "            else:\n",
    "                # perform greedy sampling distribution\n",
    "                _, x_t = T.max(probs, dim=1)  # 因greedy以機率最大進行取樣，視為其中一個action   \n",
    "            x_t = x_t.detach() # detach返回的 Variable 永远不会需要梯度\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (x_t == END) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
    "            x_t = (1-is_oov)*x_t + (is_oov)*UNKNOWN_TOKEN                                             #Replace OOVs with [UNK] token\n",
    "        # -----------------------------------End loop -----------------------------------\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1) # 在第1个维度上stack, 增加新的维度进行堆叠\n",
    "            log_probs = log_probs * decoder_padding_mask # 遮罩掉為[END] or [STOP]不計算損失           #Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1) # 計算每個sample words生成的總長度               #Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs, dim=1) / lens  # 計算平均的每個句子的log loss # (bs,1)        #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy() # 取出每個sample sentence 的word id list\n",
    "            S = output2words(id_list, vocab, batch.art_oovs[i]) #Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:          #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "        return decoded_strs, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_RL(model, config, batch):\n",
    "    # Self-Critical sequence training(SCST)\n",
    "    sample_sents, RL_log_probs = RL(model, config, batch, greedy=False)   # multinomial sampling\n",
    "    with T.autograd.no_grad():        \n",
    "        greedy_sents, _ = RL(model, config, batch, greedy=True)  # greedy sampling\n",
    "\n",
    "    sample_reward = reward_function(sample_sents, batch.original_abstract) # r(w^s):通过根据概率来随机sample词生成句子的reward值\n",
    "    baseline_reward = reward_function(greedy_sents, batch.original_abstract) # r(w^):测试阶段使用greedy decoding取概率最大的词来生成句子的reward值\n",
    "\n",
    "    batch_reward = T.mean(sample_reward).item()\n",
    "    #Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    rl_loss = -(sample_reward - baseline_reward) * RL_log_probs  # SCST梯度計算公式     \n",
    "    rl_loss = T.mean(rl_loss)  \n",
    "    '''\n",
    "    公式的意思就是：对于如果当前sample到的词比测试阶段生成的词好，那么在这次词的维度上，整个式子的值就是负的（因为后面那一项一定为负），\n",
    "    这样梯度就会上升，从而提高这个词的分数st；而对于其他词，后面那一项为正，梯度就会下降，从而降低其他词的分数\n",
    "    '''                 \n",
    "    return rl_loss, batch_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(dataloader)\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "    avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    avg_meteor = []\n",
    "    outFrame = None\n",
    "    avg_time = 0\n",
    "        \n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        start = time.time() \n",
    "#         'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "#         'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                                enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                                START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "        cost = (time.time() - start)\n",
    "        avg_time += cost        \n",
    "\n",
    "        \n",
    "        rouge_1, rouge_2, rouge_l, self_Bleu_1, self_Bleu_2, self_Bleu_3, self_Bleu_4, \\\n",
    "            Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        \n",
    "        if idx %1000 ==0 and idx >0 : print(idx)\n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "        avg_self_bleu1.extend(self_Bleu_1)\n",
    "        avg_self_bleu2.extend(self_Bleu_2)\n",
    "        avg_self_bleu3.extend(self_Bleu_3)\n",
    "        avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "        avg_bleu1.extend(Bleu_1)\n",
    "        avg_bleu2.extend(Bleu_2)\n",
    "        avg_bleu3.extend(Bleu_3)\n",
    "        avg_bleu4.extend(Bleu_4)\n",
    "        avg_meteor.extend(Meteor)\n",
    "        # ----------------------------------------------------    \n",
    "    avg_time = avg_time / (num * config.batch_size) \n",
    "    \n",
    "    avg_rouge_l, outFrame = total_output(mode, writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l, \\\n",
    "        avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4, \\\n",
    "        avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "    )\n",
    "    \n",
    "    return avg_rouge_l, outFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# loss_st, loss_cost = 0,0\n",
    "# decode_st, decode_cost = 0,0\n",
    "\n",
    "# write_train_para(writer, config)\n",
    "# logger.info('------Training START--------')\n",
    "# running_avg_loss, running_avg_rl_loss = 0, 0\n",
    "# sum_total_reward = 0\n",
    "# step = 0\n",
    "\n",
    "# try:\n",
    "#     for epoch in range(config.max_epochs):\n",
    "#         for batch in train_loader:\n",
    "#             step += 1\n",
    "#             loss_st = time.time()\n",
    "#             mle_loss = train_one(model, config, batch)\n",
    "#             if config.train_rl:\n",
    "#                 rl_loss, batch_reward = train_one_RL(model, config, batch)             \n",
    "        \n",
    "#                 if step%1000 == 0 :\n",
    "#                     writer.add_scalars('scalar/RL_Loss',  \n",
    "#                        {'rl_loss': rl_loss\n",
    "#                        }, step)\n",
    "#                     writer.add_scalars('scalar/Reward',  \n",
    "#                        {'batch_reward': batch_reward\n",
    "#                        }, step)\n",
    "# #                     logger.info('epoch %d: %d, RL_Loss = %f, batch_reward = %f'\n",
    "# #                                     % (epoch, step, rl_loss, batch_reward))\n",
    "#                 sum_total_reward += batch_reward\n",
    "#             else:\n",
    "#                 rl_loss = T.FloatTensor([0]).cuda()\n",
    "#             (config.mle_weight * mle_loss + config.rl_weight * rl_loss).backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "#             '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "#             if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "#     #             clip_grad_norm_(model.parameters(), 5.0)                      \n",
    "#                 optimizer.step() # 根据累计的梯度更新网络参数\n",
    "#                 optimizer.zero_grad() # 清空过往梯度 \n",
    "#             if step%1000 == 0 :\n",
    "#                 with T.autograd.no_grad():\n",
    "#                     train_batch_loss = mle_loss.item()\n",
    "#                     train_batch_rl_loss = rl_loss.item()\n",
    "#                     val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "#                     running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "#                     running_avg_rl_loss = calc_running_avg_loss(train_batch_rl_loss, running_avg_rl_loss)\n",
    "#                     running_avg_reward = sum_total_reward / step\n",
    "# #                     logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "# #                                 % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "#                     writer.add_scalars('scalar/Loss',  \n",
    "#                        {'train_batch_loss': train_batch_loss\n",
    "#                        }, step)\n",
    "#                     writer.add_scalars('scalar_avg/loss',  \n",
    "#                        {'train_avg_loss': running_avg_loss,\n",
    "#                         'test_avg_loss': val_avg_loss\n",
    "#                        }, step)\n",
    "#                     if running_avg_reward > 0:\n",
    "# #                         logger.info('epoch %d: %d, running_avg_reward = %f'\n",
    "# #                                 % (epoch, step, running_avg_reward))\n",
    "#                         writer.add_scalars('scalar_avg/Reward',  \n",
    "#                            {'running_avg_reward': running_avg_reward\n",
    "#                            }, step)\n",
    "#                     if running_avg_rl_loss != 0:\n",
    "# #                         logger.info('epoch %d: %d, running_avg_rl_loss = %f'\n",
    "# #                                 % (epoch, step, running_avg_rl_loss))\n",
    "#                         writer.add_scalars('scalar_avg/RL_Loss',  \n",
    "#                            {'running_avg_rl_loss': running_avg_rl_loss\n",
    "#                            }, step)\n",
    "#                     loss_cost = time.time() - loss_st\n",
    "#                     if step%10000 == 0: logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "#                                 % (epoch, step, loss_cost))\n",
    "\n",
    "#             if step%10000 == 0:\n",
    "#                 save_model(config, logger, model, optimizer, step, vocab, running_avg_loss, \\\n",
    "#                            r_loss=0, title = loggerName)\n",
    "#             if step%1000 == 0 and step > 0:\n",
    "#                 decode_st = time.time()\n",
    "#                 train_rouge_l_f = decode(writer, logger, step, config, model, batch, mode = 'train') # call batch by validate_loader\n",
    "#                 test_rouge_l_f = decode(writer, logger, step, config, model, validate_loader, mode = 'test') # call batch by validate_loader\n",
    "#                 decode_cost = time.time() - decode_st\n",
    "#                 if step%10000 == 0: logger.info('epoch %d|step %d| decode cost = %f ms'% (epoch, step, decode_cost))\n",
    "\n",
    "#                 writer.add_scalars('scalar/Rouge-L',  \n",
    "#                    {'train_rouge_l_f': train_rouge_l_f,\n",
    "#                     'test_rouge_l_f': test_rouge_l_f\n",
    "#                    }, step)\n",
    "# #                 logger.info('epoch %d: %d, train_rouge_l_f = %f, test_rouge_l_f = %f'\n",
    "# #                                 % (epoch, step, train_rouge_l_f, test_rouge_l_f))\n",
    "# #         break\n",
    "#         logger.info('-------------------------------------------------------------')\n",
    "#         train_avg_acc = avg_acc(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "#         test_avg_acc = avg_acc(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "#         logger.info('epoch %d|step %d| training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "#                      % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                    \n",
    "#         logger.info('epoch %d|step %d| train_avg_acc = %f, test_avg_acc = %f' % (epoch, step, train_avg_acc, test_avg_acc))\n",
    "#         if running_avg_reward > 0:\n",
    "#             logger.info('epoch %d|step %d| running_avg_reward = %f'% (epoch, step, running_avg_reward))\n",
    "#         if running_avg_rl_loss != 0:\n",
    "#             logger.info('epoch %d|step %d| running_avg_rl_loss = %f'% (epoch, step, running_avg_rl_loss))\n",
    "#         logger.info('-------------------------------------------------------------')\n",
    "\n",
    "# except Excepation as e:\n",
    "#         print(e)\n",
    "# else:\n",
    "#     logger.info(u'------Training SUCCESS--------')  \n",
    "# finally:\n",
    "#     logger.info(u'------Training END--------')    \n",
    "# #     train_avg_acc = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "#     test_avg_acc = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "#     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc))\n",
    "#     removeLogger(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-15 00:03:03 - Pointer_generator_word2Vec - INFO: - -----------------------------------------------------------\n",
      "I0515 00:03:03.346524 140532369233728 <ipython-input-13-bf43b0eb0c4d>:17] -----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-15 00:51:11 - Pointer_generator_word2Vec - INFO: - epoch 12: train_avg_acc = 0.408073, test_avg_acc = 0.344794\n",
      "I0515 00:51:11.632448 140532369233728 <ipython-input-13-bf43b0eb0c4d>:19] epoch 12: train_avg_acc = 0.408073, test_avg_acc = 0.344794\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>keywords</th>\n",
       "      <th>reference</th>\n",
       "      <th>decoded</th>\n",
       "      <th>rouge_1</th>\n",
       "      <th>rouge_2</th>\n",
       "      <th>rouge_l</th>\n",
       "      <th>self_Bleu_1</th>\n",
       "      <th>self_Bleu_2</th>\n",
       "      <th>self_Bleu_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Bleu_1</th>\n",
       "      <th>Bleu_2</th>\n",
       "      <th>Bleu_3</th>\n",
       "      <th>Bleu_4</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>article_lens</th>\n",
       "      <th>ref_lens</th>\n",
       "      <th>overlap</th>\n",
       "      <th>overlap_percent</th>\n",
       "      <th>gen_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44160</th>\n",
       "      <td>positive fit all requirement for indoor camera...</td>\n",
       "      <td>['fit', 'less', 'quite', 'more', 'very', 'more...</td>\n",
       "      <td>for those of us who do not like instal more ap...</td>\n",
       "      <td>great indoor camera for the price</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067718</td>\n",
       "      <td>0.031479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031479</td>\n",
       "      <td>1.150198e-155</td>\n",
       "      <td>9.303220e-205</td>\n",
       "      <td>2.198601e-232</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>993</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>Ext</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44161</th>\n",
       "      <td>return the device fail miserably set and forge...</td>\n",
       "      <td>['device', 'digital']</td>\n",
       "      <td>not useable for set and forget pvr require con...</td>\n",
       "      <td>there are some good review for this product</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061160</td>\n",
       "      <td>0.035813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035813</td>\n",
       "      <td>1.510981e-155</td>\n",
       "      <td>1.283393e-204</td>\n",
       "      <td>3.103614e-232</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>992</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>38.888889</td>\n",
       "      <td>Abs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44162</th>\n",
       "      <td>this the second thule bag own and absolutely l...</td>\n",
       "      <td>['thing', 'quite', 'well', 'more', 'thin']</td>\n",
       "      <td>great design and make like so many other thule...</td>\n",
       "      <td>thule bag own and absolutely love both of them</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104685</td>\n",
       "      <td>0.198853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198853</td>\n",
       "      <td>6.292323e-155</td>\n",
       "      <td>4.846545e-204</td>\n",
       "      <td>1.119310e-231</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>991</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>Ext</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44163</th>\n",
       "      <td>was originally look for dermabenss benzoyl per...</td>\n",
       "      <td>['peroxide', 'very', 'also', 'residential']</td>\n",
       "      <td>must have in your dealing with my dog skin iss...</td>\n",
       "      <td>great for dog with itchy paw</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>0.122626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122626</td>\n",
       "      <td>3.168233e-155</td>\n",
       "      <td>2.277733e-204</td>\n",
       "      <td>5.092529e-232</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>990</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>Abs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44164</th>\n",
       "      <td>revise downward after ton issue after use new ...</td>\n",
       "      <td>['day', 'essentially', 'cheap']</td>\n",
       "      <td>bad ghost and pulsate on great color and hdr w...</td>\n",
       "      <td>worst tv ever had lot of problem</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>989</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>52.941176</td>\n",
       "      <td>Ext</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "44160  positive fit all requirement for indoor camera...   \n",
       "44161  return the device fail miserably set and forge...   \n",
       "44162  this the second thule bag own and absolutely l...   \n",
       "44163  was originally look for dermabenss benzoyl per...   \n",
       "44164  revise downward after ton issue after use new ...   \n",
       "\n",
       "                                                keywords  \\\n",
       "44160  ['fit', 'less', 'quite', 'more', 'very', 'more...   \n",
       "44161                              ['device', 'digital']   \n",
       "44162         ['thing', 'quite', 'well', 'more', 'thin']   \n",
       "44163        ['peroxide', 'very', 'also', 'residential']   \n",
       "44164                    ['day', 'essentially', 'cheap']   \n",
       "\n",
       "                                               reference  \\\n",
       "44160  for those of us who do not like instal more ap...   \n",
       "44161  not useable for set and forget pvr require con...   \n",
       "44162  great design and make like so many other thule...   \n",
       "44163  must have in your dealing with my dog skin iss...   \n",
       "44164  bad ghost and pulsate on great color and hdr w...   \n",
       "\n",
       "                                              decoded   rouge_1  rouge_2  \\\n",
       "44160               great indoor camera for the price  0.090909      0.0   \n",
       "44161     there are some good review for this product  0.076923      0.0   \n",
       "44162  thule bag own and absolutely love both of them  0.210526      0.0   \n",
       "44163                    great for dog with itchy paw  0.222222      0.0   \n",
       "44164                worst tv ever had lot of problem  0.000000      0.0   \n",
       "\n",
       "        rouge_l  self_Bleu_1  self_Bleu_2  self_Bleu_3    ...       Bleu_1  \\\n",
       "44160  0.067718     0.031479          0.0          0.0    ...     0.031479   \n",
       "44161  0.061160     0.035813          0.0          0.0    ...     0.035813   \n",
       "44162  0.104685     0.198853          0.0          0.0    ...     0.198853   \n",
       "44163  0.092593     0.122626          0.0          0.0    ...     0.122626   \n",
       "44164  0.000000     0.000000          0.0          0.0    ...     0.000000   \n",
       "\n",
       "              Bleu_2         Bleu_3         Bleu_4    Meteor  article_lens  \\\n",
       "44160  1.150198e-155  9.303220e-205  2.198601e-232  0.033333           993   \n",
       "44161  1.510981e-155  1.283393e-204  3.103614e-232  0.029412           992   \n",
       "44162  6.292323e-155  4.846545e-204  1.119310e-231  0.101010           991   \n",
       "44163  3.168233e-155  2.277733e-204  5.092529e-232  0.087719           990   \n",
       "44164   0.000000e+00   0.000000e+00   0.000000e+00  0.031250           989   \n",
       "\n",
       "       ref_lens  overlap  overlap_percent  gen_type  \n",
       "44160        16       12        75.000000       Ext  \n",
       "44161        18        7        38.888889       Abs  \n",
       "44162        10        6        60.000000       Ext  \n",
       "44163        12        6        50.000000       Abs  \n",
       "44164        17        9        52.941176       Ext  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config.load_ckpt = '0780000'\n",
    "\n",
    "# logger = getLogger(loggerName)\n",
    "# writer = SummaryWriter(writerPath)\n",
    "\n",
    "# config.load_ckpt = '0630000'\n",
    "\n",
    "# load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "# config.batch_size = 8\n",
    "# train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "epoch = 12\n",
    "# print(load_model_path)\n",
    "# if os.path.exists(load_model_path):\n",
    "#     model, optimizer, load_step = loadCheckpoint(logger, load_model_path, model, optimizer)\n",
    "# model    \n",
    "train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "logger.info('-----------------------------------------------------------')\n",
    "test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc))\n",
    "\n",
    "# !ipython nbconvert --to script Pointer_generator.ipynb\n",
    "train_outFrame.head()\n",
    "test_outFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_outFrame.to_excel(writerPath + '/%s_output.xls'% 'train')\n",
    "# test_outFrame.to_excel(writerPath + '/%s_output.xls'% 'test')\n",
    "# len(test_outFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_overlap = len(train_outFrame[train_outFrame['rouge_1']>0.9])\n",
    "# test_overlap = len(test_outFrame[test_outFrame['rouge_1']>0.9])\n",
    "# print('train_overlap : %s'%train_overlap)\n",
    "# print('test_overlap : %s'%test_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# # 非符號alpha word重疊數\n",
    "# train_outFrame['overlap'] = train_outFrame['article']\n",
    "# train_outFrame['too_overlap'] = False\n",
    "# with tqdm(total=len(train_outFrame)) as pbar:\n",
    "#     for i ,row in train_outFrame.iterrows():        \n",
    "#         pbar.update(1)\n",
    "#         pbar.set_description(\"row %s \" % (i))\n",
    "\n",
    "#         art_set = row['article'].split(' ')        \n",
    "#         ref_set = row['reference'].split(' ')\n",
    "#         overlap = set(art_set) & set(ref_set)\n",
    "#         train_outFrame.loc[i, 'overlap'] = len(overlap)\n",
    "#         if len(overlap)> len(ref_set)-3: \n",
    "#             train_outFrame.loc[i, 'too_overlap'] = True\n",
    "        \n",
    "# train_outFrame.head()    \n",
    "\n",
    "# # from tqdm import tqdm\n",
    "# # 非符號alpha word重疊數\n",
    "# test_outFrame['overlap'] = test_outFrame['article']\n",
    "# test_outFrame['too_overlap'] = False\n",
    "\n",
    "# with tqdm(total=len(test_outFrame)) as pbar:\n",
    "#     for i ,row in test_outFrame.iterrows():        \n",
    "#         pbar.update(1)\n",
    "#         pbar.set_description(\"row %s \" % (i))\n",
    "\n",
    "#         art_set = row['article'].split(' ')        \n",
    "#         ref_set = row['reference'].split(' ')\n",
    "#         overlap = set(art_set) & set(ref_set)\n",
    "#         test_outFrame.loc[i, 'overlap'] = len(overlap)\n",
    "#         if len(overlap)> len(ref_set)-3: \n",
    "#             test_outFrame.loc[i, 'too_overlap'] = True\n",
    "            \n",
    "# test_outFrame.head()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_outFrame.to_excel(writerPath + '/%s_output.xls'% 'train')\n",
    "# test_outFrame.to_excel(writerPath + '/%s_output.xls'% 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_outFrame.head()\n",
    "\n",
    "# train_overlap = len(train_outFrame[train_outFrame['too_overlap']==True])\n",
    "# test_overlap = len(test_outFrame[test_outFrame['too_overlap']==True])\n",
    "# print('train_overlap : %s'%train_overlap)\n",
    "# print('test_overlap : %s'%test_overlap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
