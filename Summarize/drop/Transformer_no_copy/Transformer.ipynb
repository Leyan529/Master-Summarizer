{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0412 15:43:27.559321 140154035328832 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-04-12 15:43:28 - Transformer_no_pretrain - INFO: - logger已啟動\n",
      "I0412 15:43:28.387253 140154035328832 train_util.py:140] logger已啟動\n"
     ]
    }
   ],
   "source": [
    "from utils import config, data\n",
    "from utils.batcher import *\n",
    "from utils.train_util import *\n",
    "from utils.initialize import loadCheckpoint, save_model\n",
    "\n",
    "from utils.write_result import *\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from beam.transormer_beam_search import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--key_attention', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_encoder', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_decoder', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--transformer', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--train_rl', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_FOP_keywords', \n",
    "                    help = 'POS_FOP_keywords / DEP_FOP_keywords / TextRank_keywords')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--rand_unif_init_mag', type=float, default=0.02)\n",
    "parser.add_argument('--trunc_norm_init_std', type=float, default=0.001)\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument('--gound_truth_prob', type=float, default=0.1)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=1000)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=50)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=8)\n",
    "parser.add_argument('--max_epochs', type=int, default=20)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=16)\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=512)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default=None, help='0002000')\n",
    "parser.add_argument('--word_emb_type', type=str, default='word2Vec', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=True, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-12 15:43:34 - Transformer_no_pretrain - INFO: - train : 37771, test : 4197\n",
      "I0412 15:43:34.294712 140154035328832 batcher.py:171] train : 37771, test : 4197\n"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab = getDataLoader(logger, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Model\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "load_step = None\n",
    "model = Model(pre_train_emb=config.pre_train_emb, \n",
    "              word_emb_type = config.word_emb_type, \n",
    "              vocab = vocab,config=config)\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=config.lr)   \n",
    "# optimizer = T.optim.Adagrad(model.parameters(),lr=config.lr, initial_accumulator_value=0.1)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (logger, config.load_ckpt)\n",
    "\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(logger, load_model_path, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model, config, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, _, \\\n",
    "        _, _, _, _= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "       \n",
    "        'Decoder data'\n",
    "        dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(batch, config, batch_first = True) # Get input and target batchs for training decoder\n",
    "\n",
    "        pred = model(enc_batch, dec_batch, enc_padding_mask, dec_padding_mask, enc_batch_extend_vocab, extra_zeros)\n",
    "#         loss = model.label_smoothing_loss(pred, target_batch)\n",
    "        loss = model.nll_loss(pred, target_batch, dec_lens)\n",
    "#         print('loss',loss)#         \n",
    "        # >>>>>>>> DEBUG Session <<<<<<<<<\n",
    "#         print('------------------------------------')\n",
    "#         print(\"ENC\\n\")\n",
    "#         print(enc_batch.shape)\n",
    "#         print(\"DEC\\n\")\n",
    "#         print(dec_batch.shape)\n",
    "        # print(\"TGT\\n\")\n",
    "        # print(target_batch.shape)\n",
    "        # print(\"ENCP\\n\")\n",
    "        # print(enc_padding_mask.shape)\n",
    "        # print(\"DECP\\n\")\n",
    "        # print(dec_padding_mask.shape)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "#     batch = next(iter(validate_loader))\n",
    "    for batch in validate_loader:\n",
    "        loss = train_one(model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "#         break\n",
    "    model.train()\n",
    "    ave_loss = sum(losses) / len(losses)\n",
    "    return ave_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "@torch.autograd.no_grad()\n",
    "def decode(writer, logger, step, config, model, batch, mode):\n",
    "    # 動態取batch\n",
    "    model.eval()\n",
    "    config.is_predicting = True\n",
    "    if mode == 'test':\n",
    "        num = len(iter(batch))\n",
    "        select_batch = None\n",
    "        rand_b_id = randint(0,num-2)\n",
    "#         logger.info('test_batch : ' + str(num)+ ' ' + str(rand_b_id))\n",
    "        for idx, b in enumerate(batch):\n",
    "            if idx == rand_b_id:\n",
    "                select_batch = b\n",
    "                break                \n",
    "#         select_batch = next(iter(batch))\n",
    "        batch = select_batch\n",
    "        if type(batch) == torch.utils.data.dataloader.DataLoader:\n",
    "            batch = next(iter(batch))\n",
    "\n",
    "#     print(batch.enc_pad_mask.shape[0])\n",
    "    pred_ids = beam_search(config, batch, model, START, END, UNKNOWN_TOKEN)\n",
    "    config.is_predicting = False\n",
    "\n",
    "    article_sents, decoded_sents, keywords_list, \\\n",
    "    ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "#     print(prepare_result(vocab, batch, pred_ids))\n",
    "    rouge_l = write_rouge(writer, step, mode,article_sents, decoded_sents, \\\n",
    "                keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    write_bleu(writer, step, mode, article_sents, decoded_sents, \\\n",
    "               keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    write_group(writer, step, mode, article_sents, decoded_sents,\\\n",
    "                keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    return rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "@torch.autograd.no_grad()\n",
    "def avg_acc(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(iter(dataloader))\n",
    "    avg_rouge_l = []\n",
    "    model.eval()\n",
    "    config.is_predicting = True\n",
    "    for idx, batch in enumerate(dataloader): \n",
    "        pred_ids = beam_search(config, batch, model, START, END, UNKNOWN_TOKEN)\n",
    "        config.is_predicting = False\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, \\\n",
    "        ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "        rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents, \\\n",
    "                    keywords_list, ref_sents, long_seq_index, write = False)\n",
    "        avg_rouge_l.append(rouge_l)\n",
    "\n",
    "\n",
    "    avg_rouge_l = sum(avg_rouge_l) / num\n",
    "    writer.add_scalars('scalar_avg/acc',  \n",
    "                   {'%sing_avg_acc'%(mode): avg_rouge_l\n",
    "                   }, epoch)\n",
    "\n",
    "    return avg_rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL(model, config, batch, greedy):    \n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param article_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "        \n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        s_t = enc_hidden                                                                            #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(START))  # Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []                       # Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []       # Stores padding masks of generated samples\n",
    "        log_probs = []                                                                              #Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "        # Generate RL tokens and compute rl-log-loss\n",
    "        # ----------------------------------------------------------------------\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = model.embeds(x_t)\n",
    "            \n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                      ct_e, extra_zeros,\n",
    "                                                                                      enc_batch_extend_vocab,\n",
    "                                                                                      sum_temporal_srcs, prev_s, enc_key_batch, enc_key_mask)\n",
    "            \n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs) # 建立以參數probs為標準的類別分佈\n",
    "                # perform multinomial sampling\n",
    "                x_t = multi_dist.sample()  # 將下一個時間點的x_t，視為下一個action   \n",
    "                # 使用log_prob实施梯度方法 Policy Gradient，构造一个等价類別分佈的损失函数\n",
    "                log_prob = multi_dist.log_prob(x_t)  \n",
    "                log_probs.append(log_prob) #\n",
    "            else:\n",
    "                # perform greedy sampling distribution\n",
    "                _, x_t = T.max(probs, dim=1)  # 因greedy以機率最大進行取樣，視為其中一個action   \n",
    "            x_t = x_t.detach() # detach返回的 Variable 永远不会需要梯度\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (x_t == END) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
    "            x_t = (1-is_oov)*x_t + (is_oov)*UNKNOWN_TOKEN                                             #Replace OOVs with [UNK] token\n",
    "        # -----------------------------------End loop -----------------------------------\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1) # 在第1个维度上stack, 增加新的维度进行堆叠\n",
    "            log_probs = log_probs * decoder_padding_mask # 遮罩掉為[END] or [STOP]不計算損失           #Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1) # 計算每個sample words生成的總長度               #Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs, dim=1) / lens  # 計算平均的每個句子的log loss # (bs,1)        #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy() # 取出每個sample sentence 的word id list\n",
    "            S = output2words(id_list, vocab, batch.art_oovs[i]) #Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:          #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "        return decoded_strs, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_RL(model, config, batch):\n",
    "    # Self-Critical sequence training(SCST)\n",
    "    sample_sents, RL_log_probs = RL(model, config, batch, greedy=False)   # multinomial sampling\n",
    "    with T.autograd.no_grad():        \n",
    "        greedy_sents, _ = RL(model, config, batch, greedy=True)  # greedy sampling\n",
    "\n",
    "    sample_reward = reward_function(sample_sents, batch.original_abstract) # r(w^s):通过根据概率来随机sample词生成句子的reward值\n",
    "    baseline_reward = reward_function(greedy_sents, batch.original_abstract) # r(w^):测试阶段使用greedy decoding取概率最大的词来生成句子的reward值\n",
    "\n",
    "    batch_reward = T.mean(sample_reward).item()\n",
    "    #Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    rl_loss = -(sample_reward - baseline_reward) * RL_log_probs  # SCST梯度計算公式     \n",
    "    rl_loss = T.mean(rl_loss)  \n",
    "    '''\n",
    "    公式的意思就是：对于如果当前sample到的词比测试阶段生成的词好，那么在这次词的维度上，整个式子的值就是负的（因为后面那一项一定为负），\n",
    "    这样梯度就会上升，从而提高这个词的分数st；而对于其他词，后面那一项为正，梯度就会下降，从而降低其他词的分数\n",
    "    '''                 \n",
    "    return rl_loss, batch_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-12 15:43:38 - Transformer_no_pretrain - INFO: - ------Training START--------\n",
      "I0412 15:43:38.143704 140154035328832 <ipython-input-11-bc07426dfa84>:2] ------Training START--------\n",
      "2020-04-12 15:45:16 - Transformer_no_pretrain - INFO: - epoch 0: 1000, training batch loss = 5.419218, running_avg_loss loss = 5.419218, validation loss = 5.086878\n",
      "I0412 15:45:16.062202 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 0: 1000, training batch loss = 5.419218, running_avg_loss loss = 5.419218, validation loss = 5.086878\n",
      "2020-04-12 15:45:16 - Transformer_no_pretrain - INFO: - epoch 0: 1000, running_avg_reward = 0.000000\n",
      "I0412 15:45:16.063291 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 0: 1000, running_avg_reward = 0.000000\n",
      "2020-04-12 15:45:17 - Transformer_no_pretrain - INFO: - epoch 0: 1000, train_rouge_l_f = 0.114388, test_rouge_l_f = 0.113384\n",
      "I0412 15:45:17.513018 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 0: 1000, train_rouge_l_f = 0.114388, test_rouge_l_f = 0.113384\n",
      "2020-04-12 15:46:46 - Transformer_no_pretrain - INFO: - epoch 0: 2000, training batch loss = 4.136676, running_avg_loss loss = 5.406392, validation loss = 4.811998\n",
      "I0412 15:46:46.755110 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 0: 2000, training batch loss = 4.136676, running_avg_loss loss = 5.406392, validation loss = 4.811998\n",
      "2020-04-12 15:46:46 - Transformer_no_pretrain - INFO: - epoch 0: 2000, running_avg_reward = 0.000000\n",
      "I0412 15:46:46.756906 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 0: 2000, running_avg_reward = 0.000000\n",
      "2020-04-12 15:46:47 - Transformer_no_pretrain - INFO: - epoch 0: 2000, train_rouge_l_f = 0.136291, test_rouge_l_f = 0.108262\n",
      "I0412 15:46:47.217228 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 0: 2000, train_rouge_l_f = 0.136291, test_rouge_l_f = 0.108262\n",
      "2020-04-12 15:48:15 - Transformer_no_pretrain - INFO: - epoch 0: 3000, training batch loss = 4.206058, running_avg_loss loss = 5.394389, validation loss = 4.695543\n",
      "I0412 15:48:15.960699 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 0: 3000, training batch loss = 4.206058, running_avg_loss loss = 5.394389, validation loss = 4.695543\n",
      "2020-04-12 15:48:15 - Transformer_no_pretrain - INFO: - epoch 0: 3000, running_avg_reward = 0.000000\n",
      "I0412 15:48:15.961711 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 0: 3000, running_avg_reward = 0.000000\n",
      "2020-04-12 15:48:16 - Transformer_no_pretrain - INFO: - epoch 0: 3000, train_rouge_l_f = 0.152730, test_rouge_l_f = 0.094582\n",
      "I0412 15:48:16.847645 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 0: 3000, train_rouge_l_f = 0.152730, test_rouge_l_f = 0.094582\n",
      "2020-04-12 15:49:47 - Transformer_no_pretrain - INFO: - epoch 0: 4000, training batch loss = 4.980359, running_avg_loss loss = 5.390249, validation loss = 4.620914\n",
      "I0412 15:49:47.170790 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 0: 4000, training batch loss = 4.980359, running_avg_loss loss = 5.390249, validation loss = 4.620914\n",
      "2020-04-12 15:49:47 - Transformer_no_pretrain - INFO: - epoch 0: 4000, running_avg_reward = 0.000000\n",
      "I0412 15:49:47.171743 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 0: 4000, running_avg_reward = 0.000000\n",
      "2020-04-12 15:49:48 - Transformer_no_pretrain - INFO: - epoch 0: 4000, train_rouge_l_f = 0.089104, test_rouge_l_f = 0.125356\n",
      "I0412 15:49:48.098498 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 0: 4000, train_rouge_l_f = 0.089104, test_rouge_l_f = 0.125356\n",
      "2020-04-12 16:05:35 - Transformer_no_pretrain - INFO: - epoch 0: 4722, test_avg_acc = 0.087003, test_avg_acc = 0.088624\n",
      "I0412 16:05:35.408862 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 0: 4722, test_avg_acc = 0.087003, test_avg_acc = 0.088624\n",
      "2020-04-12 16:06:08 - Transformer_no_pretrain - INFO: - epoch 1: 5000, training batch loss = 3.980711, running_avg_loss loss = 5.376153, validation loss = 4.561781\n",
      "I0412 16:06:08.997978 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 1: 5000, training batch loss = 3.980711, running_avg_loss loss = 5.376153, validation loss = 4.561781\n",
      "2020-04-12 16:06:08 - Transformer_no_pretrain - INFO: - epoch 1: 5000, running_avg_reward = 0.000000\n",
      "I0412 16:06:08.999823 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 1: 5000, running_avg_reward = 0.000000\n",
      "2020-04-12 16:06:09 - Transformer_no_pretrain - INFO: - Saving model step 5000 to model/saved_models/Transformer_no_pretrain/0005000.tar...\n",
      "I0412 16:06:09.003017 140154035328832 initialize.py:225] Saving model step 5000 to model/saved_models/Transformer_no_pretrain/0005000.tar...\n",
      "2020-04-12 16:06:14 - Transformer_no_pretrain - INFO: - epoch 1: 5000, train_rouge_l_f = 0.167999, test_rouge_l_f = 0.116326\n",
      "I0412 16:06:14.826275 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 1: 5000, train_rouge_l_f = 0.167999, test_rouge_l_f = 0.116326\n",
      "2020-04-12 16:07:46 - Transformer_no_pretrain - INFO: - epoch 1: 6000, training batch loss = 4.171458, running_avg_loss loss = 5.364106, validation loss = 4.539309\n",
      "I0412 16:07:46.441624 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 1: 6000, training batch loss = 4.171458, running_avg_loss loss = 5.364106, validation loss = 4.539309\n",
      "2020-04-12 16:07:46 - Transformer_no_pretrain - INFO: - epoch 1: 6000, running_avg_reward = 0.000000\n",
      "I0412 16:07:46.443309 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 1: 6000, running_avg_reward = 0.000000\n",
      "2020-04-12 16:07:47 - Transformer_no_pretrain - INFO: - epoch 1: 6000, train_rouge_l_f = 0.096284, test_rouge_l_f = 0.099581\n",
      "I0412 16:07:47.506800 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 1: 6000, train_rouge_l_f = 0.096284, test_rouge_l_f = 0.099581\n",
      "2020-04-12 16:09:19 - Transformer_no_pretrain - INFO: - epoch 1: 7000, training batch loss = 3.682353, running_avg_loss loss = 5.347289, validation loss = 4.508605\n",
      "I0412 16:09:19.423246 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 1: 7000, training batch loss = 3.682353, running_avg_loss loss = 5.347289, validation loss = 4.508605\n",
      "2020-04-12 16:09:19 - Transformer_no_pretrain - INFO: - epoch 1: 7000, running_avg_reward = 0.000000\n",
      "I0412 16:09:19.424948 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 1: 7000, running_avg_reward = 0.000000\n",
      "2020-04-12 16:09:24 - Transformer_no_pretrain - INFO: - epoch 1: 7000, train_rouge_l_f = 0.174732, test_rouge_l_f = 0.079992\n",
      "I0412 16:09:24.154235 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 1: 7000, train_rouge_l_f = 0.174732, test_rouge_l_f = 0.079992\n",
      "2020-04-12 16:10:57 - Transformer_no_pretrain - INFO: - epoch 1: 8000, training batch loss = 4.958845, running_avg_loss loss = 5.343404, validation loss = 4.486660\n",
      "I0412 16:10:57.591322 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 1: 8000, training batch loss = 4.958845, running_avg_loss loss = 5.343404, validation loss = 4.486660\n",
      "2020-04-12 16:10:57 - Transformer_no_pretrain - INFO: - epoch 1: 8000, running_avg_reward = 0.000000\n",
      "I0412 16:10:57.593072 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 1: 8000, running_avg_reward = 0.000000\n",
      "2020-04-12 16:10:58 - Transformer_no_pretrain - INFO: - epoch 1: 8000, train_rouge_l_f = 0.134544, test_rouge_l_f = 0.184622\n",
      "I0412 16:10:58.966103 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 1: 8000, train_rouge_l_f = 0.134544, test_rouge_l_f = 0.184622\n",
      "2020-04-12 16:12:30 - Transformer_no_pretrain - INFO: - epoch 1: 9000, training batch loss = 3.995578, running_avg_loss loss = 5.329926, validation loss = 4.463916\n",
      "I0412 16:12:30.435939 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 1: 9000, training batch loss = 3.995578, running_avg_loss loss = 5.329926, validation loss = 4.463916\n",
      "2020-04-12 16:12:30 - Transformer_no_pretrain - INFO: - epoch 1: 9000, running_avg_reward = 0.000000\n",
      "I0412 16:12:30.437224 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 1: 9000, running_avg_reward = 0.000000\n",
      "2020-04-12 16:12:31 - Transformer_no_pretrain - INFO: - epoch 1: 9000, train_rouge_l_f = 0.123523, test_rouge_l_f = 0.115357\n",
      "I0412 16:12:31.285997 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 1: 9000, train_rouge_l_f = 0.123523, test_rouge_l_f = 0.115357\n",
      "2020-04-12 17:03:11 - Transformer_no_pretrain - INFO: - epoch 1: 9444, test_avg_acc = 0.130897, test_avg_acc = 0.133482\n",
      "I0412 17:03:11.964717 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 1: 9444, test_avg_acc = 0.130897, test_avg_acc = 0.133482\n",
      "2020-04-12 17:04:06 - Transformer_no_pretrain - INFO: - epoch 2: 10000, training batch loss = 3.855898, running_avg_loss loss = 5.315186, validation loss = 4.454036\n",
      "I0412 17:04:06.989487 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 2: 10000, training batch loss = 3.855898, running_avg_loss loss = 5.315186, validation loss = 4.454036\n",
      "2020-04-12 17:04:06 - Transformer_no_pretrain - INFO: - epoch 2: 10000, running_avg_reward = 0.000000\n",
      "I0412 17:04:06.991586 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 2: 10000, running_avg_reward = 0.000000\n",
      "2020-04-12 17:04:06 - Transformer_no_pretrain - INFO: - Saving model step 10000 to model/saved_models/Transformer_no_pretrain/0010000.tar...\n",
      "I0412 17:04:06.995042 140154035328832 initialize.py:225] Saving model step 10000 to model/saved_models/Transformer_no_pretrain/0010000.tar...\n",
      "2020-04-12 17:04:12 - Transformer_no_pretrain - INFO: - epoch 2: 10000, train_rouge_l_f = 0.122887, test_rouge_l_f = 0.166965\n",
      "I0412 17:04:12.886474 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 2: 10000, train_rouge_l_f = 0.122887, test_rouge_l_f = 0.166965\n",
      "2020-04-12 17:05:45 - Transformer_no_pretrain - INFO: - epoch 2: 11000, training batch loss = 3.213797, running_avg_loss loss = 5.294172, validation loss = 4.452640\n",
      "I0412 17:05:45.176108 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 2: 11000, training batch loss = 3.213797, running_avg_loss loss = 5.294172, validation loss = 4.452640\n",
      "2020-04-12 17:05:45 - Transformer_no_pretrain - INFO: - epoch 2: 11000, running_avg_reward = 0.000000\n",
      "I0412 17:05:45.177864 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 2: 11000, running_avg_reward = 0.000000\n",
      "2020-04-12 17:05:46 - Transformer_no_pretrain - INFO: - epoch 2: 11000, train_rouge_l_f = 0.191047, test_rouge_l_f = 0.080991\n",
      "I0412 17:05:46.890323 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 2: 11000, train_rouge_l_f = 0.191047, test_rouge_l_f = 0.080991\n",
      "2020-04-12 17:07:19 - Transformer_no_pretrain - INFO: - epoch 2: 12000, training batch loss = 4.043764, running_avg_loss loss = 5.281668, validation loss = 4.429269\n",
      "I0412 17:07:19.332494 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 2: 12000, training batch loss = 4.043764, running_avg_loss loss = 5.281668, validation loss = 4.429269\n",
      "2020-04-12 17:07:19 - Transformer_no_pretrain - INFO: - epoch 2: 12000, running_avg_reward = 0.000000\n",
      "I0412 17:07:19.334240 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 2: 12000, running_avg_reward = 0.000000\n",
      "2020-04-12 17:07:21 - Transformer_no_pretrain - INFO: - epoch 2: 12000, train_rouge_l_f = 0.210421, test_rouge_l_f = 0.228686\n",
      "I0412 17:07:21.883493 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 2: 12000, train_rouge_l_f = 0.210421, test_rouge_l_f = 0.228686\n",
      "2020-04-12 17:08:55 - Transformer_no_pretrain - INFO: - epoch 2: 13000, training batch loss = 4.287924, running_avg_loss loss = 5.271730, validation loss = 4.419144\n",
      "I0412 17:08:55.821081 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 2: 13000, training batch loss = 4.287924, running_avg_loss loss = 5.271730, validation loss = 4.419144\n",
      "2020-04-12 17:08:55 - Transformer_no_pretrain - INFO: - epoch 2: 13000, running_avg_reward = 0.000000\n",
      "I0412 17:08:55.822744 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 2: 13000, running_avg_reward = 0.000000\n",
      "2020-04-12 17:08:57 - Transformer_no_pretrain - INFO: - epoch 2: 13000, train_rouge_l_f = 0.163847, test_rouge_l_f = 0.192233\n",
      "I0412 17:08:57.663114 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 2: 13000, train_rouge_l_f = 0.163847, test_rouge_l_f = 0.192233\n",
      "2020-04-12 17:10:29 - Transformer_no_pretrain - INFO: - epoch 2: 14000, training batch loss = 4.521360, running_avg_loss loss = 5.264227, validation loss = 4.407363\n",
      "I0412 17:10:29.926965 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 2: 14000, training batch loss = 4.521360, running_avg_loss loss = 5.264227, validation loss = 4.407363\n",
      "2020-04-12 17:10:29 - Transformer_no_pretrain - INFO: - epoch 2: 14000, running_avg_reward = 0.000000\n",
      "I0412 17:10:29.928664 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 2: 14000, running_avg_reward = 0.000000\n",
      "2020-04-12 17:10:31 - Transformer_no_pretrain - INFO: - epoch 2: 14000, train_rouge_l_f = 0.037589, test_rouge_l_f = 0.153845\n",
      "I0412 17:10:31.885698 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 2: 14000, train_rouge_l_f = 0.037589, test_rouge_l_f = 0.153845\n",
      "2020-04-12 17:47:01 - Transformer_no_pretrain - INFO: - epoch 2: 14166, test_avg_acc = 0.125488, test_avg_acc = 0.124439\n",
      "I0412 17:47:01.457880 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 2: 14166, test_avg_acc = 0.125488, test_avg_acc = 0.124439\n",
      "2020-04-12 17:48:21 - Transformer_no_pretrain - INFO: - epoch 3: 15000, training batch loss = 4.987896, running_avg_loss loss = 5.261463, validation loss = 4.406219\n",
      "I0412 17:48:21.471158 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 3: 15000, training batch loss = 4.987896, running_avg_loss loss = 5.261463, validation loss = 4.406219\n",
      "2020-04-12 17:48:21 - Transformer_no_pretrain - INFO: - epoch 3: 15000, running_avg_reward = 0.000000\n",
      "I0412 17:48:21.472504 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 3: 15000, running_avg_reward = 0.000000\n",
      "2020-04-12 17:48:21 - Transformer_no_pretrain - INFO: - Saving model step 15000 to model/saved_models/Transformer_no_pretrain/0015000.tar...\n",
      "I0412 17:48:21.474132 140154035328832 initialize.py:225] Saving model step 15000 to model/saved_models/Transformer_no_pretrain/0015000.tar...\n",
      "2020-04-12 17:48:28 - Transformer_no_pretrain - INFO: - epoch 3: 15000, train_rouge_l_f = 0.107314, test_rouge_l_f = 0.181044\n",
      "I0412 17:48:28.389586 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 3: 15000, train_rouge_l_f = 0.107314, test_rouge_l_f = 0.181044\n",
      "2020-04-12 17:50:00 - Transformer_no_pretrain - INFO: - epoch 3: 16000, training batch loss = 3.945537, running_avg_loss loss = 5.248304, validation loss = 4.398045\n",
      "I0412 17:50:00.440429 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 3: 16000, training batch loss = 3.945537, running_avg_loss loss = 5.248304, validation loss = 4.398045\n",
      "2020-04-12 17:50:00 - Transformer_no_pretrain - INFO: - epoch 3: 16000, running_avg_reward = 0.000000\n",
      "I0412 17:50:00.442211 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 3: 16000, running_avg_reward = 0.000000\n",
      "2020-04-12 17:50:01 - Transformer_no_pretrain - INFO: - epoch 3: 16000, train_rouge_l_f = 0.083090, test_rouge_l_f = 0.133032\n",
      "I0412 17:50:01.703836 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 3: 16000, train_rouge_l_f = 0.083090, test_rouge_l_f = 0.133032\n",
      "2020-04-12 17:51:34 - Transformer_no_pretrain - INFO: - epoch 3: 17000, training batch loss = 4.488124, running_avg_loss loss = 5.240702, validation loss = 4.391150\n",
      "I0412 17:51:34.609620 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 3: 17000, training batch loss = 4.488124, running_avg_loss loss = 5.240702, validation loss = 4.391150\n",
      "2020-04-12 17:51:34 - Transformer_no_pretrain - INFO: - epoch 3: 17000, running_avg_reward = 0.000000\n",
      "I0412 17:51:34.611398 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 3: 17000, running_avg_reward = 0.000000\n",
      "2020-04-12 17:51:35 - Transformer_no_pretrain - INFO: - epoch 3: 17000, train_rouge_l_f = 0.069148, test_rouge_l_f = 0.034549\n",
      "I0412 17:51:35.363950 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 3: 17000, train_rouge_l_f = 0.069148, test_rouge_l_f = 0.034549\n",
      "2020-04-12 17:53:07 - Transformer_no_pretrain - INFO: - epoch 3: 18000, training batch loss = 4.125369, running_avg_loss loss = 5.229549, validation loss = 4.385083\n",
      "I0412 17:53:07.463461 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 3: 18000, training batch loss = 4.125369, running_avg_loss loss = 5.229549, validation loss = 4.385083\n",
      "2020-04-12 17:53:07 - Transformer_no_pretrain - INFO: - epoch 3: 18000, running_avg_reward = 0.000000\n",
      "I0412 17:53:07.465353 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 3: 18000, running_avg_reward = 0.000000\n",
      "2020-04-12 17:53:08 - Transformer_no_pretrain - INFO: - epoch 3: 18000, train_rouge_l_f = 0.104202, test_rouge_l_f = 0.140286\n",
      "I0412 17:53:08.293583 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 3: 18000, train_rouge_l_f = 0.104202, test_rouge_l_f = 0.140286\n",
      "2020-04-12 18:14:37 - Transformer_no_pretrain - INFO: - epoch 3: 18888, test_avg_acc = 0.110535, test_avg_acc = 0.112989\n",
      "I0412 18:14:37.338112 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 3: 18888, test_avg_acc = 0.110535, test_avg_acc = 0.112989\n",
      "2020-04-12 18:14:57 - Transformer_no_pretrain - INFO: - epoch 4: 19000, training batch loss = 3.439850, running_avg_loss loss = 5.211652, validation loss = 4.369272\n",
      "I0412 18:14:57.273424 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 4: 19000, training batch loss = 3.439850, running_avg_loss loss = 5.211652, validation loss = 4.369272\n",
      "2020-04-12 18:14:57 - Transformer_no_pretrain - INFO: - epoch 4: 19000, running_avg_reward = 0.000000\n",
      "I0412 18:14:57.274411 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 4: 19000, running_avg_reward = 0.000000\n",
      "2020-04-12 18:14:58 - Transformer_no_pretrain - INFO: - epoch 4: 19000, train_rouge_l_f = 0.205148, test_rouge_l_f = 0.090910\n",
      "I0412 18:14:58.075432 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 4: 19000, train_rouge_l_f = 0.205148, test_rouge_l_f = 0.090910\n",
      "2020-04-12 18:16:32 - Transformer_no_pretrain - INFO: - epoch 4: 20000, training batch loss = 4.054265, running_avg_loss loss = 5.200078, validation loss = 4.387937\n",
      "I0412 18:16:32.241724 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 4: 20000, training batch loss = 4.054265, running_avg_loss loss = 5.200078, validation loss = 4.387937\n",
      "2020-04-12 18:16:32 - Transformer_no_pretrain - INFO: - epoch 4: 20000, running_avg_reward = 0.000000\n",
      "I0412 18:16:32.243430 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 4: 20000, running_avg_reward = 0.000000\n",
      "2020-04-12 18:16:32 - Transformer_no_pretrain - INFO: - Saving model step 20000 to model/saved_models/Transformer_no_pretrain/0020000.tar...\n",
      "I0412 18:16:32.248268 140154035328832 initialize.py:225] Saving model step 20000 to model/saved_models/Transformer_no_pretrain/0020000.tar...\n",
      "2020-04-12 18:16:38 - Transformer_no_pretrain - INFO: - epoch 4: 20000, train_rouge_l_f = 0.078131, test_rouge_l_f = 0.088904\n",
      "I0412 18:16:38.452223 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 4: 20000, train_rouge_l_f = 0.078131, test_rouge_l_f = 0.088904\n",
      "2020-04-12 18:18:10 - Transformer_no_pretrain - INFO: - epoch 4: 21000, training batch loss = 4.320230, running_avg_loss loss = 5.191280, validation loss = 4.380854\n",
      "I0412 18:18:10.861148 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 4: 21000, training batch loss = 4.320230, running_avg_loss loss = 5.191280, validation loss = 4.380854\n",
      "2020-04-12 18:18:10 - Transformer_no_pretrain - INFO: - epoch 4: 21000, running_avg_reward = 0.000000\n",
      "I0412 18:18:10.862106 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 4: 21000, running_avg_reward = 0.000000\n",
      "2020-04-12 18:18:12 - Transformer_no_pretrain - INFO: - epoch 4: 21000, train_rouge_l_f = 0.103802, test_rouge_l_f = 0.114568\n",
      "I0412 18:18:12.606932 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 4: 21000, train_rouge_l_f = 0.103802, test_rouge_l_f = 0.114568\n",
      "2020-04-12 18:19:47 - Transformer_no_pretrain - INFO: - epoch 4: 22000, training batch loss = 3.925605, running_avg_loss loss = 5.178623, validation loss = 4.366135\n",
      "I0412 18:19:47.900948 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 4: 22000, training batch loss = 3.925605, running_avg_loss loss = 5.178623, validation loss = 4.366135\n",
      "2020-04-12 18:19:47 - Transformer_no_pretrain - INFO: - epoch 4: 22000, running_avg_reward = 0.000000\n",
      "I0412 18:19:47.902595 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 4: 22000, running_avg_reward = 0.000000\n",
      "2020-04-12 18:19:50 - Transformer_no_pretrain - INFO: - epoch 4: 22000, train_rouge_l_f = 0.131301, test_rouge_l_f = 0.198511\n",
      "I0412 18:19:50.437337 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 4: 22000, train_rouge_l_f = 0.131301, test_rouge_l_f = 0.198511\n",
      "2020-04-12 18:21:23 - Transformer_no_pretrain - INFO: - epoch 4: 23000, training batch loss = 3.746599, running_avg_loss loss = 5.164303, validation loss = 4.354339\n",
      "I0412 18:21:23.446885 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 4: 23000, training batch loss = 3.746599, running_avg_loss loss = 5.164303, validation loss = 4.354339\n",
      "2020-04-12 18:21:23 - Transformer_no_pretrain - INFO: - epoch 4: 23000, running_avg_reward = 0.000000\n",
      "I0412 18:21:23.448883 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 4: 23000, running_avg_reward = 0.000000\n",
      "2020-04-12 18:21:24 - Transformer_no_pretrain - INFO: - epoch 4: 23000, train_rouge_l_f = 0.106369, test_rouge_l_f = 0.177136\n",
      "I0412 18:21:24.762992 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 4: 23000, train_rouge_l_f = 0.106369, test_rouge_l_f = 0.177136\n",
      "2020-04-12 18:51:04 - Transformer_no_pretrain - INFO: - epoch 4: 23610, test_avg_acc = 0.124270, test_avg_acc = 0.126875\n",
      "I0412 18:51:04.455692 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 4: 23610, test_avg_acc = 0.124270, test_avg_acc = 0.126875\n",
      "2020-04-12 18:51:44 - Transformer_no_pretrain - INFO: - epoch 5: 24000, training batch loss = 3.870939, running_avg_loss loss = 5.151369, validation loss = 4.370090\n",
      "I0412 18:51:44.988843 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 5: 24000, training batch loss = 3.870939, running_avg_loss loss = 5.151369, validation loss = 4.370090\n",
      "2020-04-12 18:51:44 - Transformer_no_pretrain - INFO: - epoch 5: 24000, running_avg_reward = 0.000000\n",
      "I0412 18:51:44.990193 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 5: 24000, running_avg_reward = 0.000000\n",
      "2020-04-12 18:51:47 - Transformer_no_pretrain - INFO: - epoch 5: 24000, train_rouge_l_f = 0.145036, test_rouge_l_f = 0.099751\n",
      "I0412 18:51:47.644150 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 5: 24000, train_rouge_l_f = 0.145036, test_rouge_l_f = 0.099751\n",
      "2020-04-12 18:53:20 - Transformer_no_pretrain - INFO: - epoch 5: 25000, training batch loss = 3.565928, running_avg_loss loss = 5.135515, validation loss = 4.356076\n",
      "I0412 18:53:20.480565 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 5: 25000, training batch loss = 3.565928, running_avg_loss loss = 5.135515, validation loss = 4.356076\n",
      "2020-04-12 18:53:20 - Transformer_no_pretrain - INFO: - epoch 5: 25000, running_avg_reward = 0.000000\n",
      "I0412 18:53:20.482236 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 5: 25000, running_avg_reward = 0.000000\n",
      "2020-04-12 18:53:20 - Transformer_no_pretrain - INFO: - Saving model step 25000 to model/saved_models/Transformer_no_pretrain/0025000.tar...\n",
      "I0412 18:53:20.485922 140154035328832 initialize.py:225] Saving model step 25000 to model/saved_models/Transformer_no_pretrain/0025000.tar...\n",
      "2020-04-12 18:53:28 - Transformer_no_pretrain - INFO: - epoch 5: 25000, train_rouge_l_f = 0.154457, test_rouge_l_f = 0.088617\n",
      "I0412 18:53:28.700349 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 5: 25000, train_rouge_l_f = 0.154457, test_rouge_l_f = 0.088617\n",
      "2020-04-12 18:55:01 - Transformer_no_pretrain - INFO: - epoch 5: 26000, training batch loss = 3.653753, running_avg_loss loss = 5.120697, validation loss = 4.363725\n",
      "I0412 18:55:01.676608 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 5: 26000, training batch loss = 3.653753, running_avg_loss loss = 5.120697, validation loss = 4.363725\n",
      "2020-04-12 18:55:01 - Transformer_no_pretrain - INFO: - epoch 5: 26000, running_avg_reward = 0.000000\n",
      "I0412 18:55:01.678339 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 5: 26000, running_avg_reward = 0.000000\n",
      "2020-04-12 18:55:05 - Transformer_no_pretrain - INFO: - epoch 5: 26000, train_rouge_l_f = 0.137855, test_rouge_l_f = 0.133126\n",
      "I0412 18:55:05.703693 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 5: 26000, train_rouge_l_f = 0.137855, test_rouge_l_f = 0.133126\n",
      "2020-04-12 18:56:37 - Transformer_no_pretrain - INFO: - epoch 5: 27000, training batch loss = 3.588323, running_avg_loss loss = 5.105373, validation loss = 4.354550\n",
      "I0412 18:56:37.820733 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 5: 27000, training batch loss = 3.588323, running_avg_loss loss = 5.105373, validation loss = 4.354550\n",
      "2020-04-12 18:56:37 - Transformer_no_pretrain - INFO: - epoch 5: 27000, running_avg_reward = 0.000000\n",
      "I0412 18:56:37.822407 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 5: 27000, running_avg_reward = 0.000000\n",
      "2020-04-12 18:56:41 - Transformer_no_pretrain - INFO: - epoch 5: 27000, train_rouge_l_f = 0.126592, test_rouge_l_f = 0.195069\n",
      "I0412 18:56:41.457874 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 5: 27000, train_rouge_l_f = 0.126592, test_rouge_l_f = 0.195069\n",
      "2020-04-12 18:58:14 - Transformer_no_pretrain - INFO: - epoch 5: 28000, training batch loss = 3.907351, running_avg_loss loss = 5.093393, validation loss = 4.338991\n",
      "I0412 18:58:14.514660 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 5: 28000, training batch loss = 3.907351, running_avg_loss loss = 5.093393, validation loss = 4.338991\n",
      "2020-04-12 18:58:14 - Transformer_no_pretrain - INFO: - epoch 5: 28000, running_avg_reward = 0.000000\n",
      "I0412 18:58:14.515636 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 5: 28000, running_avg_reward = 0.000000\n",
      "2020-04-12 18:58:18 - Transformer_no_pretrain - INFO: - epoch 5: 28000, train_rouge_l_f = 0.081990, test_rouge_l_f = 0.148492\n",
      "I0412 18:58:18.913527 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 5: 28000, train_rouge_l_f = 0.081990, test_rouge_l_f = 0.148492\n",
      "2020-04-12 19:29:26 - Transformer_no_pretrain - INFO: - epoch 5: 28332, test_avg_acc = 0.116067, test_avg_acc = 0.117453\n",
      "I0412 19:29:26.048048 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 5: 28332, test_avg_acc = 0.116067, test_avg_acc = 0.117453\n",
      "2020-04-12 19:30:30 - Transformer_no_pretrain - INFO: - epoch 6: 29000, training batch loss = 3.383467, running_avg_loss loss = 5.076294, validation loss = 4.349447\n",
      "I0412 19:30:30.779839 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 6: 29000, training batch loss = 3.383467, running_avg_loss loss = 5.076294, validation loss = 4.349447\n",
      "2020-04-12 19:30:30 - Transformer_no_pretrain - INFO: - epoch 6: 29000, running_avg_reward = 0.000000\n",
      "I0412 19:30:30.781094 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 6: 29000, running_avg_reward = 0.000000\n",
      "2020-04-12 19:30:31 - Transformer_no_pretrain - INFO: - epoch 6: 29000, train_rouge_l_f = 0.131358, test_rouge_l_f = 0.097687\n",
      "I0412 19:30:31.707010 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 6: 29000, train_rouge_l_f = 0.131358, test_rouge_l_f = 0.097687\n",
      "2020-04-12 19:32:07 - Transformer_no_pretrain - INFO: - epoch 6: 30000, training batch loss = 4.244002, running_avg_loss loss = 5.067971, validation loss = 4.341412\n",
      "I0412 19:32:07.192451 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 6: 30000, training batch loss = 4.244002, running_avg_loss loss = 5.067971, validation loss = 4.341412\n",
      "2020-04-12 19:32:07 - Transformer_no_pretrain - INFO: - epoch 6: 30000, running_avg_reward = 0.000000\n",
      "I0412 19:32:07.194110 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 6: 30000, running_avg_reward = 0.000000\n",
      "2020-04-12 19:32:07 - Transformer_no_pretrain - INFO: - Saving model step 30000 to model/saved_models/Transformer_no_pretrain/0030000.tar...\n",
      "I0412 19:32:07.198747 140154035328832 initialize.py:225] Saving model step 30000 to model/saved_models/Transformer_no_pretrain/0030000.tar...\n",
      "2020-04-12 19:32:14 - Transformer_no_pretrain - INFO: - epoch 6: 30000, train_rouge_l_f = 0.118148, test_rouge_l_f = 0.172104\n",
      "I0412 19:32:14.148532 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 6: 30000, train_rouge_l_f = 0.118148, test_rouge_l_f = 0.172104\n",
      "2020-04-12 19:33:47 - Transformer_no_pretrain - INFO: - epoch 6: 31000, training batch loss = 4.129752, running_avg_loss loss = 5.058589, validation loss = 4.336203\n",
      "I0412 19:33:47.073434 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 6: 31000, training batch loss = 4.129752, running_avg_loss loss = 5.058589, validation loss = 4.336203\n",
      "2020-04-12 19:33:47 - Transformer_no_pretrain - INFO: - epoch 6: 31000, running_avg_reward = 0.000000\n",
      "I0412 19:33:47.075225 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 6: 31000, running_avg_reward = 0.000000\n",
      "2020-04-12 19:33:48 - Transformer_no_pretrain - INFO: - epoch 6: 31000, train_rouge_l_f = 0.105347, test_rouge_l_f = 0.185055\n",
      "I0412 19:33:48.649514 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 6: 31000, train_rouge_l_f = 0.105347, test_rouge_l_f = 0.185055\n",
      "2020-04-12 19:35:21 - Transformer_no_pretrain - INFO: - epoch 6: 32000, training batch loss = 3.987574, running_avg_loss loss = 5.047878, validation loss = 4.328848\n",
      "I0412 19:35:21.333011 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 6: 32000, training batch loss = 3.987574, running_avg_loss loss = 5.047878, validation loss = 4.328848\n",
      "2020-04-12 19:35:21 - Transformer_no_pretrain - INFO: - epoch 6: 32000, running_avg_reward = 0.000000\n",
      "I0412 19:35:21.334912 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 6: 32000, running_avg_reward = 0.000000\n",
      "2020-04-12 19:35:23 - Transformer_no_pretrain - INFO: - epoch 6: 32000, train_rouge_l_f = 0.114903, test_rouge_l_f = 0.155851\n",
      "I0412 19:35:23.857451 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 6: 32000, train_rouge_l_f = 0.114903, test_rouge_l_f = 0.155851\n",
      "2020-04-12 19:36:55 - Transformer_no_pretrain - INFO: - epoch 6: 33000, training batch loss = 3.349287, running_avg_loss loss = 5.030893, validation loss = 4.333175\n",
      "I0412 19:36:55.218446 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 6: 33000, training batch loss = 3.349287, running_avg_loss loss = 5.030893, validation loss = 4.333175\n",
      "2020-04-12 19:36:55 - Transformer_no_pretrain - INFO: - epoch 6: 33000, running_avg_reward = 0.000000\n",
      "I0412 19:36:55.220227 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 6: 33000, running_avg_reward = 0.000000\n",
      "2020-04-12 19:36:57 - Transformer_no_pretrain - INFO: - epoch 6: 33000, train_rouge_l_f = 0.165765, test_rouge_l_f = 0.097025\n",
      "I0412 19:36:57.591651 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 6: 33000, train_rouge_l_f = 0.165765, test_rouge_l_f = 0.097025\n",
      "2020-04-12 20:55:42 - Transformer_no_pretrain - INFO: - epoch 6: 33054, test_avg_acc = 0.112670, test_avg_acc = 0.114217\n",
      "I0412 20:55:42.884452 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 6: 33054, test_avg_acc = 0.112670, test_avg_acc = 0.114217\n",
      "2020-04-12 20:57:11 - Transformer_no_pretrain - INFO: - epoch 7: 34000, training batch loss = 3.738265, running_avg_loss loss = 5.017966, validation loss = 4.348744\n",
      "I0412 20:57:11.777536 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 7: 34000, training batch loss = 3.738265, running_avg_loss loss = 5.017966, validation loss = 4.348744\n",
      "2020-04-12 20:57:11 - Transformer_no_pretrain - INFO: - epoch 7: 34000, running_avg_reward = 0.000000\n",
      "I0412 20:57:11.779236 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 7: 34000, running_avg_reward = 0.000000\n",
      "2020-04-12 20:57:15 - Transformer_no_pretrain - INFO: - epoch 7: 34000, train_rouge_l_f = 0.149921, test_rouge_l_f = 0.149452\n",
      "I0412 20:57:15.005544 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 7: 34000, train_rouge_l_f = 0.149921, test_rouge_l_f = 0.149452\n",
      "2020-04-12 20:58:46 - Transformer_no_pretrain - INFO: - epoch 7: 35000, training batch loss = 3.513888, running_avg_loss loss = 5.002925, validation loss = 4.335860\n",
      "I0412 20:58:46.677837 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 7: 35000, training batch loss = 3.513888, running_avg_loss loss = 5.002925, validation loss = 4.335860\n",
      "2020-04-12 20:58:46 - Transformer_no_pretrain - INFO: - epoch 7: 35000, running_avg_reward = 0.000000\n",
      "I0412 20:58:46.679779 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 7: 35000, running_avg_reward = 0.000000\n",
      "2020-04-12 20:58:46 - Transformer_no_pretrain - INFO: - Saving model step 35000 to model/saved_models/Transformer_no_pretrain/0035000.tar...\n",
      "I0412 20:58:46.685331 140154035328832 initialize.py:225] Saving model step 35000 to model/saved_models/Transformer_no_pretrain/0035000.tar...\n",
      "2020-04-12 20:58:52 - Transformer_no_pretrain - INFO: - epoch 7: 35000, train_rouge_l_f = 0.070220, test_rouge_l_f = 0.039169\n",
      "I0412 20:58:52.610861 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 7: 35000, train_rouge_l_f = 0.070220, test_rouge_l_f = 0.039169\n",
      "2020-04-12 21:00:25 - Transformer_no_pretrain - INFO: - epoch 7: 36000, training batch loss = 4.293658, running_avg_loss loss = 4.995833, validation loss = 4.328663\n",
      "I0412 21:00:25.994499 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 7: 36000, training batch loss = 4.293658, running_avg_loss loss = 4.995833, validation loss = 4.328663\n",
      "2020-04-12 21:00:25 - Transformer_no_pretrain - INFO: - epoch 7: 36000, running_avg_reward = 0.000000\n",
      "I0412 21:00:25.996238 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 7: 36000, running_avg_reward = 0.000000\n",
      "2020-04-12 21:00:26 - Transformer_no_pretrain - INFO: - epoch 7: 36000, train_rouge_l_f = 0.039531, test_rouge_l_f = 0.042979\n",
      "I0412 21:00:26.702021 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 7: 36000, train_rouge_l_f = 0.039531, test_rouge_l_f = 0.042979\n",
      "2020-04-12 21:01:59 - Transformer_no_pretrain - INFO: - epoch 7: 37000, training batch loss = 4.177295, running_avg_loss loss = 4.987647, validation loss = 4.313342\n",
      "I0412 21:01:59.423311 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 7: 37000, training batch loss = 4.177295, running_avg_loss loss = 4.987647, validation loss = 4.313342\n",
      "2020-04-12 21:01:59 - Transformer_no_pretrain - INFO: - epoch 7: 37000, running_avg_reward = 0.000000\n",
      "I0412 21:01:59.424977 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 7: 37000, running_avg_reward = 0.000000\n",
      "2020-04-12 21:02:02 - Transformer_no_pretrain - INFO: - epoch 7: 37000, train_rouge_l_f = 0.112082, test_rouge_l_f = 0.176870\n",
      "I0412 21:02:02.176878 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 7: 37000, train_rouge_l_f = 0.112082, test_rouge_l_f = 0.176870\n",
      "2020-04-12 21:42:23 - Transformer_no_pretrain - INFO: - epoch 7: 37776, test_avg_acc = 0.123901, test_avg_acc = 0.124405\n",
      "I0412 21:42:23.663141 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 7: 37776, test_avg_acc = 0.123901, test_avg_acc = 0.124405\n",
      "2020-04-12 21:42:53 - Transformer_no_pretrain - INFO: - epoch 8: 38000, training batch loss = 3.668696, running_avg_loss loss = 4.974458, validation loss = 4.320114\n",
      "I0412 21:42:53.598932 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 8: 38000, training batch loss = 3.668696, running_avg_loss loss = 4.974458, validation loss = 4.320114\n",
      "2020-04-12 21:42:53 - Transformer_no_pretrain - INFO: - epoch 8: 38000, running_avg_reward = 0.000000\n",
      "I0412 21:42:53.600605 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 8: 38000, running_avg_reward = 0.000000\n",
      "2020-04-12 21:42:54 - Transformer_no_pretrain - INFO: - epoch 8: 38000, train_rouge_l_f = 0.060197, test_rouge_l_f = 0.050538\n",
      "I0412 21:42:54.893601 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 8: 38000, train_rouge_l_f = 0.060197, test_rouge_l_f = 0.050538\n",
      "2020-04-12 21:44:28 - Transformer_no_pretrain - INFO: - epoch 8: 39000, training batch loss = 3.827315, running_avg_loss loss = 4.962986, validation loss = 4.327366\n",
      "I0412 21:44:28.948201 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 8: 39000, training batch loss = 3.827315, running_avg_loss loss = 4.962986, validation loss = 4.327366\n",
      "2020-04-12 21:44:28 - Transformer_no_pretrain - INFO: - epoch 8: 39000, running_avg_reward = 0.000000\n",
      "I0412 21:44:28.949388 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 8: 39000, running_avg_reward = 0.000000\n",
      "2020-04-12 21:44:32 - Transformer_no_pretrain - INFO: - epoch 8: 39000, train_rouge_l_f = 0.067725, test_rouge_l_f = 0.120578\n",
      "I0412 21:44:32.141776 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 8: 39000, train_rouge_l_f = 0.067725, test_rouge_l_f = 0.120578\n",
      "2020-04-12 21:46:05 - Transformer_no_pretrain - INFO: - epoch 8: 40000, training batch loss = 4.135248, running_avg_loss loss = 4.954709, validation loss = 4.322765\n",
      "I0412 21:46:05.912409 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 8: 40000, training batch loss = 4.135248, running_avg_loss loss = 4.954709, validation loss = 4.322765\n",
      "2020-04-12 21:46:05 - Transformer_no_pretrain - INFO: - epoch 8: 40000, running_avg_reward = 0.000000\n",
      "I0412 21:46:05.913690 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 8: 40000, running_avg_reward = 0.000000\n",
      "2020-04-12 21:46:05 - Transformer_no_pretrain - INFO: - Saving model step 40000 to model/saved_models/Transformer_no_pretrain/0040000.tar...\n",
      "I0412 21:46:05.915703 140154035328832 initialize.py:225] Saving model step 40000 to model/saved_models/Transformer_no_pretrain/0040000.tar...\n",
      "2020-04-12 21:46:13 - Transformer_no_pretrain - INFO: - epoch 8: 40000, train_rouge_l_f = 0.066509, test_rouge_l_f = 0.128464\n",
      "I0412 21:46:13.002970 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 8: 40000, train_rouge_l_f = 0.066509, test_rouge_l_f = 0.128464\n",
      "2020-04-12 21:47:47 - Transformer_no_pretrain - INFO: - epoch 8: 41000, training batch loss = 4.110034, running_avg_loss loss = 4.946262, validation loss = 4.315927\n",
      "I0412 21:47:47.240228 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 8: 41000, training batch loss = 4.110034, running_avg_loss loss = 4.946262, validation loss = 4.315927\n",
      "2020-04-12 21:47:47 - Transformer_no_pretrain - INFO: - epoch 8: 41000, running_avg_reward = 0.000000\n",
      "I0412 21:47:47.241603 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 8: 41000, running_avg_reward = 0.000000\n",
      "2020-04-12 21:47:48 - Transformer_no_pretrain - INFO: - epoch 8: 41000, train_rouge_l_f = 0.101092, test_rouge_l_f = 0.108719\n",
      "I0412 21:47:48.492643 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 8: 41000, train_rouge_l_f = 0.101092, test_rouge_l_f = 0.108719\n",
      "2020-04-12 21:49:21 - Transformer_no_pretrain - INFO: - epoch 8: 42000, training batch loss = 2.828444, running_avg_loss loss = 4.925084, validation loss = 4.306463\n",
      "I0412 21:49:21.156298 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 8: 42000, training batch loss = 2.828444, running_avg_loss loss = 4.925084, validation loss = 4.306463\n",
      "2020-04-12 21:49:21 - Transformer_no_pretrain - INFO: - epoch 8: 42000, running_avg_reward = 0.000000\n",
      "I0412 21:49:21.157939 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 8: 42000, running_avg_reward = 0.000000\n",
      "2020-04-12 21:49:25 - Transformer_no_pretrain - INFO: - epoch 8: 42000, train_rouge_l_f = 0.147900, test_rouge_l_f = 0.144883\n",
      "I0412 21:49:25.657893 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 8: 42000, train_rouge_l_f = 0.147900, test_rouge_l_f = 0.144883\n",
      "2020-04-12 23:38:00 - Transformer_no_pretrain - INFO: - epoch 8: 42498, test_avg_acc = 0.104277, test_avg_acc = 0.105125\n",
      "I0412 23:38:00.588743 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 8: 42498, test_avg_acc = 0.104277, test_avg_acc = 0.105125\n",
      "2020-04-12 23:38:51 - Transformer_no_pretrain - INFO: - epoch 9: 43000, training batch loss = 3.433660, running_avg_loss loss = 4.910170, validation loss = 4.316330\n",
      "I0412 23:38:51.811046 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 9: 43000, training batch loss = 3.433660, running_avg_loss loss = 4.910170, validation loss = 4.316330\n",
      "2020-04-12 23:38:51 - Transformer_no_pretrain - INFO: - epoch 9: 43000, running_avg_reward = 0.000000\n",
      "I0412 23:38:51.812003 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 9: 43000, running_avg_reward = 0.000000\n",
      "2020-04-12 23:38:55 - Transformer_no_pretrain - INFO: - epoch 9: 43000, train_rouge_l_f = 0.095063, test_rouge_l_f = 0.123642\n",
      "I0412 23:38:55.421823 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 9: 43000, train_rouge_l_f = 0.095063, test_rouge_l_f = 0.123642\n",
      "2020-04-12 23:40:24 - Transformer_no_pretrain - INFO: - epoch 9: 44000, training batch loss = 3.287539, running_avg_loss loss = 4.893944, validation loss = 4.311584\n",
      "I0412 23:40:24.638069 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 9: 44000, training batch loss = 3.287539, running_avg_loss loss = 4.893944, validation loss = 4.311584\n",
      "2020-04-12 23:40:24 - Transformer_no_pretrain - INFO: - epoch 9: 44000, running_avg_reward = 0.000000\n",
      "I0412 23:40:24.639737 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 9: 44000, running_avg_reward = 0.000000\n",
      "2020-04-12 23:40:26 - Transformer_no_pretrain - INFO: - epoch 9: 44000, train_rouge_l_f = 0.197059, test_rouge_l_f = 0.114403\n",
      "I0412 23:40:26.775822 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 9: 44000, train_rouge_l_f = 0.197059, test_rouge_l_f = 0.114403\n",
      "2020-04-12 23:41:58 - Transformer_no_pretrain - INFO: - epoch 9: 45000, training batch loss = 3.388048, running_avg_loss loss = 4.878885, validation loss = 4.316305\n",
      "I0412 23:41:58.425825 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 9: 45000, training batch loss = 3.388048, running_avg_loss loss = 4.878885, validation loss = 4.316305\n",
      "2020-04-12 23:41:58 - Transformer_no_pretrain - INFO: - epoch 9: 45000, running_avg_reward = 0.000000\n",
      "I0412 23:41:58.427042 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 9: 45000, running_avg_reward = 0.000000\n",
      "2020-04-12 23:41:58 - Transformer_no_pretrain - INFO: - Saving model step 45000 to model/saved_models/Transformer_no_pretrain/0045000.tar...\n",
      "I0412 23:41:58.428656 140154035328832 initialize.py:225] Saving model step 45000 to model/saved_models/Transformer_no_pretrain/0045000.tar...\n",
      "2020-04-12 23:42:05 - Transformer_no_pretrain - INFO: - epoch 9: 45000, train_rouge_l_f = 0.130662, test_rouge_l_f = 0.110446\n",
      "I0412 23:42:05.294295 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 9: 45000, train_rouge_l_f = 0.130662, test_rouge_l_f = 0.110446\n",
      "2020-04-12 23:43:37 - Transformer_no_pretrain - INFO: - epoch 9: 46000, training batch loss = 3.580969, running_avg_loss loss = 4.865905, validation loss = 4.299731\n",
      "I0412 23:43:37.091907 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 9: 46000, training batch loss = 3.580969, running_avg_loss loss = 4.865905, validation loss = 4.299731\n",
      "2020-04-12 23:43:37 - Transformer_no_pretrain - INFO: - epoch 9: 46000, running_avg_reward = 0.000000\n",
      "I0412 23:43:37.093562 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 9: 46000, running_avg_reward = 0.000000\n",
      "2020-04-12 23:43:41 - Transformer_no_pretrain - INFO: - epoch 9: 46000, train_rouge_l_f = 0.101921, test_rouge_l_f = 0.121717\n",
      "I0412 23:43:41.823477 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 9: 46000, train_rouge_l_f = 0.101921, test_rouge_l_f = 0.121717\n",
      "2020-04-12 23:45:14 - Transformer_no_pretrain - INFO: - epoch 9: 47000, training batch loss = 3.555894, running_avg_loss loss = 4.852805, validation loss = 4.277803\n",
      "I0412 23:45:14.063763 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 9: 47000, training batch loss = 3.555894, running_avg_loss loss = 4.852805, validation loss = 4.277803\n",
      "2020-04-12 23:45:14 - Transformer_no_pretrain - INFO: - epoch 9: 47000, running_avg_reward = 0.000000\n",
      "I0412 23:45:14.064668 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 9: 47000, running_avg_reward = 0.000000\n",
      "2020-04-12 23:45:18 - Transformer_no_pretrain - INFO: - epoch 9: 47000, train_rouge_l_f = 0.183310, test_rouge_l_f = 0.138471\n",
      "I0412 23:45:18.338116 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 9: 47000, train_rouge_l_f = 0.183310, test_rouge_l_f = 0.138471\n",
      "2020-04-13 02:27:18 - Transformer_no_pretrain - INFO: - epoch 9: 47220, test_avg_acc = 0.141593, test_avg_acc = 0.142971\n",
      "I0413 02:27:18.766055 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 9: 47220, test_avg_acc = 0.141593, test_avg_acc = 0.142971\n",
      "2020-04-13 02:28:32 - Transformer_no_pretrain - INFO: - epoch 10: 48000, training batch loss = 3.501170, running_avg_loss loss = 4.839289, validation loss = 4.293194\n",
      "I0413 02:28:32.873674 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 10: 48000, training batch loss = 3.501170, running_avg_loss loss = 4.839289, validation loss = 4.293194\n",
      "2020-04-13 02:28:32 - Transformer_no_pretrain - INFO: - epoch 10: 48000, running_avg_reward = 0.000000\n",
      "I0413 02:28:32.875357 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 10: 48000, running_avg_reward = 0.000000\n",
      "2020-04-13 02:28:36 - Transformer_no_pretrain - INFO: - epoch 10: 48000, train_rouge_l_f = 0.086550, test_rouge_l_f = 0.114692\n",
      "I0413 02:28:36.431050 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 10: 48000, train_rouge_l_f = 0.086550, test_rouge_l_f = 0.114692\n",
      "2020-04-13 02:30:09 - Transformer_no_pretrain - INFO: - epoch 10: 49000, training batch loss = 3.529242, running_avg_loss loss = 4.826189, validation loss = 4.298144\n",
      "I0413 02:30:09.114349 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 10: 49000, training batch loss = 3.529242, running_avg_loss loss = 4.826189, validation loss = 4.298144\n",
      "2020-04-13 02:30:09 - Transformer_no_pretrain - INFO: - epoch 10: 49000, running_avg_reward = 0.000000\n",
      "I0413 02:30:09.116157 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 10: 49000, running_avg_reward = 0.000000\n",
      "2020-04-13 02:30:10 - Transformer_no_pretrain - INFO: - epoch 10: 49000, train_rouge_l_f = 0.032242, test_rouge_l_f = 0.015625\n",
      "I0413 02:30:10.772405 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 10: 49000, train_rouge_l_f = 0.032242, test_rouge_l_f = 0.015625\n",
      "2020-04-13 02:31:43 - Transformer_no_pretrain - INFO: - epoch 10: 50000, training batch loss = 3.446373, running_avg_loss loss = 4.812390, validation loss = 4.282207\n",
      "I0413 02:31:43.315903 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 10: 50000, training batch loss = 3.446373, running_avg_loss loss = 4.812390, validation loss = 4.282207\n",
      "2020-04-13 02:31:43 - Transformer_no_pretrain - INFO: - epoch 10: 50000, running_avg_reward = 0.000000\n",
      "I0413 02:31:43.317547 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 10: 50000, running_avg_reward = 0.000000\n",
      "2020-04-13 02:31:43 - Transformer_no_pretrain - INFO: - Saving model step 50000 to model/saved_models/Transformer_no_pretrain/0050000.tar...\n",
      "I0413 02:31:43.320164 140154035328832 initialize.py:225] Saving model step 50000 to model/saved_models/Transformer_no_pretrain/0050000.tar...\n",
      "2020-04-13 02:31:51 - Transformer_no_pretrain - INFO: - epoch 10: 50000, train_rouge_l_f = 0.108976, test_rouge_l_f = 0.174390\n",
      "I0413 02:31:51.741811 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 10: 50000, train_rouge_l_f = 0.108976, test_rouge_l_f = 0.174390\n",
      "2020-04-13 02:33:21 - Transformer_no_pretrain - INFO: - epoch 10: 51000, training batch loss = 3.871025, running_avg_loss loss = 4.802977, validation loss = 4.266942\n",
      "I0413 02:33:21.784563 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 10: 51000, training batch loss = 3.871025, running_avg_loss loss = 4.802977, validation loss = 4.266942\n",
      "2020-04-13 02:33:21 - Transformer_no_pretrain - INFO: - epoch 10: 51000, running_avg_reward = 0.000000\n",
      "I0413 02:33:21.786214 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 10: 51000, running_avg_reward = 0.000000\n",
      "2020-04-13 02:33:26 - Transformer_no_pretrain - INFO: - epoch 10: 51000, train_rouge_l_f = 0.172465, test_rouge_l_f = 0.048214\n",
      "I0413 02:33:26.355387 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 10: 51000, train_rouge_l_f = 0.172465, test_rouge_l_f = 0.048214\n",
      "2020-04-13 04:24:54 - Transformer_no_pretrain - INFO: - epoch 10: 51942, test_avg_acc = 0.111372, test_avg_acc = 0.112774\n",
      "I0413 04:24:54.842863 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 10: 51942, test_avg_acc = 0.111372, test_avg_acc = 0.112774\n",
      "2020-04-13 04:25:10 - Transformer_no_pretrain - INFO: - epoch 11: 52000, training batch loss = 3.022538, running_avg_loss loss = 4.785172, validation loss = 4.254268\n",
      "I0413 04:25:10.125821 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 11: 52000, training batch loss = 3.022538, running_avg_loss loss = 4.785172, validation loss = 4.254268\n",
      "2020-04-13 04:25:10 - Transformer_no_pretrain - INFO: - epoch 11: 52000, running_avg_reward = 0.000000\n",
      "I0413 04:25:10.127315 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 11: 52000, running_avg_reward = 0.000000\n",
      "2020-04-13 04:25:13 - Transformer_no_pretrain - INFO: - epoch 11: 52000, train_rouge_l_f = 0.039728, test_rouge_l_f = 0.044394\n",
      "I0413 04:25:13.783291 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 11: 52000, train_rouge_l_f = 0.039728, test_rouge_l_f = 0.044394\n",
      "2020-04-13 04:26:44 - Transformer_no_pretrain - INFO: - epoch 11: 53000, training batch loss = 2.997087, running_avg_loss loss = 4.767292, validation loss = 4.273868\n",
      "I0413 04:26:44.856992 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 11: 53000, training batch loss = 2.997087, running_avg_loss loss = 4.767292, validation loss = 4.273868\n",
      "2020-04-13 04:26:44 - Transformer_no_pretrain - INFO: - epoch 11: 53000, running_avg_reward = 0.000000\n",
      "I0413 04:26:44.857923 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 11: 53000, running_avg_reward = 0.000000\n",
      "2020-04-13 04:26:48 - Transformer_no_pretrain - INFO: - epoch 11: 53000, train_rouge_l_f = 0.145491, test_rouge_l_f = 0.091239\n",
      "I0413 04:26:48.548238 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 11: 53000, train_rouge_l_f = 0.145491, test_rouge_l_f = 0.091239\n",
      "2020-04-13 04:28:18 - Transformer_no_pretrain - INFO: - epoch 11: 54000, training batch loss = 3.316897, running_avg_loss loss = 4.752788, validation loss = 4.264223\n",
      "I0413 04:28:18.603800 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 11: 54000, training batch loss = 3.316897, running_avg_loss loss = 4.752788, validation loss = 4.264223\n",
      "2020-04-13 04:28:18 - Transformer_no_pretrain - INFO: - epoch 11: 54000, running_avg_reward = 0.000000\n",
      "I0413 04:28:18.605872 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 11: 54000, running_avg_reward = 0.000000\n",
      "2020-04-13 04:28:22 - Transformer_no_pretrain - INFO: - epoch 11: 54000, train_rouge_l_f = 0.096021, test_rouge_l_f = 0.155379\n",
      "I0413 04:28:22.287497 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 11: 54000, train_rouge_l_f = 0.096021, test_rouge_l_f = 0.155379\n",
      "2020-04-13 04:29:52 - Transformer_no_pretrain - INFO: - epoch 11: 55000, training batch loss = 3.712820, running_avg_loss loss = 4.742388, validation loss = 4.252774\n",
      "I0413 04:29:52.123095 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 11: 55000, training batch loss = 3.712820, running_avg_loss loss = 4.742388, validation loss = 4.252774\n",
      "2020-04-13 04:29:52 - Transformer_no_pretrain - INFO: - epoch 11: 55000, running_avg_reward = 0.000000\n",
      "I0413 04:29:52.124848 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 11: 55000, running_avg_reward = 0.000000\n",
      "2020-04-13 04:29:52 - Transformer_no_pretrain - INFO: - Saving model step 55000 to model/saved_models/Transformer_no_pretrain/0055000.tar...\n",
      "I0413 04:29:52.127874 140154035328832 initialize.py:225] Saving model step 55000 to model/saved_models/Transformer_no_pretrain/0055000.tar...\n",
      "2020-04-13 04:29:56 - Transformer_no_pretrain - INFO: - epoch 11: 55000, train_rouge_l_f = 0.032124, test_rouge_l_f = 0.094913\n",
      "I0413 04:29:56.068185 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 11: 55000, train_rouge_l_f = 0.032124, test_rouge_l_f = 0.094913\n",
      "2020-04-13 04:31:27 - Transformer_no_pretrain - INFO: - epoch 11: 56000, training batch loss = 3.472217, running_avg_loss loss = 4.729686, validation loss = 4.236817\n",
      "I0413 04:31:27.945892 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 11: 56000, training batch loss = 3.472217, running_avg_loss loss = 4.729686, validation loss = 4.236817\n",
      "2020-04-13 04:31:27 - Transformer_no_pretrain - INFO: - epoch 11: 56000, running_avg_reward = 0.000000\n",
      "I0413 04:31:27.947233 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 11: 56000, running_avg_reward = 0.000000\n",
      "2020-04-13 04:31:32 - Transformer_no_pretrain - INFO: - epoch 11: 56000, train_rouge_l_f = 0.027116, test_rouge_l_f = 0.023173\n",
      "I0413 04:31:32.434133 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 11: 56000, train_rouge_l_f = 0.027116, test_rouge_l_f = 0.023173\n",
      "2020-04-13 06:30:07 - Transformer_no_pretrain - INFO: - epoch 11: 56664, test_avg_acc = 0.054436, test_avg_acc = 0.054345\n",
      "I0413 06:30:07.711739 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 11: 56664, test_avg_acc = 0.054436, test_avg_acc = 0.054345\n",
      "2020-04-13 06:30:45 - Transformer_no_pretrain - INFO: - epoch 12: 57000, training batch loss = 3.645405, running_avg_loss loss = 4.718843, validation loss = 4.241417\n",
      "I0413 06:30:45.981442 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 12: 57000, training batch loss = 3.645405, running_avg_loss loss = 4.718843, validation loss = 4.241417\n",
      "2020-04-13 06:30:45 - Transformer_no_pretrain - INFO: - epoch 12: 57000, running_avg_reward = 0.000000\n",
      "I0413 06:30:45.983124 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 12: 57000, running_avg_reward = 0.000000\n",
      "2020-04-13 06:30:49 - Transformer_no_pretrain - INFO: - epoch 12: 57000, train_rouge_l_f = 0.104938, test_rouge_l_f = 0.108847\n",
      "I0413 06:30:49.393665 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 12: 57000, train_rouge_l_f = 0.104938, test_rouge_l_f = 0.108847\n",
      "2020-04-13 06:32:21 - Transformer_no_pretrain - INFO: - epoch 12: 58000, training batch loss = 3.062295, running_avg_loss loss = 4.702278, validation loss = 4.242490\n",
      "I0413 06:32:21.658506 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 12: 58000, training batch loss = 3.062295, running_avg_loss loss = 4.702278, validation loss = 4.242490\n",
      "2020-04-13 06:32:21 - Transformer_no_pretrain - INFO: - epoch 12: 58000, running_avg_reward = 0.000000\n",
      "I0413 06:32:21.660316 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 12: 58000, running_avg_reward = 0.000000\n",
      "2020-04-13 06:32:26 - Transformer_no_pretrain - INFO: - epoch 12: 58000, train_rouge_l_f = 0.154151, test_rouge_l_f = 0.103106\n",
      "I0413 06:32:26.826045 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 12: 58000, train_rouge_l_f = 0.154151, test_rouge_l_f = 0.103106\n",
      "2020-04-13 06:34:01 - Transformer_no_pretrain - INFO: - epoch 12: 59000, training batch loss = 2.903434, running_avg_loss loss = 4.684289, validation loss = 4.244156\n",
      "I0413 06:34:01.261895 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 12: 59000, training batch loss = 2.903434, running_avg_loss loss = 4.684289, validation loss = 4.244156\n",
      "2020-04-13 06:34:01 - Transformer_no_pretrain - INFO: - epoch 12: 59000, running_avg_reward = 0.000000\n",
      "I0413 06:34:01.263627 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 12: 59000, running_avg_reward = 0.000000\n",
      "2020-04-13 06:34:06 - Transformer_no_pretrain - INFO: - epoch 12: 59000, train_rouge_l_f = 0.128266, test_rouge_l_f = 0.157434\n",
      "I0413 06:34:06.100404 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 12: 59000, train_rouge_l_f = 0.128266, test_rouge_l_f = 0.157434\n",
      "2020-04-13 06:35:34 - Transformer_no_pretrain - INFO: - epoch 12: 60000, training batch loss = 3.625015, running_avg_loss loss = 4.673697, validation loss = 4.229227\n",
      "I0413 06:35:34.034400 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 12: 60000, training batch loss = 3.625015, running_avg_loss loss = 4.673697, validation loss = 4.229227\n",
      "2020-04-13 06:35:34 - Transformer_no_pretrain - INFO: - epoch 12: 60000, running_avg_reward = 0.000000\n",
      "I0413 06:35:34.036044 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 12: 60000, running_avg_reward = 0.000000\n",
      "2020-04-13 06:35:34 - Transformer_no_pretrain - INFO: - Saving model step 60000 to model/saved_models/Transformer_no_pretrain/0060000.tar...\n",
      "I0413 06:35:34.038325 140154035328832 initialize.py:225] Saving model step 60000 to model/saved_models/Transformer_no_pretrain/0060000.tar...\n",
      "2020-04-13 06:35:39 - Transformer_no_pretrain - INFO: - epoch 12: 60000, train_rouge_l_f = 0.096440, test_rouge_l_f = 0.148215\n",
      "I0413 06:35:39.896494 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 12: 60000, train_rouge_l_f = 0.096440, test_rouge_l_f = 0.148215\n",
      "2020-04-13 06:37:11 - Transformer_no_pretrain - INFO: - epoch 12: 61000, training batch loss = 3.417709, running_avg_loss loss = 4.661137, validation loss = 4.214506\n",
      "I0413 06:37:11.459866 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 12: 61000, training batch loss = 3.417709, running_avg_loss loss = 4.661137, validation loss = 4.214506\n",
      "2020-04-13 06:37:11 - Transformer_no_pretrain - INFO: - epoch 12: 61000, running_avg_reward = 0.000000\n",
      "I0413 06:37:11.461952 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 12: 61000, running_avg_reward = 0.000000\n",
      "2020-04-13 06:37:16 - Transformer_no_pretrain - INFO: - epoch 12: 61000, train_rouge_l_f = 0.150868, test_rouge_l_f = 0.109990\n",
      "I0413 06:37:16.998424 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 12: 61000, train_rouge_l_f = 0.150868, test_rouge_l_f = 0.109990\n",
      "2020-04-13 09:29:03 - Transformer_no_pretrain - INFO: - epoch 12: 61386, test_avg_acc = 0.033836, test_avg_acc = 0.034292\n",
      "I0413 09:29:03.799441 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 12: 61386, test_avg_acc = 0.033836, test_avg_acc = 0.034292\n",
      "2020-04-13 09:30:04 - Transformer_no_pretrain - INFO: - epoch 13: 62000, training batch loss = 3.682946, running_avg_loss loss = 4.651355, validation loss = 4.223827\n",
      "I0413 09:30:04.520293 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 13: 62000, training batch loss = 3.682946, running_avg_loss loss = 4.651355, validation loss = 4.223827\n",
      "2020-04-13 09:30:04 - Transformer_no_pretrain - INFO: - epoch 13: 62000, running_avg_reward = 0.000000\n",
      "I0413 09:30:04.522050 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 13: 62000, running_avg_reward = 0.000000\n",
      "2020-04-13 09:30:09 - Transformer_no_pretrain - INFO: - epoch 13: 62000, train_rouge_l_f = 0.019595, test_rouge_l_f = 0.000000\n",
      "I0413 09:30:09.336055 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 13: 62000, train_rouge_l_f = 0.019595, test_rouge_l_f = 0.000000\n",
      "2020-04-13 09:31:41 - Transformer_no_pretrain - INFO: - epoch 13: 63000, training batch loss = 3.137824, running_avg_loss loss = 4.636220, validation loss = 4.211288\n",
      "I0413 09:31:41.033760 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 13: 63000, training batch loss = 3.137824, running_avg_loss loss = 4.636220, validation loss = 4.211288\n",
      "2020-04-13 09:31:41 - Transformer_no_pretrain - INFO: - epoch 13: 63000, running_avg_reward = 0.000000\n",
      "I0413 09:31:41.035095 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 13: 63000, running_avg_reward = 0.000000\n",
      "2020-04-13 09:31:46 - Transformer_no_pretrain - INFO: - epoch 13: 63000, train_rouge_l_f = 0.029212, test_rouge_l_f = 0.115012\n",
      "I0413 09:31:46.693310 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 13: 63000, train_rouge_l_f = 0.029212, test_rouge_l_f = 0.115012\n",
      "2020-04-13 09:33:20 - Transformer_no_pretrain - INFO: - epoch 13: 64000, training batch loss = 3.613394, running_avg_loss loss = 4.625991, validation loss = 4.197793\n",
      "I0413 09:33:20.253988 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 13: 64000, training batch loss = 3.613394, running_avg_loss loss = 4.625991, validation loss = 4.197793\n",
      "2020-04-13 09:33:20 - Transformer_no_pretrain - INFO: - epoch 13: 64000, running_avg_reward = 0.000000\n",
      "I0413 09:33:20.255858 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 13: 64000, running_avg_reward = 0.000000\n",
      "2020-04-13 09:33:24 - Transformer_no_pretrain - INFO: - epoch 13: 64000, train_rouge_l_f = 0.059722, test_rouge_l_f = 0.058063\n",
      "I0413 09:33:24.970701 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 13: 64000, train_rouge_l_f = 0.059722, test_rouge_l_f = 0.058063\n",
      "2020-04-13 09:34:55 - Transformer_no_pretrain - INFO: - epoch 13: 65000, training batch loss = 3.377299, running_avg_loss loss = 4.613504, validation loss = 4.178462\n",
      "I0413 09:34:55.829818 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 13: 65000, training batch loss = 3.377299, running_avg_loss loss = 4.613504, validation loss = 4.178462\n",
      "2020-04-13 09:34:55 - Transformer_no_pretrain - INFO: - epoch 13: 65000, running_avg_reward = 0.000000\n",
      "I0413 09:34:55.831511 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 13: 65000, running_avg_reward = 0.000000\n",
      "2020-04-13 09:34:55 - Transformer_no_pretrain - INFO: - Saving model step 65000 to model/saved_models/Transformer_no_pretrain/0065000.tar...\n",
      "I0413 09:34:55.834195 140154035328832 initialize.py:225] Saving model step 65000 to model/saved_models/Transformer_no_pretrain/0065000.tar...\n",
      "2020-04-13 09:35:00 - Transformer_no_pretrain - INFO: - epoch 13: 65000, train_rouge_l_f = 0.097151, test_rouge_l_f = 0.058567\n",
      "I0413 09:35:00.805227 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 13: 65000, train_rouge_l_f = 0.097151, test_rouge_l_f = 0.058567\n",
      "2020-04-13 09:36:34 - Transformer_no_pretrain - INFO: - epoch 13: 66000, training batch loss = 3.769585, running_avg_loss loss = 4.605065, validation loss = 4.154963\n",
      "I0413 09:36:34.491775 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 13: 66000, training batch loss = 3.769585, running_avg_loss loss = 4.605065, validation loss = 4.154963\n",
      "2020-04-13 09:36:34 - Transformer_no_pretrain - INFO: - epoch 13: 66000, running_avg_reward = 0.000000\n",
      "I0413 09:36:34.493014 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 13: 66000, running_avg_reward = 0.000000\n",
      "2020-04-13 09:36:40 - Transformer_no_pretrain - INFO: - epoch 13: 66000, train_rouge_l_f = 0.013889, test_rouge_l_f = 0.083130\n",
      "I0413 09:36:40.197863 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 13: 66000, train_rouge_l_f = 0.013889, test_rouge_l_f = 0.083130\n",
      "2020-04-13 12:29:25 - Transformer_no_pretrain - INFO: - epoch 13: 66108, test_avg_acc = 0.072117, test_avg_acc = 0.071518\n",
      "I0413 12:29:25.897254 140154035328832 <ipython-input-11-bc07426dfa84>:71] epoch 13: 66108, test_avg_acc = 0.072117, test_avg_acc = 0.071518\n",
      "2020-04-13 12:30:49 - Transformer_no_pretrain - INFO: - epoch 14: 67000, training batch loss = 2.924295, running_avg_loss loss = 4.588258, validation loss = 4.159473\n",
      "I0413 12:30:49.860551 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 14: 67000, training batch loss = 2.924295, running_avg_loss loss = 4.588258, validation loss = 4.159473\n",
      "2020-04-13 12:30:49 - Transformer_no_pretrain - INFO: - epoch 14: 67000, running_avg_reward = 0.000000\n",
      "I0413 12:30:49.862277 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 14: 67000, running_avg_reward = 0.000000\n",
      "2020-04-13 12:30:54 - Transformer_no_pretrain - INFO: - epoch 14: 67000, train_rouge_l_f = 0.048304, test_rouge_l_f = 0.085530\n",
      "I0413 12:30:54.041311 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 14: 67000, train_rouge_l_f = 0.048304, test_rouge_l_f = 0.085530\n",
      "2020-04-13 12:32:26 - Transformer_no_pretrain - INFO: - epoch 14: 68000, training batch loss = 2.918651, running_avg_loss loss = 4.571561, validation loss = 4.144813\n",
      "I0413 12:32:26.997826 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 14: 68000, training batch loss = 2.918651, running_avg_loss loss = 4.571561, validation loss = 4.144813\n",
      "2020-04-13 12:32:26 - Transformer_no_pretrain - INFO: - epoch 14: 68000, running_avg_reward = 0.000000\n",
      "I0413 12:32:26.998741 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 14: 68000, running_avg_reward = 0.000000\n",
      "2020-04-13 12:32:31 - Transformer_no_pretrain - INFO: - epoch 14: 68000, train_rouge_l_f = 0.073753, test_rouge_l_f = 0.069511\n",
      "I0413 12:32:31.506512 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 14: 68000, train_rouge_l_f = 0.073753, test_rouge_l_f = 0.069511\n",
      "2020-04-13 12:34:05 - Transformer_no_pretrain - INFO: - epoch 14: 69000, training batch loss = 2.810857, running_avg_loss loss = 4.553954, validation loss = 4.127690\n",
      "I0413 12:34:05.765117 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 14: 69000, training batch loss = 2.810857, running_avg_loss loss = 4.553954, validation loss = 4.127690\n",
      "2020-04-13 12:34:05 - Transformer_no_pretrain - INFO: - epoch 14: 69000, running_avg_reward = 0.000000\n",
      "I0413 12:34:05.766855 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 14: 69000, running_avg_reward = 0.000000\n",
      "2020-04-13 12:34:10 - Transformer_no_pretrain - INFO: - epoch 14: 69000, train_rouge_l_f = 0.063779, test_rouge_l_f = 0.121361\n",
      "I0413 12:34:10.849596 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 14: 69000, train_rouge_l_f = 0.063779, test_rouge_l_f = 0.121361\n",
      "2020-04-13 12:35:46 - Transformer_no_pretrain - INFO: - epoch 14: 70000, training batch loss = 3.617516, running_avg_loss loss = 4.544590, validation loss = 4.107788\n",
      "I0413 12:35:46.282659 140154035328832 <ipython-input-11-bc07426dfa84>:41] epoch 14: 70000, training batch loss = 3.617516, running_avg_loss loss = 4.544590, validation loss = 4.107788\n",
      "2020-04-13 12:35:46 - Transformer_no_pretrain - INFO: - epoch 14: 70000, running_avg_reward = 0.000000\n",
      "I0413 12:35:46.284279 140154035328832 <ipython-input-11-bc07426dfa84>:43] epoch 14: 70000, running_avg_reward = 0.000000\n",
      "2020-04-13 12:35:46 - Transformer_no_pretrain - INFO: - Saving model step 70000 to model/saved_models/Transformer_no_pretrain/0070000.tar...\n",
      "I0413 12:35:46.286780 140154035328832 initialize.py:225] Saving model step 70000 to model/saved_models/Transformer_no_pretrain/0070000.tar...\n",
      "2020-04-13 12:35:51 - Transformer_no_pretrain - INFO: - epoch 14: 70000, train_rouge_l_f = 0.096814, test_rouge_l_f = 0.152892\n",
      "I0413 12:35:51.992819 140154035328832 <ipython-input-11-bc07426dfa84>:67] epoch 14: 70000, train_rouge_l_f = 0.096814, test_rouge_l_f = 0.152892\n"
     ]
    }
   ],
   "source": [
    "write_train_para(writer, config)\n",
    "logger.info('------Training START--------')\n",
    "running_avg_loss = 0\n",
    "sum_total_reward = 0\n",
    "step = 0\n",
    "try:\n",
    "    for epoch in range(config.max_epochs):\n",
    "        for batch in train_loader:\n",
    "            step += 1\n",
    "            mle_loss = train_one(model, config, batch)\n",
    "            if config.train_rl:\n",
    "                rl_loss, batch_reward = train_one_RL(model, config, batch)             \n",
    "                writer.add_scalars('scalar/RL_Loss',  \n",
    "                       {'rl_loss': rl_loss\n",
    "                       }, step)\n",
    "                writer.add_scalars('scalar/Reward',  \n",
    "                       {'batch_reward': batch_reward\n",
    "                       }, step)\n",
    "                \n",
    "                if step%1000 == 0 :\n",
    "                    logger.info('epoch %d: %d, RL_Loss = %f, batch_reward = %f'\n",
    "                                    % (epoch, step, rl_loss, batch_reward))\n",
    "                sum_total_reward += batch_reward\n",
    "            else:\n",
    "                rl_loss = T.FloatTensor([0]).cuda()\n",
    "            (config.mle_weight * mle_loss + config.rl_weight * rl_loss).backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "            '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "            if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "    #             clip_grad_norm_(model.parameters(), 5.0)                      \n",
    "                optimizer.step() # 根据累计的梯度更新网络参数\n",
    "                optimizer.zero_grad() # 清空过往梯度 \n",
    "\n",
    "            if step%1000 == 0 :\n",
    "                with T.autograd.no_grad():\n",
    "                    train_batch_loss = mle_loss.item()\n",
    "                    val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                    running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "                    running_avg_reward = sum_total_reward / step\n",
    "                    logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "                                % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                    writer.add_scalars('scalar/Loss',  \n",
    "                       {'train_batch_loss': train_batch_loss\n",
    "                       }, step)\n",
    "                    writer.add_scalars('scalar_avg/loss',  \n",
    "                       {'train_avg_loss': running_avg_loss,\n",
    "                        'test_avg_loss': val_avg_loss\n",
    "                       }, step)\n",
    "                    if running_avg_reward > 0:\n",
    "                        logger.info('epoch %d: %d, running_avg_reward = %f'\n",
    "                                % (epoch, step, running_avg_reward))\n",
    "                        writer.add_scalars('scalar_avg/Reward',  \n",
    "                           {'running_avg_reward': running_avg_reward\n",
    "                           }, step)\n",
    "\n",
    "            if step%5000 == 0:\n",
    "                save_model(config, logger, model, optimizer, step, vocab, running_avg_loss, \\\n",
    "                           r_loss=0, title = loggerName)\n",
    "            if step%1000 == 0 and step > 0:\n",
    "                train_rouge_l_f = decode(writer, logger, step, config, model, batch, mode = 'train') # call batch by validate_loader\n",
    "                test_rouge_l_f = decode(writer, logger, step, config, model, validate_loader, mode = 'test') # call batch by validate_loader\n",
    "\n",
    "                writer.add_scalars('scalar/Rouge-L',  \n",
    "                   {'train_rouge_l_f': train_rouge_l_f,\n",
    "                    'test_rouge_l_f': test_rouge_l_f\n",
    "                   }, step)\n",
    "                logger.info('epoch %d: %d, train_rouge_l_f = %f, test_rouge_l_f = %f'\n",
    "                                % (epoch, step, train_rouge_l_f, test_rouge_l_f))\n",
    "\n",
    "        train_avg_acc = avg_acc(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "        test_avg_acc = avg_acc(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "        logger.info('epoch %d: %d, test_avg_acc = %f, test_avg_acc = %f' % (epoch, step, train_avg_acc, test_avg_acc))\n",
    "except Excepation as e:\n",
    "        print(e)\n",
    "else:\n",
    "    logger.info(u'------Training SUCCESS--------')  \n",
    "finally:\n",
    "    logger.info(u'------Training END--------')                \n",
    "    removeLogger(logger)\n",
    "# no converge & external attention False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(2, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input.scatter_(dim, index, src)\n",
    "# 将src中数据根据index中的索引按照dim的方向填进input中\n",
    "torch.zeros(400, 50000).scatter_(1, torch.tensor([[1]\n",
    "                                           ]), x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
