{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0508 01:09:55.552205 140523512821568 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-05-08 01:09:56 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - logger已啟動\n",
      "I0508 01:09:56.468331 140523512821568 train_util.py:102] logger已啟動\n"
     ]
    }
   ],
   "source": [
    "from utils import config\n",
    "from utils.seq2seq import data\n",
    "\n",
    "from utils.seq2seq.batcher import *\n",
    "from utils.seq2seq.train_util import *\n",
    "from utils.seq2seq.rl_util import *\n",
    "from utils.seq2seq.initialize import loadCheckpoint, save_model\n",
    "from utils.seq2seq.write_result import *\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from translate.seq2seq_beam import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from utils.seq2seq.rl_util import *\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--key_attention', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--intra_encoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--intra_decoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--copy', type=bool, default=True, help = 'True/False') # for transformer\n",
    "\n",
    "parser.add_argument('--model_type', type=str, default='seq2seq', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--train_rl', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='DEP_keys', \n",
    "                    help = 'POS_keys / DEP_keys / Noun_adj_keys / TextRank_keys')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--rand_unif_init_mag', type=float, default=0.02)\n",
    "parser.add_argument('--trunc_norm_init_std', type=float, default=0.001)\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument('--gound_truth_prob', type=float, default=0.1)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=1000)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=50)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=8)\n",
    "parser.add_argument('--max_epochs', type=int, default=15)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=16)\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=300)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default=None, help='0002000')\n",
    "parser.add_argument('--word_emb_type', type=str, default='word2Vec', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=True, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-08 01:10:58 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - train : 397524, test : 44170\n",
      "I0508 01:10:58.418067 140523512821568 batcher.py:180] train : 397524, test : 44170\n",
      "2020-05-08 01:10:58 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - train batches : 49690, test batches : 5521\n",
      "I0508 01:10:58.748825 140523512821568 batcher.py:194] train batches : 49690, test batches : 5521\n"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "train_batches = len(iter(train_loader))\n",
    "test_batches = len(iter(validate_loader))\n",
    "save_steps = int(train_batches/1000)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0508 01:10:59.401710 140523512821568 utils_any2vec.py:341] loading projection weights from ../Train-Data/Mix6_mainCat/Embedding/word2Vec/word2Vec.300d.txt\n",
      "I0508 01:11:09.721817 140523512821568 utils_any2vec.py:405] loaded (49676, 300) matrix from ../Train-Data/Mix6_mainCat/Embedding/word2Vec/word2Vec.300d.txt\n"
     ]
    }
   ],
   "source": [
    "from seq2seq import Model\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "load_step = None\n",
    "model = Model(pre_train_emb=config.pre_train_emb, \n",
    "              word_emb_type = config.word_emb_type, \n",
    "              vocab = vocab)\n",
    "\n",
    "model = model.cuda()\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=config.lr)   \n",
    "# optimizer = T.optim.Adagrad(model.parameters(),lr=config.lr, initial_accumulator_value=0.1)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(logger, load_model_path, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model, config, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    " \n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input \n",
    "#         print(enc_key_batch.shape)\n",
    "#         print(enc_key_batch[0])\n",
    "#         print(enc_key_batch[0])\n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        'Decoder data'\n",
    "        dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(batch, config, batch_first = True) # Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(START))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "            use_gound_truth = get_cuda((T.rand(len(enc_out)) > config.gound_truth_prob)).long()  # Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "            x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t  # Select decoder input based on use_ground_truth probabilities\n",
    "            x_t = model.embeds(x_t)  \n",
    "            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s, _, _ = model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                      ct_e, extra_zeros,\n",
    "                                                                                      enc_batch_extend_vocab,\n",
    "                                                                                      sum_temporal_srcs, prev_s, enc_key_batch, enc_key_mask)\n",
    "            target = target_batch[:, t]\n",
    "            log_probs = T.log(final_dist + config.eps)\n",
    "            step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=PAD)\n",
    "            step_losses.append(step_loss)\n",
    "            x_t = T.multinomial(final_dist,1).squeeze()  # Sample words from final distribution which can be used as input in next time step\n",
    "\n",
    "            is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t.detach() + (is_oov) * UNKNOWN_TOKEN  # Replace OOVs with [UNK] token\n",
    "\n",
    "        losses = T.sum(T.stack(step_losses, 1), 1)  # unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens  # Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)  # Average batch loss\n",
    "        return mle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "#     batch = next(iter(validate_loader))\n",
    "    val_num = len(iter(validate_loader))\n",
    "    for idx, batch in enumerate(validate_loader):\n",
    "        loss = train_one(model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "        if idx>= val_num/10: break\n",
    "    model.train()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "@torch.autograd.no_grad()\n",
    "def decode(writer, logger, step, config, model, batch, mode):\n",
    "    # 動態取batch\n",
    "    if mode == 'test':\n",
    "#         num = len(iter(batch))\n",
    "#         select_batch = None\n",
    "#         rand_b_id = randint(0,num-1)\n",
    "# #         logger.info('test_batch : ' + str(num)+ ' ' + str(rand_b_id))\n",
    "#         for idx, b in enumerate(batch):\n",
    "#             if idx == rand_b_id:\n",
    "#                 select_batch = b\n",
    "#                 break\n",
    "        select_batch = next(iter(batch))\n",
    "        batch = select_batch\n",
    "        if type(batch) == torch.utils.data.dataloader.DataLoader:\n",
    "            batch = next(iter(batch))\n",
    "    'Encoder data'\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "    enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input   \n",
    "    enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "    'Feed encoder data to predict'\n",
    "    pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                           enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                           START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "    article_sents, decoded_sents, keywords_list, \\\n",
    "    ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "    rouge_1, rouge_2, rouge_l = write_rouge(writer, step, mode,article_sents, decoded_sents, \\\n",
    "                keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    write_bleu(writer, step, mode, article_sents, decoded_sents, \\\n",
    "               keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    write_group(writer, step, mode, article_sents, decoded_sents,\\\n",
    "                keywords_list, ref_sents, long_seq_index)\n",
    "\n",
    "    return rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import time\n",
    "@torch.autograd.no_grad()\n",
    "def avg_acc(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(iter(dataloader))\n",
    "    avg_rouge_l = []\n",
    "    acc_st, acc_cost = 0, 0\n",
    "    avg_acc_cost = 0\n",
    "    for idx, batch in enumerate(dataloader): \n",
    "        if idx >= num/100: break\n",
    "        acc_st = time.time()\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "        'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                               enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                               START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, \\\n",
    "        ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "        rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents, \\\n",
    "                    keywords_list, ref_sents, long_seq_index, write = False)\n",
    "        avg_rouge_l.append(rouge_l)\n",
    "        acc_cost = time.time() - acc_st\n",
    "        avg_acc_cost += acc_cost\n",
    "\n",
    "\n",
    "    avg_rouge_l = sum(avg_rouge_l) / len(avg_rouge_l)\n",
    "    writer.add_scalars('scalar_avg/acc',  \n",
    "                   {'%sing_avg_acc'%(mode): avg_rouge_l\n",
    "                   }, epoch)\n",
    "#     avg_acc_cost = avg_acc_cost / len(avg_rouge_l)\n",
    "#     print('decode 1% batches %s data, cost time %s ms' % (mode, avg_acc_cost ))\n",
    "    return avg_rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL(model, config, batch, greedy):    \n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param article_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "        \n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        s_t = enc_hidden                                                                            #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(START))  # Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []                       # Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []       # Stores padding masks of generated samples\n",
    "        log_probs = []                                                                              #Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "        # Generate RL tokens and compute rl-log-loss\n",
    "        # ----------------------------------------------------------------------\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = model.embeds(x_t)\n",
    "            \n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s, _, _ = model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                      ct_e, extra_zeros,\n",
    "                                                                                      enc_batch_extend_vocab,\n",
    "                                                                                      sum_temporal_srcs, prev_s, enc_key_batch, enc_key_mask)\n",
    "            \n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs) # 建立以參數probs為標準的類別分佈\n",
    "                # perform multinomial sampling\n",
    "                x_t = multi_dist.sample()  # 將下一個時間點的x_t，視為下一個action   \n",
    "                # 使用log_prob实施梯度方法 Policy Gradient，构造一个等价類別分佈的损失函数\n",
    "                log_prob = multi_dist.log_prob(x_t)  \n",
    "                log_probs.append(log_prob) #\n",
    "            else:\n",
    "                # perform greedy sampling distribution\n",
    "                _, x_t = T.max(probs, dim=1)  # 因greedy以機率最大進行取樣，視為其中一個action   \n",
    "            x_t = x_t.detach() # detach返回的 Variable 永远不会需要梯度\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (x_t == END) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
    "            x_t = (1-is_oov)*x_t + (is_oov)*UNKNOWN_TOKEN                                             #Replace OOVs with [UNK] token\n",
    "        # -----------------------------------End loop -----------------------------------\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1) # 在第1个维度上stack, 增加新的维度进行堆叠\n",
    "            log_probs = log_probs * decoder_padding_mask # 遮罩掉為[END] or [STOP]不計算損失           #Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1) # 計算每個sample words生成的總長度               #Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs, dim=1) / lens  # 計算平均的每個句子的log loss # (bs,1)        #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy() # 取出每個sample sentence 的word id list\n",
    "            S = output2words(id_list, vocab, batch.art_oovs[i]) #Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:          #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "        return decoded_strs, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_RL(model, config, batch):\n",
    "    # Self-Critical sequence training(SCST)\n",
    "    sample_sents, RL_log_probs = RL(model, config, batch, greedy=False)   # multinomial sampling\n",
    "    with T.autograd.no_grad():        \n",
    "        greedy_sents, _ = RL(model, config, batch, greedy=True)  # greedy sampling\n",
    "\n",
    "    sample_reward = reward_function(sample_sents, batch.original_abstract) # r(w^s):通过根据概率来随机sample词生成句子的reward值\n",
    "    baseline_reward = reward_function(greedy_sents, batch.original_abstract) # r(w^):测试阶段使用greedy decoding取概率最大的词来生成句子的reward值\n",
    "\n",
    "    batch_reward = T.mean(sample_reward).item()\n",
    "    #Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    rl_loss = -(sample_reward - baseline_reward) * RL_log_probs  # SCST梯度計算公式     \n",
    "    rl_loss = T.mean(rl_loss)  \n",
    "    '''\n",
    "    公式的意思就是：对于如果当前sample到的词比测试阶段生成的词好，那么在这次词的维度上，整个式子的值就是负的（因为后面那一项一定为负），\n",
    "    这样梯度就会上升，从而提高这个词的分数st；而对于其他词，后面那一项为正，梯度就会下降，从而降低其他词的分数\n",
    "    '''                 \n",
    "    return rl_loss, batch_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(dataloader)\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "    avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    avg_meteor = []\n",
    "    outFrame = None\n",
    "    avg_time = 0\n",
    "        \n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        start = time.time() \n",
    "#         'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "#         'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                                enc_batch_extend_vocab, enc_key_batch, enc_key_lens, model, \n",
    "                                START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "        cost = (time.time() - start)\n",
    "        avg_time += cost        \n",
    "\n",
    "        \n",
    "        rouge_1, rouge_2, rouge_l, self_Bleu_1, self_Bleu_2, self_Bleu_3, self_Bleu_4, \\\n",
    "            Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        \n",
    "        if idx %1000 ==0 and idx >0 : print(idx)\n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "        avg_self_bleu1.extend(self_Bleu_1)\n",
    "        avg_self_bleu2.extend(self_Bleu_2)\n",
    "        avg_self_bleu3.extend(self_Bleu_3)\n",
    "        avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "        avg_bleu1.extend(Bleu_1)\n",
    "        avg_bleu2.extend(Bleu_2)\n",
    "        avg_bleu3.extend(Bleu_3)\n",
    "        avg_bleu4.extend(Bleu_4)\n",
    "        avg_meteor.extend(Meteor)\n",
    "        # ----------------------------------------------------    \n",
    "    avg_time = avg_time / (num * config.batch_size) \n",
    "    \n",
    "    avg_rouge_l, outFrame = total_output(mode, writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l, \\\n",
    "        avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4, \\\n",
    "        avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "    )\n",
    "    \n",
    "    return avg_rouge_l, outFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-08 01:11:13 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - ------Training START--------\n",
      "I0508 01:11:13.751104 140523512821568 <ipython-input-12-a32ebc327372>:6] ------Training START--------\n",
      "2020-05-08 05:17:07 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 0: 49000, training batch loss = 3.177378, running_avg_loss loss = 3.954701, validation loss = 2.086899\n",
      "I0508 05:17:07.974321 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 0: 49000, training batch loss = 3.177378, running_avg_loss loss = 3.954701, validation loss = 2.086899\n",
      "2020-05-08 05:17:07 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 0|step 49000| compute loss cost = 28.208091 ms\n",
      "I0508 05:17:07.976514 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 0|step 49000| compute loss cost = 28.208091 ms\n",
      "2020-05-08 05:17:07 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 49000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0049000.tar...\n",
      "I0508 05:17:07.978752 140523512821568 initialize.py:229] Saving model step 49000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0049000.tar...\n",
      "2020-05-08 05:17:09 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 0|step 49000| decode cost = 1.081221 ms\n",
      "I0508 05:17:09.634683 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 0|step 49000| decode cost = 1.081221 ms\n",
      "2020-05-08 05:20:14 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0508 05:20:14.546927 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-08 05:26:19 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 0|step 49690| train_avg_acc = 0.330064, test_avg_acc = 0.542523\n",
      "I0508 05:26:19.338441 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 0|step 49690| train_avg_acc = 0.330064, test_avg_acc = 0.542523\n",
      "2020-05-08 05:26:19 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0508 05:26:19.339811 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-08 09:27:47 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 1: 98000, training batch loss = 3.255230, running_avg_loss loss = 3.541196, validation loss = 1.977096\n",
      "I0508 09:27:47.597398 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 1: 98000, training batch loss = 3.255230, running_avg_loss loss = 3.541196, validation loss = 1.977096\n",
      "2020-05-08 09:27:47 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 1|step 98000| compute loss cost = 27.416645 ms\n",
      "I0508 09:27:47.599411 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 1|step 98000| compute loss cost = 27.416645 ms\n",
      "2020-05-08 09:27:47 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 98000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0098000.tar...\n",
      "I0508 09:27:47.602157 140523512821568 initialize.py:229] Saving model step 98000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0098000.tar...\n",
      "2020-05-08 09:27:49 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 1|step 98000| decode cost = 1.139265 ms\n",
      "I0508 09:27:49.270558 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 1|step 98000| decode cost = 1.139265 ms\n",
      "2020-05-08 09:34:36 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0508 09:34:36.172144 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-08 09:40:28 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 1|step 99380| train_avg_acc = 0.338752, test_avg_acc = 0.555457\n",
      "I0508 09:40:28.875875 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 1|step 99380| train_avg_acc = 0.338752, test_avg_acc = 0.555457\n",
      "2020-05-08 09:40:28 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0508 09:40:28.877658 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-08 13:38:47 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 2: 147000, training batch loss = 2.237505, running_avg_loss loss = 3.243556, validation loss = 1.934876\n",
      "I0508 13:38:47.569718 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 2: 147000, training batch loss = 2.237505, running_avg_loss loss = 3.243556, validation loss = 1.934876\n",
      "2020-05-08 13:38:47 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 2|step 147000| compute loss cost = 27.839165 ms\n",
      "I0508 13:38:47.571174 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 2|step 147000| compute loss cost = 27.839165 ms\n",
      "2020-05-08 13:38:47 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 147000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0147000.tar...\n",
      "I0508 13:38:47.573372 140523512821568 initialize.py:229] Saving model step 147000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0147000.tar...\n",
      "2020-05-08 13:38:49 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 2|step 147000| decode cost = 1.471172 ms\n",
      "I0508 13:38:49.629703 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 2|step 147000| decode cost = 1.471172 ms\n",
      "2020-05-08 13:49:04 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0508 13:49:04.216846 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-08 13:54:51 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 2|step 149070| train_avg_acc = 0.350027, test_avg_acc = 0.567297\n",
      "I0508 13:54:51.965519 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 2|step 149070| train_avg_acc = 0.350027, test_avg_acc = 0.567297\n",
      "2020-05-08 13:54:51 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0508 13:54:51.967303 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-08 17:47:43 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 3: 196000, training batch loss = 2.863460, running_avg_loss loss = 3.037597, validation loss = 1.930958\n",
      "I0508 17:47:43.107710 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 3: 196000, training batch loss = 2.863460, running_avg_loss loss = 3.037597, validation loss = 1.930958\n",
      "2020-05-08 17:47:43 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 3|step 196000| compute loss cost = 27.933906 ms\n",
      "I0508 17:47:43.109753 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 3|step 196000| compute loss cost = 27.933906 ms\n",
      "2020-05-08 17:47:43 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 196000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0196000.tar...\n",
      "I0508 17:47:43.112442 140523512821568 initialize.py:229] Saving model step 196000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0196000.tar...\n",
      "2020-05-08 17:47:44 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 3|step 196000| decode cost = 1.159066 ms\n",
      "I0508 17:47:44.829183 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 3|step 196000| decode cost = 1.159066 ms\n",
      "2020-05-08 18:01:19 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0508 18:01:19.804187 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-08 18:07:19 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 3|step 198760| train_avg_acc = 0.355047, test_avg_acc = 0.557737\n",
      "I0508 18:07:19.030119 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 3|step 198760| train_avg_acc = 0.355047, test_avg_acc = 0.557737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-08 18:07:19 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0508 18:07:19.031401 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-08 21:58:47 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 4: 245000, training batch loss = 2.990861, running_avg_loss loss = 2.837416, validation loss = 1.913325\n",
      "I0508 21:58:47.264647 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 4: 245000, training batch loss = 2.990861, running_avg_loss loss = 2.837416, validation loss = 1.913325\n",
      "2020-05-08 21:58:47 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 4|step 245000| compute loss cost = 28.190302 ms\n",
      "I0508 21:58:47.266974 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 4|step 245000| compute loss cost = 28.190302 ms\n",
      "2020-05-08 21:58:47 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 245000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0245000.tar...\n",
      "I0508 21:58:47.269905 140523512821568 initialize.py:229] Saving model step 245000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0245000.tar...\n",
      "2020-05-08 21:58:49 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 4|step 245000| decode cost = 1.086710 ms\n",
      "I0508 21:58:49.009548 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 4|step 245000| decode cost = 1.086710 ms\n",
      "2020-05-08 22:16:01 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0508 22:16:01.029180 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-08 22:21:44 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 4|step 248450| train_avg_acc = 0.360253, test_avg_acc = 0.567176\n",
      "I0508 22:21:44.556522 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 4|step 248450| train_avg_acc = 0.360253, test_avg_acc = 0.567176\n",
      "2020-05-08 22:21:44 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0508 22:21:44.558326 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-09 02:10:50 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 5: 294000, training batch loss = 1.274991, running_avg_loss loss = 2.669666, validation loss = 1.914696\n",
      "I0509 02:10:50.519912 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 5: 294000, training batch loss = 1.274991, running_avg_loss loss = 2.669666, validation loss = 1.914696\n",
      "2020-05-09 02:10:50 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 5|step 294000| compute loss cost = 27.832031 ms\n",
      "I0509 02:10:50.522060 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 5|step 294000| compute loss cost = 27.832031 ms\n",
      "2020-05-09 02:10:50 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 294000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0294000.tar...\n",
      "I0509 02:10:50.524907 140523512821568 initialize.py:229] Saving model step 294000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0294000.tar...\n",
      "2020-05-09 02:10:52 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 5|step 294000| decode cost = 1.019105 ms\n",
      "I0509 02:10:52.132982 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 5|step 294000| decode cost = 1.019105 ms\n",
      "2020-05-09 02:31:32 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0509 02:31:32.236713 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-09 02:37:18 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 5|step 298140| train_avg_acc = 0.361977, test_avg_acc = 0.569632\n",
      "I0509 02:37:18.604898 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 5|step 298140| train_avg_acc = 0.361977, test_avg_acc = 0.569632\n",
      "2020-05-09 02:37:18 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0509 02:37:18.605963 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-09 06:20:41 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 6: 343000, training batch loss = 2.995233, running_avg_loss loss = 2.635435, validation loss = 1.930295\n",
      "I0509 06:20:41.814587 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 6: 343000, training batch loss = 2.995233, running_avg_loss loss = 2.635435, validation loss = 1.930295\n",
      "2020-05-09 06:20:41 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 6|step 343000| compute loss cost = 28.362687 ms\n",
      "I0509 06:20:41.816596 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 6|step 343000| compute loss cost = 28.362687 ms\n",
      "2020-05-09 06:20:41 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 343000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0343000.tar...\n",
      "I0509 06:20:41.818730 140523512821568 initialize.py:229] Saving model step 343000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0343000.tar...\n",
      "2020-05-09 06:20:43 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 6|step 343000| decode cost = 1.176807 ms\n",
      "I0509 06:20:43.552246 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 6|step 343000| decode cost = 1.176807 ms\n",
      "2020-05-09 06:44:22 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0509 06:44:22.161196 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-09 06:50:07 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 6|step 347830| train_avg_acc = 0.367005, test_avg_acc = 0.560316\n",
      "I0509 06:50:07.689938 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 6|step 347830| train_avg_acc = 0.367005, test_avg_acc = 0.560316\n",
      "2020-05-09 06:50:07 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0509 06:50:07.691060 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-09 10:30:45 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 7: 392000, training batch loss = 2.311992, running_avg_loss loss = 2.567482, validation loss = 1.932495\n",
      "I0509 10:30:45.500359 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 7: 392000, training batch loss = 2.311992, running_avg_loss loss = 2.567482, validation loss = 1.932495\n",
      "2020-05-09 10:30:45 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 7|step 392000| compute loss cost = 29.467368 ms\n",
      "I0509 10:30:45.501703 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 7|step 392000| compute loss cost = 29.467368 ms\n",
      "2020-05-09 10:30:45 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 392000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0392000.tar...\n",
      "I0509 10:30:45.503645 140523512821568 initialize.py:229] Saving model step 392000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0392000.tar...\n",
      "2020-05-09 10:30:47 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 7|step 392000| decode cost = 1.260866 ms\n",
      "I0509 10:30:47.373101 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 7|step 392000| decode cost = 1.260866 ms\n",
      "2020-05-09 10:58:52 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0509 10:58:52.543889 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-09 11:04:48 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 7|step 397520| train_avg_acc = 0.374955, test_avg_acc = 0.558091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0509 11:04:48.958062 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 7|step 397520| train_avg_acc = 0.374955, test_avg_acc = 0.558091\n",
      "2020-05-09 11:04:48 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0509 11:04:48.959341 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-09 14:48:34 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 8: 441000, training batch loss = 3.306699, running_avg_loss loss = 2.509025, validation loss = 1.955695\n",
      "I0509 14:48:34.049046 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 8: 441000, training batch loss = 3.306699, running_avg_loss loss = 2.509025, validation loss = 1.955695\n",
      "2020-05-09 14:48:34 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 8|step 441000| compute loss cost = 29.619678 ms\n",
      "I0509 14:48:34.051339 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 8|step 441000| compute loss cost = 29.619678 ms\n",
      "2020-05-09 14:48:34 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 441000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0441000.tar...\n",
      "I0509 14:48:34.054378 140523512821568 initialize.py:229] Saving model step 441000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0441000.tar...\n",
      "2020-05-09 14:48:38 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 8|step 441000| decode cost = 1.529265 ms\n",
      "I0509 14:48:38.192089 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 8|step 441000| decode cost = 1.529265 ms\n",
      "2020-05-09 15:20:35 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0509 15:20:35.895607 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-09 15:26:30 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 8|step 447210| train_avg_acc = 0.381991, test_avg_acc = 0.567842\n",
      "I0509 15:26:30.334240 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 8|step 447210| train_avg_acc = 0.381991, test_avg_acc = 0.567842\n",
      "2020-05-09 15:26:30 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0509 15:26:30.335552 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-09 19:03:58 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 9: 490000, training batch loss = 2.595302, running_avg_loss loss = 2.483165, validation loss = 1.962052\n",
      "I0509 19:03:58.712503 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 9: 490000, training batch loss = 2.595302, running_avg_loss loss = 2.483165, validation loss = 1.962052\n",
      "2020-05-09 19:03:58 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 9|step 490000| compute loss cost = 27.842679 ms\n",
      "I0509 19:03:58.714872 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 9|step 490000| compute loss cost = 27.842679 ms\n",
      "2020-05-09 19:03:58 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 490000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0490000.tar...\n",
      "I0509 19:03:58.718236 140523512821568 initialize.py:229] Saving model step 490000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0490000.tar...\n",
      "2020-05-09 19:04:00 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 9|step 490000| decode cost = 1.099952 ms\n",
      "I0509 19:04:00.357798 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 9|step 490000| decode cost = 1.099952 ms\n",
      "2020-05-09 19:38:27 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0509 19:38:27.112182 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-09 19:44:11 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 9|step 496900| train_avg_acc = 0.375965, test_avg_acc = 0.572292\n",
      "I0509 19:44:11.648584 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 9|step 496900| train_avg_acc = 0.375965, test_avg_acc = 0.572292\n",
      "2020-05-09 19:44:11 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0509 19:44:11.649687 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-09 23:16:50 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 10: 539000, training batch loss = 2.680080, running_avg_loss loss = 2.456875, validation loss = 1.983558\n",
      "I0509 23:16:50.486938 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 10: 539000, training batch loss = 2.680080, running_avg_loss loss = 2.456875, validation loss = 1.983558\n",
      "2020-05-09 23:16:50 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 10|step 539000| compute loss cost = 28.171551 ms\n",
      "I0509 23:16:50.488568 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 10|step 539000| compute loss cost = 28.171551 ms\n",
      "2020-05-09 23:16:50 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 539000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0539000.tar...\n",
      "I0509 23:16:50.491590 140523512821568 initialize.py:229] Saving model step 539000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0539000.tar...\n",
      "2020-05-09 23:16:52 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 10|step 539000| decode cost = 1.335573 ms\n",
      "I0509 23:16:52.429496 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 10|step 539000| decode cost = 1.335573 ms\n",
      "2020-05-09 23:54:35 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0509 23:54:35.156551 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-10 00:00:15 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 10|step 546590| train_avg_acc = 0.394161, test_avg_acc = 0.571480\n",
      "I0510 00:00:15.550633 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 10|step 546590| train_avg_acc = 0.394161, test_avg_acc = 0.571480\n",
      "2020-05-10 00:00:15 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0510 00:00:15.552089 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-10 03:26:57 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 11: 588000, training batch loss = 3.643629, running_avg_loss loss = 2.384948, validation loss = 1.995669\n",
      "I0510 03:26:57.386435 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 11: 588000, training batch loss = 3.643629, running_avg_loss loss = 2.384948, validation loss = 1.995669\n",
      "2020-05-10 03:26:57 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 11|step 588000| compute loss cost = 28.044259 ms\n",
      "I0510 03:26:57.388405 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 11|step 588000| compute loss cost = 28.044259 ms\n",
      "2020-05-10 03:26:57 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 588000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0588000.tar...\n",
      "I0510 03:26:57.391301 140523512821568 initialize.py:229] Saving model step 588000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0588000.tar...\n",
      "2020-05-10 03:26:59 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 11|step 588000| decode cost = 1.245156 ms\n",
      "I0510 03:26:59.216983 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 11|step 588000| decode cost = 1.245156 ms\n",
      "2020-05-10 04:08:14 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0510 04:08:14.953455 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-10 04:13:49 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 11|step 596280| train_avg_acc = 0.387228, test_avg_acc = 0.568441\n",
      "I0510 04:13:49.393364 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 11|step 596280| train_avg_acc = 0.387228, test_avg_acc = 0.568441\n",
      "2020-05-10 04:13:49 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0510 04:13:49.394511 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-10 07:36:52 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 12: 637000, training batch loss = 2.118286, running_avg_loss loss = 2.297320, validation loss = 2.010700\n",
      "I0510 07:36:52.628941 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 12: 637000, training batch loss = 2.118286, running_avg_loss loss = 2.297320, validation loss = 2.010700\n",
      "2020-05-10 07:36:52 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 12|step 637000| compute loss cost = 27.890768 ms\n",
      "I0510 07:36:52.631024 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 12|step 637000| compute loss cost = 27.890768 ms\n",
      "2020-05-10 07:36:52 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 637000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0637000.tar...\n",
      "I0510 07:36:52.633939 140523512821568 initialize.py:229] Saving model step 637000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0637000.tar...\n",
      "2020-05-10 07:36:54 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 12|step 637000| decode cost = 1.112094 ms\n",
      "I0510 07:36:54.288354 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 12|step 637000| decode cost = 1.112094 ms\n",
      "2020-05-10 08:21:09 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0510 08:21:09.698005 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-10 08:26:45 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 12|step 645970| train_avg_acc = 0.402731, test_avg_acc = 0.557345\n",
      "I0510 08:26:45.629956 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 12|step 645970| train_avg_acc = 0.402731, test_avg_acc = 0.557345\n",
      "2020-05-10 08:26:45 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0510 08:26:45.631054 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-10 11:46:25 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 13: 686000, training batch loss = 2.821480, running_avg_loss loss = 2.266175, validation loss = 2.036508\n",
      "I0510 11:46:25.230480 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 13: 686000, training batch loss = 2.821480, running_avg_loss loss = 2.266175, validation loss = 2.036508\n",
      "2020-05-10 11:46:25 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 13|step 686000| compute loss cost = 28.289657 ms\n",
      "I0510 11:46:25.231724 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 13|step 686000| compute loss cost = 28.289657 ms\n",
      "2020-05-10 11:46:25 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 686000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0686000.tar...\n",
      "I0510 11:46:25.233516 140523512821568 initialize.py:229] Saving model step 686000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0686000.tar...\n",
      "2020-05-10 11:46:26 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 13|step 686000| decode cost = 1.108923 ms\n",
      "I0510 11:46:26.904851 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 13|step 686000| decode cost = 1.108923 ms\n",
      "2020-05-10 12:34:19 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0510 12:34:19.024364 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-10 12:39:55 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 13|step 695660| train_avg_acc = 0.383950, test_avg_acc = 0.562810\n",
      "I0510 12:39:55.780694 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 13|step 695660| train_avg_acc = 0.383950, test_avg_acc = 0.562810\n",
      "2020-05-10 12:39:55 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0510 12:39:55.782477 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-10 15:56:05 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 14: 735000, training batch loss = 2.570415, running_avg_loss loss = 2.262743, validation loss = 2.057983\n",
      "I0510 15:56:05.761334 140523512821568 <ipython-input-12-a32ebc327372>:49] epoch 14: 735000, training batch loss = 2.570415, running_avg_loss loss = 2.262743, validation loss = 2.057983\n",
      "2020-05-10 15:56:05 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 14|step 735000| compute loss cost = 28.211370 ms\n",
      "I0510 15:56:05.763798 140523512821568 <ipython-input-12-a32ebc327372>:71] epoch 14|step 735000| compute loss cost = 28.211370 ms\n",
      "2020-05-10 15:56:05 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - Saving model step 735000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0735000.tar...\n",
      "I0510 15:56:05.766911 140523512821568 initialize.py:229] Saving model step 735000 to model/saved_models/Pointer_generator_word2Vec_Intra_Atten_Key_Atten/0735000.tar...\n",
      "2020-05-10 15:56:07 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 14|step 735000| decode cost = 1.232460 ms\n",
      "I0510 15:56:07.583125 140523512821568 <ipython-input-12-a32ebc327372>:81] epoch 14|step 735000| decode cost = 1.232460 ms\n",
      "2020-05-10 16:47:35 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0510 16:47:35.541523 140523512821568 <ipython-input-12-a32ebc327372>:90] -------------------------------------------------------------\n",
      "2020-05-10 16:53:10 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 14|step 745350| train_avg_acc = 0.397555, test_avg_acc = 0.563153\n",
      "I0510 16:53:10.596133 140523512821568 <ipython-input-12-a32ebc327372>:93] epoch 14|step 745350| train_avg_acc = 0.397555, test_avg_acc = 0.563153\n",
      "2020-05-10 16:53:10 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - -------------------------------------------------------------\n",
      "I0510 16:53:10.597929 140523512821568 <ipython-input-12-a32ebc327372>:98] -------------------------------------------------------------\n",
      "2020-05-10 16:53:10 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - ------Training SUCCESS--------\n",
      "I0510 16:53:10.599646 140523512821568 <ipython-input-12-a32ebc327372>:103] ------Training SUCCESS--------\n",
      "2020-05-10 16:53:10 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - ------Training END--------\n",
      "I0510 16:53:10.600874 140523512821568 <ipython-input-12-a32ebc327372>:105] ------Training END--------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 04:24:46 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - epoch 14: train_avg_acc = 0.401106, test_avg_acc = 0.343998\n",
      "I0511 04:24:46.464147 140523512821568 <ipython-input-12-a32ebc327372>:108] epoch 14: train_avg_acc = 0.401106, test_avg_acc = 0.343998\n",
      "2020-05-11 04:24:46 - Pointer_generator_word2Vec_Intra_Atten_Key_Atten - INFO: - logger已關閉\n",
      "I0511 04:24:46.465187 140523512821568 train_util.py:106] logger已關閉\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "loss_st, loss_cost = 0,0\n",
    "decode_st, decode_cost = 0,0\n",
    "\n",
    "write_train_para(writer, config)\n",
    "logger.info('------Training START--------')\n",
    "running_avg_loss, running_avg_rl_loss = 0, 0\n",
    "sum_total_reward = 0\n",
    "step = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(config.max_epochs):\n",
    "        for batch in train_loader:\n",
    "            step += 1\n",
    "            loss_st = time.time()\n",
    "            mle_loss = train_one(model, config, batch)\n",
    "            if config.train_rl:\n",
    "                rl_loss, batch_reward = train_one_RL(model, config, batch)             \n",
    "        \n",
    "                if step%1000 == 0 :\n",
    "                    writer.add_scalars('scalar/RL_Loss',  \n",
    "                       {'rl_loss': rl_loss\n",
    "                       }, step)\n",
    "                    writer.add_scalars('scalar/Reward',  \n",
    "                       {'batch_reward': batch_reward\n",
    "                       }, step)\n",
    "#                     logger.info('epoch %d: %d, RL_Loss = %f, batch_reward = %f'\n",
    "#                                     % (epoch, step, rl_loss, batch_reward))\n",
    "                sum_total_reward += batch_reward\n",
    "            else:\n",
    "                rl_loss = T.FloatTensor([0]).cuda()\n",
    "            (config.mle_weight * mle_loss + config.rl_weight * rl_loss).backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "            '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "            if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "    #             clip_grad_norm_(model.parameters(), 5.0)                      \n",
    "                optimizer.step() # 根据累计的梯度更新网络参数\n",
    "                optimizer.zero_grad() # 清空过往梯度 \n",
    "            if step%1000 == 0 :\n",
    "                with T.autograd.no_grad():\n",
    "                    train_batch_loss = mle_loss.item()\n",
    "                    train_batch_rl_loss = rl_loss.item()\n",
    "                    val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                    running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "                    running_avg_rl_loss = calc_running_avg_loss(train_batch_rl_loss, running_avg_rl_loss)\n",
    "                    running_avg_reward = sum_total_reward / step\n",
    "                    if step % save_steps == 0:\n",
    "                        logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "                                    % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                    writer.add_scalars('scalar/Loss',  \n",
    "                       {'train_batch_loss': train_batch_loss\n",
    "                       }, step)\n",
    "                    writer.add_scalars('scalar_avg/loss',  \n",
    "                       {'train_avg_loss': running_avg_loss,\n",
    "                        'test_avg_loss': val_avg_loss\n",
    "                       }, step)\n",
    "                    if running_avg_reward > 0:\n",
    "#                         logger.info('epoch %d: %d, running_avg_reward = %f'\n",
    "#                                 % (epoch, step, running_avg_reward))\n",
    "                        writer.add_scalars('scalar_avg/Reward',  \n",
    "                           {'running_avg_reward': running_avg_reward\n",
    "                           }, step)\n",
    "                    if running_avg_rl_loss != 0:\n",
    "#                         logger.info('epoch %d: %d, running_avg_rl_loss = %f'\n",
    "#                                 % (epoch, step, running_avg_rl_loss))\n",
    "                        writer.add_scalars('scalar_avg/RL_Loss',  \n",
    "                           {'running_avg_rl_loss': running_avg_rl_loss\n",
    "                           }, step)\n",
    "                    loss_cost = time.time() - loss_st\n",
    "                    if step % save_steps == 0: logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "                                % (epoch, step, loss_cost))\n",
    "\n",
    "            if step % save_steps == 0:\n",
    "                save_model(config, logger, model, optimizer, step, vocab, running_avg_loss, \\\n",
    "                           r_loss=0, title = loggerName)\n",
    "            if step%1000 == 0 and step > 0:\n",
    "                decode_st = time.time()\n",
    "                train_rouge_l_f = decode(writer, logger, step, config, model, batch, mode = 'train') # call batch by validate_loader\n",
    "                test_rouge_l_f = decode(writer, logger, step, config, model, validate_loader, mode = 'test') # call batch by validate_loader\n",
    "                decode_cost = time.time() - decode_st\n",
    "                if step%save_steps == 0: logger.info('epoch %d|step %d| decode cost = %f ms'% (epoch, step, decode_cost))\n",
    "\n",
    "                writer.add_scalars('scalar/Rouge-L',  \n",
    "                   {'train_rouge_l_f': train_rouge_l_f,\n",
    "                    'test_rouge_l_f': test_rouge_l_f\n",
    "                   }, step)\n",
    "#                 logger.info('epoch %d: %d, train_rouge_l_f = %f, test_rouge_l_f = %f'\n",
    "#                                 % (epoch, step, train_rouge_l_f, test_rouge_l_f))\n",
    "#         break\n",
    "        logger.info('-------------------------------------------------------------')\n",
    "        train_avg_acc = avg_acc(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "        test_avg_acc = avg_acc(writer, logger, epoch, config, model, validate_loader, mode = 'test')                   \n",
    "        logger.info('epoch %d|step %d| train_avg_acc = %f, test_avg_acc = %f' % (epoch, step, train_avg_acc, test_avg_acc))\n",
    "        if running_avg_reward > 0:\n",
    "            logger.info('epoch %d|step %d| running_avg_reward = %f'% (epoch, step, running_avg_reward))\n",
    "        if running_avg_rl_loss != 0:\n",
    "            logger.info('epoch %d|step %d| running_avg_rl_loss = %f'% (epoch, step, running_avg_rl_loss))\n",
    "        logger.info('-------------------------------------------------------------')\n",
    "\n",
    "except Excepation as e:\n",
    "        print(e)\n",
    "else:\n",
    "    logger.info(u'------Training SUCCESS--------')  \n",
    "finally:\n",
    "    logger.info(u'------Training END--------')    \n",
    "    train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "    test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "    logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc))\n",
    "    removeLogger(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>keywords</th>\n",
       "      <th>reference</th>\n",
       "      <th>decoded</th>\n",
       "      <th>rouge_1</th>\n",
       "      <th>rouge_2</th>\n",
       "      <th>rouge_l</th>\n",
       "      <th>self_Bleu_1</th>\n",
       "      <th>self_Bleu_2</th>\n",
       "      <th>self_Bleu_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Bleu_1</th>\n",
       "      <th>Bleu_2</th>\n",
       "      <th>Bleu_3</th>\n",
       "      <th>Bleu_4</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>article_lens</th>\n",
       "      <th>ref_lens</th>\n",
       "      <th>overlap</th>\n",
       "      <th>overlap_percent</th>\n",
       "      <th>gen_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44160</th>\n",
       "      <td>positive fit all requirement for indoor camera...</td>\n",
       "      <td>['fit', 'generic']</td>\n",
       "      <td>for those of us who do not like instal more ap...</td>\n",
       "      <td>great indoor camera for the price</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.067718</td>\n",
       "      <td>0.031479</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031479</td>\n",
       "      <td>1.150198e-155</td>\n",
       "      <td>9.303220e-205</td>\n",
       "      <td>2.198601e-232</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>993</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>Ext</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44161</th>\n",
       "      <td>return the device fail miserably set and forge...</td>\n",
       "      <td>['review', 'recording']</td>\n",
       "      <td>not useable for set and forget pvr require con...</td>\n",
       "      <td>device fail to miserably set up and forget pvr</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.246914</td>\n",
       "      <td>0.163502</td>\n",
       "      <td>9.196986e-02</td>\n",
       "      <td>5.255421e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163502</td>\n",
       "      <td>1.226265e-01</td>\n",
       "      <td>9.373961e-02</td>\n",
       "      <td>1.594801e-78</td>\n",
       "      <td>0.219298</td>\n",
       "      <td>992</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>38.888889</td>\n",
       "      <td>Abs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44162</th>\n",
       "      <td>this the second thule bag own and absolutely l...</td>\n",
       "      <td>['thing', 'thin']</td>\n",
       "      <td>great design and make like so many other thule...</td>\n",
       "      <td>thule bag of own and absolutely love both of them</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.104685</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.670943e-155</td>\n",
       "      <td>5.231030e-204</td>\n",
       "      <td>1.218332e-231</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>991</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>Ext</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44163</th>\n",
       "      <td>was originally look for dermabenss benzoyl per...</td>\n",
       "      <td>['product', 'spray']</td>\n",
       "      <td>must have in your dealing with my dog skin iss...</td>\n",
       "      <td>highly recommend it our dog suffer from season...</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.091575</td>\n",
       "      <td>0.079615</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079615</td>\n",
       "      <td>3.562756e-155</td>\n",
       "      <td>3.087326e-204</td>\n",
       "      <td>7.536728e-232</td>\n",
       "      <td>0.085470</td>\n",
       "      <td>990</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>Abs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44164</th>\n",
       "      <td>revise downward after ton issue after use new ...</td>\n",
       "      <td>['day', 'wider']</td>\n",
       "      <td>bad ghost and pulsate on great color and hdr w...</td>\n",
       "      <td>the mitsubishi diamondvision dlp set</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>989</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>52.941176</td>\n",
       "      <td>Ext</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "44160  positive fit all requirement for indoor camera...   \n",
       "44161  return the device fail miserably set and forge...   \n",
       "44162  this the second thule bag own and absolutely l...   \n",
       "44163  was originally look for dermabenss benzoyl per...   \n",
       "44164  revise downward after ton issue after use new ...   \n",
       "\n",
       "                      keywords  \\\n",
       "44160       ['fit', 'generic']   \n",
       "44161  ['review', 'recording']   \n",
       "44162        ['thing', 'thin']   \n",
       "44163     ['product', 'spray']   \n",
       "44164         ['day', 'wider']   \n",
       "\n",
       "                                               reference  \\\n",
       "44160  for those of us who do not like instal more ap...   \n",
       "44161  not useable for set and forget pvr require con...   \n",
       "44162  great design and make like so many other thule...   \n",
       "44163  must have in your dealing with my dog skin iss...   \n",
       "44164  bad ghost and pulsate on great color and hdr w...   \n",
       "\n",
       "                                                 decoded   rouge_1  rouge_2  \\\n",
       "44160                  great indoor camera for the price  0.090909     0.00   \n",
       "44161     device fail to miserably set up and forget pvr  0.296296     0.16   \n",
       "44162  thule bag of own and absolutely love both of them  0.210526     0.00   \n",
       "44163  highly recommend it our dog suffer from season...  0.095238     0.00   \n",
       "44164               the mitsubishi diamondvision dlp set  0.000000     0.00   \n",
       "\n",
       "        rouge_l  self_Bleu_1    self_Bleu_2    self_Bleu_3    ...     \\\n",
       "44160  0.067718     0.031479   0.000000e+00   0.000000e+00    ...      \n",
       "44161  0.246914     0.163502   9.196986e-02   5.255421e-02    ...      \n",
       "44162  0.104685     0.200000  2.225074e-308  2.225074e-308    ...      \n",
       "44163  0.091575     0.079615   0.000000e+00   0.000000e+00    ...      \n",
       "44164  0.000000     0.000000   0.000000e+00   0.000000e+00    ...      \n",
       "\n",
       "         Bleu_1         Bleu_2         Bleu_3         Bleu_4    Meteor  \\\n",
       "44160  0.031479  1.150198e-155  9.303220e-205  2.198601e-232  0.033333   \n",
       "44161  0.163502   1.226265e-01   9.373961e-02   1.594801e-78  0.219298   \n",
       "44162  0.200000  6.670943e-155  5.231030e-204  1.218332e-231  0.100000   \n",
       "44163  0.079615  3.562756e-155  3.087326e-204  7.536728e-232  0.085470   \n",
       "44164  0.000000   0.000000e+00   0.000000e+00   0.000000e+00  0.000000   \n",
       "\n",
       "       article_lens  ref_lens  overlap  overlap_percent  gen_type  \n",
       "44160           993        16       12        75.000000       Ext  \n",
       "44161           992        18        7        38.888889       Abs  \n",
       "44162           991        10        6        60.000000       Ext  \n",
       "44163           990        12        6        50.000000       Abs  \n",
       "44164           989        17        9        52.941176       Ext  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_outFrame.head()\n",
    "test_outFrame.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
