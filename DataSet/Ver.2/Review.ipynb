{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make data dict from category1 : Electronics category2 : Cameras\n",
      "Connect to MongoDB\n",
      "conn_mongo -- uri: mongodb://root:1234@localhost:27017/admin?authMechanism=SCRAM-SHA-1\n",
      "Auth :  True\n",
      "Connect to db : Amazon \n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# NEW_PROD_DICT \n",
    "from data_util.product import *\n",
    "from data_util.MongoDB import *\n",
    "from data_util.stopwords import *\n",
    "from data_util.preprocess import *\n",
    "#%%\n",
    "import spacy\n",
    "# gpu = spacy.prefer_gpu()\n",
    "# print('GPU:', gpu)\n",
    "# pip install -U spacy[cuda100]\n",
    "# python -m spacy validate\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "import re\n",
    "# from stopwords import *\n",
    "import nltk\n",
    "# from preprocess import *\n",
    "# from feature import *\n",
    "\n",
    "from textblob import TextBlob\n",
    "import collections\n",
    "\n",
    "from spacy.symbols import cop, acomp, amod, conj, neg, nn, nsubj, dobj\n",
    "from spacy.symbols import VERB, NOUN, PROPN, ADJ, ADV, AUX, PART\n",
    "\n",
    "pattern_counter = collections.Counter()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "# from MongoDB import MongoDB\n",
    "import os\n",
    "\n",
    "import dateutil.parser as parser\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Spec From Mongo\n",
    "# from product import *\n",
    "# mongoObj = DVD_Player()\n",
    "mongoObj = Cameras()\n",
    "# mongoObj = Cell_Phones()\n",
    "# mongoObj = GPS()\n",
    "# mongoObj = Keyboards()\n",
    "#--------------------------------#\n",
    "# mongoObj = Home_Kitchen()\n",
    "# mongoObj = Cloth_Shoes_Jewelry()\n",
    "# mongoObj = Grocery_Gourmet_Food()\n",
    "# mongoObj = Automotive()\n",
    "# mongoObj = Toys_Games()\n",
    "#--------------------------------#\n",
    "'''\n",
    "cd D:\\WorkSpace\\JupyterWorkSpace\\Text-Summarizer-BERT2\\\n",
    "D:\n",
    "activate tensorflow\n",
    "python makeDataDict.py\n",
    "'''\n",
    "main_cat, category1, category2, cond_date = mongoObj.getAttr()\n",
    "print(\"make data dict from category1 : %s category2 : %s\" % (category1, category2))\n",
    "\n",
    "# Connect MongoDB\n",
    "print(\"Connect to MongoDB\")\n",
    "mongo = MongoDB()\n",
    "mongo.conn_db(db_name='Amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer \n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "import re\n",
    "from data_util.stopwords import *\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def remove_word5(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    for k, v in contractions.items():\n",
    "        if k in text:\n",
    "            text = text.replace(k, v)\n",
    "\n",
    "    for k in html_escape_table:\n",
    "        if k in text:\n",
    "            text = text.replace(k, \"\")\n",
    "\n",
    "    text = text.replace(\"-\", '')   \n",
    "    text = text.replace(\"\\\"\", '')\n",
    "    text = text.replace(\"\\n\", '')  \n",
    "    text = text.replace('\" ', '')\n",
    "    text = text.replace(' \"', '')\n",
    "\n",
    "    remove_chars = '[\"#$%&\\'\\\"\\()*+:<=>@★【】《》“”‘’[\\\\]^_`{|}~]+'\n",
    "    text = re.sub(remove_chars, \"\", text)  # remove number and segment\n",
    "\n",
    "    text = text.replace(\"\\\\\", '')\n",
    "    text = text.replace(\"/\", '')\n",
    "    return text\n",
    "\n",
    "def process(text):\n",
    "    if type(text) == list: \n",
    "        if len(text) == 1 : text = text[0]\n",
    "        else: text = \"\\n\".join(text)\n",
    "    text = remove_tags(text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = remove_word5(text)\n",
    "    \n",
    "#     dash_words = []\n",
    "#     pattern = re.compile(r\"([\\d\\w\\.-]+[-'//.][\\d\\w\\.-]+)\")  # |  ([\\(](\\w)+[\\)])\n",
    "#     dash_words.extend(pattern.findall(text))\n",
    "\n",
    "#     for sym in dash_words:    \n",
    "#         text_sym = sym.replace('-','_').replace('.','_')\n",
    "#         text = text.replace(sym,text_sym)    \n",
    "    \n",
    "    text = [line.strip() for line in text.split(\"\\n\") if line != '']\n",
    "    text = \"\\n\".join(text)\n",
    "    \n",
    "    text = \" \".join(bert_tokenizer.tokenize(text))\n",
    "    text = text.replace(\" ##\",\"\")\n",
    "    text = text.split(\" . \")\n",
    "    text = [line.replace(\" .\",\"\") for line in text]\n",
    "    text = [line for line in text if len(line) > 0]\n",
    "\n",
    "    return text\n",
    "\n",
    "rev = \"\"\"\n",
    "<a data-hook=\"product-link-linked\" class=\"a-link-normal\" href=\"/Crocheting-For-Dummies/dp/B001C4PKLW/ref=cm_cr_arp_d_rvw_txt?ie=UTF8\">Crocheting For Dummies</a>Though the sample version for the Kindle included the entire table of contents, there were no images in the sample.  How can you tell if it's a good illustration if you don't get a sample of an illustration?  Thanks!\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "斷詞辭典 已取得\n",
      "negative-words.txt\n",
      "positive-words.txt\n",
      "total-words 已取得\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'crochet for dummiesthough the sample version for the kindle include the entire table of content , there were no image in the sample how can you tell if it is a good illustration if you do not get a sample of an illustration ?thanks !'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_util.stopwords import *\n",
    "from data_util.preprocess import *\n",
    "from data_util.eda import *\n",
    "\n",
    "def lemm_keyword(text):\n",
    "    text_keywords = []\n",
    "    lemm_sents = []\n",
    "    for line in text:\n",
    "        lemm_text = lemm_sent_process4(line, remove_stopwords=False, summary=False, mode=\"spacy\",withdot=False)\n",
    "        lemm_text = lemm_text.replace(\"\\n\",'')\n",
    "        text_keywords2 = PF_rule_POS(lemm_text).run()\n",
    "        lemm_sents.append(lemm_text)\n",
    "        text_keywords.extend(text_keywords2)\n",
    "    lemm_sents = \" \".join(lemm_sents)\n",
    "    return lemm_sents, text_keywords\n",
    "\n",
    "rev = process(rev)\n",
    "rev\n",
    "lemm_keyword(rev)[0]\n",
    "\n",
    "# summ = \"\"\"\n",
    "# New hobby 4 a seventy-four year old man, Amazon.com\n",
    "# \"\"\"\n",
    "\n",
    "# def lemm_summ(text):\n",
    "#     text_keywords = []\n",
    "#     lemm_sents = []\n",
    "#     for line in text:\n",
    "#         lemm_text = lemm_sent_process4(line, remove_stopwords=False, summary=False, mode=\"spacy\",withdot=False)\n",
    "#         lemm_text = lemm_text.replace(\"\\n\",'')\n",
    "#         lemm_sents.append(lemm_text)\n",
    "#     lemm_sents = \" \".join(lemm_sents)\n",
    "#     return lemm_sents\n",
    "\n",
    "# summ = process(summ)\n",
    "# summ\n",
    "# lemm_summ(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make product reviews feature from 194194 reviews...\n",
      "Search reviews finished...\n"
     ]
    }
   ],
   "source": [
    "def loadProdReviewData():\n",
    "    global category1, category2, main_cat, cond_date\n",
    "    rev_db_col = 'new_reviews2'\n",
    "    review_cursor = mongo.searchInDB(mongoObj.getReviewKey(), db_col=rev_db_col)\n",
    "    docCount = review_cursor.count()\n",
    "    print(\"make product reviews feature from %s reviews...\" % (docCount))\n",
    "    print(\"Search reviews finished...\")  \n",
    "    return review_cursor , docCount    \n",
    "\n",
    "review_cursor , docCount = loadProdReviewData()\n",
    "nlp = en_core_web_sm.load()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11255/194194 [42:36<14:07:36,  3.60it/s]"
     ]
    }
   ],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "# fn = 'Cameras/prod_keywords.txt'\n",
    "# print('load %s keywords...' % (fn))\n",
    "# total_keywords = set()\n",
    "# feature_counter = Counter()\n",
    "# with open(fn, 'r', encoding='utf-8') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         k, v = line.split(\":\")\n",
    "#         total_keywords.add(k)\n",
    "            \n",
    "asin_list, review_list, overall_list, vote_list, summary_list, review_ID_list , cheat_num_list = \\\n",
    "[] , [] , [] , [] , [] , [] , []\n",
    "\n",
    "with tqdm(total=docCount) as pbar:\n",
    "    for i2, rev in enumerate(review_cursor):\n",
    "        asin, review, overall, vote, summary, review_ID = \\\n",
    "            rev[\"asin\"], rev[\"reviewText\"], rev['overall'], rev['vote'], rev['summary'], str(rev['unixReviewTime'])       \n",
    "        try:            \n",
    "            review = process(review)\n",
    "            lemm_review , rev_keywords = lemm_keyword(review)\n",
    "            feature_counter.update(rev_keywords)\n",
    "            lemm_review_len = len(lemm_review.split(\" \"))\n",
    "        except Exception as e :\n",
    "            print(e)   \n",
    "        # ---------------------------------------------------------------------------------------------\n",
    "        try:            \n",
    "            summary = process(summary)\n",
    "            lemm_summary = lemm_summ(summary)\n",
    "            lemm_summary = '<s> ' + lemm_summary + \" </s>\"\n",
    "            lemm_summary_len = len(lemm_summary.split(\" \"))\n",
    "        except Exception as e :\n",
    "            print(e)           \n",
    "        # ----------------------------------------------------------------------------------------------\n",
    "        rev_token_set = set(lemm_review.split(\" \"))\n",
    "        summ_token_set = set(lemm_summary.split(\" \"))\n",
    "        # if (rev_token_set & summ_token_set) < 10: continue # for cheat data set\n",
    "#         if len(summ_token_set & total_keywords) == 0: continue\n",
    "        cheat_num = len(rev_token_set & summ_token_set) \n",
    "        # ----------------------------------------------------------------------------------------------\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\"%s training-pair \" % (category2))\n",
    "        \n",
    "        asin_list.append(asin)\n",
    "        review_list.append(lemm_review)\n",
    "        overall_list.append(overall)\n",
    "        vote_list.append(vote)\n",
    "        summary_list.append(lemm_summary)\n",
    "        review_ID_list.append(review_ID)\n",
    "        cheat_num_list.append(cheat_num)        \n",
    "        \n",
    "    df = pd.DataFrame({\"asin\":asin_list, \"review\": review_list, \"overall\": overall_list, \"vote\": vote_list,\n",
    "                        \"summary\": summary_list , \"review_ID\": review_ID_list, \"cheat_num\": cheat_num_list})\n",
    "\n",
    "if not os.path.exists(category2):\n",
    "    os.makedirs(category2)\n",
    "\n",
    "csv_path = '%s/review.xlsx'%(category2)     \n",
    "df.head()\n",
    "df.to_excel(csv_path, encoding='utf8')\n",
    "print(csv_path + \" Write finished\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = OrderedDict(sorted(feature_counter.items(), key=lambda pair: pair[1], reverse=True))\n",
    "important_features = [(word, important_features[word]) for word in important_features if important_features[word] > 0]\n",
    "print(\"Count : %s\" % (len(important_features)))\n",
    "\n",
    "fn3 = '%s/review_keywords.txt'%(category2)\n",
    "with open(fn3, 'w', encoding=\"utf-8\") as f:\n",
    "    total_keywords = set()\n",
    "    for word, v in important_features:\n",
    "        f.write(\"%s:%s \\n\" % (word, v))\n",
    "        total_keywords.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script Review.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
