{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 02:31:51.582449 140501341763392 file_utils.py:35] PyTorch version 1.3.1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  30526 bert tokens now\n",
      "We have added 3 XL tokens\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from bert_lstm import Model\n",
    "\n",
    "\n",
    "from data_util import config\n",
    "from data_util import bert_data as data\n",
    "from data_util.bert_batcher import Batcher\n",
    "from data_util.bert_data import Vocab\n",
    "from write_result import *\n",
    "# from data_util import data\n",
    "# from data_util.batcher import Batcher\n",
    "# from data_util.data import Vocab\n",
    "\n",
    "\n",
    "from train_bert_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse\n",
    "import torchsnooper\n",
    "import logging\n",
    "transformers_logger = logging.getLogger(\"transformers.tokenization_utils\")\n",
    "transformers_logger.setLevel(logging.ERROR)\n",
    "transformers_logger.disabled = True\n",
    "\n",
    "# -------- Test Packages -------\n",
    "from bert_enc_beam_search import *\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# from pytorch_pretrained_bert import BertModel\n",
    "from transformers import BertModel, BertTokenizer \n",
    "from transformers import TransfoXLTokenizer, TransfoXLModel, TransfoXLConfig\n",
    "\n",
    "config.batch_size = 2\n",
    "config.emb_dim = 768\n",
    "\n",
    "# config.hidden_dim = 512\n",
    "config.hidden_dim = 768\n",
    "\n",
    "config.max_enc_steps = 500\n",
    "config.lr = 0.001 # 0.001\n",
    "\n",
    "# config.keywords = \"TextRank_keywords\"\n",
    "# config.max_key_num = 8\n",
    "\n",
    "config.intra_encoder = False\n",
    "config.intra_decoder = False\n",
    "# config.data_type = 'Cameras'\n",
    "\n",
    "config.max_dec_steps = 60\t\t#99% of the titles are within length 15\n",
    "config.min_dec_steps= 8\n",
    "config.ber_layer = 11 # last_layer [0~11]\n",
    "config.key_attention = False\n",
    "# help(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## batch_size : 2\n",
      "## beam_size : 16\n",
      "## ber_layer : 11\n",
      "## data_type : Cameras_new5\n",
      "## emb_dim : 768\n",
      "## eps : 1e-12\n",
      "## gound_truth_prob : 0.1\n",
      "## hidden_dim : 768\n",
      "## intra_decoder : False\n",
      "## intra_encoder : False\n",
      "## key_attention : False\n",
      "## keywords : POS_FOP_keywords\n",
      "## loggerName : Text-Summary\n",
      "## lr : 0.001\n",
      "## max_dec_steps : 60\n",
      "## max_enc_steps : 500\n",
      "## max_epochs : 100\n",
      "## max_iterations : 500000\n",
      "## max_key_num : 8\n",
      "## min_dec_steps : 8\n",
      "## rand_unif_init_mag : 0.02\n",
      "## trunc_norm_init_std : 0.0001\n",
      "## vocab_size : 100000\n",
      "## word_emb_type : word2Vec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_str = ''\n",
    "for a in dir(config):\n",
    "    if type(getattr(config, a)) in [str,int,float,bool] \\\n",
    "    and 'path' not in str(a) \\\n",
    "    and '__' not in str(a) \\\n",
    "    and 'info' not in str(a):\n",
    "\n",
    "        info_str += '## %s : %s\\n'%(a,getattr(config, a))\n",
    "\n",
    "# [print(a,getattr(config, a)) for a in dir(config)\n",
    "# if type(getattr(config, a)) in [str,int,float]\n",
    "#  and 'path' not in str(a)\n",
    "#  and '__' not in str(a)\n",
    "#  and 'info' not in str(a)\n",
    "# ]\n",
    "print(info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def getLogger(loggerName, loggerPath):\n",
    "    # 設置logger\n",
    "    logger = logging.getLogger(loggerName)  # 不加名稱設置root logger\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s: - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    logging.Filter(loggerName)\n",
    "\n",
    "    # 使用FileHandler輸出到文件\n",
    "    directory = os.path.dirname(loggerPath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fh = logging.FileHandler(loggerPath)\n",
    "\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # 使用StreamHandler輸出到屏幕\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    ch.setFormatter(formatter)\n",
    "    # 添加兩個Handler\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    # Handler只啟動一次\n",
    "    # 設置logger\n",
    "    logger.info(u'logger已啟動')\n",
    "    return logger\n",
    "\n",
    "def removeLogger(logger):\n",
    "    logger.info(u'logger已關閉')\n",
    "    handlers = logger.handlers[:]\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_words :  ['battery', 'auxiliary', 'battery', 'backup', 'underwater', 'amazing']\n",
      "key_words :  ['action', 'affordable', 'screen', 'lcd', 'clip', 'small']\n"
     ]
    }
   ],
   "source": [
    "def test_batch():\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    # with torchsnooper.snoop():\n",
    "    while batch is not None:\n",
    "        example_list = batch.example_list\n",
    "        for ex in example_list:\n",
    "            r = str(ex.original_review)\n",
    "            s = str(ex.original_summary)\n",
    "            k = str(ex.key_words)\n",
    "            sent = ex.original_summary_sents\n",
    "#             print(\"original_review_sents:\", r)\n",
    "#             print(\"original_summary_sents : \", s)\n",
    "            print(\"key_words : \", k)\n",
    "#             print('------------------------------------------------------------\\n')\n",
    "        batch = batcher.next_batch()        \n",
    "        break\n",
    "test_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bin Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 16497\n",
      "\n",
      "test : 2061\n",
      "\n",
      "valid : 2062\n",
      "\n"
     ]
    }
   ],
   "source": [
    " with open(config.bin_info,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    [print(line) for line in lines]\n",
    "    train_num = int(lines[0].split(\":\")[1])\n",
    "    test_num = int(lines[1].split(\":\")[1])\n",
    "    val_num = int(lines[2].split(\":\")[1])\n",
    "    # f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    # f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    # f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View model summary\n",
    "#### 只有torchsummaryX成功\n",
    "#### 日後將以此模擬呈現結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 02:31:57.557062 140501341763392 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/eagleuser/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0317 02:31:57.637035 140501341763392 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0317 02:31:59.331117 140501341763392 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/eagleuser/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenizer) 30526\n",
      "test success\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model2 import Encoder,Model\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "encoder = Encoder().to(device)    \n",
    "\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "     \n",
    "# bert_model = TransfoXLModel(xl_config) # 更改參數以傳入TransfoXLModel\n",
    "# bert_model = get_cuda(TransfoXLModel.from_pretrained('transfo-xl-wt103'))\n",
    "# all_hidden_states, _ = bert_model(enc_batch)[-2:]\n",
    "   \n",
    "# summary(encoder, enc_batch, enc_padding_mask, enc_lens) # encoder summary (ok)\n",
    "\n",
    "print('test success')\n",
    "# model = Model(False,'word2Vec',vocab)\n",
    "# enc_out, enc_hidden = model.encoder(enc_batch, enc_padding_mask, enc_lens) # encoder summary (ok)\n",
    "# print(enc_hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 02:32:10.222320 140501341763392 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/eagleuser/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0317 02:32:10.256061 140501341763392 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0317 02:32:11.913739 140501341763392 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/eagleuser/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenizer) 30526\n",
      "decoder time step 0\n",
      "=================================================================================\n",
      "                             Kernel Shape    Output Shape     Params  \\\n",
      "Layer                                                                  \n",
      "0_x_context                   [1536, 768]        [2, 768]  1.180416M   \n",
      "1_lstm                                  -        [2, 768]  4.724736M   \n",
      "2_enc_attention.Linear_W_h    [768, 1536]  [2, 415, 1536]  1.179648M   \n",
      "3_enc_attention.Linear_W_s   [1536, 1536]       [2, 1536]  2.360832M   \n",
      "4_enc_attention.Linear_v        [1536, 1]     [2, 415, 1]     1.536k   \n",
      "5_dec_attention                         -        [2, 768]          -   \n",
      "6_p_gen_linear                  [3840, 1]          [2, 1]     3.841k   \n",
      "7_V                           [2304, 768]        [2, 768]   1.77024M   \n",
      "8_V1                        [768, 100000]     [2, 100000]      76.9M   \n",
      "\n",
      "                            Mult-Adds  \n",
      "Layer                                  \n",
      "0_x_context                 1.179648M  \n",
      "1_lstm                      4.718592M  \n",
      "2_enc_attention.Linear_W_h  1.179648M  \n",
      "3_enc_attention.Linear_W_s  2.359296M  \n",
      "4_enc_attention.Linear_v       1.536k  \n",
      "5_dec_attention                     -  \n",
      "6_p_gen_linear                  3.84k  \n",
      "7_V                         1.769472M  \n",
      "8_V1                            76.8M  \n",
      "---------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params          88.121249M\n",
      "Trainable params      88.121249M\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             88.012032M\n",
      "=================================================================================\n",
      "finish inner loop\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model2 import Decoder,Model\n",
    "from train_bert_util import *\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# decoder = Decoder().to(device)    \n",
    "\n",
    "model = Model(False,'word2Vec',vocab)\n",
    "# vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "# batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "#                        batch_size=config.batch_size, single_pass=False)\n",
    "# batch = batcher.next_batch()\n",
    "# enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "# enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "# encoder = Encoder().to(device)   \n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# print('enc_batch',enc_batch.shape)\n",
    "# print(enc_batch[0][:5])\n",
    "\n",
    "enc_out, enc_hidden = model.encoder(enc_batch, enc_padding_mask, enc_lens) # encoder summary (ok)\n",
    "# print('enc_out',enc_out.shape)\n",
    "# print('enc_hidden',enc_hidden.shape)\n",
    "# print('encoder success')  \n",
    "\n",
    "start_decoding = vocab.word2id(bert_data.START_DECODING) # start_decoding = 30524\n",
    "stop_decoding = vocab.word2id(bert_data.STOP_DECODING) # stop_decoding = 30525\n",
    "    \n",
    "# train_batch_MLE\n",
    "dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "step_losses = []\n",
    "# s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "s_t = (enc_hidden,enc_hidden)\n",
    "# x_t 為decoder每一個time step 的batch input\n",
    "x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(start_decoding)) # initial batch decode word                            #Input to the decoder\n",
    "prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "sum_temporal_srcs = None     \n",
    "unk_id = vocab.word2id(data.PAD_TOKEN)\n",
    "\n",
    "# print('x_t',x_t.shape)\n",
    "# print(vocab.vocab)     \n",
    "# print('enc_hidden',enc_hidden.shape)\n",
    "\n",
    "# s_t = s_t[:,0,:]\n",
    "for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "    print('decoder time step %s'%t)\n",
    "#     T.rand(len(enc_out)) tensor([0.6797, 0.7603])\n",
    "    use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "    # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "    # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "    x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities   \n",
    "    x_t, _ = model.encoder(x_t, None, None) # encoder summary (ok)\n",
    "#     enc_key_batch = model.embeds(enc_key_batch)  \n",
    "\n",
    "    # use the output vector corresponding to [CLS] to initialize the hidden state and cell state of LSTM decoder\n",
    "#     print('s_t1',s_t[0].shape);print('s_t1',s_t[1].shape)\n",
    "    \n",
    "    final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "    x_t, s_t, enc_out, enc_padding_mask, context, \n",
    "    extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "    None, None)    \n",
    "#     print('s_t2',s_t[0].shape);print('s_t2',s_t[1].shape)\n",
    "    target = target_batch[:, t]\n",
    "    log_probs = T.log(final_dist + config.eps)\n",
    "    step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=unk_id)\n",
    "    step_losses.append(step_loss)\n",
    "#     x_t = T.multinomial(final_dist,1).squeeze()  # Sample words from final distribution which can be used as input in next time step\n",
    "    \n",
    "#     is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "#     x_t = (1 - is_oov) * x_t.detach() + (is_oov) * unk_id  # Replace OOVs with [UNK] token\n",
    "  \n",
    "#     print('s_t',s_t.shape)\n",
    "    decoder_summary = summary(model.decoder, x_t, s_t, enc_out, \n",
    "                              enc_padding_mask, context, extra_zeros,\n",
    "                              enc_batch_extend_vocab, \n",
    "                              sum_temporal_srcs, prev_s,None, \n",
    "                              enc_key_lens) # encoder summary\n",
    "    print('finish inner loop'); print('-------------------------------------------------\\n')\n",
    "    break\n",
    "# decoder_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "def write_enc_graph():\n",
    "    encoder_writer = SummaryWriter('runs/Pointer-Generator/word2Vec/Encoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    encoder = Encoder().to(device) \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "#     enc_batch = Variable(torch.rand(enc_batch.shape)).to(device) \n",
    "    enc_lens = torch.from_numpy(enc_lens).to(device) \n",
    "\n",
    "    encoder_writer.add_graph(encoder, (enc_batch, enc_lens), verbose=True)\n",
    "    encoder_writer.close()\n",
    "\n",
    "def write_dec_graph():\n",
    "    decoder_writer = SummaryWriter('runs/Pointer-Generator/word2Vec/Decoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    # decoder = Decoder().to(device)    \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    model = Model(False,'word2Vec',vocab)\n",
    "    \n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "#     enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "    # train_batch_MLE\n",
    "    dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "    step_losses = []\n",
    "    s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "    # x_t 為decoder每一個time step 的batch input\n",
    "    x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "    prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    sum_temporal_srcs = None     \n",
    "\n",
    "\n",
    "    for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "        use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "        # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "        # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "        if t == 0 :temp_batch = dec_batch[:, t]\n",
    "        x_t = use_gound_truth * temp_batch + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "#         x_t = model.embeds(x_t)\n",
    "    #     final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "        final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "        x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "        extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "        enc_key_batch, enc_key_lens)        \n",
    "\n",
    "\n",
    "        #         decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "#         x_t = Variable(torch.rand(x_t.shape)).to(device) \n",
    "        #             s_t = Variable(torch.rand(s_t.shape)).to(device)\n",
    "#         enc_out = Variable(torch.rand(enc_out.shape)).to(device)\n",
    "#         enc_padding_mask = Variable(torch.rand(enc_padding_mask.shape)).to(device,dtype=torch.long)\n",
    "#         context = Variable(torch.rand(context.shape)).to(device)\n",
    "#         extra_zeros = Variable(torch.rand(extra_zeros.shape)).to(device)\n",
    "#         enc_batch_extend_vocab = Variable(torch.rand(enc_batch_extend_vocab.shape)).to(device)\n",
    "        #             sum_temporal_srcs = Variable(torch.rand(sum_temporal_srcs.shape)).to(device)\n",
    "        #             prev_s = Variable(torch.rand(prev_s.shape)).to(device)\n",
    "#         enc_key_batch = Variable(torch.rand(enc_key_batch.shape)).to(device)\n",
    "        enc_key_lens = torch.from_numpy(enc_key_lens).to(device) \n",
    "        \n",
    "        decoder_writer.add_graph(model.decoder, \n",
    "                         (x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens), verbose=True)\n",
    "        decoder_writer.close()\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 02:32:30 - Text-Summary - INFO: - logger已啟動\n",
      "I0317 02:32:30.541391 140501341763392 <ipython-input-3-00f9962e7fdd>:30] logger已啟動\n",
      "2020-03-17 02:32:34 - Text-Summary - INFO: - ------Training Setting--------\n",
      "I0317 02:32:34.850296 140501341763392 bert_enc_lstm_dec_run.py:60] ------Training Setting--------\n",
      "2020-03-17 02:32:34 - Text-Summary - INFO: - Traing Type :Cameras_new5\n",
      "I0317 02:32:34.943624 140501341763392 bert_enc_lstm_dec_run.py:62] Traing Type :Cameras_new5\n",
      "2020-03-17 02:32:35 - Text-Summary - INFO: - Training mle: True, mle weight: 1.00\n",
      "I0317 02:32:35.096759 140501341763392 bert_enc_lstm_dec_run.py:64] Training mle: True, mle weight: 1.00\n",
      "2020-03-17 02:32:35 - Text-Summary - INFO: - use pre_train_word2Vec vocab_size 100000 \n",
      "\n",
      "I0317 02:32:35.271745 140501341763392 bert_enc_lstm_dec_run.py:71] use pre_train_word2Vec vocab_size 100000 \n",
      "\n",
      "2020-03-17 02:32:35 - Text-Summary - INFO: - intra_encoder: False intra_decoder: False \n",
      "\n",
      "I0317 02:32:35.474201 140501341763392 bert_enc_lstm_dec_run.py:76] intra_encoder: False intra_decoder: False \n",
      "\n",
      "I0317 02:32:43.258224 140501341763392 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/eagleuser/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0317 02:32:43.430212 140501341763392 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0317 02:32:45.784815 140501341763392 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/eagleuser/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenizer) 30526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 02:33:13 - Text-Summary - INFO: - ------Training START--------\n",
      "I0317 02:33:13.841950 140501341763392 bert_enc_lstm_dec_run.py:465] ------Training START--------\n",
      "2020-03-17 02:33:24 - Text-Summary - INFO: - iter: 0 train_rouge_l_f: 0.092 test_rouge_l_f: 0.033 \n",
      "\n",
      "I0317 02:33:24.792073 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 0 train_rouge_l_f: 0.092 test_rouge_l_f: 0.033 \n",
      "\n",
      "2020-03-17 02:52:42 - Text-Summary - INFO: - iter: 1000 train_rouge_l_f: 0.060 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 02:52:42.841036 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 1000 train_rouge_l_f: 0.060 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 03:11:19 - Text-Summary - INFO: - iter: 2000 train_rouge_l_f: 0.078 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 03:11:19.257725 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 2000 train_rouge_l_f: 0.078 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 03:30:48 - Text-Summary - INFO: - iter: 3000 train_rouge_l_f: 0.034 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 03:30:48.585386 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 3000 train_rouge_l_f: 0.034 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 03:50:33 - Text-Summary - INFO: - iter: 4000 train_rouge_l_f: 0.074 test_rouge_l_f: 0.076 \n",
      "\n",
      "I0317 03:50:33.171771 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 4000 train_rouge_l_f: 0.074 test_rouge_l_f: 0.076 \n",
      "\n",
      "2020-03-17 04:09:33 - Text-Summary - INFO: - iter: 5000 train_rouge_l_f: 0.038 test_rouge_l_f: 0.076 \n",
      "\n",
      "I0317 04:09:33.662648 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 5000 train_rouge_l_f: 0.038 test_rouge_l_f: 0.076 \n",
      "\n",
      "2020-03-17 04:29:11 - Text-Summary - INFO: - iter: 6000 train_rouge_l_f: 0.037 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 04:29:11.294770 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 6000 train_rouge_l_f: 0.037 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 04:49:31 - Text-Summary - INFO: - iter: 7000 train_rouge_l_f: 0.146 test_rouge_l_f: 0.075 \n",
      "\n",
      "I0317 04:49:31.112557 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 7000 train_rouge_l_f: 0.146 test_rouge_l_f: 0.075 \n",
      "\n",
      "2020-03-17 05:09:45 - Text-Summary - INFO: - iter: 8000 train_rouge_l_f: 0.000 test_rouge_l_f: 0.075 \n",
      "\n",
      "I0317 05:09:45.499106 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 8000 train_rouge_l_f: 0.000 test_rouge_l_f: 0.075 \n",
      "\n",
      "2020-03-17 05:30:16 - Text-Summary - INFO: - iter: 9000 train_rouge_l_f: 0.060 test_rouge_l_f: 0.034 \n",
      "\n",
      "I0317 05:30:16.977709 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 9000 train_rouge_l_f: 0.060 test_rouge_l_f: 0.034 \n",
      "\n",
      "2020-03-17 05:51:03 - Text-Summary - INFO: - iter: 10000 train_rouge_l_f: 0.000 test_rouge_l_f: 0.152 \n",
      "\n",
      "I0317 05:51:03.719579 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 10000 train_rouge_l_f: 0.000 test_rouge_l_f: 0.152 \n",
      "\n",
      "2020-03-17 06:11:51 - Text-Summary - INFO: - iter: 11000 train_rouge_l_f: 0.080 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 06:11:51.759454 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 11000 train_rouge_l_f: 0.080 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 06:32:03 - Text-Summary - INFO: - iter: 12000 train_rouge_l_f: 0.321 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 06:32:03.840960 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 12000 train_rouge_l_f: 0.321 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 06:51:08 - Text-Summary - INFO: - iter: 13000 train_rouge_l_f: 0.086 test_rouge_l_f: 0.075 \n",
      "\n",
      "I0317 06:51:08.989862 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 13000 train_rouge_l_f: 0.086 test_rouge_l_f: 0.075 \n",
      "\n",
      "2020-03-17 07:11:37 - Text-Summary - INFO: - iter: 14000 train_rouge_l_f: 0.185 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 07:11:37.699694 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 14000 train_rouge_l_f: 0.185 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 07:31:12 - Text-Summary - INFO: - iter: 15000 train_rouge_l_f: 0.093 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 07:31:12.974056 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 15000 train_rouge_l_f: 0.093 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 07:51:43 - Text-Summary - INFO: - iter: 16000 train_rouge_l_f: 0.237 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 07:51:43.826278 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 16000 train_rouge_l_f: 0.237 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 08:12:18 - Text-Summary - INFO: - iter: 17000 train_rouge_l_f: 0.045 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 08:12:18.408455 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 17000 train_rouge_l_f: 0.045 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 08:32:48 - Text-Summary - INFO: - iter: 18000 train_rouge_l_f: 0.035 test_rouge_l_f: 0.076 \n",
      "\n",
      "I0317 08:32:48.369027 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 18000 train_rouge_l_f: 0.035 test_rouge_l_f: 0.076 \n",
      "\n",
      "2020-03-17 08:53:19 - Text-Summary - INFO: - iter: 19000 train_rouge_l_f: 0.060 test_rouge_l_f: 0.116 \n",
      "\n",
      "I0317 08:53:19.233800 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 19000 train_rouge_l_f: 0.060 test_rouge_l_f: 0.116 \n",
      "\n",
      "2020-03-17 09:12:11 - Text-Summary - INFO: - iter: 20000 train_rouge_l_f: 0.000 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 09:12:11.633232 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 20000 train_rouge_l_f: 0.000 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 09:32:19 - Text-Summary - INFO: - iter: 21000 train_rouge_l_f: 0.138 test_rouge_l_f: 0.076 \n",
      "\n",
      "I0317 09:32:19.477590 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 21000 train_rouge_l_f: 0.138 test_rouge_l_f: 0.076 \n",
      "\n",
      "2020-03-17 09:53:04 - Text-Summary - INFO: - iter: 22000 train_rouge_l_f: 0.129 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 09:53:04.439935 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 22000 train_rouge_l_f: 0.129 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 10:13:10 - Text-Summary - INFO: - iter: 23000 train_rouge_l_f: 0.245 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 10:13:10.932360 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 23000 train_rouge_l_f: 0.245 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 10:33:41 - Text-Summary - INFO: - iter: 24000 train_rouge_l_f: 0.119 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 10:33:41.786919 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 24000 train_rouge_l_f: 0.119 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 10:54:09 - Text-Summary - INFO: - iter: 25000 train_rouge_l_f: 0.000 test_rouge_l_f: 0.116 \n",
      "\n",
      "I0317 10:54:09.716196 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 25000 train_rouge_l_f: 0.000 test_rouge_l_f: 0.116 \n",
      "\n",
      "2020-03-17 11:13:34 - Text-Summary - INFO: - iter: 26000 train_rouge_l_f: 0.048 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 11:13:34.566117 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 26000 train_rouge_l_f: 0.048 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 11:33:19 - Text-Summary - INFO: - iter: 27000 train_rouge_l_f: 0.082 test_rouge_l_f: 0.076 \n",
      "\n",
      "I0317 11:33:19.258817 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 27000 train_rouge_l_f: 0.082 test_rouge_l_f: 0.076 \n",
      "\n",
      "2020-03-17 11:54:01 - Text-Summary - INFO: - iter: 28000 train_rouge_l_f: 0.044 test_rouge_l_f: 0.075 \n",
      "\n",
      "I0317 11:54:01.019217 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 28000 train_rouge_l_f: 0.044 test_rouge_l_f: 0.075 \n",
      "\n",
      "2020-03-17 12:14:28 - Text-Summary - INFO: - iter: 29000 train_rouge_l_f: 0.089 test_rouge_l_f: 0.076 \n",
      "\n",
      "I0317 12:14:28.088808 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 29000 train_rouge_l_f: 0.089 test_rouge_l_f: 0.076 \n",
      "\n",
      "2020-03-17 12:35:04 - Text-Summary - INFO: - iter: 30000 train_rouge_l_f: 0.087 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 12:35:04.484098 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 30000 train_rouge_l_f: 0.087 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 12:55:22 - Text-Summary - INFO: - iter: 31000 train_rouge_l_f: 0.048 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 12:55:22.246232 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 31000 train_rouge_l_f: 0.048 test_rouge_l_f: 0.074 \n",
      "\n",
      "2020-03-17 13:14:42 - Text-Summary - INFO: - iter: 32000 train_rouge_l_f: 0.159 test_rouge_l_f: 0.074 \n",
      "\n",
      "I0317 13:14:42.336056 140501341763392 bert_enc_lstm_dec_run.py:416] iter: 32000 train_rouge_l_f: 0.159 test_rouge_l_f: 0.074 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/u012869752/article/details/72513141\n",
    "# 由于在jupyter notebook中，args不为空\n",
    "from glob import glob\n",
    "# nvidia-smi -pm 1\n",
    "from bert_enc_lstm_dec_run import *\n",
    "\n",
    "if __name__ == \"__main__\":   \n",
    "    try:\n",
    "        # --------------------------Training ----------------------------------\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--train_mle', type=bool, default=True)\n",
    "        parser.add_argument('--train_rl', type=bool, default=False)\n",
    "        parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "    #         parser.add_argument('--load_model', type=str, default='/0015000_3.29_0.00.tar')\n",
    "        parser.add_argument('--load_model', type=str, default=None)\n",
    "        parser.add_argument('--new_lr', type=float, default=None)\n",
    "        parser.add_argument('--multi_device', type=bool, default=True)\n",
    "        parser.add_argument('--view', type=bool, default=True)\n",
    "        parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "        parser.add_argument('--word_emb_type', type=str, default='word2Vec')\n",
    "        parser.add_argument('--train_action', type=bool, default=True)\n",
    "        opt = parser.parse_args(args=[])\n",
    "\n",
    "        today = dt.now()\n",
    "    #         loggerPath = \"LOG//%s-(%s_%s_%s)-(%s:%s:%s)\"%(opt.word_emb_type,\n",
    "    #                   today.year,today.month,today.day,\n",
    "    #                   today.hour,today.minute,today.second)\n",
    "    #         logger = getLogger(config.loggerName,loggerPath)   \n",
    "\n",
    "        loggerPath = \"LOG//%s\"%(opt.word_emb_type)\n",
    "        logger = getLogger(config.loggerName,loggerPath) \n",
    "\n",
    "        if opt.load_model == None:\n",
    "            shutil.rmtree('runs/Pointer-Generator/bert', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/bert/exp-4', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/bert/Eecoder', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/bert/Decoder', ignore_errors=True) # clear previous \n",
    "\n",
    "        writer = SummaryWriter('runs/Pointer-Generator/bert/exp-4')\n",
    "        writer.add_text('Train_Para/',info_str,0)\n",
    "    #         write_enc_graph()\n",
    "    #         write_dec_graph()\n",
    "\n",
    "        if opt.train_action: \n",
    "            train_action(opt, logger, writer, train_num)\n",
    "\n",
    "    except KeyError as e:\n",
    "        traceback = sys.exc_info()[2]\n",
    "        print(sys.exc_info())\n",
    "        print(traceback.tb_lineno)\n",
    "        print(e)\n",
    "    finally:\n",
    "        removeLogger(logger)\n",
    "        # export scalar data to JSON for external processing\n",
    "        # tensorboard --logdir /home/eagleuser/Users/leyan/Text-Summarizer-FOP/TensorBoard\n",
    "#         tensorboard --logdir ./runs\n",
    "#         if not os.path.exists('TensorBoard'): os.makedirs('TensorBoard')\n",
    "#         writer.export_scalars_to_json(\"TensorBoard/test.json\")\n",
    "        writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
