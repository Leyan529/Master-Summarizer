{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "I0227 02:50:57.223042  4168 file_utils.py:35] PyTorch version 0.4.1 available.\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I0227 02:51:03.122018  4168 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\user\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0227 02:51:04.053584  4168 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\user\\.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0227 02:51:04.054581  4168 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0227 02:51:04.968182  4168 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at C:\\Users\\user\\.cache\\torch\\transformers\\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "\n",
    "# Transformers has a unified API\n",
    "# for 10 transformer architectures and 30 pretrained weights.\n",
    "#          Model          | Tokenizer          | Pretrained weights shortcut\n",
    "MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),\n",
    "          (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),\n",
    "          (GPT2Model,       GPT2Tokenizer,       'gpt2'),\n",
    "          (CTRLModel,       CTRLTokenizer,       'ctrl'),\n",
    "          (TransfoXLModel,  TransfoXLTokenizer,  'transfo-xl-wt103'),\n",
    "          (XLNetModel,      XLNetTokenizer,      'xlnet-base-cased'),\n",
    "          (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
    "          (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased'),\n",
    "          (RobertaModel,    RobertaTokenizer,    'roberta-base'),\n",
    "          (XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-base'),\n",
    "         ]\n",
    "\n",
    "model_class, tokenizer_class, pretrained_weights = BertModel, BertTokenizer, 'bert-base-uncased'\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "# config = BertConfig(output_hidden_states=True, output_attentions=True)\n",
    "model = model_class.from_pretrained(pretrained_weights,\n",
    "                                    output_hidden_states=True,\n",
    "                                    output_attentions=True,\n",
    "                                   max_position_embeddings = 512)\n",
    "\n",
    "\n",
    "\n",
    "# bert_emb_model = BertEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0227 02:51:08.105601  4168 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\user\\.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0227 02:51:08.110582  4168 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0227 02:51:09.032651  4168 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at C:\\Users\\user\\.cache\\torch\\transformers\\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_ids = torch.tensor([tokenizer.encode(text)]) # 自己加符號了 \"[CLS]\" , \"[SEP]\"\n",
    "\n",
    "# print(input_ids.shape)\n",
    "# all_hidden_states, all_attentions = model(input_ids)[-2:]\n",
    "\n",
    "PreTrainmodel = BertForPreTraining.from_pretrained('bert-base-uncased',\n",
    "                                    output_hidden_states=True,\n",
    "                                    output_attentions=True,\n",
    "                                   max_position_embeddings = 512)\n",
    "\n",
    "prediction_scores, seq_relationship_scores , hidden_states , _ = PreTrainmodel(torch.tensor([tokenizer.encode('happy')]))\n",
    "len(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', 'are', 'the', 'creative', 'and', 'resource', '##ful', 'type', 'you', 'will', 'notice', 'the', 'sw', '##ive', '##l', 'inner', 'zoom', 'lens', 'is', 'useful', 'to', 'capture', 'photo', 'from', 'just', 'about', 'any', 'angle', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n['if', 'you', 'are', 'the', 'creative', 'and', 'resource', '##ful', 'type', 'you', 'will', 'notice', 'the', 'sw', '##ive', '##l', 'inner', 'zoom', 'lens', 'is', 'useful', 'to', 'capture', 'photo', 'from', 'just', 'about', 'any', 'angle', '.']\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'all of you who have own expensive camera will appreciate this priceless feeling . '\n",
    "text = 'if you are the creative and resourceful type you will notice the swivel inner zoom lens is useful to capture photo from just about any angle . '\n",
    "print(tokenizer.tokenize(text))\n",
    "\n",
    "'''\n",
    "['if', 'you', 'are', 'the', 'creative', 'and', 'resource', '##ful', 'type', 'you', 'will', 'notice', 'the', 'sw', '##ive', '##l', 'inner', 'zoom', 'lens', 'is', 'useful', 'to', 'capture', 'photo', 'from', 'just', 'about', 'any', 'angle', '.']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0227 02:51:12.514340  4168 tokenization_utils.py:552] Adding [start] to the vocabulary\n",
      "I0227 02:51:12.516333  4168 tokenization_utils.py:629] Assigning [START] to the bos_token key of the tokenizer\n",
      "I0227 02:51:12.517329  4168 tokenization_utils.py:552] Adding [stop] to the vocabulary\n",
      "I0227 02:51:12.519325  4168 tokenization_utils.py:629] Assigning [STOP] to the eos_token key of the tokenizer\n",
      "I0227 02:51:12.521319  4168 tokenization_utils.py:629] Assigning [UNK] to the unk_token key of the tokenizer\n",
      "I0227 02:51:12.524310  4168 tokenization_utils.py:552] Adding <s> to the vocabulary\n",
      "I0227 02:51:12.525308  4168 tokenization_utils.py:552] Adding </s> to the vocabulary\n",
      "I0227 02:51:12.526307  4168 tokenization_utils.py:552] Adding [START] to the vocabulary\n",
      "I0227 02:51:12.528300  4168 tokenization_utils.py:552] Adding [STOP] to the vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 4 tokens\n",
      "<s> 30524 <s>\n",
      "</s> 30525 </s>\n",
      "[START] 30526 [START]\n",
      "[STOP] 30527 [STOP]\n",
      "[UNK] 100 [UNK]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'bos_token':'[START]','eos_token':'[STOP]','unk_token':'[UNK]'})\n",
    "num_added_toks = tokenizer.add_tokens(['<s>', '</s>','[START]','[STOP]','[UNK]'])\n",
    "\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "a = tokenizer.convert_tokens_to_ids('<s>')\n",
    "print('<s>',a , tokenizer.convert_ids_to_tokens(a))\n",
    "b = tokenizer.convert_tokens_to_ids('</s>')\n",
    "print('</s>',b , tokenizer.convert_ids_to_tokens(b))\n",
    "c = tokenizer.convert_tokens_to_ids('[START]')\n",
    "print('[START]',c , tokenizer.convert_ids_to_tokens(c))\n",
    "\n",
    "d = tokenizer.convert_tokens_to_ids('[STOP]')\n",
    "print('[STOP]',d , tokenizer.convert_ids_to_tokens(d))\n",
    "\n",
    "e = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "print('[UNK]',e , tokenizer.convert_ids_to_tokens(e))\n",
    "# help(tokenizer.add_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'bananas', '.']\n",
      "['do', 'you', '?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(tokenizer.encode_plus)\n",
    "orig_text = 'I like bananas.'\n",
    "edit_text = 'Do you?'\n",
    "\n",
    "orig_tokens = tokenizer.tokenize(orig_text)\n",
    "edit_tokens = tokenizer.tokenize(edit_text)\n",
    "\n",
    "seqs = tokenizer.encode_plus(orig_tokens,\n",
    "                             edit_tokens,\n",
    "                             orig_tokens,\n",
    "                             return_attention_mask=True,\n",
    "                             return_tensors='pt',\n",
    "                             pad_to_max_length=5,\n",
    "                            max_length=20)\n",
    "\n",
    "print(orig_tokens)\n",
    "print(edit_tokens)\n",
    "seqs['token_type_ids']\n",
    "# orig_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method encode_plus in module transformers.tokenization_utils:\n",
      "\n",
      "encode_plus(text, text_pair=None, add_special_tokens=True, max_length=None, stride=0, truncation_strategy='longest_first', pad_to_max_length=False, return_tensors=None, return_token_type_ids=True, return_attention_mask=True, return_overflowing_tokens=False, return_special_tokens_mask=False, **kwargs) method of transformers.tokenization_bert.BertTokenizer instance\n",
      "    Returns a dictionary containing the encoded sequence or sequence pair and additional informations:\n",
      "    the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n",
      "    \n",
      "    Args:\n",
      "        text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      "            the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      "            method)\n",
      "        text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n",
      "            string using the `tokenize` method) or a list of integers (tokenized string ids using the\n",
      "            `convert_tokens_to_ids` method)\n",
      "        add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n",
      "            to their model.\n",
      "        max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n",
      "            If there are overflowing tokens, those will be added to the returned dictionary\n",
      "        stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n",
      "            from the main sequence returned. The value of this argument defines the number of additional tokens.\n",
      "        truncation_strategy: string selected in the following options:\n",
      "            - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
      "                starting from the longest one at each token (when there is a pair of input sequences)\n",
      "            - 'only_first': Only truncate the first sequence\n",
      "            - 'only_second': Only truncate the second sequence\n",
      "            - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
      "        pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n",
      "            padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n",
      "            The tokenizer padding sides are handled by the following strings:\n",
      "            - 'left': pads on the left of the sequences\n",
      "            - 'right': pads on the right of the sequences   \n",
      "            Defaults to False: no padding.\n",
      "        return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n",
      "            or PyTorch torch.Tensor instead of a list of python integers.\n",
      "        return_token_type_ids: (optional) Set to False to avoid returning token_type_ids (default True).\n",
      "        return_attention_mask: (optional) Set to False to avoir returning attention mask (default True)\n",
      "        return_overflowing_tokens: (optional) Set to True to return overflowing token information (default False).\n",
      "        return_special_tokens_mask: (optional) Set to True to return special tokens mask information (default False).\n",
      "        **kwargs: passed to the `self.tokenize()` method\n",
      "    \n",
      "    Return:\n",
      "        A Dictionary of shape::\n",
      "    \n",
      "            {\n",
      "                input_ids: list[int],\n",
      "                token_type_ids: list[int] if return_token_type_ids is True (default)\n",
      "                attention_mask: list[int] if return_attention_mask is True (default)\n",
      "                overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
      "                num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n",
      "                special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
      "            }\n",
      "    \n",
      "        With the fields:\n",
      "            ``input_ids``: list of token ids to be fed to a model\n",
      "            ``token_type_ids``: list of token type ids to be fed to a model\n",
      "            ``attention_mask``: list of indices specifying which tokens should be attended to by the model\n",
      "            ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n",
      "            ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n",
      "            ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n",
      "            tokens and 1 specifying sequence tokens.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.encode_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'here', 'is', 'some', 'text', 'to', 'en', '##code', '</s>']\n",
      "[101, 30524, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 30525, 102]\n"
     ]
    }
   ],
   "source": [
    "text = \"<s> Here is some text to encode </s>\"\n",
    "# Encode text\n",
    "input_ids = torch.tensor([tokenizer.encode(\"<s> Here is some text to encode </s>\", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "print(tokenizer.tokenize(\"<s> Here is some text to encode </s>\"))\n",
    "print(tokenizer.encode(\"<s> Here is some text to encode </s>\"))\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "['[start]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(101))\n",
    "print(tokenizer.convert_ids_to_tokens([30522]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2573, 2104, 2300, 1010, 2307, 3976, 2005, 3617, 102]\n",
      "[CLS] works under water , great price for digital [SEP]\n",
      "[101, 2573, 2104, 2300, 1010, 2307, 3976, 2005, 3617, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] works under water , great price for digital [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "text = \"works under water, great price for digital \"\n",
    "# Encode text\n",
    "enc_ids = tokenizer.encode(text, max_length=512,\n",
    "                               add_special_tokens=True,\n",
    "                               )  \n",
    "encod_str = \" \".join(tokenizer.convert_ids_to_tokens(enc_ids))\n",
    "print(enc_ids)\n",
    "print(encod_str)\n",
    "\n",
    "plus = tokenizer.encode_plus(encod_str, max_length=20, pad_to_max_length=True,\n",
    "                               add_special_tokens=False,return_attention_mask=True\n",
    "                               )  \n",
    "enc_ids2 ,attention_mask = plus['input_ids'], plus['attention_mask']\n",
    "encod_str2 = \" \".join(tokenizer.convert_ids_to_tokens(enc_ids2))\n",
    "print(enc_ids2)\n",
    "print(attention_mask)\n",
    "\n",
    "print(encod_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'wu', '##han', 'fall', '##ed', 'in', 'love', 'with', 'pneumonia', '.', '</s>']\n",
      "[101, 30524, 8814, 4819, 2991, 2098, 1999, 2293, 2007, 18583, 1012, 30525, 102]\n",
      "tensor([[  101, 30524,  8814,  4819,  2991,  2098,  1999,  2293,  2007, 18583,\n",
      "          1012, 30525,   102,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "text = \"<s> Wuhan falled in love with pneumonia . </s>\"\n",
    "# Encode text\n",
    "input_ids = tokenizer.encode(text, max_length=15,\n",
    "                               add_special_tokens=True, pad_to_max_length=True,\n",
    "                               return_tensors='pt')  \n",
    "\n",
    "print(tokenizer.tokenize(text))\n",
    "print(tokenizer.encode(text))\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0227 02:51:14.228383  4168 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\user\\.cache\\torch\\transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0227 02:51:15.197024  4168 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at C:\\Users\\user\\.cache\\torch\\transformers\\6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.fc076a4d5f1edf25ea3a2bd66e9f6f295dcd64c81dfef5b3f5a3eb2a82751ad1\n",
      "I0227 02:51:15.198021  4168 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0227 02:51:16.095619  4168 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at C:\\Users\\user\\.cache\\torch\\transformers\\54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n"
     ]
    }
   ],
   "source": [
    "# pretrained_weights = 'bert-base-uncased'\n",
    "pretrained_weights ='bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "\n",
    "# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\n",
    "BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n",
    "                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]\n",
    "\n",
    "model_class = BertForPreTraining\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "# model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "# Models can return full list of hidden-states & attentions weights at each layer\n",
    "model = model_class.from_pretrained(pretrained_weights,\n",
    "                                    output_hidden_states=True,\n",
    "                                    output_attentions=True,\n",
    "                                   max_position_embeddings = 512)\n",
    "\n",
    "# model = model_class.from_pretrained(output_hidden_states=True,\n",
    "#                                     output_attentions=True,\n",
    "#                                    max_position_embeddings = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 352])\n",
      "25\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
    "every thing work great except the bag . \n",
    "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
    "every thing work great except the bag . \n",
    "one of the front buckle was defective and would not close . \n",
    "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
    "you will basically need to buy a new camera bag anyways .\n",
    "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
    "every thing work great except the bag . \n",
    "one of the front buckle was defective and would not close . \n",
    "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
    "you will basically need to buy a new camera bag anyways .\n",
    "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
    "every thing work great except the bag . \n",
    "one of the front buckle was defective and would not close . \n",
    "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
    "you will basically need to buy a new camera bag anyways .\n",
    "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
    "every thing work great except the bag . \n",
    "one of the front buckle was defective and would not close . \n",
    "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
    "you will basically need to buy a new camera bag anyways .\n",
    "'''\n",
    "\n",
    "\n",
    "# text = '''\n",
    "# i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
    "# every thing work great except the bag . \n",
    "# one of the front buckle was defective and would not close . \n",
    "# so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
    "# you will basically need to buy a new camera bag anyways .\n",
    "# '''\n",
    "# text = \"[CLS]\" + text + \"[SEP]\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(text)]) # 自己加符號了 \"[CLS]\" , \"[SEP]\"\n",
    "\n",
    "print(input_ids.shape)\n",
    "all_hidden_states, all_attentions = model(input_ids)[-2:]\n",
    "print(len(list(all_hidden_states)))\n",
    "print(len(list(all_attentions)))\n",
    "# print(all_attentions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good',\n",
       " 'color',\n",
       " 'good',\n",
       " 'picture',\n",
       " 'quality',\n",
       " 'but',\n",
       " 'no',\n",
       " 'auto',\n",
       " 'focus',\n",
       " 'how',\n",
       " 'can',\n",
       " 'that',\n",
       " 'be']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'good color good picture quality but no auto focus how can that be'\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on tuple object:\n",
      "\n",
      "class tuple(object)\n",
      " |  tuple() -> empty tuple\n",
      " |  tuple(iterable) -> tuple initialized from iterable's items\n",
      " |  \n",
      " |  If the argument is a tuple, the return value is the same object.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  count(...)\n",
      " |      T.count(value) -> integer -- return number of occurrences of value\n",
      " |  \n",
      " |  index(...)\n",
      " |      T.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransfoXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0227 02:52:58.875157  4168 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-vocab.bin from cache at C:\\Users\\user\\.cache\\torch\\transformers\\b24cb708726fd43cbf1a382da9ed3908263e4fb8a156f9e0a4f45b7540c69caa.a6a9c41b856e5c31c9f125dd6a7ed4b833fbcefda148b627871d4171b25cffd1\n",
      "I0227 02:53:00.094725  4168 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-config.json from cache at C:\\Users\\user\\.cache\\torch\\transformers\\a6dfd6a3896b3ae4c1a3c5f26ff1f1827c26c15b679de9212a04060eaf1237df.4f34ea1bcf7fb8fa015d300b4847b2030f36e7d2c6e4c92075a244afa0cc3d67\n",
      "I0227 02:53:00.097718  4168 configuration_utils.py:199] Model config {\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"TransfoXLLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": 0,\n",
      "  \"clamp_len\": 1000,\n",
      "  \"cutoffs\": [\n",
      "    20000,\n",
      "    40000,\n",
      "    200000\n",
      "  ],\n",
      "  \"d_embed\": 1024,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 4096,\n",
      "  \"d_model\": 1024,\n",
      "  \"div_val\": 4,\n",
      "  \"dropatt\": 0.0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"ext_len\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"init\": \"normal\",\n",
      "  \"init_range\": 0.01,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"mem_len\": 1600,\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 18,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pre_lnorm\": false,\n",
      "  \"proj_init_std\": 0.01,\n",
      "  \"pruned_heads\": {},\n",
      "  \"same_length\": true,\n",
      "  \"sample_softmax\": -1,\n",
      "  \"tgt_len\": 128,\n",
      "  \"tie_projs\": [\n",
      "    false,\n",
      "    true,\n",
      "    true,\n",
      "    true\n",
      "  ],\n",
      "  \"tie_weight\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"untie_r\": true,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 267735\n",
      "}\n",
      "\n",
      "I0227 02:53:01.034163  4168 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin from cache at C:\\Users\\user\\.cache\\torch\\transformers\\12642ff7d0279757d8356bfd86a729d9697018a0c93ad042de1d0d2cc17fd57b.e9704971f27275ec067a00a67e6a5f0b05b4306b3f714a96e9f763d8fb612671\n",
      "I0227 02:53:10.559115  4168 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-config.json from cache at C:\\Users\\user\\.cache\\torch\\transformers\\a6dfd6a3896b3ae4c1a3c5f26ff1f1827c26c15b679de9212a04060eaf1237df.4f34ea1bcf7fb8fa015d300b4847b2030f36e7d2c6e4c92075a244afa0cc3d67\n",
      "I0227 02:53:10.560080  4168 configuration_utils.py:199] Model config {\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"TransfoXLLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": 0,\n",
      "  \"clamp_len\": 1000,\n",
      "  \"cutoffs\": [\n",
      "    20000,\n",
      "    40000,\n",
      "    200000\n",
      "  ],\n",
      "  \"d_embed\": 1024,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 4096,\n",
      "  \"d_model\": 1024,\n",
      "  \"div_val\": 4,\n",
      "  \"dropatt\": 0.0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"ext_len\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"init\": \"normal\",\n",
      "  \"init_range\": 0.01,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"mem_len\": 1600,\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 18,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pre_lnorm\": false,\n",
      "  \"proj_init_std\": 0.01,\n",
      "  \"pruned_heads\": {},\n",
      "  \"same_length\": true,\n",
      "  \"sample_softmax\": -1,\n",
      "  \"tgt_len\": 128,\n",
      "  \"tie_projs\": [\n",
      "    false,\n",
      "    true,\n",
      "    true,\n",
      "    true\n",
      "  ],\n",
      "  \"tie_weight\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"untie_r\": true,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 267735\n",
      "}\n",
      "\n",
      "I0227 02:53:11.477625  4168 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin from cache at C:\\Users\\user\\.cache\\torch\\transformers\\12642ff7d0279757d8356bfd86a729d9697018a0c93ad042de1d0d2cc17fd57b.e9704971f27275ec067a00a67e6a5f0b05b4306b3f714a96e9f763d8fb612671\n"
     ]
    }
   ],
   "source": [
    "# from pytorch_pretrained_bert import TransfoXLTokenizer, TransfoXLModel, TransfoXLLMHeadModel\n",
    "\n",
    "# from pytorch_pretrained_bert import TransfoXLConfig, TransfoXLCorpus\n",
    "\n",
    "from transformers import TransfoXLLMHeadModel\n",
    "'''\n",
    "TransfoXLLMHeadModel提供形狀為[batch_size，sequence_length，n_tokens]的對數概率輸出\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "model_class, tokenizer_class, pretrained_weights = TransfoXLModel, TransfoXLTokenizer, 'transfo-xl-wt103'\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
    "model = TransfoXLModel.from_pretrained('transfo-xl-wt103')\n",
    "head_model = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method convert_tokens_to_ids in module transformers.tokenization_utils:\n",
      "\n",
      "convert_tokens_to_ids(tokens) method of transformers.tokenization_transfo_xl.TransfoXLTokenizer instance\n",
      "    Converts a single token, or a sequence of tokens, (str/unicode) in a single integer id\n",
      "    (resp. a sequence of ids), using the vocabulary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.convert_tokens_to_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([522, 1024])\n",
      "torch.Size([1600, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "T = 522 # maximum length\n",
    "padded_tokens = tokens + ['[PAD]' for _ in range(T - len(tokens))]\n",
    "attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "\n",
    "#Converting everything to torch tensors before feeding them to bert_model\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "attn_mask = torch.tensor(attn_mask).unsqueeze(0) #Shape : [1, 12]\n",
    "\n",
    "last_hidden_states, mems = model(token_ids)[-2:]\n",
    "# mems. Can be used to speed up sequential decoding and attend to longer context.\n",
    "\n",
    "print(last_hidden_states[0].shape)\n",
    "#Out: torch.Size([1, 12, 768])\n",
    "print(mems[0].shape)\n",
    "#Out: torch.Size([1, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I have two dogs and one is a\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "\n",
    "# pred_score, mems = head_model(input_ids)[:2]\n",
    "word_dist, mems = head_model(token_ids)[:2]\n",
    "word_dist = word_dist[0,-1] # torch.Size([267735])\n",
    "next_word_id = torch.argmax(word_dist).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word = tokenizer.convert_ids_to_tokens([next_word_id])[0]\n",
    "next_word\n",
    "# help(head_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have two dogs and one is a male\n",
      "I have two dogs and one is a male named\n",
      "I have two dogs and one is a male named <unk>\n",
      "I have two dogs and one is a male named <unk> .\n",
      "I have two dogs and one is a male named <unk> . <eos>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "    \n",
    "    word_dist, mems = head_model(token_ids)[:2]\n",
    "    word_dist = word_dist[0,-1] # torch.Size([267735])\n",
    "    next_word_id = torch.argmax(word_dist).item()\n",
    "    next_word = tokenizer.convert_ids_to_tokens([next_word_id])[0]\n",
    "    text = text + \" \" + next_word\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0227 02:54:01.712908  4168 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json from cache at C:\\Users\\user\\.cache\\torch\\transformers\\4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
      "I0227 02:54:01.714901  4168 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt from cache at C:\\Users\\user\\.cache\\torch\\transformers\\0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
      "W0227 02:54:01.763771  4168 tokenization_openai.py:100] ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "I0227 02:54:02.931648  4168 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json from cache at C:\\Users\\user\\.cache\\torch\\transformers\\a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.1bb6b73eca45938a31700ee9b8193eee21734d2e12bb8098bd7cf94a82070c71\n",
      "I0227 02:54:02.932647  4168 configuration_utils.py:199] Model config {\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "I0227 02:54:03.857263  4168 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin from cache at C:\\Users\\user\\.cache\\torch\\transformers\\e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0227 02:54:07.611423  4168 tokenization_utils.py:552] Adding [START] to the vocabulary\n",
      "I0227 02:54:07.613420  4168 tokenization_utils.py:629] Assigning [START] to the bos_token key of the tokenizer\n",
      "I0227 02:54:07.614418  4168 tokenization_utils.py:552] Adding [STOP] to the vocabulary\n",
      "I0227 02:54:07.615414  4168 tokenization_utils.py:629] Assigning [STOP] to the eos_token key of the tokenizer\n",
      "I0227 02:54:07.616412  4168 tokenization_utils.py:552] Adding [UNK] to the vocabulary\n",
      "I0227 02:54:07.617408  4168 tokenization_utils.py:629] Assigning [UNK] to the unk_token key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 0 tokens\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e431c7ef9690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<s>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<s>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'</s>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'</s>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[1;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[0;32m   1227\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_id_to_token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1229\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1230\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'bos_token':'[START]','eos_token':'[STOP]','unk_token':'[UNK]'})\n",
    "num_added_toks = tokenizer.add_tokens(['<s>', '</s>','[START]','[STOP]','[UNK]'])\n",
    "\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "a = tokenizer.convert_tokens_to_ids('<s>')\n",
    "print('<s>',a , tokenizer.convert_ids_to_tokens(a))\n",
    "b = tokenizer.convert_tokens_to_ids('</s>')\n",
    "print('</s>',b , tokenizer.convert_ids_to_tokens(b))\n",
    "c = tokenizer.convert_tokens_to_ids('[START]')\n",
    "print('[START]',c , tokenizer.convert_ids_to_tokens(c))\n",
    "\n",
    "d = tokenizer.convert_tokens_to_ids('[STOP]')\n",
    "print('[STOP]',d , tokenizer.convert_ids_to_tokens(d))\n",
    "\n",
    "e = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "print('[UNK]',e , tokenizer.convert_ids_to_tokens(e))\n",
    "# help(tokenizer.add_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
