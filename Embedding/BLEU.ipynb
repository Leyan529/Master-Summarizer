{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = '<s> but my experience with this set up was bad </s> <s> wire everything up quickly to see if it </s>'\n",
    "candidate = '[UNK] but my experience with this set up was bad [UNK] [UNK] wire everything up quickly to see if it would work [UNK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = reference.replace(\"<s>\",\"\").replace(\"</s>\",\"\")\n",
    "candidate = candidate.replace(\"[UNK]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference = [['this', 'is', 'small', 'test']]\n",
    "# candidate = ['this', 'is', 'a', 'test']\n",
    "reference = reference.split()\n",
    "candidate = candidate.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sents = [\n",
    "    \"but my experience with this set up was bad\",\n",
    "    \"poor picture quality\"   \n",
    "]\n",
    "\n",
    "ref_sents = [\n",
    "    \"but my experience with this set up was bad wire everything up quickly to see if it\",\n",
    "    \"blurry photo and pixelate noise are the downfall of this camera\"   \n",
    "]\n",
    "\n",
    "# ref_sents  = [\n",
    "#     \"but my experience with this set up was bad\",\n",
    "#     \"poor picture quality\"   \n",
    "# ]\n",
    "\n",
    "# decoded_sents = [\n",
    "#     \"but my experience with this set up was bad wire everything up quickly to see if it\",\n",
    "#     \"blurry photo and pixelate noise are the downfall of this camera\"   \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_decode_sents = [decode.split(\" \") for decode in decoded_sents]\n",
    "bleu_ref_sents = [[ref.split(\" \")] for ref in ref_sents]\n",
    "\n",
    "# bleu_ref_sents = [decode.split(\" \") for decode in decoded_sents]\n",
    "# bleu_decode_sents = [[ref.split(\" \")] for ref in ref_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual BLEU-1: 0.197698\n",
      "Individual BLEU-2: 0.210878\n",
      "Individual BLEU-3: 0.230647\n",
      "Individual BLEU-4: 0.225940\n"
     ]
    }
   ],
   "source": [
    "# n-gram individual BLEU\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# reference = [['this', 'is', 'a', 'test']]\n",
    "# candidate = ['this', 'is', 'a', 'test']\n",
    "print('Individual BLEU-1: %f' % corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(1, 0, 0, 0)))\n",
    "print('Individual BLEU-2: %f' % corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0, 1, 0, 0)))\n",
    "print('Individual BLEU-3: %f' % corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0, 0, 1, 0)))\n",
    "print('Individual BLEU-4: %f' % corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative BLEU-1: 0.197698\n",
      "Cumulative BLEU-2: 0.204181\n",
      "Cumulative BLEU-3: 0.213105\n",
      "Cumulative BLEU-4: 0.215896\n"
     ]
    }
   ],
   "source": [
    "print('Cumulative BLEU-1: %f' % corpus_bleu(bleu_ref_sents, bleu_decode_sents, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative BLEU-2: %f' % corpus_bleu(bleu_ref_sents, bleu_decode_sents, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative BLEU-3: %f' % corpus_bleu(bleu_ref_sents, bleu_decode_sents, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative BLEU-4: %f' % corpus_bleu(bleu_ref_sents, bleu_decode_sents, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function corpus_bleu in module nltk.translate.bleu_score:\n",
      "\n",
      "corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=None, auto_reweigh=False)\n",
      "    Calculate a single corpus-level BLEU score (aka. system-level BLEU) for all\n",
      "    the hypotheses and their respective references.\n",
      "    \n",
      "    Instead of averaging the sentence level BLEU scores (i.e. marco-average\n",
      "    precision), the original BLEU metric (Papineni et al. 2002) accounts for\n",
      "    the micro-average precision (i.e. summing the numerators and denominators\n",
      "    for each hypothesis-reference(s) pairs before the division).\n",
      "    \n",
      "    >>> hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n",
      "    ...         'ensures', 'that', 'the', 'military', 'always',\n",
      "    ...         'obeys', 'the', 'commands', 'of', 'the', 'party']\n",
      "    >>> ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
      "    ...          'ensures', 'that', 'the', 'military', 'will', 'forever',\n",
      "    ...          'heed', 'Party', 'commands']\n",
      "    >>> ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n",
      "    ...          'guarantees', 'the', 'military', 'forces', 'always',\n",
      "    ...          'being', 'under', 'the', 'command', 'of', 'the', 'Party']\n",
      "    >>> ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n",
      "    ...          'army', 'always', 'to', 'heed', 'the', 'directions',\n",
      "    ...          'of', 'the', 'party']\n",
      "    \n",
      "    >>> hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\n",
      "    ...         'interested', 'in', 'world', 'history']\n",
      "    >>> ref2a = ['he', 'was', 'interested', 'in', 'world', 'history',\n",
      "    ...          'because', 'he', 'read', 'the', 'book']\n",
      "    \n",
      "    >>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\n",
      "    >>> hypotheses = [hyp1, hyp2]\n",
      "    >>> corpus_bleu(list_of_references, hypotheses) # doctest: +ELLIPSIS\n",
      "    0.5920...\n",
      "    \n",
      "    The example below show that corpus_bleu() is different from averaging\n",
      "    sentence_bleu() for hypotheses\n",
      "    \n",
      "    >>> score1 = sentence_bleu([ref1a, ref1b, ref1c], hyp1)\n",
      "    >>> score2 = sentence_bleu([ref2a], hyp2)\n",
      "    >>> (score1 + score2) / 2 # doctest: +ELLIPSIS\n",
      "    0.6223...\n",
      "    \n",
      "    :param list_of_references: a corpus of lists of reference sentences, w.r.t. hypotheses\n",
      "    :type list_of_references: list(list(list(str)))\n",
      "    :param hypotheses: a list of hypothesis sentences\n",
      "    :type hypotheses: list(list(str))\n",
      "    :param weights: weights for unigrams, bigrams, trigrams and so on\n",
      "    :type weights: list(float)\n",
      "    :param smoothing_function:\n",
      "    :type smoothing_function: SmoothingFunction\n",
      "    :param auto_reweigh: Option to re-normalize the weights uniformly.\n",
      "    :type auto_reweigh: bool\n",
      "    :return: The corpus-level BLEU score.\n",
      "    :rtype: float\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(corpus_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单独的N-Gram分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual BLEU-1: 0.894737\n",
      "Individual BLEU-2: 0.888889\n",
      "Individual BLEU-3: 0.882353\n",
      "Individual BLEU-4: 0.875000\n"
     ]
    }
   ],
   "source": [
    "# n-gram individual BLEU\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# reference = [['this', 'is', 'a', 'test']]\n",
    "# candidate = ['this', 'is', 'a', 'test']\n",
    "print('Individual BLEU-1: %f' % corpus_bleu([[reference],[reference]], [candidate,candidate], weights=(1, 0, 0, 0)))\n",
    "print('Individual BLEU-2: %f' % corpus_bleu([[reference],[reference]], [candidate,candidate], weights=(0, 1, 0, 0)))\n",
    "print('Individual BLEU-3: %f' % corpus_bleu([[reference],[reference]], [candidate,candidate], weights=(0, 0, 1, 0)))\n",
    "print('Individual BLEU-4: %f' % corpus_bleu([[reference],[reference]], [candidate,candidate], weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 累加的N-Gram分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative BLEU-1: 0.894737\n",
      "Cumulative BLEU-2: 0.891808\n",
      "Cumulative BLEU-3: 0.889695\n",
      "Cumulative BLEU-4: 0.885214\n"
     ]
    }
   ],
   "source": [
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "# reference = [['this', 'is', 'small', 'test']]\n",
    "# candidate = ['this', 'is', 'a', 'test']\n",
    "print('Cumulative BLEU-1: %f' % corpus_bleu([[reference],[reference]], [candidate,candidate], weights=(1, 0, 0, 0)))\n",
    "print('Cumulative BLEU-2: %f' % corpus_bleu([[reference],[reference]], [candidate,candidate], weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative BLEU-3: %f' % corpus_bleu([[reference],[reference]], [candidate,candidate], weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative BLEU-4: %f' % corpus_bleu([[reference],[reference]], [candidate,candidate], weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(corpus_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METEOR 分數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = '<s> but my experience with this set up was bad </s> <s> wire everything up quickly to see if it </s>'\n",
    "candidate = '[UNK] but my experience with this set up was bad [UNK] [UNK] wire everything up quickly to see if it would work [UNK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = reference.replace(\"<s>\",\"\").replace(\"</s>\",\"\")\n",
    "candidate = candidate.replace(\"[UNK]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n",
    "    \n",
    "reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n",
    "reference3 = 'It is the practical guide for the army always to heed the directions of the party'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meteor: 0.739828\n"
     ]
    }
   ],
   "source": [
    "print('Meteor: %f' % meteor_score([reference1, reference2, reference3], hypothesis1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function meteor_score in module nltk.translate.meteor_score:\n",
      "\n",
      "meteor_score(references, hypothesis, preprocess=<method 'lower' of 'str' objects>, stemmer=<PorterStemmer>, wordnet=<WordNetCorpusReader in '.../corpora/wordnet' (not loaded yet)>, alpha=0.9, beta=3, gamma=0.5)\n",
      "    Calculates METEOR score for hypothesis with multiple references as \n",
      "    described in \"Meteor: An Automatic Metric for MT Evaluation with \n",
      "    HighLevels of Correlation with Human Judgments\" by Alon Lavie and \n",
      "    Abhaya Agarwal, in Proceedings of ACL. \n",
      "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
      "    \n",
      "    \n",
      "    In case of multiple references the best score is chosen. This method \n",
      "    iterates over single_meteor_score and picks the best pair among all \n",
      "    the references for a given hypothesis\n",
      "    \n",
      "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
      "    >>> hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n",
      "    \n",
      "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
      "    >>> reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n",
      "    >>> reference3 = 'It is the practical guide for the army always to heed the directions of the party'\n",
      "    \n",
      "    >>> round(meteor_score([reference1, reference2, reference3], hypothesis1),4)\n",
      "    0.7398\n",
      "    \n",
      "        If there is no words match during the alignment the method returns the \n",
      "        score as 0. We can safely  return a zero instead of raising a \n",
      "        division by zero error as no match usually implies a bad translation. \n",
      "    \n",
      "    >>> round(meteor_score(['this is a cat'], 'non matching hypothesis'),4) \n",
      "    0.0\n",
      "    \n",
      "    :param references: reference sentences\n",
      "    :type references: list(str)\n",
      "    :param hypothesis: a hypothesis sentence\n",
      "    :type hypothesis: str\n",
      "    :param preprocess: preprocessing function (default str.lower)\n",
      "    :type preprocess: method\n",
      "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
      "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
      "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
      "    :type wordnet: WordNetCorpusReader\n",
      "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
      "    :type alpha: float\n",
      "    :param beta: parameter for controlling shape of penalty as a function \n",
      "                 of as a function of fragmentation.\n",
      "    :type beta: float\n",
      "    :param gamma: relative weight assigned to fragmentation penality.\n",
      "    :type gamma: float\n",
      "    :return: The sentence-level METEOR score.\n",
      "    :rtype: float\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(meteor_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
