{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0527 08:30:56.537786 140688719603520 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-05-27 08:30:58 - Pointer_generator_word2Vec_Intra_Atten - INFO: - logger已啟動\n",
      "I0527 08:30:58.795985 140688719603520 train_util.py:106] logger已啟動\n"
     ]
    }
   ],
   "source": [
    "from utils import config\n",
    "from utils.seq2seq import data\n",
    "\n",
    "from utils.seq2seq.batcher import *\n",
    "\n",
    "from utils.seq2seq.train_util import *\n",
    "from utils.seq2seq.rl_util import *\n",
    "from utils.seq2seq.initialize import loadCheckpoint, save_model\n",
    "from utils.seq2seq.write_result import *\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from translate.seq2seq_beam import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from utils.seq2seq.rl_util import *\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "eval_gpu = 0\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--key_attention', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_encoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--intra_decoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--copy', type=bool, default=True, help = 'True/False') # for transformer\n",
    "\n",
    "\n",
    "parser.add_argument('--model_type', type=str, default='seq2seq', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--train_rl', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_keys', \n",
    "                    help = 'POS_keys / DEP_keys / Noun_adj_keys / TextRank_keys')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--rand_unif_init_mag', type=float, default=0.02)\n",
    "parser.add_argument('--trunc_norm_init_std', type=float, default=0.001)\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument('--gound_truth_prob', type=float, default=0.1)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=500)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=20)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=5)\n",
    "parser.add_argument('--max_epochs', type=int, default=15)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=16)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=300)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default='0204750', help='0002000')\n",
    "parser.add_argument('--word_emb_type', type=str, default='word2Vec', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=True, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)\n",
    "\n",
    "eval_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-27 08:32:11 - Pointer_generator_word2Vec_Intra_Atten - INFO: - train : 504075, test : 56009\n",
      "I0527 08:32:11.625055 140688719603520 batcher.py:186] train : 504075, test : 56009\n",
      "2020-05-27 08:32:12 - Pointer_generator_word2Vec_Intra_Atten - INFO: - train batches : 15752, test batches : 1750\n",
      "I0527 08:32:12.081622 140688719603520 batcher.py:210] train batches : 15752, test batches : 1750\n"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "train_batches = len(iter(train_loader))\n",
    "test_batches = len(iter(validate_loader))\n",
    "save_steps = int(train_batches/250)*250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0527 08:32:13.046588 140688719603520 utils_any2vec.py:341] loading projection weights from ../Train-Data/Mix6_mainCat_best/Embedding/word2Vec/word2Vec.300d.txt\n",
      "I0527 08:32:25.410172 140688719603520 utils_any2vec.py:405] loaded (49676, 300) matrix from ../Train-Data/Mix6_mainCat_best/Embedding/word2Vec/word2Vec.300d.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/saved_models/Pointer_generator_word2Vec_Intra_Atten/0204750.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-27 08:32:35 - Pointer_generator_word2Vec_Intra_Atten - INFO: - Loaded model at model/saved_models/Pointer_generator_word2Vec_Intra_Atten/0204750.tar\n",
      "I0527 08:32:35.275532 140688719603520 initialize.py:212] Loaded model at model/saved_models/Pointer_generator_word2Vec_Intra_Atten/0204750.tar\n",
      "2020-05-27 08:32:35 - Pointer_generator_word2Vec_Intra_Atten - INFO: - Loaded model step = 204750, loss = 1.54, r_loss = 0.00 \n",
      "I0527 08:32:35.277233 140688719603520 initialize.py:213] Loaded model step = 204750, loss = 1.54, r_loss = 0.00 \n"
     ]
    }
   ],
   "source": [
    "from seq2seq import Model\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.distributed as dist\n",
    "\n",
    "from parallel import DataParallelModel, DataParallelCriterion\n",
    "# https://gist.github.com/thomwolf/7e2407fbd5945f07821adae3d9fd1312\n",
    "\n",
    "\n",
    "load_step = None\n",
    "model = Model(pre_train_emb=config.pre_train_emb, \n",
    "              word_emb_type = config.word_emb_type, \n",
    "              vocab = vocab)\n",
    "\n",
    "# model = model.cuda()\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=config.lr)   \n",
    "# optimizer = T.optim.Adagrad(model.parameters(),lr=config.lr, initial_accumulator_value=0.1)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(logger, load_model_path, model, optimizer)\n",
    "    # 若偵測到model切換成eval\n",
    "    eval_model = True\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(eval_gpu)\n",
    "    \n",
    "else:    \n",
    "    model.to('cuda:%s' % 0) #BCW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        With label smoothing,\n",
    "        KL-divergence between q_{smoothed ground truth prob.}(w)\n",
    "        and p_{prob. computed by model}(w) is minimized.\n",
    "        \"\"\"\n",
    "        def __init__(self, ignore_index):\n",
    "            super(NLLLoss, self).__init__()\n",
    "#             step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=PAD)\n",
    "            self.NLL = nn.NLLLoss(ignore_index=ignore_index, reduction='sum')\n",
    "\n",
    "        def forward(self, out, tar):  \n",
    "            # target dimension[0] / 2\n",
    "            # tar = target.contiguous().view(-1) \n",
    "            # out = output.contiguous().view(target.size(0),-1)\n",
    "\n",
    "            target = tar.contiguous().view(-1)\n",
    "            output = out[:tar.size(0)]\n",
    "            normalize = output.size(0) * output.size(1)\n",
    "            output = output.contiguous().view(target.size(0),-1)\n",
    "            loss = self.NLL(output, target) / normalize\n",
    "            \n",
    "            return loss\n",
    "\n",
    "if not eval_model:\n",
    "    criterion = NLLLoss(ignore_index=PAD)\n",
    "    parallel_model = DataParallelModel(model) # Encapsulate the model\n",
    "    parallel_loss = DataParallelCriterion(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sents(enc_out, inds, vocab, art_oovs):\n",
    "    decoded_strs = []\n",
    "    for i in range(len(enc_out)):\n",
    "        id_list = inds[i].tolist() # 取出每個sample sentence 的word id list\n",
    "        S = output2words(id_list, vocab, art_oovs[i]) #Generate sentence corresponding to sampled words\n",
    "        try:\n",
    "            end_idx = S.index(data.STOP_DECODING)\n",
    "            S = S[:end_idx]\n",
    "        except ValueError:\n",
    "            S = S\n",
    "        if len(S) < 2:          #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "            S = [\"xxx\"]\n",
    "        S = \" \".join(S)\n",
    "        decoded_strs.append(S)\n",
    "    return decoded_strs\n",
    "\n",
    "def merge_res(res):\n",
    "    ((inds1, log_probs1, enc_out1),(inds2, log_probs2, enc_out2)) = res\n",
    "    inds = T.cat([inds1, inds2], dim = 0).cpu()\n",
    "    log_probs = T.cat([log_probs1, log_probs2], dim = 0)\n",
    "    enc_out = T.cat([enc_out1, enc_out2], dim = 0).cpu()\n",
    "\n",
    "#     inds, log_probs, enc_out = res\n",
    "#     inds = inds.cpu()\n",
    "#     enc_out = enc_out.cpu()\n",
    "    return inds, log_probs, enc_out\n",
    "\n",
    "def train_one_rl(package, inputs):\n",
    "    config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e,                                 max_dec_len, dec_batch, target_batch = package\n",
    "    \n",
    "    rl_loss, batch_reward = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e,                                 max_dec_len, dec_batch, target_batch, train_rl = True, art_oovs = inputs.art_oovs, original_abstract = inputs.original_abstract, vocab = vocab)\n",
    "    rl_loss = nn.parallel.gather(rl_loss, 0).mean() \n",
    "    return rl_loss, batch_reward\n",
    "\n",
    "def train_one(package):\n",
    "    model.train()\n",
    "    config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch = package   \n",
    "\n",
    "    pred_probs = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch)\n",
    "    target = target_batch\n",
    "    loss = parallel_loss(config.mle_weight, pred_probs, target)\n",
    "    \n",
    "    return loss, pred_probs\n",
    "\n",
    "def write_res(inputs, batch_probs):\n",
    "    decoded_sents = []\n",
    "    for i, probs in enurmerate(batch_probs):\n",
    "        sents = []\n",
    "        for prob in probs:\n",
    "            _id = T.max(probs, dim=1)[1]\n",
    "            _id = _id.detach()\n",
    "            sents.append(_id)\n",
    "        decoded_sents.append(seq)\n",
    "            \n",
    "    output2words()        \n",
    "    article_sents = [article for article in inputs.original_article]\n",
    "    ref_sents = [ref for ref in inputs.original_abstract]\n",
    "#     decoded_sents = [summarize(article, words=30) for article in article_sents]\n",
    "#     decoded_sents = [sent if len(sent) > 5 else \"xxx xxx xxx xxx xxx\" for sent in decoded_sents]\n",
    "        \n",
    "#     article_sents, decoded_sents, keywords_list, \\\n",
    "#     ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "#     rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents, \\\n",
    "#                 keywords_list, ref_sents, long_seq_index, write = False)\n",
    "#     avg_rouge_l.append(rouge_l)\n",
    "#     acc_cost = time.time() - acc_st\n",
    "#     avg_acc_cost.append(acc_cost)\n",
    "    \n",
    "    return seq_sents\n",
    "\n",
    "def get_package(inputs):\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(inputs, config, batch_first = True)\n",
    "\n",
    "    dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(inputs, config, batch_first = True) # Get input and target batchs for training decoder            \n",
    "\n",
    "    max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0]    \n",
    "    # ----------------------------------------------------\n",
    "    package = (config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch)\n",
    "    \n",
    "    inner_c = package[1] != max(package[4].tolist())[0]\n",
    "\n",
    "    return inner_c, package\n",
    "\n",
    "\n",
    "# for inputs in train_loader:  \n",
    "#     # MLE test\n",
    "#     # ----------------------------------------------------\n",
    "#     # pred_probs = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "#     #                             max_dec_len, dec_batch, target_batch)\n",
    "#     # # pass\n",
    "#     # target = target_batch\n",
    "#     # loss = parallel_loss(config.mle_weight, pred_probs, target)\n",
    "#     loss = train_one(package)\n",
    "# #     loss.backward() # Backward pass \n",
    "# #     optimizer.step() # Optimizer step\n",
    "#     print('loss : ',loss)\n",
    "#     # pass\n",
    "#     # ----------------------------------------------------   \n",
    "#     if config.train_rl:\n",
    "#         rl_loss, batch_reward = train_one_rl(package)\n",
    "#         print('rl_loss : ',rl_loss, 'batch_reward : ',batch_reward)\n",
    "#     else:\n",
    "#         rl_loss = T.FloatTensor([0]).cuda()        \n",
    "    \n",
    "#     (config.mle_weight * loss + config.rl_weight * rl_loss).backward() # Backward pass   \n",
    "#     optimizer.step() # Optimizer step\n",
    "#     optimizer.zero_grad() # 清空过往梯度 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "#     batch = next(iter(validate_loader))\n",
    "    val_num = len(iter(validate_loader))\n",
    "    for idx, batch in enumerate(validate_loader):\n",
    "        inner_c, package = get_package(batch)\n",
    "        if inner_c: continue\n",
    "        loss, _ = train_one(package)\n",
    "#         loss = train_one(model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "#         if idx>= val_num/40: break\n",
    "#     model.train()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del parallel_model, parallel_loss\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(dataloader)\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "    avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    avg_meteor = []\n",
    "    outFrame = None\n",
    "    avg_time = 0\n",
    "        \n",
    "    for idx, inputs in enumerate(dataloader):\n",
    "        start = time.time() \n",
    "#         'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "            ct_e, enc_key_batch, enc_key_mask, enc_key_lens = get_input_from_batch(inputs, config, batch_first = True)\n",
    "        max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0] \n",
    "        \n",
    "        if (max_enc_len != max(enc_lens.tolist())[0]): continue\n",
    "\n",
    "        enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = model.encoder(enc_batch, enc_lens, max_enc_len)\n",
    "        \n",
    "#         'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                                enc_batch_extend_vocab, enc_key_batch, enc_key_mask, model, \n",
    "                                START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, ref_sents, long_seq_index = prepare_result(vocab, inputs, pred_ids)\n",
    "        cost = (time.time() - start)\n",
    "        avg_time += cost        \n",
    "\n",
    "        \n",
    "        rouge_1, rouge_2, rouge_l, self_Bleu_1, self_Bleu_2, self_Bleu_3, self_Bleu_4,             Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        \n",
    "        if idx %1000 ==0 and idx >0 : print(idx)\n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "        avg_self_bleu1.extend(self_Bleu_1)\n",
    "        avg_self_bleu2.extend(self_Bleu_2)\n",
    "        avg_self_bleu3.extend(self_Bleu_3)\n",
    "        avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "        avg_bleu1.extend(Bleu_1)\n",
    "        avg_bleu2.extend(Bleu_2)\n",
    "        avg_bleu3.extend(Bleu_3)\n",
    "        avg_bleu4.extend(Bleu_4)\n",
    "        avg_meteor.extend(Meteor)\n",
    "        # ----------------------------------------------------    \n",
    "    avg_time = avg_time / (num * config.batch_size) \n",
    "    \n",
    "    avg_rouge_l, outFrame = total_output(mode, writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l,         avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4,         avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "    )\n",
    "    \n",
    "    return avg_rouge_l, outFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-27 08:33:47 - Pointer_generator_word2Vec_Intra_Atten - INFO: - train : 504075, test : 56009\n",
      "I0527 08:33:47.524726 140688719603520 batcher.py:186] train : 504075, test : 56009\n",
      "2020-05-27 08:33:47 - Pointer_generator_word2Vec_Intra_Atten - INFO: - train batches : 15752, test batches : 1750\n",
      "I0527 08:33:47.998079 140688719603520 batcher.py:210] train batches : 15752, test batches : 1750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-27 09:23:57 - Pointer_generator_word2Vec_Intra_Atten - INFO: - epoch 13: test_avg_acc = 0.354980\n",
      "I0527 09:23:57.193564 140688719603520 <ipython-input-8-c8d2e1c34b51>:160] epoch 13: test_avg_acc = 0.354980\n",
      "2020-05-27 09:23:57 - Pointer_generator_word2Vec_Intra_Atten - INFO: - logger已關閉\n",
      "I0527 09:23:57.195485 140688719603520 train_util.py:110] logger已關閉\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test \n",
      " ['Accuracy result:\\n', '##-- Rouge --##\\n', 'testing_avg_rouge_1: 0.3838549233182839 \\n', 'testing_avg_rouge_2: 0.2556870543479391 \\n', 'testing_avg_rouge_l: 0.354980012086971 \\n', '##-- SELF-BLEU --##\\n', 'testing_avg_self_bleu1: 0.33870950982388953 \\n', 'testing_avg_self_bleu2: 0.23508876064302392 \\n', 'testing_avg_self_bleu3: 0.19443434992550554 \\n', 'testing_avg_self_bleu4: 0.16725112494011551 \\n', '##-- BLEU --##\\n', 'testing_avg_bleu1: 0.33870950982388953 \\n', 'testing_avg_bleu2: 0.2580444916562384 \\n', 'testing_avg_bleu3: 0.22213981663671817 \\n', 'testing_avg_bleu4: 0.19561463160645642 \\n', '##-- Meteor --##\\n', 'testing_avg_meteor: 0.34942041087378667 \\n', 'Num : 51712 Execute Time: 0.0510904541015625 \\n']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "loss_st, loss_cost = 0,0\n",
    "decode_st, decode_cost = 0,0\n",
    "last_save_step = 0\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "if not eval_model:\n",
    "\n",
    "    write_train_para(writer, config)\n",
    "    logger.info('------Training START--------')\n",
    "    running_avg_loss, running_avg_rl_loss = 0, 0\n",
    "    sum_total_reward = 0\n",
    "    step = 0\n",
    "    print_step = 250\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(config, logger, vocab, loggerName, patience=3, verbose=True)\n",
    "    try:\n",
    "        for epoch in range(1, config.max_epochs+1):\n",
    "            for batch in train_loader:\n",
    "                step += 1; \n",
    "                loss_st = time.time()\n",
    "                inner_c, package = get_package(batch)\n",
    "                if inner_c: continue\n",
    "                parallel_model.module.train()\n",
    "                mle_loss, pred_probs = train_one(package)\n",
    "                if config.train_rl:\n",
    "                    rl_loss, batch_reward = train_one_rl(package, batch)             \n",
    "\n",
    "                    if step%print_step == 0 :\n",
    "                        writer.add_scalars('scalar/RL_Loss',  \n",
    "                           {'rl_loss': rl_loss\n",
    "                           }, step)\n",
    "                        writer.add_scalars('scalar/Reward',  \n",
    "                           {'batch_reward': batch_reward\n",
    "                           }, step)\n",
    "    #                     logger.info('epoch %d: %d, RL_Loss = %f, batch_reward = %f'\n",
    "    #                                     % (epoch, step, rl_loss, batch_reward))\n",
    "                    sum_total_reward += batch_reward\n",
    "                else:\n",
    "                    rl_loss = T.FloatTensor([0]).cuda()\n",
    "\n",
    "                (config.mle_weight * mle_loss + config.rl_weight * rl_loss).backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "                '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "                if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "        #             clip_grad_norm_(model.parameters(), 5.0)                      \n",
    "                    optimizer.step() # 根据累计的梯度更新网络参数\n",
    "                    optimizer.zero_grad() # 清空过往梯度 \n",
    "                if step%print_step == 0 :\n",
    "                    with T.autograd.no_grad():\n",
    "                        train_batch_loss = mle_loss.item()\n",
    "                        train_batch_rl_loss = rl_loss.item()\n",
    "#                         val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                        running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "                        running_avg_rl_loss = calc_running_avg_loss(train_batch_rl_loss, running_avg_rl_loss)\n",
    "                        running_avg_reward = sum_total_reward / step\n",
    "#                         if step % save_steps == 0:\n",
    "#                             logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "#                                         % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                        writer.add_scalars('scalar/Loss',  \n",
    "                           {'train_batch_loss': train_batch_loss\n",
    "                           }, step)\n",
    "                        writer.add_scalars('scalar_avg/loss',  \n",
    "                           {'train_avg_loss': running_avg_loss\n",
    "#                             'test_avg_loss': val_avg_loss\n",
    "                           }, step)\n",
    "                        if running_avg_reward > 0:\n",
    "    #                         logger.info('epoch %d: %d, running_avg_reward = %f'\n",
    "    #                                 % (epoch, step, running_avg_reward))\n",
    "                            writer.add_scalars('scalar_avg/Reward',  \n",
    "                               {'running_avg_reward': running_avg_reward\n",
    "                               }, step)\n",
    "                        if running_avg_rl_loss != 0:\n",
    "    #                         logger.info('epoch %d: %d, running_avg_rl_loss = %f'\n",
    "    #                                 % (epoch, step, running_avg_rl_loss))\n",
    "                            writer.add_scalars('scalar_avg/RL_Loss',  \n",
    "                               {'running_avg_rl_loss': running_avg_rl_loss\n",
    "                               }, step)\n",
    "                                                    \n",
    "                \n",
    "                if step % save_steps == 0:\n",
    "                    parallel_model.module.eval()\n",
    "                    logger.info('epoch : %s' % epoch)\n",
    "                    val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                    logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "                                        % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                    writer.add_scalars('scalar_avg/loss',  \n",
    "                           {'train_avg_loss': running_avg_loss,\n",
    "                            'test_avg_loss': val_avg_loss\n",
    "                           }, step)\n",
    "                    '''（讀取所儲存模型引數後，再進行並行化操作，否則無法利用之前的程式碼進行讀取）'''\n",
    "                    save_model(config, logger, parallel_model, optimizer, step, vocab, val_avg_loss, \\\n",
    "                               r_loss=0, title = loggerName)\n",
    "                    loss_cost = time.time() - loss_st\n",
    "                    logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "                                    % (epoch, step, loss_cost))\n",
    "                    writer.add_scalars('scalar_avg/epoch_loss',  \n",
    "                       {'train_avg_loss': running_avg_loss,\n",
    "                        'test_avg_loss': val_avg_loss\n",
    "                       }, epoch)\n",
    "                    last_save_step = step\n",
    "\n",
    "            logger.info('-------------------------------------------------------------')\n",
    "\n",
    "            if running_avg_reward > 0:\n",
    "                logger.info('epoch %d|step %d| running_avg_reward = %f'% (epoch, step, running_avg_reward))\n",
    "            if running_avg_rl_loss != 0:\n",
    "                logger.info('epoch %d|step %d| running_avg_rl_loss = %f'% (epoch, step, running_avg_rl_loss))\n",
    "            logger.info('-------------------------------------------------------------')\n",
    "\n",
    "            early_stopping(parallel_model, optimizer, step, val_loss) # update patience\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(\"Early stopping epoch %s\"%(epoch))\n",
    "                break\n",
    "\n",
    "    except Excepation as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        logger.info(u'------Training SUCCESS--------')  \n",
    "    finally:\n",
    "        logger.info(u'------Training END--------')   \n",
    "        logger.info(\"stopping epoch %s\"%(epoch))        \n",
    "        logger.info(\"last_save_step %s\"%(last_save_step))  \n",
    "        removeLogger(logger)\n",
    "\n",
    "else: # EVAL\n",
    "    load_ep = float(config.load_ckpt) / float(save_steps)\n",
    "    config.batch_size = 32\n",
    "    train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "    train_batches = len(iter(train_loader))\n",
    "    test_batches = len(iter(validate_loader))\n",
    "#     save_steps = int(train_batches/250)*250\n",
    "    model.cuda(eval_gpu) \n",
    "    model.eval()\n",
    "    '''先將test_avg_acc調起來再decode train_'''\n",
    "#     train_avg_acc, train_outFrame = decode_write_all(writer, logger, load_ep, config, model, train_loader, mode = 'train')\n",
    "    test_avg_acc, test_outFrame = decode_write_all(writer, logger, load_ep, config, model, validate_loader, mode = 'test')\n",
    "#     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (load_ep, train_avg_acc, test_avg_acc)) \n",
    "    logger.info('epoch %d: test_avg_acc = %f' % (load_ep, test_avg_acc)) \n",
    "    removeLogger(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article', 'keywords', 'reference', 'decoded', 'rouge_1', 'rouge_2',\n",
       "       'rouge_l', 'self_Bleu_1', 'self_Bleu_2', 'self_Bleu_3', 'self_Bleu_4',\n",
       "       'Bleu_1', 'Bleu_2', 'Bleu_3', 'Bleu_4', 'Meteor', 'article_lens',\n",
       "       'ref_lens', 'overlap', 'overlap_percent', 'gen_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge_1</th>\n",
       "      <th>article</th>\n",
       "      <th>reference</th>\n",
       "      <th>decoded</th>\n",
       "      <th>gen_type</th>\n",
       "      <th>overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51707</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>like research thing and know bit about whateve...</td>\n",
       "      <td>like to research thing and know bit about what...</td>\n",
       "      <td>like research thing and know bit about whateve...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51704</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>update may 2016 . after have this collar for y...</td>\n",
       "      <td>this collar is great tool and communication de...</td>\n",
       "      <td>great collar great customer service from amazon</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51678</th>\n",
       "      <td>0.526316</td>\n",
       "      <td>this was the best investment make for our seni...</td>\n",
       "      <td>our senior puppy love it best purchase we make</td>\n",
       "      <td>this was the best investment make for our seni...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51674</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>first opinion that the olympus e520 absolutely...</td>\n",
       "      <td>excellent camera best value for the money</td>\n",
       "      <td>the best dslr camera you will find</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51672</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>okay here are initial impression about the pro...</td>\n",
       "      <td>superb tv for the price excellent picture but ...</td>\n",
       "      <td>great tv for the price but could be better</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51654</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>have have this case for month now buy this aft...</td>\n",
       "      <td>great htpc case if you have spacious media con...</td>\n",
       "      <td>great htpc case at great price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51650</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>receive this printer january and get set succe...</td>\n",
       "      <td>good printer customer support has pro and con</td>\n",
       "      <td>the customer support system were good and some...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51621</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>really like the fz70 especially the super wide...</td>\n",
       "      <td>fantastic zoom and extra wide angle camera gre...</td>\n",
       "      <td>great wide angle lens for the beginner</td>\n",
       "      <td>Ext</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51608</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>columbia pfg fit columbia make some fantastic ...</td>\n",
       "      <td>compare my favorite blood gut iii to other pfg...</td>\n",
       "      <td>comparison of columbia pfg bahama bonehead blo...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51605</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>old eye love lot light this unit kill outdoors...</td>\n",
       "      <td>my old eye love lot of light this unit kill it</td>\n",
       "      <td>my old eye love lot of light</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51601</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>aesthetic out beauty obviously subjective but ...</td>\n",
       "      <td>the best android wear watch but wait for price...</td>\n",
       "      <td>but this is the best looking watch for android...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51598</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>aesthetic out beauty obviously subjective but ...</td>\n",
       "      <td>the best android wear watch but wait for price...</td>\n",
       "      <td>but this is the best looking watch for android...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51585</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>rx100 review some background first . have own ...</td>\n",
       "      <td>have always want discretely sized excellent qu...</td>\n",
       "      <td>have always want discretely sized excellent qu...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51582</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>matter how good the hardware the system basica...</td>\n",
       "      <td>system is only as good as the documention and ...</td>\n",
       "      <td>matter how good the hardware the system is bas...</td>\n",
       "      <td>Abs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51577</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>think tank photo urban approach camera bag wea...</td>\n",
       "      <td>choose the urban approach because it fit my ne...</td>\n",
       "      <td>the urban approach because it fit need better ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51560</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>purchase the black tanto and the cold steel le...</td>\n",
       "      <td>compare favorably to the cold steel tanto</td>\n",
       "      <td>comparison of the cold steel leatherneck tanto</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51532</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>short answer this has become new photography b...</td>\n",
       "      <td>manfrotto befree carbon fiber tripod my new ph...</td>\n",
       "      <td>this has become my new photography best friend...</td>\n",
       "      <td>Abs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51531</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>had been look for battery case for iphone and ...</td>\n",
       "      <td>great iphone battery case for the price</td>\n",
       "      <td>had been look for battery case for my iphone</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51524</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>ok . will start off say for the price this goo...</td>\n",
       "      <td>good phone for price but has downfall</td>\n",
       "      <td>great beginner phone for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51499</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>overall good product and fairly easy setup als...</td>\n",
       "      <td>overall good product and fairly easy to set up</td>\n",
       "      <td>overall good product and fairly easy to setup ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51492</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>this little sharrk contender the portable powe...</td>\n",
       "      <td>sharrk portable power bank power pack mah</td>\n",
       "      <td>great portable power pack at great price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51442</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>use quite few tablet here there eventually set...</td>\n",
       "      <td>best tablet pc on the market if you do not use...</td>\n",
       "      <td>the best tablet on the market</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51434</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>the nbsp class href arp rvw txt utf8 logitech ...</td>\n",
       "      <td>logitech quickcam pro great all around webcam</td>\n",
       "      <td>logitech good all around webcam at good price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51418</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>initial thought while the battery maybe for so...</td>\n",
       "      <td>my review of the zerolemon galaxy mah battery ...</td>\n",
       "      <td>great battery case for the zerolemon battery</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51409</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>overall love the otis flexible memory cable cl...</td>\n",
       "      <td>review of the otis flexible memory cable clean...</td>\n",
       "      <td>love the otis flexible memory cable</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51397</th>\n",
       "      <td>0.526316</td>\n",
       "      <td>overall will say that contempt with purchase ....</td>\n",
       "      <td>there are several good thing and several bad t...</td>\n",
       "      <td>there are several good thing about this produc...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51367</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>buy one these yesterday for 149 local price cl...</td>\n",
       "      <td>terrible gps combine with so so dash cam at gr...</td>\n",
       "      <td>good dash cam at good price</td>\n",
       "      <td>Abs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51324</th>\n",
       "      <td>0.965517</td>\n",
       "      <td>first let say why star without doubt best valu...</td>\n",
       "      <td>me say why star without doubt best value besid...</td>\n",
       "      <td>let me say why star without doubt best value b...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51318</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>overall good product for the money . camera ha...</td>\n",
       "      <td>overall good system for the money customer ser...</td>\n",
       "      <td>overall good product for the money</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51317</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>the atrix great phone but the outdated version...</td>\n",
       "      <td>outdated version of android great phone but bu...</td>\n",
       "      <td>the atrix is great phone but the outdated vers...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>great for light float but you are go maneuver ...</td>\n",
       "      <td>great for light float but if you are go to</td>\n",
       "      <td>great for light float but you are go to maneuver</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>this great product . come with mfi certify cab...</td>\n",
       "      <td>this is great product it come with ft mfi certify</td>\n",
       "      <td>this is great product it come with mfi certify...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>husband use them and like that can put multipl...</td>\n",
       "      <td>grip feel pretty good even with lot of weight</td>\n",
       "      <td>grip feel pretty good even with lot of weight</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>get this watch for kid look great timing was a...</td>\n",
       "      <td>get this watch for my kid it look great timing...</td>\n",
       "      <td>get this watch for kid look great timing was a...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>surprisingly this local dim tv . advertise ein...</td>\n",
       "      <td>surprisingly this tv is full array local dim tv</td>\n",
       "      <td>surprisingly this local dim tv but fact</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>nice hat make vietnam and with non functional ...</td>\n",
       "      <td>nice hat make in vietnam and with non functional</td>\n",
       "      <td>nice hat make vietnam and with non functional ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>dog treat arrive schedule and the dog enjoy th...</td>\n",
       "      <td>was satisfied with the ordering process and wi...</td>\n",
       "      <td>my dog treat arrive schedule and the dog enjoy...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>wow amazing product although its not brand the...</td>\n",
       "      <td>wow amazing product although its not brand the...</td>\n",
       "      <td>wow amazing product although its not brand the...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>good bag but close with the sun shade attach ....</td>\n",
       "      <td>good bag but will not close with the sun shade...</td>\n",
       "      <td>good bag but close up with the sun shade</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.740741</td>\n",
       "      <td>okay thought would more sturdy . but for cocka...</td>\n",
       "      <td>it is okay think it would be more sturdy but f...</td>\n",
       "      <td>okay thought would more sturdy but for cockate...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>get puncture with minute ride . tiny shard gla...</td>\n",
       "      <td>get puncture right away with in minute of star...</td>\n",
       "      <td>get puncture with minute ride tiny shard</td>\n",
       "      <td>Abs</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>the gold piece around the phone case break ver...</td>\n",
       "      <td>the gold piece around the phone case break in</td>\n",
       "      <td>the gold piece around the phone case break ver...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>total body workout you also learn how pace you...</td>\n",
       "      <td>total body workout you also learn how to pace</td>\n",
       "      <td>total body workout you also learn how to pace</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.538462</td>\n",
       "      <td>all our camping friend have buy when our old o...</td>\n",
       "      <td>so we buy it when our old one die and it is gr...</td>\n",
       "      <td>great pack and the wheel are handy</td>\n",
       "      <td>Ext</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.944444</td>\n",
       "      <td>use with the standard cover and although you c...</td>\n",
       "      <td>use with the standard cover and although you c...</td>\n",
       "      <td>use with the standard cover and although you c...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>very bad product this the second one buy first...</td>\n",
       "      <td>very bad product this is the second one buy first</td>\n",
       "      <td>very bad product this is the second one buy</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>buy 200 dollar phone and come with power cord ...</td>\n",
       "      <td>buy dollar phone and it does not come</td>\n",
       "      <td>buy dollar phone and come with power cord what...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>decent bag but rip pretty easily canyoneere tr...</td>\n",
       "      <td>it is decent bag but it rip pretty easily on c...</td>\n",
       "      <td>decent bag but rip pretty easily canyoneere tr...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.842105</td>\n",
       "      <td>will last long time . cat like it . like how s...</td>\n",
       "      <td>my cat like it did not like how strong the smell</td>\n",
       "      <td>my cat like it like how strong the smell was</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>thank you yes did receive mini camera and the ...</td>\n",
       "      <td>did receive my mini camera and the timing was ...</td>\n",
       "      <td>thank you yes did receive mini camera and the ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>solid beginner lens for anyone who want get ou...</td>\n",
       "      <td>solid beginner lens for anyone who want to get...</td>\n",
       "      <td>solid beginner lens for anyone who want to get...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>conure love these and not big fan pellet . all...</td>\n",
       "      <td>my conure love these and she is not big fan</td>\n",
       "      <td>my conure love these and not big fan of pellet</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>very nice and great quality for the price . st...</td>\n",
       "      <td>very nice and great quality for the price</td>\n",
       "      <td>very nice and great quality for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>dog love and love all the natural ingredient ....</td>\n",
       "      <td>my dog love it and love all the natural ingred...</td>\n",
       "      <td>my dog love it and love all the natural ingred...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>star the fisheye lens the best not like other ...</td>\n",
       "      <td>the fisheye lens is the best it is not like ot...</td>\n",
       "      <td>the fisheye lens is the best not like other wi...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.896552</td>\n",
       "      <td>buy this for husband galaxy and work great . l...</td>\n",
       "      <td>for my husband galaxy and it is work great he ...</td>\n",
       "      <td>buy this for my husband galaxy and it work gre...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>very solid . more like solid chrome than cheap...</td>\n",
       "      <td>more like solid chrome than cheap pot metal</td>\n",
       "      <td>very more like solid chrome than cheap very ni...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>this excellent case love the kickstand and its...</td>\n",
       "      <td>this is an excellent case love the kickstand a...</td>\n",
       "      <td>this is an excellent case love the kickstand a...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.761905</td>\n",
       "      <td>yes dog love and has richness however look the...</td>\n",
       "      <td>yes my dog love and it has richness however</td>\n",
       "      <td>yes my dog love and has richness however look ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>this great quality especially consider the pri...</td>\n",
       "      <td>this is great quality especially consider the ...</td>\n",
       "      <td>this is great quality especially consider the ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18975 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rouge_1                                            article  \\\n",
       "51707  0.888889  like research thing and know bit about whateve...   \n",
       "51704  0.444444  update may 2016 . after have this collar for y...   \n",
       "51678  0.526316  this was the best investment make for our seni...   \n",
       "51674  0.428571  first opinion that the olympus e520 absolutely...   \n",
       "51672  0.666667  okay here are initial impression about the pro...   \n",
       "51654  0.428571  have have this case for month now buy this aft...   \n",
       "51650  0.470588  receive this printer january and get set succe...   \n",
       "51621  0.571429  really like the fz70 especially the super wide...   \n",
       "51608  0.521739  columbia pfg fit columbia make some fantastic ...   \n",
       "51605  0.777778  old eye love lot light this unit kill outdoors...   \n",
       "51601  0.700000  aesthetic out beauty obviously subjective but ...   \n",
       "51598  0.700000  aesthetic out beauty obviously subjective but ...   \n",
       "51585  0.933333  rx100 review some background first . have own ...   \n",
       "51582  0.421053  matter how good the hardware the system basica...   \n",
       "51577  0.800000  think tank photo urban approach camera bag wea...   \n",
       "51560  0.571429  purchase the black tanto and the cold steel le...   \n",
       "51532  0.500000  short answer this has become new photography b...   \n",
       "51531  0.533333  had been look for battery case for iphone and ...   \n",
       "51524  0.461538  ok . will start off say for the price this goo...   \n",
       "51499  0.700000  overall good product and fairly easy setup als...   \n",
       "51492  0.500000  this little sharrk contender the portable powe...   \n",
       "51442  0.555556  use quite few tablet here there eventually set...   \n",
       "51434  0.571429  the nbsp class href arp rvw txt utf8 logitech ...   \n",
       "51418  0.533333  initial thought while the battery maybe for so...   \n",
       "51409  0.666667  overall love the otis flexible memory cable cl...   \n",
       "51397  0.526316  overall will say that contempt with purchase ....   \n",
       "51367  0.533333  buy one these yesterday for 149 local price cl...   \n",
       "51324  0.965517  first let say why star without doubt best valu...   \n",
       "51318  0.666667  overall good product for the money . camera ha...   \n",
       "51317  0.545455  the atrix great phone but the outdated version...   \n",
       "...         ...                                                ...   \n",
       "53     0.900000  great for light float but you are go maneuver ...   \n",
       "52     0.900000  this great product . come with mfi certify cab...   \n",
       "58     1.000000  husband use them and like that can put multipl...   \n",
       "8      0.941176  get this watch for kid look great timing was a...   \n",
       "14     0.666667  surprisingly this local dim tv . advertise ein...   \n",
       "12     0.888889  nice hat make vietnam and with non functional ...   \n",
       "11     0.636364  dog treat arrive schedule and the dog enjoy th...   \n",
       "10     0.947368  wow amazing product although its not brand the...   \n",
       "9      0.800000  good bag but close with the sun shade attach ....   \n",
       "4      0.740741  okay thought would more sturdy . but for cocka...   \n",
       "6      0.555556  get puncture with minute ride . tiny shard gla...   \n",
       "3      0.823529  the gold piece around the phone case break ver...   \n",
       "2      1.000000  total body workout you also learn how pace you...   \n",
       "16     0.538462  all our camping friend have buy when our old o...   \n",
       "15     0.944444  use with the standard cover and although you c...   \n",
       "17     0.947368  very bad product this the second one buy first...   \n",
       "18     0.454545  buy 200 dollar phone and come with power cord ...   \n",
       "19     0.800000  decent bag but rip pretty easily canyoneere tr...   \n",
       "20     0.842105  will last long time . cat like it . like how s...   \n",
       "0      0.933333  thank you yes did receive mini camera and the ...   \n",
       "21     0.833333  solid beginner lens for anyone who want get ou...   \n",
       "22     0.800000  conure love these and not big fan pellet . all...   \n",
       "23     1.000000  very nice and great quality for the price . st...   \n",
       "24     1.000000  dog love and love all the natural ingredient ....   \n",
       "25     0.916667  star the fisheye lens the best not like other ...   \n",
       "26     0.896552  buy this for husband galaxy and work great . l...   \n",
       "27     0.705882  very solid . more like solid chrome than cheap...   \n",
       "28     1.000000  this excellent case love the kickstand and its...   \n",
       "29     0.761905  yes dog love and has richness however look the...   \n",
       "31     1.000000  this great quality especially consider the pri...   \n",
       "\n",
       "                                               reference  \\\n",
       "51707  like to research thing and know bit about what...   \n",
       "51704  this collar is great tool and communication de...   \n",
       "51678     our senior puppy love it best purchase we make   \n",
       "51674          excellent camera best value for the money   \n",
       "51672  superb tv for the price excellent picture but ...   \n",
       "51654  great htpc case if you have spacious media con...   \n",
       "51650      good printer customer support has pro and con   \n",
       "51621  fantastic zoom and extra wide angle camera gre...   \n",
       "51608  compare my favorite blood gut iii to other pfg...   \n",
       "51605     my old eye love lot of light this unit kill it   \n",
       "51601  the best android wear watch but wait for price...   \n",
       "51598  the best android wear watch but wait for price...   \n",
       "51585  have always want discretely sized excellent qu...   \n",
       "51582  system is only as good as the documention and ...   \n",
       "51577  choose the urban approach because it fit my ne...   \n",
       "51560          compare favorably to the cold steel tanto   \n",
       "51532  manfrotto befree carbon fiber tripod my new ph...   \n",
       "51531            great iphone battery case for the price   \n",
       "51524              good phone for price but has downfall   \n",
       "51499     overall good product and fairly easy to set up   \n",
       "51492          sharrk portable power bank power pack mah   \n",
       "51442  best tablet pc on the market if you do not use...   \n",
       "51434      logitech quickcam pro great all around webcam   \n",
       "51418  my review of the zerolemon galaxy mah battery ...   \n",
       "51409  review of the otis flexible memory cable clean...   \n",
       "51397  there are several good thing and several bad t...   \n",
       "51367  terrible gps combine with so so dash cam at gr...   \n",
       "51324  me say why star without doubt best value besid...   \n",
       "51318  overall good system for the money customer ser...   \n",
       "51317  outdated version of android great phone but bu...   \n",
       "...                                                  ...   \n",
       "53            great for light float but if you are go to   \n",
       "52     this is great product it come with ft mfi certify   \n",
       "58         grip feel pretty good even with lot of weight   \n",
       "8      get this watch for my kid it look great timing...   \n",
       "14       surprisingly this tv is full array local dim tv   \n",
       "12      nice hat make in vietnam and with non functional   \n",
       "11     was satisfied with the ordering process and wi...   \n",
       "10     wow amazing product although its not brand the...   \n",
       "9      good bag but will not close with the sun shade...   \n",
       "4      it is okay think it would be more sturdy but f...   \n",
       "6      get puncture right away with in minute of star...   \n",
       "3          the gold piece around the phone case break in   \n",
       "2          total body workout you also learn how to pace   \n",
       "16     so we buy it when our old one die and it is gr...   \n",
       "15     use with the standard cover and although you c...   \n",
       "17     very bad product this is the second one buy first   \n",
       "18                 buy dollar phone and it does not come   \n",
       "19     it is decent bag but it rip pretty easily on c...   \n",
       "20      my cat like it did not like how strong the smell   \n",
       "0      did receive my mini camera and the timing was ...   \n",
       "21     solid beginner lens for anyone who want to get...   \n",
       "22           my conure love these and she is not big fan   \n",
       "23             very nice and great quality for the price   \n",
       "24     my dog love it and love all the natural ingred...   \n",
       "25     the fisheye lens is the best it is not like ot...   \n",
       "26     for my husband galaxy and it is work great he ...   \n",
       "27           more like solid chrome than cheap pot metal   \n",
       "28     this is an excellent case love the kickstand a...   \n",
       "29           yes my dog love and it has richness however   \n",
       "31     this is great quality especially consider the ...   \n",
       "\n",
       "                                                 decoded gen_type  overlap  \n",
       "51707  like research thing and know bit about whateve...      Ext        8  \n",
       "51704    great collar great customer service from amazon      Ext        8  \n",
       "51678  this was the best investment make for our seni...      Ext        5  \n",
       "51674                 the best dslr camera you will find      Ext        5  \n",
       "51672         great tv for the price but could be better      Ext        7  \n",
       "51654                     great htpc case at great price      Ext        6  \n",
       "51650  the customer support system were good and some...      Ext        7  \n",
       "51621             great wide angle lens for the beginner      Ext       11  \n",
       "51608  comparison of columbia pfg bahama bonehead blo...      Ext       10  \n",
       "51605                       my old eye love lot of light      Ext        9  \n",
       "51601  but this is the best looking watch for android...      Ext        9  \n",
       "51598  but this is the best looking watch for android...      Ext        9  \n",
       "51585  have always want discretely sized excellent qu...      Ext       14  \n",
       "51582  matter how good the hardware the system is bas...      Abs        4  \n",
       "51577  the urban approach because it fit need better ...      Ext       17  \n",
       "51560     comparison of the cold steel leatherneck tanto      Ext        5  \n",
       "51532  this has become my new photography best friend...      Abs        4  \n",
       "51531       had been look for battery case for my iphone      Ext        6  \n",
       "51524                 great beginner phone for the price      Ext        7  \n",
       "51499  overall good product and fairly easy to setup ...      Ext        7  \n",
       "51492           great portable power pack at great price      Ext        5  \n",
       "51442                      the best tablet on the market      Ext        9  \n",
       "51434      logitech good all around webcam at good price      Ext        7  \n",
       "51418       great battery case for the zerolemon battery      Ext        5  \n",
       "51409                love the otis flexible memory cable      Ext        8  \n",
       "51397  there are several good thing about this produc...      Ext        8  \n",
       "51367                        good dash cam at good price      Abs        4  \n",
       "51324  let me say why star without doubt best value b...      Ext       14  \n",
       "51318                 overall good product for the money      Ext        7  \n",
       "51317  the atrix is great phone but the outdated vers...      Ext        7  \n",
       "...                                                  ...      ...      ...  \n",
       "53      great for light float but you are go to maneuver      Ext        8  \n",
       "52     this is great product it come with mfi certify...      Ext        7  \n",
       "58         grip feel pretty good even with lot of weight      Ext        8  \n",
       "8      get this watch for kid look great timing was a...      Ext       16  \n",
       "14               surprisingly this local dim tv but fact      Ext        5  \n",
       "12     nice hat make vietnam and with non functional ...      Ext        8  \n",
       "11     my dog treat arrive schedule and the dog enjoy...      Ext       10  \n",
       "10     wow amazing product although its not brand the...      Ext        9  \n",
       "9               good bag but close up with the sun shade      Ext        9  \n",
       "4      okay thought would more sturdy but for cockate...      Ext       10  \n",
       "6               get puncture with minute ride tiny shard      Abs        5  \n",
       "3      the gold piece around the phone case break ver...      Ext        8  \n",
       "2          total body workout you also learn how to pace      Ext        8  \n",
       "16                    great pack and the wheel are handy      Ext       15  \n",
       "15     use with the standard cover and although you c...      Ext       19  \n",
       "17           very bad product this is the second one buy      Ext        9  \n",
       "18     buy dollar phone and come with power cord what...      Ext        5  \n",
       "19     decent bag but rip pretty easily canyoneere tr...      Ext       10  \n",
       "20          my cat like it like how strong the smell was      Ext        8  \n",
       "0      thank you yes did receive mini camera and the ...      Ext       16  \n",
       "21     solid beginner lens for anyone who want to get...      Ext        9  \n",
       "22        my conure love these and not big fan of pellet      Ext        7  \n",
       "23             very nice and great quality for the price      Ext        8  \n",
       "24     my dog love it and love all the natural ingred...      Ext        8  \n",
       "25     the fisheye lens is the best not like other wi...      Ext       11  \n",
       "26     buy this for my husband galaxy and it work gre...      Ext       11  \n",
       "27     very more like solid chrome than cheap very ni...      Ext        6  \n",
       "28     this is an excellent case love the kickstand a...      Ext        9  \n",
       "29     yes my dog love and has richness however look ...      Ext        7  \n",
       "31     this is great quality especially consider the ...      Ext        7  \n",
       "\n",
       "[18975 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outFrame[test_outFrame[\"rouge_1\"]>=0.4][['rouge_1','article', 'reference', 'decoded', 'gen_type','overlap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_16 epoch_6\n",
    "# testing_avg_rouge_1: 0.3873426628114616 \\n', \n",
    "# 'testing_avg_rouge_2: 0.25943944916828854 \\n', \n",
    "# 'testing_avg_rouge_l: 0.3614074094052472 \\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('Leyan': conda)",
   "language": "python",
   "name": "python36764bitleyancondaa378f3cedbcc4b3f906a2276b3eef765"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
