{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eagleuser/.conda/envs/Leyan/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0530 21:09:17.276201 139743483926336 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/eagleuser/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_bert_token_sents(text):\n",
    "    text = bert_tokenizer.tokenize(text)\n",
    "    # 判斷詞綴是否在wordnet字典當中，否則將其視為詞根\n",
    "    text = [i.replace(\"##\",'') if wn.synsets(i.replace(\"##\",'')) \n",
    "         else i for i in text \n",
    "    ]\n",
    "\n",
    "    i = 1\n",
    "    while(i<len(text)):\n",
    "        if '##' in text[i-1]:\n",
    "            temp = (text[i-1] + text[i]).replace(\"##\",'')\n",
    "            if wn.synsets(temp):\n",
    "                pos = i-1\n",
    "                text.remove(text[i])\n",
    "                text.remove(text[i-1])            \n",
    "                text.insert(pos, temp)\n",
    "                i += 1\n",
    "        elif len(text[i])==1:\n",
    "            pos = i-1\n",
    "            temp = text[i-1] + text[i]\n",
    "            text.remove(text[i])\n",
    "            text.remove(text[i-1])            \n",
    "            text.insert(pos, temp)\n",
    "            i += 1       \n",
    "        i += 1\n",
    "\n",
    "    text = \" \".join(text).replace(\" ##\",\"\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amazon been it little speed purchase'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = 'qualityneed amazonbeen itlittle speedpurchase descriptionpurchase airstation everupdate elsewherebuy'\n",
    "text = 'amazonbeen itlittle speedpurchase'\n",
    "# text = 'glitchthis'\n",
    "# text = 'stand...purchase'\n",
    "nltk_bert_token_sents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
