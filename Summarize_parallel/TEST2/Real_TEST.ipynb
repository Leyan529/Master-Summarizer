{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "from utils import config\n",
    "from utils.seq2seq.batcher import *\n",
    "from utils.seq2seq.train_util import *\n",
    "import argparse\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--key_attention', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--intra_encoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--intra_decoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--copy', type=bool, default=True, help = 'True/False') # for transformer\n",
    "\n",
    "parser.add_argument('--model_type', type=str, default='seq2seq', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--train_rl', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='Noun_adj_keys', \n",
    "                    help = 'POS_keys / DEP_keys / Noun_adj_keys / TextRank_keys')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--rand_unif_init_mag', type=float, default=0.02)\n",
    "parser.add_argument('--trunc_norm_init_std', type=float, default=0.001)\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument('--gound_truth_prob', type=float, default=0.1)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=500)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=20)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=5)\n",
    "parser.add_argument('--max_epochs', type=int, default=15)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=16)\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=300)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--word_emb_type', type=str, default='word2Vec', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=True, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "    \n",
    "    \n",
    "    \n",
    "folder = 'B07YN7CMXS'\n",
    "\n",
    "# B00CJICDSS\n",
    "# B00DVPS4IQ\n",
    "# B01HGM33HG\n",
    "# B003VVP0KU\n",
    "# B004NBZB2Y\n",
    "# B07YN7CMXS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0720 11:41:46.072339 140308382467904 file_utils.py:35] PyTorch version 1.4.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer_generator_word2Vec_Intra_Atten_Key_Atten(Noun_adj_keys)_RL.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0720 11:41:49.281333 140308382467904 utils_any2vec.py:341] loading projection weights from /home/eagleuser/Users/leyan//Train-Data/Mix6_mainCat_best/Embedding/word2Vec/word2Vec.300d.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eagleuser/Users/leyan//Train-Data/Mix6_mainCat_best/Embedding/word2Vec/word2Vec.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0720 11:41:57.361805 140308382467904 utils_any2vec.py:405] loaded (49676, 300) matrix from /home/eagleuser/Users/leyan//Train-Data/Mix6_mainCat_best/Embedding/word2Vec/word2Vec.300d.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (encoder): Encoder(\n",
       "    (lstm): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
       "    (reduce_h): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (reduce_c): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (enc_attention): encoder_attention(\n",
       "      (W_h): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      (W_s): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (W_t): Linear(in_features=300, out_features=1024, bias=True)\n",
       "      (v): Linear(in_features=1024, out_features=1, bias=False)\n",
       "    )\n",
       "    (dec_attention): decoder_attention(\n",
       "      (W_prev): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (W_s): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (W_t): Linear(in_features=300, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (x_context): Linear(in_features=1324, out_features=300, bias=True)\n",
       "    (x_key_context): Linear(in_features=1624, out_features=300, bias=True)\n",
       "    (lstm): LSTMCell(300, 512)\n",
       "    (p_gen_linear): Linear(in_features=2860, out_features=1, bias=True)\n",
       "    (V): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (V1): Linear(in_features=512, out_features=50000, bias=True)\n",
       "  )\n",
       "  (embeds): Embedding(50000, 300)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from create_model.pg_multi_head import Model \n",
    "# load_model_path = 'Pointer_MultiHead.tar' \n",
    "\n",
    "from create_model.pg import Model \n",
    "load_model_path = 'Pointer_generator_word2Vec_Intra_Atten_Key_Atten(Noun_adj_keys)_RL.tar'\n",
    "\n",
    "\n",
    "\n",
    "import torch as T\n",
    "\n",
    "T.backends.cudnn.benchmark = True \n",
    "checkpoint = T.load(load_model_path)\n",
    "vocab = checkpoint['vocab']\n",
    "print(load_model_path)\n",
    "\n",
    "\n",
    "model = Model(pre_train_emb=config.pre_train_emb, \n",
    "              word_emb_type = config.word_emb_type, \n",
    "              vocab = vocab)\n",
    "\n",
    "model = model.cuda()\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils.seq2seq.batcher import Example, Batch\n",
    "from translate.seq2seq_beam import *\n",
    "\n",
    "config.batch_size = 1\n",
    "config.gound_truth_prob = 0.0\n",
    "\n",
    "def generate(data):\n",
    "    # ready data\n",
    "    ex = Example(config, vocab, data)\n",
    "    b = Batch([ex])\n",
    "    b.enc_pad_mask\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "                    get_input_from_batch(b, config, batch_first = True)\n",
    "    dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "            get_output_from_batch(b, config, batch_first = True)\n",
    "    max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0] \n",
    "    # encode\n",
    "    enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "    enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "    # model generate\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens, max_enc_len)\n",
    "\n",
    "    # 'Feed encoder data to predict'\n",
    "    pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                            enc_batch_extend_vocab, enc_key_batch, enc_key_mask, model, \n",
    "                            START, END, UNKNOWN_TOKEN)[0]\n",
    "    pred_words = [vocab.id2word(d) for d in pred_ids]\n",
    "    pred_words = \" \".join(pred_words) \n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B07YN7CMXS row 71: 100%|██████████| 72/72 [00:08<00:00,  8.81it/s]\n"
     ]
    }
   ],
   "source": [
    "fn = 'NEW_CRAWL2/%s/total.xlsx'%(folder)\n",
    "df = pd.read_excel(fn)\n",
    "df['pred_summary'] = ''\n",
    "df = df[df['summary_conflict']==False]\n",
    "df = df[df['summary_polarity']>0.1]\n",
    "df = df[df['summary_subjectivity']>0.1]\n",
    "df = df[df['Noun_adj_keys']!='[]']\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# idx = 0\n",
    "\n",
    "df['pred_summary'] = ''\n",
    "\n",
    "\n",
    "\n",
    "with tqdm(total=len(df)) as pbar:\n",
    "    for i ,row in df.iterrows(): \n",
    "        data = df.iloc[i].to_dict()\n",
    "        data['review_ID'] = str(i)\n",
    "        data['review'] = data['lemm_reviewtext']\n",
    "        data['summary'] = data['lemm_summary'].replace(\"<s> \",\" \").replace(\" </s>\",\"\")\n",
    "        summary_len = len(data['summary'].split(\" \"))\n",
    "        if summary_len <3: continue \n",
    "        pred_words = generate(data)\n",
    "#         print(data['summary'])\n",
    "        df.loc[i,'summary'] = data['summary'] \n",
    "        df.loc[i,'pred_summary'] = pred_words          \n",
    "        pbar.set_description(\"%s row %s\" % (folder, i, ))\n",
    "        pbar.update(1)\n",
    "        \n",
    "def func(text):\n",
    "    return len(text.split(\" \"))        \n",
    "        \n",
    "df['pred_summary_len'] = df['pred_summary'].apply(func)\n",
    "df['summary_len'] = df['summary'].apply(func)        \n",
    "df['dff_len'] = df['pred_summary_len'] - df['summary_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "def evaluate(data):\n",
    "    ref_sents = data['summary']\n",
    "    decoded_sents = data['pred_summary']\n",
    "    # Rouge\n",
    "    rouge = Rouge() \n",
    "    scores = rouge.get_scores(decoded_sents, ref_sents, avg = False)\n",
    "    score = scores[0]   \n",
    "#     rouge_1_p = round(score['rouge-1']['p'],2)\n",
    "#     rouge_2_p = round(score['rouge-2']['p'],2)\n",
    "#     rouge_l_p = round(score['rouge-l']['p'],2)\n",
    "\n",
    "    rouge_1_r = round(score['rouge-1']['r'],2)\n",
    "    rouge_2_r = round(score['rouge-2']['r'],2)\n",
    "    rouge_l_r = round(score['rouge-l']['r'],2)\n",
    "\n",
    "#     rouge_1_f = round(score['rouge-1']['f'],2)\n",
    "#     rouge_2_f = round(score['rouge-2']['f'],2)\n",
    "#     rouge_l_f = round(score['rouge-l']['f'],2)\n",
    "    \n",
    "    # BLEU\n",
    "    Bleu_1 = round(sentence_bleu([ref_sents.split(\" \")], decoded_sents.split(\" \"), weights=(1, 0, 0, 0)), 2)\n",
    "    Bleu_2 = round(sentence_bleu([ref_sents.split(\" \")], decoded_sents.split(\" \"), weights=(0.5, 0.5, 0, 0)), 2)\n",
    "    Bleu_3 = round(sentence_bleu([ref_sents.split(\" \")], decoded_sents.split(\" \"), weights=(0.33, 0.33, 0.33, 0)), 2)\n",
    "    Bleu_4 = round(sentence_bleu([ref_sents.split(\" \")], decoded_sents.split(\" \"), weights=(0.25, 0.25, 0.25, 0.25)), 2)\n",
    "    \n",
    "    #METEOR\n",
    "    Meteor = round(single_meteor_score(ref_sents, decoded_sents), 2)  \n",
    "    \n",
    "    eval_scores = {}\n",
    "    \n",
    "\n",
    "#     eval_scores['rouge_1_p'] = rouge_1_p\n",
    "#     eval_scores['rouge_2_p'] = rouge_2_p\n",
    "#     eval_scores['rouge_l_p'] = rouge_l_p\n",
    "    \n",
    "    eval_scores['rouge_1_r'] = rouge_1_r\n",
    "    eval_scores['rouge_2_r'] = rouge_2_r\n",
    "    eval_scores['rouge_l_r'] = rouge_l_r\n",
    "    \n",
    "#     eval_scores['rouge_1_f'] = rouge_1_f\n",
    "#     eval_scores['rouge_2_f'] = rouge_2_f\n",
    "#     eval_scores['rouge_l_f'] = rouge_l_f\n",
    "    \n",
    "    eval_scores['Bleu_1'] = Bleu_1\n",
    "    eval_scores['Bleu_2'] = Bleu_2\n",
    "    eval_scores['Bleu_3'] = Bleu_3\n",
    "    eval_scores['Bleu_4'] = Bleu_4\n",
    "    \n",
    "    eval_scores['Meteor'] = Meteor\n",
    "    return eval_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['summary','pred_summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B07YN7CMXS 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B07YN7CMXS row 1:   1%|▏         | 1/72 [00:01<01:29,  1.26s/it]/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "B07YN7CMXS row 71: 100%|██████████| 72/72 [00:02<00:00, 34.72it/s]\n"
     ]
    }
   ],
   "source": [
    "df = df[df['pred_summary']!='']\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(folder,len(df))\n",
    "with tqdm(total=len(df)) as pbar:\n",
    "    for i ,row in df.iterrows(): \n",
    "        data = df.iloc[i].to_dict()\n",
    "#         print(data)\n",
    "        eval_scores = evaluate(data) \n",
    "        for key ,score in eval_scores.items():\n",
    "            df.loc[i,key] = score\n",
    "        pbar.set_description(\"%s row %s\" % (folder, i, ))\n",
    "        pbar.update(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred num 72\n"
     ]
    }
   ],
   "source": [
    "df.to_excel('%s.xlsx'%folder, encoding='utf8', engine='xlsxwriter')\n",
    "print('pred num',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()\n",
    "with open(\"%s_res.txt\"%(folder), 'w',encoding='utf-8') as f:\n",
    "    f.write('##-- Rouge-1 --##\\n')\n",
    "#     f.write('rouge_1_p: '+ str(df.rouge_1_p.mean())+ '\\n')\n",
    "    f.write('rouge_1_r: '+ str(df.rouge_1_r.mean())+ '\\n')\n",
    "#     f.write('rouge_1_f: '+ str(df.rouge_1_f.mean())+ '\\n')\n",
    "    \n",
    "    f.write('##-- Rouge-2 --##\\n')\n",
    "#     f.write('rouge_2_p: '+ str(df.rouge_2_p.mean())+ '\\n')\n",
    "    f.write('rouge_2_r: '+ str(df.rouge_2_r.mean())+ '\\n')\n",
    "#     f.write('rouge_2_f: '+ str(df.rouge_2_f.mean())+ '\\n')\n",
    "    \n",
    "    f.write('##-- Rouge-l --##\\n')\n",
    "#     f.write('rouge_l_p: '+ str(df.rouge_l_p.mean())+ '\\n')\n",
    "    f.write('rouge_l_r: '+ str(df.rouge_l_r.mean())+ '\\n')\n",
    "#     f.write('rouge_l_f: '+ str(df.rouge_l_f.mean())+ '\\n')\n",
    "    \n",
    "    f.write('##-- BLEU --##\\n')\n",
    "    f.write('Bleu_1: '+ str(df.Bleu_1.mean())+ '\\n')\n",
    "    f.write('Bleu_2: '+ str(df.Bleu_2.mean())+ '\\n')\n",
    "    f.write('Bleu_3: '+ str(df.Bleu_3.mean())+ '\\n')\n",
    "    f.write('Bleu_4: '+ str(df.Bleu_4.mean())+ '\\n')\n",
    "    \n",
    "    f.write('##-- Meteor --##\\n')\n",
    "    f.write('Meteor: '+ str(df.Meteor.mean())+ '\\n')\n",
    "    \n",
    "    f.write('dff_len: '+ str(df.dff_len.mean())+ '\\n')\n",
    "\n",
    "# for folder in os.listdir('NEW_CRAWL'):\n",
    "#     fn = 'NEW_CRAWL/%s/total.xlsx'%(folder)\n",
    "#     df = pd.read_excel(fn)\n",
    "    \n",
    "#     print(fn)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2537499999999999\n",
      "3.638888888888889\n"
     ]
    }
   ],
   "source": [
    "print(df.rouge_l_r.mean())\n",
    "print(df.dff_len.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
