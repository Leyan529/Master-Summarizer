{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 14:21:50.408060 139644430731072 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-04-29 14:21:51 - Pointer_Transformer_NoPretrain - INFO: - logger已啟動\n",
      "I0429 14:21:51.161508 139644430731072 train_util.py:119] logger已啟動\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loggerName Pointer_Transformer_NoPretrain\n",
      "writerPath runs/Mix6_mainCat/Pointer_Transformer/NoPretrain/exp\n"
     ]
    }
   ],
   "source": [
    "from utils import config\n",
    "from utils.bert import data\n",
    "\n",
    "from utils.bert.batcher import *\n",
    "from utils.bert.train_util import *\n",
    "from utils.bert.initialize import loadCheckpoint, save_model\n",
    "from utils.bert.write_result import *\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_type', type=str, default='transformer', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--copy', type=bool, default=True, choices=[True, False])\n",
    "parser.add_argument(\"-encoder\", default='Transformer', type=str, choices=['bert', 'Transformer'])\n",
    "parser.add_argument(\"-max_pos\", default=1000, type=int)\n",
    "parser.add_argument(\"-use_bert_emb\", type=str2bool, nargs='?',const=False,default=False, choices=[False, True])\n",
    "\n",
    "parser.add_argument(\"-lr_bert\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-lr_dec\", default=2e-3, type=float)\n",
    "parser.add_argument(\"-share_emb\", type=str2bool, nargs='?', const=True, default=False)\n",
    "parser.add_argument(\"-finetune_bert\", type=bool, default=True)\n",
    "    \n",
    "'''\n",
    "原transformer paper核心參數\n",
    "dropout = 0.1\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "emb_dim(d_model) : 512\n",
    "ff_embed_dim = 2048    \n",
    "\n",
    "bert_config = BertConfig(self.encoder.model.config.vocab_size, hidden_size=768,\n",
    "                                     num_hidden_layers=12, num_attention_heads=8,\n",
    "                                     intermediate_size= 3072,\n",
    "                                     hidden_dropout_prob=0.1,\n",
    "                                     attention_probs_dropout_prob=0.1)\n",
    "'''\n",
    "parser.add_argument(\"-enc_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-enc_layers\", default=6, type=int)\n",
    "parser.add_argument(\"-enc_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-enc_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-enc_ff_size\", default=2048, type=int)\n",
    "\n",
    "parser.add_argument(\"-dec_dropout\", default=0.2, type=float)\n",
    "parser.add_argument(\"-dec_layers\", default=6, type=int)\n",
    "parser.add_argument(\"-dec_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-dec_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-dec_ff_size\", default=2048, type=int)\n",
    "parser.add_argument(\"-sep_optim\", type=str2bool, nargs='?',const=True,default=False, choices=[False, True])\n",
    "\n",
    "parser.add_argument(\"-param_init\", default=0, type=float)\n",
    "parser.add_argument(\"-param_init_glorot\", type=str2bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-optim\", default='adam', type=str)\n",
    "parser.add_argument(\"-lr\", default=1, type=float)\n",
    "parser.add_argument(\"-beta1\", default= 0.9, type=float)\n",
    "parser.add_argument(\"-beta2\", default=0.999, type=float)\n",
    "parser.add_argument(\"-warmup_steps\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_bert\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_dec\", default=8000, type=int)\n",
    "parser.add_argument(\"-max_grad_norm\", default=0, type=float)\n",
    "\n",
    "parser.add_argument(\"-block_trigram\", type=str2bool, nargs='?', const=True, default=True)\n",
    "\n",
    "\n",
    "parser.add_argument('--train_rl', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_keys', \n",
    "                    help = 'POS_keys / DEP_keys / Noun_adj_keys / TextRank_keys')\n",
    "\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument(\"-label_smoothing\", default=0.1, type=float)\n",
    "parser.add_argument(\"-generator_shard_size\", default=32, type=int)\n",
    "parser.add_argument(\"-alpha\",  default=0.6, type=float)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=1000)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=50)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=8)\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=16)\n",
    "parser.add_argument('--batch_size', type=int, default=6)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=512)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default='0000010', help='0000010')\n",
    "# parser.add_argument('--word_emb_type', type=str, default='glove', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=False, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 14:21:52.285758 139644430731072 tokenization.py:157] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "{'BOS': 1, 'EOS': 2, 'PAD': 0, 'EOQ': 3, 'SEP': 102, 'CLS': 101, 'UNK': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 14:22:59 - Pointer_Transformer_NoPretrain - INFO: - train : 439076, test : 48787\n",
      "I0429 14:22:59.165392 139644430731072 batcher.py:209] train : 439076, test : 48787\n",
      "2020-04-29 14:22:59 - Pointer_Transformer_NoPretrain - INFO: - train batches : 73179, test batches : 8131\n",
      "I0429 14:22:59.568717 139644430731072 batcher.py:224] train batches : 73179, test batches : 8131\n"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab, symbols = getDataLoader(logger, config)\n",
    "tokenizer = vocab.tokenizer\n",
    "train_batches = len(iter(train_loader))\n",
    "test_batches = len(iter(validate_loader))\n",
    "save_steps = int(train_batches/1000)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0429 14:23:00.718711 139644430731072 modeling_utils.py:199] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0429 14:23:00.721032 139644430731072 modeling_utils.py:216] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0429 14:23:01.762507 139644430731072 modeling_utils.py:533] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 768])\n"
     ]
    }
   ],
   "source": [
    "from utils.transformer.loss import *\n",
    "from utils.transformer.optimizers import Optimizer\n",
    "from transformer import *\n",
    "from utils.transformer.predictor import build_predictor\n",
    "\n",
    "model = AbsSummarizer(config)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(config, logger, load_model_path, model)\n",
    "else:    \n",
    "    if (config.sep_optim):\n",
    "        optim_bert = Optimizer(\n",
    "            config.optim, config.lr_bert, config.max_grad_norm,\n",
    "            beta1=config.beta1, beta2=config.beta2,\n",
    "            decay_method='noam',\n",
    "            warmup_steps=config.warmup_steps_bert)\n",
    "\n",
    "        optim_dec = Optimizer(\n",
    "            config.optim, config.lr_dec, config.max_grad_norm,\n",
    "            beta1=config.beta1, beta2=config.beta2,\n",
    "            decay_method='noam',\n",
    "            warmup_steps=config.warmup_steps_dec)\n",
    "        \n",
    "        params = [(n, p) for n, p in list(model.named_parameters()) if n.startswith('encoder.model')]\n",
    "        optim_bert.set_parameters(params)\n",
    "\n",
    "        params = [(n, p) for n, p in list(model.named_parameters()) if not n.startswith('encoder.model')]\n",
    "        optim_dec.set_parameters(params)\n",
    "\n",
    "        optimizer = [optim_bert, optim_dec]\n",
    "    else:\n",
    "        optimizer = Optimizer(\n",
    "            config.optim, config.lr, config.max_grad_norm,\n",
    "            beta1=config.beta1, beta2=config.beta2,\n",
    "            decay_method='noam',\n",
    "            warmup_steps=config.warmup_steps)\n",
    "        optimizer.set_parameters(list(model.named_parameters()))\n",
    "        optimizer = [optimizer]\n",
    "model = get_cuda(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model, config, batch):\n",
    "    normalization = 0\n",
    "    'Encoder data'\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, _, _, _, _, _, enc_seg, enc_cls, enc_cls_mask = get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "    'Decoder data'\n",
    "    dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = get_output_from_batch(batch, config, batch_first = True) # Get input and target \n",
    "\n",
    "    \n",
    "    num_tokens = dec_batch[:, 1:].ne(0).sum()\n",
    "    normalization += num_tokens.item()    \n",
    "\n",
    "    pred, state = model(enc_batch, dec_batch, enc_seg, \n",
    "        enc_cls, enc_padding_mask, dec_padding_mask, enc_cls_mask, \n",
    "        extra_zeros, enc_batch_extend_vocab)\n",
    "    criterion = choose_criterion(config, model.vocab_size)\n",
    "    loss, num_correct, target = compute_loss(model, criterion, pred, dec_batch[:,1:], num_tokens, tokenizer)\n",
    "    # loss = loss / normalization  # Normalized losses; (batch_size)\n",
    "    # --------------------------------------------------------------------------------\n",
    "    acc = accuracy(num_correct, num_tokens)\n",
    "    cross_entropy = xent(loss, num_tokens)\n",
    "    perplexity = ppl(loss, num_tokens)\n",
    "\n",
    "#     print(\"num_tokens:%s; acc: %6.2f; perplexity: %5.2f; cross entropy loss: %4.2f\" \n",
    "#                             % (num_tokens,\n",
    "#                             acc,\n",
    "#                             perplexity,\n",
    "#                             cross_entropy\n",
    "#                             ))\n",
    "    # >>>>>>>> DEBUG Session <<<<<<<<<\n",
    "    # print('------------------------------------')\n",
    "    # print(\"ENC\\n\")\n",
    "    # print(enc_batch.shape)\n",
    "    # print(\"DEC\\n\")\n",
    "    # print(dec_batch.shape)\n",
    "    # print(\"TGT\\n\")\n",
    "    # print(target_batch.shape)\n",
    "    # print(\"ENCP\\n\")\n",
    "    # print(enc_padding_mask.shape)\n",
    "    # print(\"DECP\\n\")\n",
    "    # print(dec_padding_mask.shape)\n",
    "    # print(\"enc_seg\\n\")\n",
    "    # print(enc_seg.shape)\n",
    "    # print(\"enc_cls\\n\")\n",
    "    # print(enc_cls.shape)\n",
    "    # print(\"enc_cls_mask\\n\")\n",
    "    # print(enc_cls_mask.shape)\n",
    "    return loss.div(float(normalization))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, model):\n",
    "    losses = []\n",
    "    # batch = next(iter(validate_loader))\n",
    "    val_num = len(iter(validate_loader))\n",
    "    for idx, batch in enumerate(validate_loader):\n",
    "        loss = train_one(model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "        if idx>= val_num/10: break\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "@torch.autograd.no_grad()\n",
    "def decode(writer, logger, step, config, model, batch, mode):\n",
    "    # 動態取batch\n",
    "    if mode == 'test':\n",
    "        # num = len(iter(batch))\n",
    "        # select_batch = None\n",
    "        # rand_b_id = randint(0,num-1)\n",
    "        # logger.info('test_batch : ' + str(num)+ ' ' + str(rand_b_id))\n",
    "        # for idx, b in enumerate(batch):\n",
    "        #     if idx == rand_b_id:\n",
    "        #         select_batch = b\n",
    "        #         break\n",
    "        select_batch = next(iter(batch))\n",
    "        batch = select_batch\n",
    "        if type(batch) == torch.utils.data.dataloader.DataLoader:\n",
    "            batch = next(iter(batch))\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    '''\n",
    "    batch_data = self.translate_batch(batch)\n",
    "    translations = self.from_batch(batch_data)\n",
    "    '''\n",
    "    gold_tgt_len = batch.dec_tgt.size(1)\n",
    "    setattr(config, 'min_length',gold_tgt_len + 20)\n",
    "    setattr(config, 'max_length',gold_tgt_len + 60)\n",
    "    predictor = build_predictor(config, tokenizer, symbols, model, logger)\n",
    "\n",
    "    # 'Encoder data'\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, _, _, _, _, _, enc_seg, enc_cls, enc_cls_mask =         get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "    # 'Decoder data'\n",
    "    dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = get_output_from_batch(batch, config, batch_first = True) # Get input and target \n",
    "\n",
    "    setattr(batch, 'src',enc_batch)\n",
    "    setattr(batch, 'segs',enc_seg)\n",
    "    setattr(batch, 'mask_src',enc_padding_mask)\n",
    "\n",
    "    batch_data = predictor.translate_batch(batch)\n",
    "    translations = predictor.from_batch(batch_data) # translation = (pred_sents, gold_sent, raw_src)\n",
    "    article_sents = [t[2] for t in translations]\n",
    "    decoded_sents = [t[0] for t in translations]\n",
    "    ref_sents = [t[1] for t in translations]\n",
    "    keywords_list = [str(word_list) for word_list in batch.key_words]\n",
    "#     print('decoded_sents',decoded_sents)\n",
    "    # ---------------------------------------------------------------------------\n",
    "    rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents,                     keywords_list, ref_sents, 0, write = False)\n",
    "    write_bleu(writer, step, mode, article_sents, decoded_sents,                keywords_list, ref_sents, 0)\n",
    "\n",
    "    write_group(writer, step, mode, article_sents, decoded_sents,                keywords_list, ref_sents, 0)\n",
    "\n",
    "    return rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import time\n",
    "@torch.autograd.no_grad()\n",
    "def avg_acc(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(iter(dataloader))\n",
    "    avg_rouge_l = []\n",
    "    acc_st, acc_cost = 0, 0\n",
    "    avg_acc_cost = []\n",
    "    for idx, batch in enumerate(dataloader): \n",
    "        if idx >= num/10000: break\n",
    "        acc_st = time.time()\n",
    "        # ---------------------------------------------------------------------------\n",
    "        '''\n",
    "        batch_data = self.translate_batch(batch)\n",
    "        translations = self.from_batch(batch_data)\n",
    "        '''\n",
    "        gold_tgt_len = batch.dec_tgt.size(1)\n",
    "        setattr(config, 'min_length',gold_tgt_len + 20)\n",
    "        setattr(config, 'max_length',gold_tgt_len + 60)\n",
    "        predictor = build_predictor(config, tokenizer, symbols, model, logger)\n",
    "\n",
    "        # 'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, _, _, _, _, _, enc_seg, enc_cls, enc_cls_mask = get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        # 'Decoder data'\n",
    "        dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = get_output_from_batch(batch, config, batch_first = True) # Get input and target \n",
    "\n",
    "        setattr(batch, 'src',enc_batch)\n",
    "        setattr(batch, 'segs',enc_seg)\n",
    "        setattr(batch, 'mask_src',enc_padding_mask)\n",
    "\n",
    "        batch_data = predictor.translate_batch(batch)\n",
    "        translations = predictor.from_batch(batch_data) # translation = (pred_sents, gold_sent, raw_src)\n",
    "        article_sents = [t[2] for t in translations]\n",
    "        decoded_sents = [t[0] for t in translations]\n",
    "        ref_sents = [t[1] for t in translations]\n",
    "        keywords_list = [str(word_list) for word_list in batch.key_words]\n",
    "\n",
    "\n",
    "        rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents,                         keywords_list, ref_sents, 0, write = False)\n",
    "        # ---------------------------------------------------------------------------\n",
    "        avg_rouge_l.append(rouge_l)\n",
    "        acc_cost = time.time() - acc_st\n",
    "        avg_acc_cost.append(acc_cost)\n",
    "\n",
    "\n",
    "    avg_rouge_l = sum(avg_rouge_l) / len(avg_rouge_l)\n",
    "    writer.add_scalars('scalar_avg/acc',  \n",
    "                   {'%sing_avg_acc'%(mode): avg_rouge_l\n",
    "                   }, epoch)\n",
    "    avg_acc_cost = sum(avg_acc_cost) / len(avg_acc_cost)\n",
    "    return avg_rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(iter(dataloader))\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    outFrame = None\n",
    "    avg_time = 0\n",
    "    \n",
    "    rouge = Rouge()  \n",
    "    \n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        start = time.time() \n",
    "        gold_tgt_len = batch.dec_tgt.size(1)\n",
    "        setattr(config, 'min_length',gold_tgt_len + 20)\n",
    "        setattr(config, 'max_length',gold_tgt_len + 60)\n",
    "        predictor = build_predictor(config, tokenizer, symbols, model, logger)\n",
    "\n",
    "        # 'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, _, \\\n",
    "        _, _, _, _, enc_seg, enc_cls, enc_cls_mask = \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        # 'Decoder data'\n",
    "        dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(batch, config, batch_first = True) # Get input and target \n",
    "\n",
    "        setattr(batch, 'src',enc_batch)\n",
    "        setattr(batch, 'segs',enc_seg)\n",
    "        setattr(batch, 'mask_src',enc_padding_mask)\n",
    "\n",
    "        batch_data = predictor.translate_batch(batch)\n",
    "        translations = predictor.from_batch(batch_data) # translation = (pred_sents, gold_sent, raw_src)\n",
    "        article_sents = [t[2] for t in translations]\n",
    "        decoded_sents = [t[0] for t in translations]\n",
    "        ref_sents = [t[1] for t in translations]\n",
    "        keywords_list = [str(word_list) for word_list in batch.key_words]\n",
    "        cost = (time.time() - start)\n",
    "\n",
    "        avg_time += cost        \n",
    "\n",
    "        overlap = [len(set(article_sents[i].split(\" \")) & set(ref_sents[i].split(\" \"))) for i in range(len(article_sents))]\n",
    "        too_overlap = [overlap[i] > len(set(ref_sents[i].split(\" \")))-3 for i in range(len(article_sents))]\n",
    "        scores = rouge.get_scores(decoded_sents, ref_sents, avg = False)\n",
    "        rouge_1 = [score['rouge-1']['f'] for score in scores]\n",
    "        rouge_2 = [score['rouge-2']['f'] for score in scores]\n",
    "        rouge_l = [score['rouge-l']['f'] for score in scores]\n",
    "\n",
    "        batch_frame = {\n",
    "            'article':article_sents,\n",
    "            'keywords':keywords_list,\n",
    "            'reference':ref_sents,\n",
    "            'decoded':decoded_sents,\n",
    "            'ref_lens': [len(r.split(\" \")) for r in ref_sents],\n",
    "            'overlap': overlap,\n",
    "            'too_overlap': too_overlap,\n",
    "            'rouge_1':rouge_1,\n",
    "            'rouge_2':rouge_2,\n",
    "            'rouge_l':rouge_l,\n",
    "            'article_lens': [len(r.split(\" \")) for r in article_sents],\n",
    "#             'Bleu_1':Bleu_1,\n",
    "#             'Bleu_2':Bleu_2,\n",
    "#             'Bleu_3':Bleu_3,\n",
    "#             'Bleu_4':Bleu_4,\n",
    "        }\n",
    "        batch_frame = pd.DataFrame(batch_frame)\n",
    "        if idx %1000 ==0 and idx >0 : print(idx)\n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)        \n",
    "#         avg_bleu1.append(Bleu_1)\n",
    "#         avg_bleu2.append(Bleu_2)\n",
    "#         avg_bleu3.append(Bleu_3)\n",
    "#         avg_bleu4.append(Bleu_4)\n",
    "        # ----------------------------------------------------\n",
    "#     print(avg_rouge_1)\n",
    "    avg_rouge_1 = sum(avg_rouge_1) / len(avg_rouge_1)\n",
    "    avg_rouge_2 = sum(avg_rouge_2) / len(avg_rouge_2)\n",
    "    avg_rouge_l = sum(avg_rouge_l) / len(avg_rouge_l)\n",
    "    writer.add_scalars('Rouge_avg/mode',  \n",
    "                    {'avg_rouge_1': avg_rouge_1,\n",
    "                    'avg_rouge_2': avg_rouge_2,\n",
    "                    'avg_rouge_l': avg_rouge_l\n",
    "                    }, epoch)\n",
    "    # --------------------------------------               \n",
    "#     avg_bleu1 = sum(avg_bleu1)/len(avg_bleu1)\n",
    "#     avg_bleu2 = sum(avg_bleu2)/len(avg_bleu2)\n",
    "#     avg_bleu3 = sum(avg_bleu3)/len(avg_bleu3)\n",
    "#     avg_bleu4 = sum(avg_bleu4)/len(avg_bleu4)\n",
    "    \n",
    "#     writer.add_scalars('BLEU_avg/mode',  \n",
    "#                     {\n",
    "#                     '%sing_avg_bleu1'%(mode): avg_bleu1,\n",
    "#                     '%sing_avg_bleu1'%(mode): avg_bleu2,\n",
    "#                     '%sing_avg_bleu1'%(mode): avg_bleu3,\n",
    "#                     '%sing_avg_bleu1'%(mode): avg_bleu4,                   \n",
    "#                     }, epoch)\n",
    "    # --------------------------------------      \n",
    "#     outFrame.to_excel(writerPath + '/%s_output.xls'% mode)\n",
    "    avg_time = avg_time / (num * config.batch_size) \n",
    "    with open(writerPath + '/%s_res.txt'% mode, 'w', encoding='utf-8') as f:\n",
    "        f.write('Accuracy result:\\n')\n",
    "        f.write('##-- Rouge --##\\n')\n",
    "        f.write('%sing_avg_rouge_1: %s \\n'%(mode, avg_rouge_1))\n",
    "        f.write('%sing_avg_rouge_2: %s \\n'%(mode, avg_rouge_2))\n",
    "        f.write('%sing_avg_rouge_l: %s \\n'%(mode, avg_rouge_l))\n",
    "\n",
    "#         f.write('##-- BLEU --##\\n')\n",
    "#         f.write('%sing_avg_bleu1: %s \\n'%(mode, avg_bleu1))\n",
    "#         f.write('%sing_avg_bleu2: %s \\n'%(mode, avg_bleu2))\n",
    "#         f.write('%sing_avg_bleu3: %s \\n'%(mode, avg_bleu3))\n",
    "#         f.write('%sing_avg_bleu4: %s \\n'%(mode, avg_bleu4))\n",
    "\n",
    "        f.write('Execute Time: %s \\n' % avg_time)        \n",
    "    # --------------------------------------     \n",
    "    outFrame = outFrame.sort_values(by=['article_lens'], ascending = False)\n",
    "    outFrame = outFrame[:1000]\n",
    "    outFrame.to_excel(writerPath + '/%s_output.xls'% mode)\n",
    "    return avg_rouge_l, outFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 14:23:09 - Pointer_Transformer_NoPretrain - INFO: - ------Training START--------\n",
      "I0429 14:23:09.126385 139644430731072 <ipython-input-11-659d4f858dd8>:6] ------Training START--------\n",
      "2020-04-29 20:50:18 - Pointer_Transformer_NoPretrain - INFO: - epoch 0: 73000, training batch loss = 5.324286, running_avg_loss loss = 5.633060, validation loss = 5.815689\n",
      "I0429 20:50:18.028122 139644430731072 <ipython-input-11-659d4f858dd8>:41] epoch 0: 73000, training batch loss = 5.324286, running_avg_loss loss = 5.633060, validation loss = 5.815689\n",
      "2020-04-29 20:50:18 - Pointer_Transformer_NoPretrain - INFO: - epoch 0|step 73000| compute loss cost = 23.778488 ms\n",
      "I0429 20:50:18.030856 139644430731072 <ipython-input-11-659d4f858dd8>:63] epoch 0|step 73000| compute loss cost = 23.778488 ms\n",
      "2020-04-29 20:50:18 - Pointer_Transformer_NoPretrain - INFO: - Saving model step 73000 to model/saved_models/Pointer_Transformer_NoPretrain/0073000.tar...\n",
      "I0429 20:50:18.034990 139644430731072 initialize.py:219] Saving model step 73000 to model/saved_models/Pointer_Transformer_NoPretrain/0073000.tar...\n",
      "2020-04-29 20:50:31 - Pointer_Transformer_NoPretrain - INFO: - epoch 0|step 73000| decode cost = 11.204496 ms\n",
      "I0429 20:50:31.283315 139644430731072 <ipython-input-11-659d4f858dd8>:72] epoch 0|step 73000| decode cost = 11.204496 ms\n",
      "2020-04-29 20:50:31 - Pointer_Transformer_NoPretrain - INFO: - epoch 0: 73000, train_rouge_l_f = 0.021957, test_rouge_l_f = 0.020331\n",
      "I0429 20:50:31.286781 139644430731072 <ipython-input-11-659d4f858dd8>:80] epoch 0: 73000, train_rouge_l_f = 0.021957, test_rouge_l_f = 0.020331\n",
      "2020-04-29 20:51:20 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0429 20:51:20.108924 139644430731072 <ipython-input-11-659d4f858dd8>:82] -------------------------------------------------------------\n",
      "2020-04-29 20:52:20 - Pointer_Transformer_NoPretrain - INFO: - epoch 0|step 73179| train_avg_acc = 0.015186, test_avg_acc = 0.022972\n",
      "I0429 20:52:20.923878 139644430731072 <ipython-input-11-659d4f858dd8>:85] epoch 0|step 73179| train_avg_acc = 0.015186, test_avg_acc = 0.022972\n",
      "2020-04-29 20:52:20 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0429 20:52:20.925632 139644430731072 <ipython-input-11-659d4f858dd8>:90] -------------------------------------------------------------\n",
      "2020-04-30 03:15:17 - Pointer_Transformer_NoPretrain - INFO: - epoch 1: 146000, training batch loss = 5.395567, running_avg_loss loss = 5.341407, validation loss = 5.815689\n",
      "I0430 03:15:17.976244 139644430731072 <ipython-input-11-659d4f858dd8>:41] epoch 1: 146000, training batch loss = 5.395567, running_avg_loss loss = 5.341407, validation loss = 5.815689\n",
      "2020-04-30 03:15:17 - Pointer_Transformer_NoPretrain - INFO: - epoch 1|step 146000| compute loss cost = 23.051186 ms\n",
      "I0430 03:15:17.978303 139644430731072 <ipython-input-11-659d4f858dd8>:63] epoch 1|step 146000| compute loss cost = 23.051186 ms\n",
      "2020-04-30 03:15:17 - Pointer_Transformer_NoPretrain - INFO: - Saving model step 146000 to model/saved_models/Pointer_Transformer_NoPretrain/0146000.tar...\n",
      "I0430 03:15:17.981829 139644430731072 initialize.py:219] Saving model step 146000 to model/saved_models/Pointer_Transformer_NoPretrain/0146000.tar...\n",
      "2020-04-30 03:15:31 - Pointer_Transformer_NoPretrain - INFO: - epoch 1|step 146000| decode cost = 11.290389 ms\n",
      "I0430 03:15:31.287149 139644430731072 <ipython-input-11-659d4f858dd8>:72] epoch 1|step 146000| decode cost = 11.290389 ms\n",
      "2020-04-30 03:15:31 - Pointer_Transformer_NoPretrain - INFO: - epoch 1: 146000, train_rouge_l_f = 0.006118, test_rouge_l_f = 0.000000\n",
      "I0430 03:15:31.290608 139644430731072 <ipython-input-11-659d4f858dd8>:80] epoch 1: 146000, train_rouge_l_f = 0.006118, test_rouge_l_f = 0.000000\n",
      "2020-04-30 03:17:09 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0430 03:17:09.813158 139644430731072 <ipython-input-11-659d4f858dd8>:82] -------------------------------------------------------------\n",
      "2020-04-30 03:18:03 - Pointer_Transformer_NoPretrain - INFO: - epoch 1|step 146358| train_avg_acc = 0.009483, test_avg_acc = 0.005743\n",
      "I0430 03:18:03.496886 139644430731072 <ipython-input-11-659d4f858dd8>:85] epoch 1|step 146358| train_avg_acc = 0.009483, test_avg_acc = 0.005743\n",
      "2020-04-30 03:18:03 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0430 03:18:03.498646 139644430731072 <ipython-input-11-659d4f858dd8>:90] -------------------------------------------------------------\n",
      "2020-04-30 09:35:46 - Pointer_Transformer_NoPretrain - INFO: - epoch 2: 219000, training batch loss = 4.037880, running_avg_loss loss = 5.258277, validation loss = 5.815689\n",
      "I0430 09:35:46.456651 139644430731072 <ipython-input-11-659d4f858dd8>:41] epoch 2: 219000, training batch loss = 4.037880, running_avg_loss loss = 5.258277, validation loss = 5.815689\n",
      "2020-04-30 09:35:46 - Pointer_Transformer_NoPretrain - INFO: - epoch 2|step 219000| compute loss cost = 23.859584 ms\n",
      "I0430 09:35:46.458564 139644430731072 <ipython-input-11-659d4f858dd8>:63] epoch 2|step 219000| compute loss cost = 23.859584 ms\n",
      "2020-04-30 09:35:46 - Pointer_Transformer_NoPretrain - INFO: - Saving model step 219000 to model/saved_models/Pointer_Transformer_NoPretrain/0219000.tar...\n",
      "I0430 09:35:46.461968 139644430731072 initialize.py:219] Saving model step 219000 to model/saved_models/Pointer_Transformer_NoPretrain/0219000.tar...\n",
      "2020-04-30 09:36:00 - Pointer_Transformer_NoPretrain - INFO: - epoch 2|step 219000| decode cost = 11.350336 ms\n",
      "I0430 09:36:00.122378 139644430731072 <ipython-input-11-659d4f858dd8>:72] epoch 2|step 219000| decode cost = 11.350336 ms\n",
      "2020-04-30 09:36:00 - Pointer_Transformer_NoPretrain - INFO: - epoch 2: 219000, train_rouge_l_f = 0.075805, test_rouge_l_f = 0.036784\n",
      "I0430 09:36:00.125853 139644430731072 <ipython-input-11-659d4f858dd8>:80] epoch 2: 219000, train_rouge_l_f = 0.075805, test_rouge_l_f = 0.036784\n",
      "2020-04-30 09:38:27 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0430 09:38:27.290312 139644430731072 <ipython-input-11-659d4f858dd8>:82] -------------------------------------------------------------\n",
      "2020-04-30 09:39:26 - Pointer_Transformer_NoPretrain - INFO: - epoch 2|step 219537| train_avg_acc = 0.007368, test_avg_acc = 0.026784\n",
      "I0430 09:39:26.444254 139644430731072 <ipython-input-11-659d4f858dd8>:85] epoch 2|step 219537| train_avg_acc = 0.007368, test_avg_acc = 0.026784\n",
      "2020-04-30 09:39:26 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0430 09:39:26.446018 139644430731072 <ipython-input-11-659d4f858dd8>:90] -------------------------------------------------------------\n",
      "2020-04-30 16:03:10 - Pointer_Transformer_NoPretrain - INFO: - epoch 3: 292000, training batch loss = 6.107387, running_avg_loss loss = 5.233013, validation loss = 5.815689\n",
      "I0430 16:03:10.444216 139644430731072 <ipython-input-11-659d4f858dd8>:41] epoch 3: 292000, training batch loss = 6.107387, running_avg_loss loss = 5.233013, validation loss = 5.815689\n",
      "2020-04-30 16:03:10 - Pointer_Transformer_NoPretrain - INFO: - epoch 3|step 292000| compute loss cost = 24.184973 ms\n",
      "I0430 16:03:10.446590 139644430731072 <ipython-input-11-659d4f858dd8>:63] epoch 3|step 292000| compute loss cost = 24.184973 ms\n",
      "2020-04-30 16:03:10 - Pointer_Transformer_NoPretrain - INFO: - Saving model step 292000 to model/saved_models/Pointer_Transformer_NoPretrain/0292000.tar...\n",
      "I0430 16:03:10.451715 139644430731072 initialize.py:219] Saving model step 292000 to model/saved_models/Pointer_Transformer_NoPretrain/0292000.tar...\n",
      "2020-04-30 16:03:23 - Pointer_Transformer_NoPretrain - INFO: - epoch 3|step 292000| decode cost = 11.338569 ms\n",
      "I0430 16:03:23.727808 139644430731072 <ipython-input-11-659d4f858dd8>:72] epoch 3|step 292000| decode cost = 11.338569 ms\n",
      "2020-04-30 16:03:23 - Pointer_Transformer_NoPretrain - INFO: - epoch 3: 292000, train_rouge_l_f = 0.006102, test_rouge_l_f = 0.070932\n",
      "I0430 16:03:23.731009 139644430731072 <ipython-input-11-659d4f858dd8>:80] epoch 3: 292000, train_rouge_l_f = 0.006102, test_rouge_l_f = 0.070932\n",
      "2020-04-30 16:06:45 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0430 16:06:45.698653 139644430731072 <ipython-input-11-659d4f858dd8>:82] -------------------------------------------------------------\n",
      "2020-04-30 16:07:52 - Pointer_Transformer_NoPretrain - INFO: - epoch 3|step 292716| train_avg_acc = 0.006269, test_avg_acc = 0.049836\n",
      "I0430 16:07:52.544625 139644430731072 <ipython-input-11-659d4f858dd8>:85] epoch 3|step 292716| train_avg_acc = 0.006269, test_avg_acc = 0.049836\n",
      "2020-04-30 16:07:52 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0430 16:07:52.546486 139644430731072 <ipython-input-11-659d4f858dd8>:90] -------------------------------------------------------------\n",
      "2020-04-30 22:34:38 - Pointer_Transformer_NoPretrain - INFO: - epoch 4: 365000, training batch loss = 4.447283, running_avg_loss loss = 5.229193, validation loss = 5.815689\n",
      "I0430 22:34:38.059729 139644430731072 <ipython-input-11-659d4f858dd8>:41] epoch 4: 365000, training batch loss = 4.447283, running_avg_loss loss = 5.229193, validation loss = 5.815689\n",
      "2020-04-30 22:34:38 - Pointer_Transformer_NoPretrain - INFO: - epoch 4|step 365000| compute loss cost = 25.512311 ms\n",
      "I0430 22:34:38.061752 139644430731072 <ipython-input-11-659d4f858dd8>:63] epoch 4|step 365000| compute loss cost = 25.512311 ms\n",
      "2020-04-30 22:34:38 - Pointer_Transformer_NoPretrain - INFO: - Saving model step 365000 to model/saved_models/Pointer_Transformer_NoPretrain/0365000.tar...\n",
      "I0430 22:34:38.067494 139644430731072 initialize.py:219] Saving model step 365000 to model/saved_models/Pointer_Transformer_NoPretrain/0365000.tar...\n",
      "2020-04-30 22:34:54 - Pointer_Transformer_NoPretrain - INFO: - epoch 4|step 365000| decode cost = 14.428594 ms\n",
      "I0430 22:34:54.556588 139644430731072 <ipython-input-11-659d4f858dd8>:72] epoch 4|step 365000| decode cost = 14.428594 ms\n",
      "2020-04-30 22:34:54 - Pointer_Transformer_NoPretrain - INFO: - epoch 4: 365000, train_rouge_l_f = 0.021681, test_rouge_l_f = 0.036839\n",
      "I0430 22:34:54.558469 139644430731072 <ipython-input-11-659d4f858dd8>:80] epoch 4: 365000, train_rouge_l_f = 0.021681, test_rouge_l_f = 0.036839\n",
      "2020-04-30 22:39:08 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0430 22:39:08.621649 139644430731072 <ipython-input-11-659d4f858dd8>:82] -------------------------------------------------------------\n",
      "2020-04-30 22:40:05 - Pointer_Transformer_NoPretrain - INFO: - epoch 4|step 365895| train_avg_acc = 0.015476, test_avg_acc = 0.011732\n",
      "I0430 22:40:05.443372 139644430731072 <ipython-input-11-659d4f858dd8>:85] epoch 4|step 365895| train_avg_acc = 0.015476, test_avg_acc = 0.011732\n",
      "2020-04-30 22:40:05 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0430 22:40:05.444763 139644430731072 <ipython-input-11-659d4f858dd8>:90] -------------------------------------------------------------\n",
      "2020-05-01 05:02:25 - Pointer_Transformer_NoPretrain - INFO: - epoch 5: 438000, training batch loss = 6.438815, running_avg_loss loss = 5.240527, validation loss = 5.815689\n",
      "I0501 05:02:25.436741 139644430731072 <ipython-input-11-659d4f858dd8>:41] epoch 5: 438000, training batch loss = 6.438815, running_avg_loss loss = 5.240527, validation loss = 5.815689\n",
      "2020-05-01 05:02:25 - Pointer_Transformer_NoPretrain - INFO: - epoch 5|step 438000| compute loss cost = 24.639321 ms\n",
      "I0501 05:02:25.438080 139644430731072 <ipython-input-11-659d4f858dd8>:63] epoch 5|step 438000| compute loss cost = 24.639321 ms\n",
      "2020-05-01 05:02:25 - Pointer_Transformer_NoPretrain - INFO: - Saving model step 438000 to model/saved_models/Pointer_Transformer_NoPretrain/0438000.tar...\n",
      "I0501 05:02:25.440616 139644430731072 initialize.py:219] Saving model step 438000 to model/saved_models/Pointer_Transformer_NoPretrain/0438000.tar...\n",
      "2020-05-01 05:02:39 - Pointer_Transformer_NoPretrain - INFO: - epoch 5|step 438000| decode cost = 12.103202 ms\n",
      "I0501 05:02:39.950219 139644430731072 <ipython-input-11-659d4f858dd8>:72] epoch 5|step 438000| decode cost = 12.103202 ms\n",
      "2020-05-01 05:02:39 - Pointer_Transformer_NoPretrain - INFO: - epoch 5: 438000, train_rouge_l_f = 0.000000, test_rouge_l_f = 0.032357\n",
      "I0501 05:02:39.953037 139644430731072 <ipython-input-11-659d4f858dd8>:80] epoch 5: 438000, train_rouge_l_f = 0.000000, test_rouge_l_f = 0.032357\n",
      "2020-05-01 05:08:22 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0501 05:08:22.894603 139644430731072 <ipython-input-11-659d4f858dd8>:82] -------------------------------------------------------------\n",
      "2020-05-01 05:09:25 - Pointer_Transformer_NoPretrain - INFO: - epoch 5|step 439074| train_avg_acc = 0.017093, test_avg_acc = 0.057694\n",
      "I0501 05:09:25.748571 139644430731072 <ipython-input-11-659d4f858dd8>:85] epoch 5|step 439074| train_avg_acc = 0.017093, test_avg_acc = 0.057694\n",
      "2020-05-01 05:09:25 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0501 05:09:25.750385 139644430731072 <ipython-input-11-659d4f858dd8>:90] -------------------------------------------------------------\n",
      "2020-05-01 11:31:25 - Pointer_Transformer_NoPretrain - INFO: - epoch 6: 511000, training batch loss = 5.971798, running_avg_loss loss = 5.310047, validation loss = 5.815689\n",
      "I0501 11:31:25.870110 139644430731072 <ipython-input-11-659d4f858dd8>:41] epoch 6: 511000, training batch loss = 5.971798, running_avg_loss loss = 5.310047, validation loss = 5.815689\n",
      "2020-05-01 11:31:25 - Pointer_Transformer_NoPretrain - INFO: - epoch 6|step 511000| compute loss cost = 25.131317 ms\n",
      "I0501 11:31:25.872111 139644430731072 <ipython-input-11-659d4f858dd8>:63] epoch 6|step 511000| compute loss cost = 25.131317 ms\n",
      "2020-05-01 11:31:25 - Pointer_Transformer_NoPretrain - INFO: - Saving model step 511000 to model/saved_models/Pointer_Transformer_NoPretrain/0511000.tar...\n",
      "I0501 11:31:25.875617 139644430731072 initialize.py:219] Saving model step 511000 to model/saved_models/Pointer_Transformer_NoPretrain/0511000.tar...\n",
      "2020-05-01 11:31:41 - Pointer_Transformer_NoPretrain - INFO: - epoch 6|step 511000| decode cost = 12.623416 ms\n",
      "I0501 11:31:41.160131 139644430731072 <ipython-input-11-659d4f858dd8>:72] epoch 6|step 511000| decode cost = 12.623416 ms\n",
      "2020-05-01 11:31:41 - Pointer_Transformer_NoPretrain - INFO: - epoch 6: 511000, train_rouge_l_f = 0.013808, test_rouge_l_f = 0.016880\n",
      "I0501 11:31:41.163614 139644430731072 <ipython-input-11-659d4f858dd8>:80] epoch 6: 511000, train_rouge_l_f = 0.013808, test_rouge_l_f = 0.016880\n",
      "2020-05-01 11:38:08 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0501 11:38:08.493519 139644430731072 <ipython-input-11-659d4f858dd8>:82] -------------------------------------------------------------\n",
      "2020-05-01 11:39:05 - Pointer_Transformer_NoPretrain - INFO: - epoch 6|step 512253| train_avg_acc = 0.016479, test_avg_acc = 0.056181\n",
      "I0501 11:39:05.346355 139644430731072 <ipython-input-11-659d4f858dd8>:85] epoch 6|step 512253| train_avg_acc = 0.016479, test_avg_acc = 0.056181\n",
      "2020-05-01 11:39:05 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0501 11:39:05.348314 139644430731072 <ipython-input-11-659d4f858dd8>:90] -------------------------------------------------------------\n",
      "2020-05-01 17:56:28 - Pointer_Transformer_NoPretrain - INFO: - epoch 7: 584000, training batch loss = 5.449803, running_avg_loss loss = 5.286512, validation loss = 5.815689\n",
      "I0501 17:56:28.850980 139644430731072 <ipython-input-11-659d4f858dd8>:41] epoch 7: 584000, training batch loss = 5.449803, running_avg_loss loss = 5.286512, validation loss = 5.815689\n",
      "2020-05-01 17:56:28 - Pointer_Transformer_NoPretrain - INFO: - epoch 7|step 584000| compute loss cost = 23.732564 ms\n",
      "I0501 17:56:28.852917 139644430731072 <ipython-input-11-659d4f858dd8>:63] epoch 7|step 584000| compute loss cost = 23.732564 ms\n",
      "2020-05-01 17:56:28 - Pointer_Transformer_NoPretrain - INFO: - Saving model step 584000 to model/saved_models/Pointer_Transformer_NoPretrain/0584000.tar...\n",
      "I0501 17:56:28.855972 139644430731072 initialize.py:219] Saving model step 584000 to model/saved_models/Pointer_Transformer_NoPretrain/0584000.tar...\n",
      "2020-05-01 17:56:42 - Pointer_Transformer_NoPretrain - INFO: - epoch 7|step 584000| decode cost = 11.601928 ms\n",
      "I0501 17:56:42.729223 139644430731072 <ipython-input-11-659d4f858dd8>:72] epoch 7|step 584000| decode cost = 11.601928 ms\n",
      "2020-05-01 17:56:42 - Pointer_Transformer_NoPretrain - INFO: - epoch 7: 584000, train_rouge_l_f = 0.021045, test_rouge_l_f = 0.024578\n",
      "I0501 17:56:42.731802 139644430731072 <ipython-input-11-659d4f858dd8>:80] epoch 7: 584000, train_rouge_l_f = 0.021045, test_rouge_l_f = 0.024578\n",
      "2020-05-01 18:04:00 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0501 18:04:00.208912 139644430731072 <ipython-input-11-659d4f858dd8>:82] -------------------------------------------------------------\n",
      "2020-05-01 18:04:56 - Pointer_Transformer_NoPretrain - INFO: - epoch 7|step 585432| train_avg_acc = 0.015979, test_avg_acc = 0.063587\n",
      "I0501 18:04:56.954376 139644430731072 <ipython-input-11-659d4f858dd8>:85] epoch 7|step 585432| train_avg_acc = 0.015979, test_avg_acc = 0.063587\n",
      "2020-05-01 18:04:56 - Pointer_Transformer_NoPretrain - INFO: - -------------------------------------------------------------\n",
      "I0501 18:04:56.956377 139644430731072 <ipython-input-11-659d4f858dd8>:90] -------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "loss_st, loss_cost = 0,0\n",
    "decode_st, decode_cost = 0,0\n",
    "\n",
    "write_train_para(writer, config)\n",
    "logger.info('------Training START--------')\n",
    "running_avg_loss, running_avg_rl_loss = 0, 0\n",
    "sum_total_reward = 0\n",
    "step = 0\n",
    "# save_steps = 10\n",
    "try:\n",
    "    for epoch in range(config.max_epochs):\n",
    "        for batch in train_loader:\n",
    "            step += 1\n",
    "            loss_st = time.time()\n",
    "            mle_loss = train_one(model, config, batch)\n",
    "            if config.train_rl:\n",
    "                rl_loss, batch_reward = train_one_RL(model, config, batch)             \n",
    "            else:\n",
    "                rl_loss = T.FloatTensor([0]).cuda()\n",
    "            (config.mle_weight * mle_loss + config.rl_weight * rl_loss).backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "            model.zero_grad() # 清空过往梯度\n",
    "            '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "            if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "                    # clip_grad_norm_(model.parameters(), 5.0)                     \n",
    "                for o in optimizer:\n",
    "                    o.step() # 根据累计的梯度更新网络参数\n",
    "                \n",
    "                    \n",
    "            if step%1000 == 0 :\n",
    "                with T.autograd.no_grad():\n",
    "                    train_batch_loss = mle_loss.item()\n",
    "                    train_batch_rl_loss = rl_loss.item()\n",
    "                    val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                    running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "                    running_avg_rl_loss = calc_running_avg_loss(train_batch_rl_loss, running_avg_rl_loss)\n",
    "                    running_avg_reward = sum_total_reward / step\n",
    "                    if step % save_steps == 0:\n",
    "                        logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "                                    % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                    writer.add_scalars('scalar/Loss',  \n",
    "                       {'train_batch_loss': train_batch_loss\n",
    "                       }, step)\n",
    "                    writer.add_scalars('scalar_avg/loss',  \n",
    "                       {'train_avg_loss': running_avg_loss,\n",
    "                        'test_avg_loss': val_avg_loss\n",
    "                       }, step)\n",
    "                    if running_avg_reward > 0:\n",
    "#                         logger.info('epoch %d: %d, running_avg_reward = %f'\n",
    "#                                 % (epoch, step, running_avg_reward))\n",
    "                        writer.add_scalars('scalar_avg/Reward',  \n",
    "                           {'running_avg_reward': running_avg_reward\n",
    "                           }, step)\n",
    "                    if running_avg_rl_loss != 0:\n",
    "#                         logger.info('epoch %d: %d, running_avg_rl_loss = %f'\n",
    "#                                 % (epoch, step, running_avg_rl_loss))\n",
    "                        writer.add_scalars('scalar_avg/RL_Loss',  \n",
    "                           {'running_avg_rl_loss': running_avg_rl_loss\n",
    "                           }, step)\n",
    "                    loss_cost = time.time() - loss_st\n",
    "                    if step % save_steps == 0: logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "                                % (epoch, step, loss_cost))\n",
    "            if step % save_steps == 0:\n",
    "                save_model(config, logger, model, optimizer, step, vocab, running_avg_loss, \\\n",
    "                           r_loss=0, title = loggerName)\n",
    "            if step%1000 == 0 and step > 0:\n",
    "                decode_st = time.time()\n",
    "                train_rouge_l_f = decode(writer, logger, step, config, model, batch, mode = 'train') # call batch by validate_loader\n",
    "                test_rouge_l_f = decode(writer, logger, step, config, model, validate_loader, mode = 'test') # call batch by validate_loader\n",
    "                decode_cost = time.time() - decode_st\n",
    "                if step%save_steps == 0: logger.info('epoch %d|step %d| decode cost = %f ms'% (epoch, step, decode_cost))\n",
    "\n",
    "                writer.add_scalars('scalar/Rouge-L',  \n",
    "                   {'train_rouge_l_f': train_rouge_l_f,\n",
    "                    'test_rouge_l_f': test_rouge_l_f\n",
    "                   }, step)\n",
    "                if step%save_steps == 0:\n",
    "                    logger.info('epoch %d: %d, train_rouge_l_f = %f, test_rouge_l_f = %f'\n",
    "                                % (epoch, step, train_rouge_l_f, test_rouge_l_f))\n",
    "#         break\n",
    "        logger.info('-------------------------------------------------------------')\n",
    "        train_avg_acc = avg_acc(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "        test_avg_acc = avg_acc(writer, logger, epoch, config, model, validate_loader, mode = 'test')                   \n",
    "        logger.info('epoch %d|step %d| train_avg_acc = %f, test_avg_acc = %f' % (epoch, step, train_avg_acc, test_avg_acc))\n",
    "        if running_avg_reward > 0:\n",
    "            logger.info('epoch %d|step %d| running_avg_reward = %f'% (epoch, step, running_avg_reward))\n",
    "        if running_avg_rl_loss != 0:\n",
    "            logger.info('epoch %d|step %d| running_avg_rl_loss = %f'% (epoch, step, running_avg_rl_loss))\n",
    "        logger.info('-------------------------------------------------------------')\n",
    "\n",
    "except Excepation as e:\n",
    "        print(e)\n",
    "else:\n",
    "    logger.info(u'------Training SUCCESS--------')  \n",
    "finally:\n",
    "    logger.info(u'------Training END--------')    \n",
    "    train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "    test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "    logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc))\n",
    "    removeLogger(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_outFrame.head()\n",
    "# test_outFrame.head()\n",
    "\n",
    "# !ipython nbconvert --to script Transformer_RUN.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
