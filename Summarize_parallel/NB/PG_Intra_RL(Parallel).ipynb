{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0528 08:47:47.602876 139654433257280 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-05-28 08:47:48 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - logger已啟動\n",
      "I0528 08:47:48.532269 139654433257280 train_util.py:106] logger已啟動\n"
     ]
    }
   ],
   "source": [
    "from utils import config\n",
    "from utils.seq2seq import data\n",
    "\n",
    "from utils.seq2seq.batcher import *\n",
    "\n",
    "from utils.seq2seq.train_util import *\n",
    "from utils.seq2seq.rl_util import *\n",
    "from utils.seq2seq.initialize import loadCheckpoint, save_model\n",
    "from utils.seq2seq.write_result import *\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from translate.seq2seq_beam import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "eval_gpu = 0\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--key_attention', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_encoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--intra_decoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--copy', type=bool, default=True, help = 'True/False') # for transformer\n",
    "\n",
    "\n",
    "parser.add_argument('--model_type', type=str, default='seq2seq', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--train_rl', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_keys', \n",
    "                    help = 'POS_keys / DEP_keys / Noun_adj_keys / TextRank_keys')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--rand_unif_init_mag', type=float, default=0.02)\n",
    "parser.add_argument('--trunc_norm_init_std', type=float, default=0.001)\n",
    "parser.add_argument('--mle_weight', type=float, default=0.5)\n",
    "parser.add_argument('--gound_truth_prob', type=float, default=0.5)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=500)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=20)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=5)\n",
    "parser.add_argument('--max_epochs', type=int, default=15)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=5)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=300)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default='', help='0002000')\n",
    "parser.add_argument('--word_emb_type', type=str, default='word2Vec', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=True, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)\n",
    "\n",
    "eval_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-28 08:49:11 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - train : 504075, test : 56009\n",
      "I0528 08:49:11.221448 139654433257280 batcher.py:186] train : 504075, test : 56009\n",
      "2020-05-28 08:49:11 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - train batches : 126018, test batches : 14002\n",
      "I0528 08:49:11.734210 139654433257280 batcher.py:210] train batches : 126018, test batches : 14002\n"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "train_batches = len(iter(train_loader))\n",
    "test_batches = len(iter(validate_loader))\n",
    "save_steps = int(train_batches/250)*250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0528 08:49:12.346186 139654433257280 utils_any2vec.py:341] loading projection weights from ../Train-Data/Mix6_mainCat_best/Embedding/word2Vec/word2Vec.300d.txt\n",
      "I0528 08:49:26.035500 139654433257280 utils_any2vec.py:405] loaded (49676, 300) matrix from ../Train-Data/Mix6_mainCat_best/Embedding/word2Vec/word2Vec.300d.txt\n"
     ]
    }
   ],
   "source": [
    "from seq2seq import Model\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.distributed as dist\n",
    "\n",
    "from parallel import DataParallelModel, DataParallelCriterion\n",
    "# https://gist.github.com/thomwolf/7e2407fbd5945f07821adae3d9fd1312\n",
    "\n",
    "\n",
    "load_step = None\n",
    "model = Model(pre_train_emb=config.pre_train_emb, \n",
    "              word_emb_type = config.word_emb_type, \n",
    "              vocab = vocab)\n",
    "\n",
    "# model = model.cuda()\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=config.lr)   \n",
    "# optimizer = T.optim.Adagrad(model.parameters(),lr=config.lr, initial_accumulator_value=0.1)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(logger, load_model_path, model, optimizer)\n",
    "    # 若偵測到model切換成eval\n",
    "    eval_model = True\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(eval_gpu)\n",
    "    \n",
    "else:    \n",
    "    model.to('cuda:%s' % 0) #BCW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        With label smoothing,\n",
    "        KL-divergence between q_{smoothed ground truth prob.}(w)\n",
    "        and p_{prob. computed by model}(w) is minimized.\n",
    "        \"\"\"\n",
    "        def __init__(self, ignore_index):\n",
    "            super(NLLLoss, self).__init__()\n",
    "#             step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=PAD)\n",
    "            self.NLL = nn.NLLLoss(ignore_index=ignore_index, reduction='sum')\n",
    "\n",
    "        def forward(self, out, tar):  \n",
    "            # target dimension[0] / 2\n",
    "            # tar = target.contiguous().view(-1) \n",
    "            # out = output.contiguous().view(target.size(0),-1)\n",
    "\n",
    "            target = tar.contiguous().view(-1)\n",
    "            output = out[:tar.size(0)]\n",
    "            normalize = output.size(0) * output.size(1)\n",
    "            output = output.contiguous().view(target.size(0),-1)\n",
    "            loss = self.NLL(output, target) / normalize\n",
    "            \n",
    "            return loss\n",
    "\n",
    "if not eval_model:\n",
    "    criterion = NLLLoss(ignore_index=PAD)\n",
    "    parallel_model = DataParallelModel(model) # Encapsulate the model\n",
    "    parallel_loss = DataParallelCriterion(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sents(enc_out, inds, vocab, art_oovs):\n",
    "    decoded_strs = []\n",
    "    for i in range(len(enc_out)):\n",
    "        id_list = inds[i].tolist() # 取出每個sample sentence 的word id list\n",
    "        S = output2words(id_list, vocab, art_oovs[i]) #Generate sentence corresponding to sampled words\n",
    "        try:\n",
    "            end_idx = S.index(data.STOP_DECODING)\n",
    "            S = S[:end_idx]\n",
    "        except ValueError:\n",
    "            S = S\n",
    "        if len(S) < 2:          #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "            S = [\"xxx\"]\n",
    "        S = \" \".join(S)\n",
    "        decoded_strs.append(S)\n",
    "    return decoded_strs\n",
    "\n",
    "# def merge_res(res):\n",
    "#     ((inds1, log_probs1, enc_out1),(inds2, log_probs2, enc_out2)) = res\n",
    "#     inds = T.cat([inds1, inds2], dim = 0).cpu()\n",
    "#     enc_out = T.cat([enc_out1, enc_out2], dim = 0).cpu()\n",
    "#     if type(log_probs1) != list:\n",
    "#         log_probs = T.cat([log_probs1, log_probs2], dim = 0)\n",
    "#         return inds, log_probs, enc_out\n",
    "#     else:\n",
    "#         return inds, _, enc_out\n",
    "\n",
    "def train_one_rl(package, inputs):\n",
    "    config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e,                                 max_dec_len, dec_batch, target_batch = package\n",
    "    \n",
    "    rl_loss, batch_reward = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e,                                 max_dec_len, dec_batch, target_batch, train_rl = True, art_oovs = inputs.art_oovs, original_abstract = inputs.original_abstract, vocab = vocab)\n",
    "    rl_loss = nn.parallel.gather(rl_loss, 0).mean() \n",
    "    return rl_loss, batch_reward\n",
    "\n",
    "def train_one(package):\n",
    "    model.train()\n",
    "    config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch = package   \n",
    "\n",
    "    pred_probs = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch)\n",
    "    target = target_batch\n",
    "    loss = parallel_loss(config.mle_weight, pred_probs, target)\n",
    "    \n",
    "    return loss, pred_probs\n",
    "\n",
    "def write_res(inputs, batch_probs):\n",
    "    decoded_sents = []\n",
    "    for i, probs in enurmerate(batch_probs):\n",
    "        sents = []\n",
    "        for prob in probs:\n",
    "            _id = T.max(probs, dim=1)[1]\n",
    "            _id = _id.detach()\n",
    "            sents.append(_id)\n",
    "        decoded_sents.append(seq)\n",
    "            \n",
    "    output2words()        \n",
    "    article_sents = [article for article in inputs.original_article]\n",
    "    ref_sents = [ref for ref in inputs.original_abstract]\n",
    "#     decoded_sents = [summarize(article, words=30) for article in article_sents]\n",
    "#     decoded_sents = [sent if len(sent) > 5 else \"xxx xxx xxx xxx xxx\" for sent in decoded_sents]\n",
    "        \n",
    "#     article_sents, decoded_sents, keywords_list, \\\n",
    "#     ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "#     rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents, \\\n",
    "#                 keywords_list, ref_sents, long_seq_index, write = False)\n",
    "#     avg_rouge_l.append(rouge_l)\n",
    "#     acc_cost = time.time() - acc_st\n",
    "#     avg_acc_cost.append(acc_cost)\n",
    "    \n",
    "    return seq_sents\n",
    "\n",
    "def get_package(inputs):\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(inputs, config, batch_first = True)\n",
    "\n",
    "    dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(inputs, config, batch_first = True) # Get input and target batchs for training decoder            \n",
    "\n",
    "    max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0]    \n",
    "    # ----------------------------------------------------\n",
    "    package = (config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch)\n",
    "    \n",
    "    inner_c = package[1] != max(package[4].tolist())[0]\n",
    "\n",
    "    return inner_c, package\n",
    "\n",
    "\n",
    "# for inputs in train_loader:  \n",
    "#     # MLE test\n",
    "#     # ----------------------------------------------------\n",
    "#     # pred_probs = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "#     #                             max_dec_len, dec_batch, target_batch)\n",
    "#     # # pass\n",
    "#     # target = target_batch\n",
    "#     # loss = parallel_loss(config.mle_weight, pred_probs, target)\n",
    "#     loss = train_one(package)\n",
    "# #     loss.backward() # Backward pass \n",
    "# #     optimizer.step() # Optimizer step\n",
    "#     print('loss : ',loss)\n",
    "#     # pass\n",
    "#     # ----------------------------------------------------   \n",
    "#     if config.train_rl:\n",
    "#         rl_loss, batch_reward = train_one_rl(package)\n",
    "#         print('rl_loss : ',rl_loss, 'batch_reward : ',batch_reward)\n",
    "#     else:\n",
    "#         rl_loss = T.FloatTensor([0]).cuda()        \n",
    "    \n",
    "#     (config.mle_weight * loss + config.rl_weight * rl_loss).backward() # Backward pass   \n",
    "#     optimizer.step() # Optimizer step\n",
    "#     optimizer.zero_grad() # 清空过往梯度 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "#     batch = next(iter(validate_loader))\n",
    "    val_num = len(iter(validate_loader))\n",
    "    for idx, batch in enumerate(validate_loader):\n",
    "        inner_c, package = get_package(batch)\n",
    "        if inner_c: continue\n",
    "        loss, _ = train_one(package)\n",
    "#         loss = train_one(model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "#         if idx>= val_num/40: break\n",
    "#     model.train()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # del parallel_model, parallel_loss\n",
    "\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "# @torch.autograd.no_grad()\n",
    "# def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "#     # 動態取batch\n",
    "#     num = len(dataloader)\n",
    "#     avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "#     avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "#     avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "#     avg_meteor = []\n",
    "#     outFrame = None\n",
    "#     avg_time = 0\n",
    "        \n",
    "#     for idx, inputs in enumerate(dataloader):\n",
    "#         start = time.time() \n",
    "# #         'Encoder data'\n",
    "#         enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "#             ct_e, enc_key_batch, enc_key_mask, enc_key_lens = get_input_from_batch(inputs, config, batch_first = True)\n",
    "#         max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0] \n",
    "        \n",
    "#         if (max_enc_len != max(enc_lens.tolist())[0]): continue\n",
    "\n",
    "#         enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "#         enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "#         enc_out, enc_hidden = model.encoder(enc_batch, enc_lens, max_enc_len)\n",
    "        \n",
    "# #         'Feed encoder data to predict'\n",
    "#         pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "#                                 enc_batch_extend_vocab, enc_key_batch, enc_key_mask, model, \n",
    "#                                 START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "#         article_sents, decoded_sents, keywords_list, ref_sents, long_seq_index = prepare_result(vocab, inputs, pred_ids)\n",
    "#         cost = (time.time() - start)\n",
    "#         avg_time += cost        \n",
    "\n",
    "        \n",
    "#         rouge_1, rouge_2, rouge_l,             Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        \n",
    "#         if idx %1000 ==0 and idx >0 : print(idx)\n",
    "#         if idx == 0: outFrame = batch_frame\n",
    "#         else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "#         # ----------------------------------------------------\n",
    "#         avg_rouge_1.extend(rouge_1)\n",
    "#         avg_rouge_2.extend(rouge_2)\n",
    "#         avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "#         # avg_self_bleu1.extend(self_Bleu_1)\n",
    "#         # avg_self_bleu2.extend(self_Bleu_2)\n",
    "#         # avg_self_bleu3.extend(self_Bleu_3)\n",
    "#         # avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "#         avg_bleu1.extend(Bleu_1)\n",
    "#         avg_bleu2.extend(Bleu_2)\n",
    "#         avg_bleu3.extend(Bleu_3)\n",
    "#         avg_bleu4.extend(Bleu_4)\n",
    "#         avg_meteor.extend(Meteor)\n",
    "#         # ----------------------------------------------------    \n",
    "#     avg_time = avg_time / (num * config.batch_size) \n",
    "    \n",
    "#     avg_rouge_l, outFrame = total_output(mode, writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l,         avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4,         avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "#     )\n",
    "    \n",
    "#     return avg_rouge_l, outFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del parallel_model, parallel_loss\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode(writer, dataloader, epoch):\n",
    "    # 動態取batch\n",
    "    num = len(dataloader)\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "    avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    avg_meteor = []\n",
    "    outFrame = None\n",
    "    avg_time = 0\n",
    "        \n",
    "    for idx, inputs in enumerate(dataloader):\n",
    "        start = time.time() \n",
    "#         'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "            ct_e, enc_key_batch, enc_key_mask, enc_key_lens = get_input_from_batch(inputs, config, batch_first = True)\n",
    "        max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0] \n",
    "        \n",
    "        if (max_enc_len != max(enc_lens.tolist())[0]): continue\n",
    "\n",
    "        enc_batch = parallel_model.module.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = parallel_model.module.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = parallel_model.module.encoder(enc_batch, enc_lens, max_enc_len)\n",
    "        \n",
    "#         'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                                enc_batch_extend_vocab, enc_key_batch, enc_key_mask, parallel_model.module, \n",
    "                                START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, ref_sents, long_seq_index = prepare_result(vocab, inputs, pred_ids)\n",
    "        cost = (time.time() - start)\n",
    "        avg_time += cost        \n",
    "\n",
    "        \n",
    "        rouge_1, rouge_2, rouge_l,             Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        \n",
    "        if idx %1000 ==0 and idx >0 : print(idx); \n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "        # avg_self_bleu1.extend(self_Bleu_1)\n",
    "        # avg_self_bleu2.extend(self_Bleu_2)\n",
    "        # avg_self_bleu3.extend(self_Bleu_3)\n",
    "        # avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "        avg_bleu1.extend(Bleu_1)\n",
    "        avg_bleu2.extend(Bleu_2)\n",
    "        avg_bleu3.extend(Bleu_3)\n",
    "        avg_bleu4.extend(Bleu_4)\n",
    "        avg_meteor.extend(Meteor)\n",
    "        # ----------------------------------------------------    \n",
    "    avg_time = avg_time / (num * config.batch_size)    \n",
    "    \n",
    "    scalar_acc = {\n",
    "        'rouge_1':sum(avg_rouge_1) / len(avg_rouge_1),\n",
    "        'rouge_2':sum(avg_rouge_2) / len(avg_rouge_2),\n",
    "        'rouge_l':sum(avg_rouge_l) / len(avg_rouge_l),\n",
    "        \n",
    "        'bleu1':sum(avg_bleu1) / len(avg_bleu1),\n",
    "        'bleu2':sum(avg_bleu2) / len(avg_bleu2),\n",
    "        'bleu3':sum(avg_bleu3) / len(avg_bleu3),\n",
    "        'bleu4':sum(avg_bleu4) / len(avg_bleu4),\n",
    "        \n",
    "        'meteor':sum(avg_meteor) / len(avg_meteor)\n",
    "    }\n",
    "    \n",
    "    for scalar_name, accuracy in scalar_acc.items():\n",
    "        if 'rouge' in scalar_name:\n",
    "            writer.add_scalars('scalar/rouge',  \n",
    "               {scalar_name: accuracy,\n",
    "               }, epoch)\n",
    "        elif 'bleu' in scalar_name:\n",
    "            writer.add_scalars('scalar/bleu',  \n",
    "               {scalar_name: accuracy,\n",
    "               }, epoch)\n",
    "        else:\n",
    "            writer.add_scalars('scalar/meteor',  \n",
    "               {scalar_name: accuracy,\n",
    "               }, epoch)\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    total_output(epoch, 'test', writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l,                  avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "    )\n",
    "    # -----------------------------------------------------------\n",
    "    outFrame = outFrame.sort_values(by=['rouge_l'], ascending=False)\n",
    "    big_frame = outFrame.head()\n",
    "    small_frame = outFrame.tail()    \n",
    "    # -----------------------------------------------------------\n",
    "    i = 0\n",
    "    for view_item in big_frame.to_dict('records'):\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### rouge_l : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + str(view_item['rouge_l']), epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### decoded : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['decoded'], epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### reference : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['reference'], epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### keywords : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['keywords'], epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### article : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['article'], epoch)\n",
    "\n",
    "        i += 1\n",
    "    # -----------------------------------------------------------\n",
    "    i = 0\n",
    "    for view_item in small_frame.to_dict('records'):\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### rouge_l : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + str(view_item['rouge_l']), epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### decoded : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['decoded'], epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### reference : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['reference'], epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### keywords : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['keywords'], epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### article : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['article'], epoch)\n",
    "        i += 1\n",
    "    return outFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-28 08:49:31 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - ------Training START--------\n",
      "I0528 08:49:31.440270 139654433257280 <ipython-input-8-4998389d8682>:10] ------Training START--------\n",
      "/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "loss_st, loss_cost = 0,0\n",
    "decode_st, decode_cost = 0,0\n",
    "last_save_step = 0\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "print_step = 250\n",
    "# save_steps = print_step\n",
    "if not eval_model:\n",
    "\n",
    "    write_train_para(writer, config)\n",
    "    logger.info('------Training START--------')\n",
    "    running_avg_loss, running_avg_rl_loss = 0, 0\n",
    "    sum_total_reward = 0\n",
    "    step = 0\n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(config, logger, vocab, loggerName, patience=3, verbose=True)\n",
    "    try:\n",
    "        for epoch in range(1, config.max_epochs+1):\n",
    "            for batch in train_loader:\n",
    "                step += 1; \n",
    "                loss_st = time.time()\n",
    "                inner_c, package = get_package(batch)\n",
    "                if inner_c: continue\n",
    "                parallel_model.module.train()\n",
    "                mle_loss, pred_probs = train_one(package)\n",
    "                if config.train_rl:\n",
    "                    rl_loss, batch_reward = train_one_rl(package, batch)             \n",
    "\n",
    "                    if step%print_step == 0 :\n",
    "                        writer.add_scalars('scalar/RL_Loss',  \n",
    "                           {'rl_loss': rl_loss\n",
    "                           }, step)\n",
    "                        writer.add_scalars('scalar/Reward',  \n",
    "                           {'batch_reward': batch_reward\n",
    "                           }, step)\n",
    "    #                     logger.info('epoch %d: %d, RL_Loss = %f, batch_reward = %f'\n",
    "    #                                     % (epoch, step, rl_loss, batch_reward))\n",
    "                    sum_total_reward += batch_reward\n",
    "                else:\n",
    "                    rl_loss = T.FloatTensor([0]).cuda()\n",
    "\n",
    "                (config.mle_weight * mle_loss + config.rl_weight * rl_loss).backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "                '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "                if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "        #             clip_grad_norm_(model.parameters(), 5.0)                      \n",
    "                    optimizer.step() # 根据累计的梯度更新网络参数\n",
    "                    optimizer.zero_grad() # 清空过往梯度 \n",
    "                if step%print_step == 0 :\n",
    "                    with T.autograd.no_grad():\n",
    "                        train_batch_loss = mle_loss.item()\n",
    "                        train_batch_rl_loss = rl_loss.item()\n",
    "#                         val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                        running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "                        running_avg_rl_loss = calc_running_avg_loss(train_batch_rl_loss, running_avg_rl_loss)\n",
    "                        running_avg_reward = sum_total_reward / step\n",
    "#                         if step % save_steps == 0:\n",
    "#                             logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "#                                         % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                        writer.add_scalars('scalar/Loss',  \n",
    "                           {'train_batch_loss': train_batch_loss\n",
    "                           }, step)\n",
    "                        writer.add_scalars('scalar_avg/loss',  \n",
    "                           {'train_avg_loss': running_avg_loss\n",
    "#                             'test_avg_loss': val_avg_loss\n",
    "                           }, step)\n",
    "                        if running_avg_reward > 0:\n",
    "    #                         logger.info('epoch %d: %d, running_avg_reward = %f'\n",
    "    #                                 % (epoch, step, running_avg_reward))\n",
    "                            writer.add_scalars('scalar_avg/Reward',  \n",
    "                               {'running_avg_reward': running_avg_reward\n",
    "                               }, step)\n",
    "                        if running_avg_rl_loss != 0:\n",
    "    #                         logger.info('epoch %d: %d, running_avg_rl_loss = %f'\n",
    "    #                                 % (epoch, step, running_avg_rl_loss))\n",
    "                            writer.add_scalars('scalar_avg/RL_Loss',  \n",
    "                               {'running_avg_rl_loss': running_avg_rl_loss\n",
    "                               }, step)\n",
    "                                                    \n",
    "                \n",
    "                if step % save_steps == 0:\n",
    "                    parallel_model.module.eval()\n",
    "                    logger.info('epoch : %s' % epoch)\n",
    "                    val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                    logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "                                        % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                    writer.add_scalars('scalar_avg/loss',  \n",
    "                           {'train_avg_loss': running_avg_loss,\n",
    "                            'test_avg_loss': val_avg_loss\n",
    "                           }, step)\n",
    "                    '''（讀取所儲存模型引數後，再進行並行化操作，否則無法利用之前的程式碼進行讀取）'''\n",
    "                    save_model(config, logger, parallel_model, optimizer, step, vocab, val_avg_loss, \\\n",
    "                               r_loss=0, title = loggerName)\n",
    "                    loss_cost = time.time() - loss_st\n",
    "                    logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "                                    % (epoch, step, loss_cost))\n",
    "                    writer.add_scalars('scalar_avg/epoch_loss',  \n",
    "                       {'train_avg_loss': running_avg_loss,\n",
    "                        'test_avg_loss': val_avg_loss\n",
    "                       }, epoch)\n",
    "                    last_save_step = step\n",
    "                    test_outFrame = decode(writer, validate_loader, epoch)                   \n",
    "\n",
    "            logger.info('-------------------------------------------------------------')\n",
    "\n",
    "            if running_avg_reward > 0:\n",
    "                logger.info('epoch %d|step %d| running_avg_reward = %f'% (epoch, step, running_avg_reward))\n",
    "            if running_avg_rl_loss != 0:\n",
    "                logger.info('epoch %d|step %d| running_avg_rl_loss = %f'% (epoch, step, running_avg_rl_loss))\n",
    "            logger.info('-------------------------------------------------------------')\n",
    "\n",
    "            early_stopping(parallel_model, optimizer, step, val_avg_loss) # update patience\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(\"Early stopping epoch %s\"%(epoch))\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        logger.info(u'------Training SUCCESS--------')  \n",
    "    finally:\n",
    "        logger.info(u'------Training END--------')   \n",
    "        logger.info(\"stopping epoch %s\"%(epoch))        \n",
    "        logger.info(\"last_save_step %s\"%(last_save_step))  \n",
    "        '''先將test_avg_acc調起來再decode train_'''\n",
    "    #     train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "#         test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, parallel_model.module, validate_loader, mode = 'test')\n",
    "    #     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc)) \n",
    "#         logger.info('epoch %d: test_avg_acc = %f' % (load_ep, test_avg_acc)) \n",
    "        removeLogger(logger)\n",
    "\n",
    "# else: # EVAL\n",
    "#     load_ep = float(config.load_ckpt) / float(save_steps)\n",
    "#     config.batch_size = 32\n",
    "#     train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "#     train_batches = len(iter(train_loader))\n",
    "#     test_batches = len(iter(validate_loader))\n",
    "# #     save_steps = int(train_batches/250)*250\n",
    "#     model.cuda(eval_gpu) \n",
    "#     model.eval()\n",
    "#     '''先將test_avg_acc調起來再decode train_'''\n",
    "# #     train_avg_acc, train_outFrame = decode_write_all(writer, logger, load_ep, config, model, train_loader, mode = 'train')\n",
    "#     test_avg_acc, test_outFrame = decode_write_all(writer, logger, load_ep, config, model, validate_loader, mode = 'test')\n",
    "# #     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (load_ep, train_avg_acc, test_avg_acc)) \n",
    "#     logger.info('epoch %d: test_avg_acc = %f' % (load_ep, test_avg_acc)) \n",
    "#     removeLogger(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_outFrame[test_outFrame[\"rouge_1\"]>=0.4][['rouge_1','article', 'reference', 'decoded', 'gen_type','overlap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_16 epoch_6\n",
    "# testing_avg_rouge_1: 0.3873426628114616 \\n', \n",
    "# 'testing_avg_rouge_2: 0.25943944916828854 \\n', \n",
    "# 'testing_avg_rouge_l: 0.3614074094052472 \\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('Leyan': conda)",
   "language": "python",
   "name": "python36764bitleyancondaa378f3cedbcc4b3f906a2276b3eef765"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
