{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0531 21:57:34.342973 140094496737088 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-05-31 21:57:35 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - logger已啟動\n",
      "I0531 21:57:35.288391 140094496737088 train_util.py:106] logger已啟動\n"
     ]
    }
   ],
   "source": [
    "from utils import config\n",
    "from utils.seq2seq import data\n",
    "\n",
    "from utils.seq2seq.batcher import *\n",
    "\n",
    "from utils.seq2seq.train_util import *\n",
    "from utils.seq2seq.rl_util import *\n",
    "from utils.seq2seq.initialize import loadCheckpoint, save_model\n",
    "from utils.seq2seq.write_result import *\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from translate.seq2seq_beam import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "eval_gpu = 0\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--key_attention', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_encoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--intra_decoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--copy', type=bool, default=True, help = 'True/False') # for transformer\n",
    "\n",
    "\n",
    "parser.add_argument('--model_type', type=str, default='seq2seq', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--train_rl', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_keys', \n",
    "                    help = 'POS_keys / DEP_keys / Noun_adj_keys / TextRank_keys')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--rand_unif_init_mag', type=float, default=0.02)\n",
    "parser.add_argument('--trunc_norm_init_std', type=float, default=0.001)\n",
    "parser.add_argument('--mle_weight', type=float, default=0.7)\n",
    "parser.add_argument('--gound_truth_prob', type=float, default=0.1)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=500)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=20)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=5)\n",
    "parser.add_argument('--max_epochs', type=int, default=15)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=5)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=300)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default='', help='0002000')\n",
    "parser.add_argument('--word_emb_type', type=str, default='word2Vec', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=True, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)\n",
    "\n",
    "eval_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-31 21:58:50 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - train : 469335, test : 52149\n",
      "I0531 21:58:50.368366 140094496737088 batcher.py:186] train : 469335, test : 52149\n",
      "2020-05-31 21:58:50 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - train batches : 14666, test batches : 1629\n",
      "I0531 21:58:50.855831 140094496737088 batcher.py:210] train batches : 14666, test batches : 1629\n"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "train_batches = len(iter(train_loader))\n",
    "test_batches = len(iter(validate_loader))\n",
    "save_steps = int(train_batches/250)*250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0531 21:58:51.432293 140094496737088 utils_any2vec.py:341] loading projection weights from ../Train-Data/Mix6_mainCat_best/Embedding/word2Vec/word2Vec.300d.txt\n",
      "I0531 21:59:03.680691 140094496737088 utils_any2vec.py:405] loaded (49676, 300) matrix from ../Train-Data/Mix6_mainCat_best/Embedding/word2Vec/word2Vec.300d.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/saved_models/Pointer_generator_word2Vec_Intra_Atten_RL/0188500.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-31 21:59:06 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - Loaded model at model/saved_models/Pointer_generator_word2Vec_Intra_Atten_RL/0188500.tar\n",
      "I0531 21:59:06.772599 140094496737088 initialize.py:212] Loaded model at model/saved_models/Pointer_generator_word2Vec_Intra_Atten_RL/0188500.tar\n",
      "2020-05-31 21:59:06 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - Loaded model step = 188500, loss = 1.61, r_loss = 0.00 \n",
      "I0531 21:59:06.774292 140094496737088 initialize.py:213] Loaded model step = 188500, loss = 1.61, r_loss = 0.00 \n"
     ]
    }
   ],
   "source": [
    "from seq2seq import Model\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.distributed as dist\n",
    "\n",
    "from parallel import DataParallelModel, DataParallelCriterion\n",
    "# https://gist.github.com/thomwolf/7e2407fbd5945f07821adae3d9fd1312\n",
    "\n",
    "\n",
    "load_step = 0\n",
    "model = Model(pre_train_emb=config.pre_train_emb, \n",
    "              word_emb_type = config.word_emb_type, \n",
    "              vocab = vocab)\n",
    "\n",
    "# model = model.cuda()\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=config.lr)   \n",
    "# optimizer = T.optim.Adagrad(model.parameters(),lr=config.lr, initial_accumulator_value=0.1)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(logger, load_model_path, model, optimizer)\n",
    "    # 若偵測到model切換成eval\n",
    "    eval_model = True\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(eval_gpu)\n",
    "    \n",
    "else:    \n",
    "    model.to('cuda:%s' % 0) #BCW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        With label smoothing,\n",
    "        KL-divergence between q_{smoothed ground truth prob.}(w)\n",
    "        and p_{prob. computed by model}(w) is minimized.\n",
    "        \"\"\"\n",
    "        def __init__(self, ignore_index):\n",
    "            super(NLLLoss, self).__init__()\n",
    "#             step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=PAD)\n",
    "            self.NLL = nn.NLLLoss(ignore_index=ignore_index, reduction='sum')\n",
    "\n",
    "        def forward(self, out, tar):  \n",
    "            # target dimension[0] / 2\n",
    "            # tar = target.contiguous().view(-1) \n",
    "            # out = output.contiguous().view(target.size(0),-1)\n",
    "\n",
    "            target = tar.contiguous().view(-1)\n",
    "            output = out[:tar.size(0)]\n",
    "            normalize = output.size(0) * output.size(1)\n",
    "            output = output.contiguous().view(target.size(0),-1)\n",
    "            loss = self.NLL(output, target) / normalize\n",
    "            \n",
    "            return loss\n",
    "\n",
    "if not eval_model:\n",
    "    criterion = NLLLoss(ignore_index=PAD)\n",
    "    parallel_model = DataParallelModel(model) # Encapsulate the model\n",
    "    parallel_loss = DataParallelCriterion(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sents(enc_out, inds, vocab, art_oovs):\n",
    "    decoded_strs = []\n",
    "    for i in range(len(enc_out)):\n",
    "        id_list = inds[i].tolist() # 取出每個sample sentence 的word id list\n",
    "        S = output2words(id_list, vocab, art_oovs[i]) #Generate sentence corresponding to sampled words\n",
    "        try:\n",
    "            end_idx = S.index(data.STOP_DECODING)\n",
    "            S = S[:end_idx]\n",
    "        except ValueError:\n",
    "            S = S\n",
    "        if len(S) < 2:          #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "            S = [\"xxx\"]\n",
    "        S = \" \".join(S)\n",
    "        decoded_strs.append(S)\n",
    "    return decoded_strs\n",
    "\n",
    "# def merge_res(res):\n",
    "#     ((inds1, log_probs1, enc_out1),(inds2, log_probs2, enc_out2)) = res\n",
    "#     inds = T.cat([inds1, inds2], dim = 0).cpu()\n",
    "#     enc_out = T.cat([enc_out1, enc_out2], dim = 0).cpu()\n",
    "#     if type(log_probs1) != list:\n",
    "#         log_probs = T.cat([log_probs1, log_probs2], dim = 0)\n",
    "#         return inds, log_probs, enc_out\n",
    "#     else:\n",
    "#         return inds, _, enc_out\n",
    "\n",
    "def train_one_rl(package, inputs):\n",
    "    config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e,                                 max_dec_len, dec_batch, target_batch = package\n",
    "    \n",
    "    rl_loss, batch_reward = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e,                                 max_dec_len, dec_batch, target_batch, train_rl = True, art_oovs = inputs.art_oovs, original_abstract = inputs.original_abstract, vocab = vocab)\n",
    "    rl_loss = nn.parallel.gather(rl_loss, 0).mean() \n",
    "    return rl_loss, batch_reward\n",
    "\n",
    "def train_one(package):\n",
    "    model.train()\n",
    "    config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch = package   \n",
    "\n",
    "    pred_probs = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch)\n",
    "    target = target_batch\n",
    "    loss = parallel_loss(config.mle_weight, pred_probs, target)\n",
    "    \n",
    "    return loss, pred_probs\n",
    "\n",
    "def write_res(inputs, batch_probs):\n",
    "    decoded_sents = []\n",
    "    for i, probs in enurmerate(batch_probs):\n",
    "        sents = []\n",
    "        for prob in probs:\n",
    "            _id = T.max(probs, dim=1)[1]\n",
    "            _id = _id.detach()\n",
    "            sents.append(_id)\n",
    "        decoded_sents.append(seq)\n",
    "            \n",
    "    output2words()        \n",
    "    article_sents = [article for article in inputs.original_article]\n",
    "    ref_sents = [ref for ref in inputs.original_abstract]\n",
    "#     decoded_sents = [summarize(article, words=30) for article in article_sents]\n",
    "#     decoded_sents = [sent if len(sent) > 5 else \"xxx xxx xxx xxx xxx\" for sent in decoded_sents]\n",
    "        \n",
    "#     article_sents, decoded_sents, keywords_list, \\\n",
    "#     ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "#     rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents, \\\n",
    "#                 keywords_list, ref_sents, long_seq_index, write = False)\n",
    "#     avg_rouge_l.append(rouge_l)\n",
    "#     acc_cost = time.time() - acc_st\n",
    "#     avg_acc_cost.append(acc_cost)\n",
    "    \n",
    "    return seq_sents\n",
    "\n",
    "def get_package(inputs):\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(inputs, config, batch_first = True)\n",
    "\n",
    "    dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(inputs, config, batch_first = True) # Get input and target batchs for training decoder            \n",
    "\n",
    "    max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0]    \n",
    "    # ----------------------------------------------------\n",
    "    package = (config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch)\n",
    "    \n",
    "    inner_c = package[1] != max(package[4].tolist())[0]\n",
    "\n",
    "    return inner_c, package\n",
    "\n",
    "\n",
    "# for inputs in train_loader:  \n",
    "#     # MLE test\n",
    "#     # ----------------------------------------------------\n",
    "#     # pred_probs = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "#     #                             max_dec_len, dec_batch, target_batch)\n",
    "#     # # pass\n",
    "#     # target = target_batch\n",
    "#     # loss = parallel_loss(config.mle_weight, pred_probs, target)\n",
    "#     loss = train_one(package)\n",
    "# #     loss.backward() # Backward pass \n",
    "# #     optimizer.step() # Optimizer step\n",
    "#     print('loss : ',loss)\n",
    "#     # pass\n",
    "#     # ----------------------------------------------------   \n",
    "#     if config.train_rl:\n",
    "#         rl_loss, batch_reward = train_one_rl(package)\n",
    "#         print('rl_loss : ',rl_loss, 'batch_reward : ',batch_reward)\n",
    "#     else:\n",
    "#         rl_loss = T.FloatTensor([0]).cuda()        \n",
    "    \n",
    "#     (config.mle_weight * loss + config.rl_weight * rl_loss).backward() # Backward pass   \n",
    "#     optimizer.step() # Optimizer step\n",
    "#     optimizer.zero_grad() # 清空过往梯度 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "#     batch = next(iter(validate_loader))\n",
    "    val_num = len(iter(validate_loader))\n",
    "    for idx, batch in enumerate(validate_loader):\n",
    "        inner_c, package = get_package(batch)\n",
    "        if inner_c: continue\n",
    "        loss, _ = train_one(package)\n",
    "#         loss = train_one(model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "#         if idx>= val_num/40: break\n",
    "#     model.train()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del parallel_model, parallel_loss\n",
    "\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "# @torch.autograd.no_grad()\n",
    "# def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "#     # 動態取batch\n",
    "#     num = len(dataloader)\n",
    "#     avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "#     avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "#     avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "#     avg_meteor = []\n",
    "#     outFrame = None\n",
    "#     avg_time = 0\n",
    "        \n",
    "#     for idx, inputs in enumerate(dataloader):\n",
    "#         start = time.time() \n",
    "# #         'Encoder data'\n",
    "#         enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "#             ct_e, enc_key_batch, enc_key_mask, enc_key_lens = get_input_from_batch(inputs, config, batch_first = True)\n",
    "#         max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0] \n",
    "        \n",
    "#         if (max_enc_len != max(enc_lens.tolist())[0]): continue\n",
    "\n",
    "#         enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "#         enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "#         enc_out, enc_hidden = model.encoder(enc_batch, enc_lens, max_enc_len)\n",
    "        \n",
    "# #         'Feed encoder data to predict'\n",
    "#         pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "#                                 enc_batch_extend_vocab, enc_key_batch, enc_key_mask, model, \n",
    "#                                 START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "#         article_sents, decoded_sents, keywords_list, ref_sents, long_seq_index = prepare_result(vocab, inputs, pred_ids)\n",
    "#         cost = (time.time() - start)\n",
    "#         avg_time += cost        \n",
    "\n",
    "        \n",
    "#         rouge_1, rouge_2, rouge_l,             Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        \n",
    "#         if idx %1000 ==0 and idx >0 : print(idx)\n",
    "#         if idx == 0: outFrame = batch_frame\n",
    "#         else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "#         # ----------------------------------------------------\n",
    "#         avg_rouge_1.extend(rouge_1)\n",
    "#         avg_rouge_2.extend(rouge_2)\n",
    "#         avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "#         # avg_self_bleu1.extend(self_Bleu_1)\n",
    "#         # avg_self_bleu2.extend(self_Bleu_2)\n",
    "#         # avg_self_bleu3.extend(self_Bleu_3)\n",
    "#         # avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "#         avg_bleu1.extend(Bleu_1)\n",
    "#         avg_bleu2.extend(Bleu_2)\n",
    "#         avg_bleu3.extend(Bleu_3)\n",
    "#         avg_bleu4.extend(Bleu_4)\n",
    "#         avg_meteor.extend(Meteor)\n",
    "#         # ----------------------------------------------------    \n",
    "#     avg_time = avg_time / (num * config.batch_size) \n",
    "    \n",
    "#     avg_rouge_l, outFrame = total_output(mode, writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l,         avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4,         avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "#     )\n",
    "    \n",
    "#     return avg_rouge_l, outFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del parallel_model, parallel_loss\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode(writer, dataloader, epoch):\n",
    "    # 動態取batch\n",
    "    num = len(dataloader)\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "    avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    avg_meteor = []\n",
    "    outFrame = None\n",
    "    avg_time = 0\n",
    "        \n",
    "    for idx, inputs in enumerate(dataloader):\n",
    "        start = time.time() \n",
    "#         'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "            ct_e, enc_key_batch, enc_key_mask, enc_key_lens = get_input_from_batch(inputs, config, batch_first = True)\n",
    "        max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0] \n",
    "        \n",
    "        if (max_enc_len != max(enc_lens.tolist())[0]): continue\n",
    "\n",
    "        enc_batch = parallel_model.module.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = parallel_model.module.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = parallel_model.module.encoder(enc_batch, enc_lens, max_enc_len)\n",
    "        \n",
    "#         'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                                enc_batch_extend_vocab, enc_key_batch, enc_key_mask, parallel_model.module, \n",
    "                                START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, ref_sents, long_seq_index = prepare_result(vocab, inputs, pred_ids)\n",
    "        cost = (time.time() - start)\n",
    "        avg_time += cost        \n",
    "\n",
    "        \n",
    "        rouge_1, rouge_2, rouge_l,             Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        \n",
    "        if idx %1000 ==0 and idx >0 : print(idx); \n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "        # avg_self_bleu1.extend(self_Bleu_1)\n",
    "        # avg_self_bleu2.extend(self_Bleu_2)\n",
    "        # avg_self_bleu3.extend(self_Bleu_3)\n",
    "        # avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "        avg_bleu1.extend(Bleu_1)\n",
    "        avg_bleu2.extend(Bleu_2)\n",
    "        avg_bleu3.extend(Bleu_3)\n",
    "        avg_bleu4.extend(Bleu_4)\n",
    "        avg_meteor.extend(Meteor)\n",
    "        # ----------------------------------------------------    \n",
    "    avg_time = avg_time / (num * config.batch_size)    \n",
    "    \n",
    "    scalar_acc = {\n",
    "        'rouge_1':sum(avg_rouge_1) / len(avg_rouge_1),\n",
    "        'rouge_2':sum(avg_rouge_2) / len(avg_rouge_2),\n",
    "        'rouge_l':sum(avg_rouge_l) / len(avg_rouge_l),\n",
    "        \n",
    "        'bleu1':sum(avg_bleu1) / len(avg_bleu1),\n",
    "        'bleu2':sum(avg_bleu2) / len(avg_bleu2),\n",
    "        'bleu3':sum(avg_bleu3) / len(avg_bleu3),\n",
    "        'bleu4':sum(avg_bleu4) / len(avg_bleu4),\n",
    "        \n",
    "        'meteor':sum(avg_meteor) / len(avg_meteor)\n",
    "    }\n",
    "    \n",
    "    for scalar_name, accuracy in scalar_acc.items():\n",
    "        if 'rouge' in scalar_name:\n",
    "            writer.add_scalars('scalar/rouge',  \n",
    "               {scalar_name: accuracy,\n",
    "               }, epoch)\n",
    "        elif 'bleu' in scalar_name:\n",
    "            writer.add_scalars('scalar/bleu',  \n",
    "               {scalar_name: accuracy,\n",
    "               }, epoch)\n",
    "        else:\n",
    "            writer.add_scalars('scalar/meteor',  \n",
    "               {scalar_name: accuracy,\n",
    "               }, epoch)\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    total_output(epoch, 'test', writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l,                  avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "    )\n",
    "    # -----------------------------------------------------------\n",
    "    outFrame = outFrame.sort_values(by=['rouge_l'], ascending=False)\n",
    "    big_frame = outFrame.head()\n",
    "    small_frame = outFrame.tail()    \n",
    "    # -----------------------------------------------------------\n",
    "    i = 0\n",
    "    for view_item in big_frame.to_dict('records'):\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### rouge_l : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + str(view_item['rouge_l']), epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### decoded : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['decoded'], epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### reference : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['reference'], epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### keywords : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['keywords'], epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### article : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['article'], epoch)\n",
    "\n",
    "        i += 1\n",
    "    # -----------------------------------------------------------\n",
    "    i = 0\n",
    "    for view_item in small_frame.to_dict('records'):\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### rouge_l : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + str(view_item['rouge_l']), epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### decoded : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['decoded'], epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### reference : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['reference'], epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### keywords : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['keywords'], epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### article : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['article'], epoch)\n",
    "        i += 1\n",
    "    return outFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-31 22:00:25 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - train : 469335, test : 52149\n",
      "I0531 22:00:25.074653 140094496737088 batcher.py:186] train : 469335, test : 52149\n",
      "2020-05-31 22:00:25 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - train batches : 14666, test batches : 1629\n",
      "I0531 22:00:25.690158 140094496737088 batcher.py:210] train batches : 14666, test batches : 1629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-31 22:45:33 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - epoch 13: test_avg_acc = 0.369765\n",
      "I0531 22:45:33.455439 140094496737088 <ipython-input-9-206e40dcbf81>:147] epoch 13: test_avg_acc = 0.369765\n",
      "2020-05-31 22:45:33 - Pointer_generator_word2Vec_Intra_Atten_RL - INFO: - logger已關閉\n",
      "I0531 22:45:33.457283 140094496737088 train_util.py:110] logger已關閉\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test \n",
      " ['Accuracy result:\\n', '##-- Rouge --##\\n', 'testing_avg_rouge_1: 0.396555456370121 \\n', 'testing_avg_rouge_2: 0.2665779144467531 \\n', 'testing_avg_rouge_l: 0.3697648393025512 \\n', '##-- SELF-BLEU --##\\n', 'testing_avg_self_bleu1: 0.353505550172693 \\n', 'testing_avg_self_bleu2: 0.24631661144311548 \\n', 'testing_avg_self_bleu3: 0.20310959626010666 \\n', 'testing_avg_self_bleu4: 0.17403881011197814 \\n', '##-- BLEU --##\\n', 'testing_avg_bleu1: 0.353505550172693 \\n', 'testing_avg_bleu2: 0.27046720168119376 \\n', 'testing_avg_bleu3: 0.23249948654803423 \\n', 'testing_avg_bleu4: 0.20396200372347756 \\n', '##-- Meteor --##\\n', 'testing_avg_meteor: 0.36083165857107247 \\n', 'Num : 47872 Execute Time: 0.049164192086717996 \\n']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "loss_st, loss_cost = 0,0\n",
    "decode_st, decode_cost = 0,0\n",
    "last_save_step = 0\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "print_step = 250\n",
    "# save_steps = print_step\n",
    "if not eval_model:\n",
    "\n",
    "    write_train_para(writer, config)\n",
    "    logger.info('------Training START--------')\n",
    "    running_avg_loss, running_avg_rl_loss = 0, 0\n",
    "    sum_total_reward = 0\n",
    "    step = 0\n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(config, logger, vocab, loggerName, patience=3, verbose=True)\n",
    "    try:\n",
    "        for epoch in range((start_ep+1), config.max_epochs+1):\n",
    "            for batch in train_loader:\n",
    "                step += 1; \n",
    "                loss_st = time.time()\n",
    "                inner_c, package = get_package(batch)\n",
    "                if inner_c: continue\n",
    "                parallel_model.module.train()\n",
    "                mle_loss, pred_probs = train_one(package)\n",
    "                if config.train_rl:\n",
    "                    rl_loss, batch_reward = train_one_rl(package, batch)             \n",
    "\n",
    "                    if step%print_step == 0 :\n",
    "                        writer.add_scalars('scalar/RL_Loss',  \n",
    "                           {'rl_loss': rl_loss\n",
    "                           }, step)\n",
    "                        writer.add_scalars('scalar/Reward',  \n",
    "                           {'batch_reward': batch_reward\n",
    "                           }, step)\n",
    "    #                     logger.info('epoch %d: %d, RL_Loss = %f, batch_reward = %f'\n",
    "    #                                     % (epoch, step, rl_loss, batch_reward))\n",
    "                    sum_total_reward += batch_reward\n",
    "                else:\n",
    "                    rl_loss = T.FloatTensor([0]).cuda()\n",
    "\n",
    "                (config.mle_weight * mle_loss + config.rl_weight * rl_loss).backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "                '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "                if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "        #             clip_grad_norm_(model.parameters(), 5.0)                      \n",
    "                    optimizer.step() # 根据累计的梯度更新网络参数\n",
    "                    optimizer.zero_grad() # 清空过往梯度 \n",
    "                if step%print_step == 0 :\n",
    "                    with T.autograd.no_grad():\n",
    "                        train_batch_loss = mle_loss.item()\n",
    "                        train_batch_rl_loss = rl_loss.item()\n",
    "#                         val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                        running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "                        running_avg_rl_loss = calc_running_avg_loss(train_batch_rl_loss, running_avg_rl_loss)\n",
    "                        running_avg_reward = sum_total_reward / step\n",
    "#                         if step % save_steps == 0:\n",
    "#                             logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "#                                         % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                        writer.add_scalars('scalar/Loss',  \n",
    "                           {'train_batch_loss': train_batch_loss\n",
    "                           }, step)\n",
    "                        writer.add_scalars('scalar_avg/loss',  \n",
    "                           {'train_avg_loss': running_avg_loss\n",
    "#                             'test_avg_loss': val_avg_loss\n",
    "                           }, step)\n",
    "                        if running_avg_reward > 0:\n",
    "    #                         logger.info('epoch %d: %d, running_avg_reward = %f'\n",
    "    #                                 % (epoch, step, running_avg_reward))\n",
    "                            writer.add_scalars('scalar_avg/Reward',  \n",
    "                               {'running_avg_reward': running_avg_reward\n",
    "                               }, step)\n",
    "                        if running_avg_rl_loss != 0:\n",
    "    #                         logger.info('epoch %d: %d, running_avg_rl_loss = %f'\n",
    "    #                                 % (epoch, step, running_avg_rl_loss))\n",
    "                            writer.add_scalars('scalar_avg/RL_Loss',  \n",
    "                               {'running_avg_rl_loss': running_avg_rl_loss\n",
    "                               }, step)\n",
    "                                                    \n",
    "                \n",
    "                if step % save_steps == 0:\n",
    "                    parallel_model.module.eval()\n",
    "                    logger.info('epoch : %s' % epoch)\n",
    "                    val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                    logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "                                        % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                    writer.add_scalars('scalar_avg/loss',  \n",
    "                           {'train_avg_loss': running_avg_loss,\n",
    "                            'test_avg_loss': val_avg_loss\n",
    "                           }, step)\n",
    "                    '''（讀取所儲存模型引數後，再進行並行化操作，否則無法利用之前的程式碼進行讀取）'''\n",
    "                    save_model(config, logger, parallel_model, optimizer, step, vocab, val_avg_loss, \\\n",
    "                               r_loss=0, title = loggerName)\n",
    "                    loss_cost = time.time() - loss_st\n",
    "                    logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "                                    % (epoch, step, loss_cost))\n",
    "                    writer.add_scalars('scalar_avg/epoch_loss',  \n",
    "                       {'train_avg_loss': running_avg_loss,\n",
    "                        'test_avg_loss': val_avg_loss\n",
    "                       }, epoch)\n",
    "                    last_save_step = step\n",
    "                    test_outFrame = decode(writer, validate_loader, epoch)                   \n",
    "\n",
    "            logger.info('-------------------------------------------------------------')\n",
    "\n",
    "            if running_avg_reward > 0:\n",
    "                logger.info('epoch %d|step %d| running_avg_reward = %f'% (epoch, step, running_avg_reward))\n",
    "            if running_avg_rl_loss != 0:\n",
    "                logger.info('epoch %d|step %d| running_avg_rl_loss = %f'% (epoch, step, running_avg_rl_loss))\n",
    "            logger.info('-------------------------------------------------------------')\n",
    "\n",
    "            early_stopping(parallel_model, optimizer, step, val_avg_loss) # update patience\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(\"Early stopping epoch %s\"%(epoch))\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        logger.info(u'------Training SUCCESS--------')  \n",
    "    finally:\n",
    "        logger.info(u'------Training END--------')   \n",
    "        logger.info(\"stopping epoch %s\"%(epoch))        \n",
    "        logger.info(\"last_save_step %s\"%(last_save_step))  \n",
    "        '''先將test_avg_acc調起來再decode train_'''\n",
    "        '''decode write all read model時發現權重更新並未存到，經過完所有訓練發現acc差很多(一個3成,一個4成?)'''\n",
    "    #     train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "#         test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, parallel_model.module, validate_loader, mode = 'test')\n",
    "    #     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc)) \n",
    "#         logger.info('epoch %d: test_avg_acc = %f' % (load_ep, test_avg_acc)) \n",
    "        removeLogger(logger)\n",
    "\n",
    "# else: # EVAL\n",
    "#     load_ep = float(config.load_ckpt) / float(save_steps)\n",
    "#     config.batch_size = 32\n",
    "#     train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "#     train_batches = len(iter(train_loader))\n",
    "#     test_batches = len(iter(validate_loader))\n",
    "# #     save_steps = int(train_batches/250)*250\n",
    "#     model.cuda(eval_gpu) \n",
    "#     model.eval()\n",
    "#     '''先將test_avg_acc調起來再decode train_'''\n",
    "# #     train_avg_acc, train_outFrame = decode_write_all(writer, logger, load_ep, config, model, train_loader, mode = 'train')\n",
    "#     test_avg_acc, test_outFrame = decode_write_all(writer, logger, load_ep, config, model, validate_loader, mode = 'test')\n",
    "# #     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (load_ep, train_avg_acc, test_avg_acc)) \n",
    "#     logger.info('epoch %d: test_avg_acc = %f' % (load_ep, test_avg_acc)) \n",
    "#     removeLogger(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article', 'keywords', 'reference', 'decoded', 'rouge_1', 'rouge_2',\n",
       "       'rouge_l', 'self_Bleu_1', 'self_Bleu_2', 'self_Bleu_3', 'self_Bleu_4',\n",
       "       'Bleu_1', 'Bleu_2', 'Bleu_3', 'Bleu_4', 'Meteor', 'article_lens',\n",
       "       'ref_lens', 'overlap', 'overlap_percent', 'gen_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge_1</th>\n",
       "      <th>article</th>\n",
       "      <th>reference</th>\n",
       "      <th>decoded</th>\n",
       "      <th>gen_type</th>\n",
       "      <th>overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47849</th>\n",
       "      <td>0.434783</td>\n",
       "      <td>have been play with this for one week the imag...</td>\n",
       "      <td>it was great tv until notice the screen unifor...</td>\n",
       "      <td>the color accuracy was great until notice ther...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47850</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>this direct comparison ozark the utg which the...</td>\n",
       "      <td>this is direct comparison of ozark vs the utg</td>\n",
       "      <td>this is direct comparison ozark to the utg which</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47851</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>this great video camera for the price . fairly...</td>\n",
       "      <td>this is great video camera for the price</td>\n",
       "      <td>this is great video camera for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47856</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>like car there are those who just want drive t...</td>\n",
       "      <td>excellent high end point and shoot camera</td>\n",
       "      <td>great for point and shoot camera</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47868</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>purchased cameras camera and 202l dnr . take t...</td>\n",
       "      <td>work well but take ton of patience</td>\n",
       "      <td>take ton of patience and time finally get ever...</td>\n",
       "      <td>Abs</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47814</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>have decide return this device . after try for...</td>\n",
       "      <td>surprised at the quality of this monitor but i...</td>\n",
       "      <td>this is not heart rate monitor</td>\n",
       "      <td>Abs</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47812</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>this very good phone system . there are many d...</td>\n",
       "      <td>this is very good phone system</td>\n",
       "      <td>this is very good phone system</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47776</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>this drain pump excellent replacement every re...</td>\n",
       "      <td>excellent replacement pump how to drain the tu...</td>\n",
       "      <td>this drain pump is excellent replacement every...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47790</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>the sony cli the perfect combination style fun...</td>\n",
       "      <td>excellent combination of style functionality a...</td>\n",
       "      <td>perfect combination of functionality and price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47793</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>first let mention that any have with this prod...</td>\n",
       "      <td>they are not terrible but they are mislead on ...</td>\n",
       "      <td>they terrible but they are mislead their rating</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47750</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>currently own the bose qc15 headphone and use ...</td>\n",
       "      <td>great sound poor battery life disappoint bluet...</td>\n",
       "      <td>horrible sound quality and poor battery life</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47731</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>very nice gps very easy use really like the la...</td>\n",
       "      <td>very nice gps very easy to use</td>\n",
       "      <td>very nice gps very easy to use</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47740</th>\n",
       "      <td>0.434783</td>\n",
       "      <td>the deal . get this phone from the seller cell...</td>\n",
       "      <td>seller and their defective product make bad re...</td>\n",
       "      <td>ton of great review you want defective product...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47691</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>had couple these was the first monopod actuall...</td>\n",
       "      <td>when get the manfrotto think it was great it w...</td>\n",
       "      <td>thought was lot lighter than the old very heav...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47685</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>this really great price for large quantity goo...</td>\n",
       "      <td>this is really great price for large quantity ...</td>\n",
       "      <td>this is really great price for large quantity ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47710</th>\n",
       "      <td>0.476190</td>\n",
       "      <td>let start off why will return the product . re...</td>\n",
       "      <td>could be great product if motorola support it</td>\n",
       "      <td>think this device could be really great only m...</td>\n",
       "      <td>Abs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47664</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>hey everyone today will review the samsung gal...</td>\n",
       "      <td>in depth review samsung galaxy note premium qu...</td>\n",
       "      <td>hill review of the samsung galaxy note</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47600</th>\n",
       "      <td>0.538462</td>\n",
       "      <td>have been japan time the last year and have fi...</td>\n",
       "      <td>great japanese sim card for convenience servic...</td>\n",
       "      <td>have find this prepaid sim card one of the bes...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47609</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>certify veteran the litter box war can say tha...</td>\n",
       "      <td>this is the best and least expensive to own au...</td>\n",
       "      <td>this is the hand down best least expensive and...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47562</th>\n",
       "      <td>0.761905</td>\n",
       "      <td>lens breathing and other lens phenomena ill ma...</td>\n",
       "      <td>zoom make the lens pretty amazing find issue w...</td>\n",
       "      <td>the lens is pretty amazing find issue with cop...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47560</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>this simply the best smartwatch available for ...</td>\n",
       "      <td>simply the best smartwatch for android on the ...</td>\n",
       "      <td>this is simply the best smartwatch available f...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47615</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>wonderful amazing car camera pro have purchase...</td>\n",
       "      <td>wonderful amazing car camera</td>\n",
       "      <td>wonderful camera pro have purchase several car...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47570</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>update 27oct11 this final review this product ...</td>\n",
       "      <td>wonderful when it work but malfunction break e...</td>\n",
       "      <td>the switch is easily because it is really good...</td>\n",
       "      <td>Abs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47520</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>go describe experience the good absolutely ama...</td>\n",
       "      <td>night vision for this price must have however</td>\n",
       "      <td>the good absolutely amazing that can have nigh...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47534</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>the canon sd1200 good camera the current price...</td>\n",
       "      <td>good ultra small camera at good price</td>\n",
       "      <td>the canon sd1200 is good camera at the current...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47537</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>this landing gear brand usb charger and outlet...</td>\n",
       "      <td>good quality usb charger and ac outlet adapter</td>\n",
       "      <td>gear usb charger and outlet adapter appear to ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47492</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>while consider myself hardcore gamer spend ple...</td>\n",
       "      <td>an excellent gaming mouse with some minor ergo...</td>\n",
       "      <td>great mouse with few minor issue</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47547</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>really want like this littler box . good for c...</td>\n",
       "      <td>really want to like this littler box</td>\n",
       "      <td>really want to like this littler box</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47502</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>satisfied with this keyboard kit . both keyboa...</td>\n",
       "      <td>work great with iphone ipad and mac</td>\n",
       "      <td>work great with my iphone and ipad</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47505</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>was plan purchase the samsung but decide with ...</td>\n",
       "      <td>excellent blu ray player with video store buil...</td>\n",
       "      <td>excellent blu ray player for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>use this for control projector screen . this u...</td>\n",
       "      <td>am use this for control an rf projector screen</td>\n",
       "      <td>use this for control projector screen this unit</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>extremely satisfied with the new rugged iphone...</td>\n",
       "      <td>extremely satisfied with the new rugged iphone...</td>\n",
       "      <td>am extremely satisfied with the new rugged iph...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>seem really good only get week ago will adjust...</td>\n",
       "      <td>seem really good only get it week ago will adjust</td>\n",
       "      <td>seem to be really good only get week ago will ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>this item misleading . picture show box and le...</td>\n",
       "      <td>this item is mislead picture show box and</td>\n",
       "      <td>this item is misleading picture show box and lead</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>easy install but adhesive not very strong all ...</td>\n",
       "      <td>easy install but adhesive is not very strong a...</td>\n",
       "      <td>easy to install but adhesive not very strong a...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>work pretty good but now start get into the re...</td>\n",
       "      <td>work pretty good but now start to get into the...</td>\n",
       "      <td>work pretty good but now start to get into the</td>\n",
       "      <td>Ext</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.761905</td>\n",
       "      <td>its good till the time you connect the smart h...</td>\n",
       "      <td>its good tv till the time you will not connect...</td>\n",
       "      <td>its good till the time you connect to the smar...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>fantastic camera . definitely not dslr series ...</td>\n",
       "      <td>fantastic camera it is definitely not dslr or an</td>\n",
       "      <td>fantastic camera definitely not dslr series bu...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>its good with smart feature . exactly what wan...</td>\n",
       "      <td>its good tv with no smart tv feature</td>\n",
       "      <td>its good with with smart feature</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>was great for couple month but then every corn...</td>\n",
       "      <td>was great for couple of month</td>\n",
       "      <td>was great for couple of month but then every c...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.526316</td>\n",
       "      <td>like fall out pocket just few inch off the gro...</td>\n",
       "      <td>do not like it it fall out of my pocket just</td>\n",
       "      <td>like fall out pocket just few inch off the</td>\n",
       "      <td>Abs</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>fit new ultegra 6800 pedal perfectly . very us...</td>\n",
       "      <td>very useful if you want to use road bike for</td>\n",
       "      <td>very useful if you want to use road bike for q...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.782609</td>\n",
       "      <td>the picture the cover awesome but sticker plac...</td>\n",
       "      <td>the picture on the cover is awesome but it is ...</td>\n",
       "      <td>the picture the cover is awesome but sticker p...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>good quality . manufacturer need find way mini...</td>\n",
       "      <td>good quality manufacturer need to find way to ...</td>\n",
       "      <td>good quality manufacturer need to find way to ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>have gaiam balance ball chair . this cover way...</td>\n",
       "      <td>have gaiam balance ball chair this cover</td>\n",
       "      <td>have gaiam balance ball chair this cover</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>beautiful sling put off the price . you have i...</td>\n",
       "      <td>beautiful sling do not be put off by the price</td>\n",
       "      <td>beautiful sling put off the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>want brainer point and shoot camera with big z...</td>\n",
       "      <td>no brainer point and shoot with big zoom lens</td>\n",
       "      <td>want brainer point and shoot camera with</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>the optical zoom like 600 len and bring object...</td>\n",
       "      <td>the optical zoom like mm lens and bring object...</td>\n",
       "      <td>the optical zoom is like 600 len</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>great fun son could play football this ball re...</td>\n",
       "      <td>great fun my son could play football</td>\n",
       "      <td>great fun my son could play football this ball...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>normally get small medium zumba pant . try the...</td>\n",
       "      <td>try the small and they fit perfect still had t...</td>\n",
       "      <td>the small and they fit perfect still had the b...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.551724</td>\n",
       "      <td>inexpensive pellet that work well some the spe...</td>\n",
       "      <td>great to use plinke or knock down soup and soda</td>\n",
       "      <td>inexpensive pellet that work well some the spe...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>great unit . after week one the camera die had...</td>\n",
       "      <td>great unit after week one of the camera</td>\n",
       "      <td>great unit after week one of the camera die</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>excellent quality food and cat love it . have ...</td>\n",
       "      <td>excellent quality food and my cat love it</td>\n",
       "      <td>excellent quality food and my cat love it</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>buy this secondary case . its light and stylis...</td>\n",
       "      <td>buy this an secondary case its light</td>\n",
       "      <td>buy this secondary case its light and stylish</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>love hold pound lens when out shoot the sun mo...</td>\n",
       "      <td>love it it hold up my pound lens</td>\n",
       "      <td>love it hold my pound lens when out</td>\n",
       "      <td>Abs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>was very satisfied with the clear football hel...</td>\n",
       "      <td>was very satisfied with the clear football hel...</td>\n",
       "      <td>was very satisfied with the clear football hel...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>very good value . lot function for the money ....</td>\n",
       "      <td>very good value lot of function for the money</td>\n",
       "      <td>very good value lot of function for the money</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>this awesome design had return because its ove...</td>\n",
       "      <td>this is an awesome design had to return becaus...</td>\n",
       "      <td>this awesome awesome design had to return beca...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>buy this thinking would match the purple bike ...</td>\n",
       "      <td>very disappointed that spend my money on this</td>\n",
       "      <td>very disappointed that spend my money on this .</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>this charger excellent buy . the coil make hug...</td>\n",
       "      <td>this charger is an excellent buy the coil make...</td>\n",
       "      <td>this charger is excellent buy the coil make it...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18465 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rouge_1                                            article  \\\n",
       "47849  0.434783  have been play with this for one week the imag...   \n",
       "47850  0.777778  this direct comparison ozark the utg which the...   \n",
       "47851  1.000000  this great video camera for the price . fairly...   \n",
       "47856  0.615385  like car there are those who just want drive t...   \n",
       "47868  0.555556  purchased cameras camera and 202l dnr . take t...   \n",
       "47814  0.421053  have decide return this device . after try for...   \n",
       "47812  1.000000  this very good phone system . there are many d...   \n",
       "47776  0.444444  this drain pump excellent replacement every re...   \n",
       "47790  0.769231  the sony cli the perfect combination style fun...   \n",
       "47793  0.875000  first let mention that any have with this prod...   \n",
       "47750  0.533333  currently own the bose qc15 headphone and use ...   \n",
       "47731  1.000000  very nice gps very easy use really like the la...   \n",
       "47740  0.434783  the deal . get this phone from the seller cell...   \n",
       "47691  0.545455  had couple these was the first monopod actuall...   \n",
       "47685  1.000000  this really great price for large quantity goo...   \n",
       "47710  0.476190  let start off why will return the product . re...   \n",
       "47664  0.421053  hey everyone today will review the samsung gal...   \n",
       "47600  0.538462  have been japan time the last year and have fi...   \n",
       "47609  0.615385  certify veteran the litter box war can say tha...   \n",
       "47562  0.761905  lens breathing and other lens phenomena ill ma...   \n",
       "47560  0.800000  this simply the best smartwatch available for ...   \n",
       "47615  0.500000  wonderful amazing car camera pro have purchase...   \n",
       "47570  0.444444  update 27oct11 this final review this product ...   \n",
       "47520  0.444444  go describe experience the good absolutely ama...   \n",
       "47534  0.500000  the canon sd1200 good camera the current price...   \n",
       "47537  0.500000  this landing gear brand usb charger and outlet...   \n",
       "47492  0.533333  while consider myself hardcore gamer spend ple...   \n",
       "47547  1.000000  really want like this littler box . good for c...   \n",
       "47502  0.857143  satisfied with this keyboard kit . both keyboa...   \n",
       "47505  0.500000  was plan purchase the samsung but decide with ...   \n",
       "...         ...                                                ...   \n",
       "3      0.750000  use this for control projector screen . this u...   \n",
       "8      0.947368  extremely satisfied with the new rugged iphone...   \n",
       "1      0.818182  seem really good only get week ago will adjust...   \n",
       "49     0.823529  this item misleading . picture show box and le...   \n",
       "63     0.900000  easy install but adhesive not very strong all ...   \n",
       "62     0.952381  work pretty good but now start get into the re...   \n",
       "61     0.761905  its good till the time you connect the smart h...   \n",
       "60     0.588235  fantastic camera . definitely not dslr series ...   \n",
       "59     0.833333  its good with smart feature . exactly what wan...   \n",
       "58     0.750000  was great for couple month but then every corn...   \n",
       "57     0.526316  like fall out pocket just few inch off the gro...   \n",
       "56     0.952381  fit new ultegra 6800 pedal perfectly . very us...   \n",
       "54     0.782609  the picture the cover awesome but sticker plac...   \n",
       "53     1.000000  good quality . manufacturer need find way mini...   \n",
       "52     1.000000  have gaiam balance ball chair . this cover way...   \n",
       "51     0.750000  beautiful sling put off the price . you have i...   \n",
       "0      0.625000  want brainer point and shoot camera with big z...   \n",
       "16     0.470588  the optical zoom like 600 len and bring object...   \n",
       "18     0.823529  great fun son could play football this ball re...   \n",
       "19     0.952381  normally get small medium zumba pant . try the...   \n",
       "20     0.551724  inexpensive pellet that work well some the spe...   \n",
       "21     0.941176  great unit . after week one the camera die had...   \n",
       "22     1.000000  excellent quality food and cat love it . have ...   \n",
       "23     0.800000  buy this secondary case . its light and stylis...   \n",
       "24     0.800000  love hold pound lens when out shoot the sun mo...   \n",
       "25     0.952381  was very satisfied with the clear football hel...   \n",
       "28     1.000000  very good value . lot function for the money ....   \n",
       "30     0.857143  this awesome design had return because its ove...   \n",
       "31     1.000000  buy this thinking would match the purple bike ...   \n",
       "17     0.857143  this charger excellent buy . the coil make hug...   \n",
       "\n",
       "                                               reference  \\\n",
       "47849  it was great tv until notice the screen unifor...   \n",
       "47850      this is direct comparison of ozark vs the utg   \n",
       "47851           this is great video camera for the price   \n",
       "47856          excellent high end point and shoot camera   \n",
       "47868                 work well but take ton of patience   \n",
       "47814  surprised at the quality of this monitor but i...   \n",
       "47812                     this is very good phone system   \n",
       "47776  excellent replacement pump how to drain the tu...   \n",
       "47790  excellent combination of style functionality a...   \n",
       "47793  they are not terrible but they are mislead on ...   \n",
       "47750  great sound poor battery life disappoint bluet...   \n",
       "47731                     very nice gps very easy to use   \n",
       "47740  seller and their defective product make bad re...   \n",
       "47691  when get the manfrotto think it was great it w...   \n",
       "47685  this is really great price for large quantity ...   \n",
       "47710      could be great product if motorola support it   \n",
       "47664  in depth review samsung galaxy note premium qu...   \n",
       "47600  great japanese sim card for convenience servic...   \n",
       "47609  this is the best and least expensive to own au...   \n",
       "47562  zoom make the lens pretty amazing find issue w...   \n",
       "47560  simply the best smartwatch for android on the ...   \n",
       "47615                       wonderful amazing car camera   \n",
       "47570  wonderful when it work but malfunction break e...   \n",
       "47520      night vision for this price must have however   \n",
       "47534              good ultra small camera at good price   \n",
       "47537     good quality usb charger and ac outlet adapter   \n",
       "47492  an excellent gaming mouse with some minor ergo...   \n",
       "47547               really want to like this littler box   \n",
       "47502                work great with iphone ipad and mac   \n",
       "47505  excellent blu ray player with video store buil...   \n",
       "...                                                  ...   \n",
       "3         am use this for control an rf projector screen   \n",
       "8      extremely satisfied with the new rugged iphone...   \n",
       "1      seem really good only get it week ago will adjust   \n",
       "49             this item is mislead picture show box and   \n",
       "63     easy install but adhesive is not very strong a...   \n",
       "62     work pretty good but now start to get into the...   \n",
       "61     its good tv till the time you will not connect...   \n",
       "60      fantastic camera it is definitely not dslr or an   \n",
       "59                  its good tv with no smart tv feature   \n",
       "58                         was great for couple of month   \n",
       "57          do not like it it fall out of my pocket just   \n",
       "56          very useful if you want to use road bike for   \n",
       "54     the picture on the cover is awesome but it is ...   \n",
       "53     good quality manufacturer need to find way to ...   \n",
       "52              have gaiam balance ball chair this cover   \n",
       "51        beautiful sling do not be put off by the price   \n",
       "0          no brainer point and shoot with big zoom lens   \n",
       "16     the optical zoom like mm lens and bring object...   \n",
       "18                  great fun my son could play football   \n",
       "19     try the small and they fit perfect still had t...   \n",
       "20       great to use plinke or knock down soup and soda   \n",
       "21               great unit after week one of the camera   \n",
       "22             excellent quality food and my cat love it   \n",
       "23                  buy this an secondary case its light   \n",
       "24                      love it it hold up my pound lens   \n",
       "25     was very satisfied with the clear football hel...   \n",
       "28         very good value lot of function for the money   \n",
       "30     this is an awesome design had to return becaus...   \n",
       "31         very disappointed that spend my money on this   \n",
       "17     this charger is an excellent buy the coil make...   \n",
       "\n",
       "                                                 decoded gen_type  overlap  \n",
       "47849  the color accuracy was great until notice ther...      Ext        8  \n",
       "47850   this is direct comparison ozark to the utg which      Ext        6  \n",
       "47851           this is great video camera for the price      Ext        7  \n",
       "47856                   great for point and shoot camera      Ext        5  \n",
       "47868  take ton of patience and time finally get ever...      Abs        3  \n",
       "47814                     this is not heart rate monitor      Abs        6  \n",
       "47812                     this is very good phone system      Ext        5  \n",
       "47776  this drain pump is excellent replacement every...      Ext        6  \n",
       "47790     perfect combination of functionality and price      Ext        5  \n",
       "47793    they terrible but they are mislead their rating      Ext        8  \n",
       "47750       horrible sound quality and poor battery life      Ext        6  \n",
       "47731                     very nice gps very easy to use      Ext        6  \n",
       "47740  ton of great review you want defective product...      Ext        8  \n",
       "47691  thought was lot lighter than the old very heav...      Ext       12  \n",
       "47685  this is really great price for large quantity ...      Ext       10  \n",
       "47710  think this device could be really great only m...      Abs        4  \n",
       "47664             hill review of the samsung galaxy note      Ext        8  \n",
       "47600  have find this prepaid sim card one of the bes...      Ext        8  \n",
       "47609  this is the hand down best least expensive and...      Ext        9  \n",
       "47562  the lens is pretty amazing find issue with cop...      Ext       10  \n",
       "47560  this is simply the best smartwatch available f...      Ext        8  \n",
       "47615  wonderful camera pro have purchase several car...      Ext        4  \n",
       "47570  the switch is easily because it is really good...      Abs        4  \n",
       "47520  the good absolutely amazing that can have nigh...      Ext        6  \n",
       "47534  the canon sd1200 is good camera at the current...      Ext        4  \n",
       "47537  gear usb charger and outlet adapter appear to ...      Ext        5  \n",
       "47492                   great mouse with few minor issue      Ext        5  \n",
       "47547               really want to like this littler box      Ext        6  \n",
       "47502                 work great with my iphone and ipad      Ext        7  \n",
       "47505             excellent blu ray player for the price      Ext        6  \n",
       "...                                                  ...      ...      ...  \n",
       "3        use this for control projector screen this unit      Ext        6  \n",
       "8      am extremely satisfied with the new rugged iph...      Ext        9  \n",
       "1      seem to be really good only get week ago will ...      Ext        9  \n",
       "49     this item is misleading picture show box and lead      Ext        6  \n",
       "63     easy to install but adhesive not very strong a...      Ext        8  \n",
       "62        work pretty good but now start to get into the      Ext       10  \n",
       "61     its good till the time you connect to the smar...      Ext        9  \n",
       "60     fantastic camera definitely not dslr series bu...      Ext        5  \n",
       "59                      its good with with smart feature      Ext        5  \n",
       "58     was great for couple of month but then every c...      Ext        5  \n",
       "57            like fall out pocket just few inch off the      Abs        5  \n",
       "56     very useful if you want to use road bike for q...      Ext        8  \n",
       "54     the picture the cover is awesome but sticker p...      Ext        9  \n",
       "53     good quality manufacturer need to find way to ...      Ext        7  \n",
       "52              have gaiam balance ball chair this cover      Ext        7  \n",
       "51                     beautiful sling put off the price      Ext        6  \n",
       "0               want brainer point and shoot camera with      Ext        8  \n",
       "16                      the optical zoom is like 600 len      Ext        8  \n",
       "18     great fun my son could play football this ball...      Ext        6  \n",
       "19     the small and they fit perfect still had the b...      Ext       12  \n",
       "20     inexpensive pellet that work well some the spe...      Ext        8  \n",
       "21           great unit after week one of the camera die      Ext        7  \n",
       "22             excellent quality food and my cat love it      Ext        7  \n",
       "23         buy this secondary case its light and stylish      Ext        6  \n",
       "24                   love it hold my pound lens when out      Abs        4  \n",
       "25     was very satisfied with the clear football hel...      Ext       11  \n",
       "28         very good value lot of function for the money      Ext        8  \n",
       "30     this awesome awesome design had to return beca...      Ext        9  \n",
       "31       very disappointed that spend my money on this .      Ext        6  \n",
       "17     this charger is excellent buy the coil make it...      Ext        9  \n",
       "\n",
       "[18465 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outFrame[test_outFrame[\"rouge_1\"]>=0.4][['rouge_1','article', 'reference', 'decoded', 'gen_type','overlap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_16 epoch_6\n",
    "# testing_avg_rouge_1: 0.3873426628114616 \\n', \n",
    "# 'testing_avg_rouge_2: 0.25943944916828854 \\n', \n",
    "# 'testing_avg_rouge_l: 0.3614074094052472 \\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('Leyan': conda)",
   "language": "python",
   "name": "python36764bitleyancondaa378f3cedbcc4b3f906a2276b3eef765"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
