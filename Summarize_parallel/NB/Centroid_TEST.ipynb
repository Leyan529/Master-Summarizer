{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import config\n",
    "from utils.seq2seq import data\n",
    "\n",
    "from utils.seq2seq.batcher import *\n",
    "from utils.seq2seq.train_util import *\n",
    "from utils.seq2seq.rl_util import *\n",
    "from utils.seq2seq.initialize import loadCheckpoint, save_model\n",
    "from utils.seq2seq.write_result import *\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from translate.seq2seq_beam import *\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils.seq2seq.data import output2words\n",
    "import argparse\n",
    "from utils.seq2seq.rl_util import *\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--key_attention', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_encoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--intra_decoder', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--copy', type=bool, default=True, help = 'True/False') # for transformer\n",
    "\n",
    "parser.add_argument('--model_type', type=str, default='seq2seq', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--train_rl', type=bool, default=True, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_keys', \n",
    "                    help = 'POS_keys / DEP_keys / Noun_adj_keys / TextRank_keys')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--rand_unif_init_mag', type=float, default=0.02)\n",
    "parser.add_argument('--trunc_norm_init_std', type=float, default=0.001)\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument('--gound_truth_prob', type=float, default=0.5)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=500)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=20)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=6)\n",
    "parser.add_argument('--max_epochs', type=int, default=15)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=5)\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=300)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default='0378000', help='0002000')\n",
    "parser.add_argument('--word_emb_type', type=str, default='word2Vec', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "# loggerName, writerPath = getName(config)    \n",
    "loggerName = 'Centroid'\n",
    "writerPath = 'runs/%s/%s/exp'% (config.data_type, loggerName)\n",
    "if not os.path.exists(writerPath): os.makedirs(writerPath)\n",
    "logger = getLogger(loggerName)\n",
    "# writer = SummaryWriter(writerPath)\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "train_batches = len(iter(train_loader))\n",
    "test_batches = len(iter(validate_loader))\n",
    "save_steps = int(train_batches/1000)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "from CentroidSummarizer import summarizer\n",
    "model_path = config.word_emb_path\n",
    "s = summarizer.Summarizer(model_path=model_path)\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(dataloader)\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "    avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    avg_meteor = []\n",
    "    outFrame = None\n",
    "    avg_time = 0    \n",
    "#     rouge = Rouge()  \n",
    "\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        start = time.time() \n",
    "        article_sents = [article for article in batch.original_article]\n",
    "        ref_sents = [ref for ref in batch.original_abstract ]\n",
    "        decoded_sents = [\n",
    "            s.summarize(text=article, summary_length=1, query_based_token=[])[0] \\\n",
    "            for article in article_sents\n",
    "        ]\n",
    "        decoded_sents = [sent if len(sent) > 5 else \"xxx xxx xxx xxx xxx\" for sent in decoded_sents]\n",
    "        \n",
    "        keywords_list = [str(word_list) for word_list in batch.key_words]\n",
    "        cost = (time.time() - start)\n",
    "        avg_time += cost        \n",
    "        try:\n",
    "            rouge_1, rouge_2, rouge_l, self_Bleu_1, self_Bleu_2, self_Bleu_3, self_Bleu_4, \\\n",
    "                Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        except Exception as e :\n",
    "            continue\n",
    "            \n",
    "        if idx %1000 ==0 and idx >0 : print(idx)\n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "        # avg_self_bleu1.extend(self_Bleu_1)\n",
    "        # avg_self_bleu2.extend(self_Bleu_2)\n",
    "        # avg_self_bleu3.extend(self_Bleu_3)\n",
    "        # avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "        avg_bleu1.extend(Bleu_1)\n",
    "        avg_bleu2.extend(Bleu_2)\n",
    "        avg_bleu3.extend(Bleu_3)\n",
    "        avg_bleu4.extend(Bleu_4)\n",
    "        avg_meteor.extend(Meteor)\n",
    "        # ----------------------------------------------------    \n",
    "    avg_time = avg_time / (num * config.batch_size) \n",
    "    \n",
    "    avg_rouge_l, outFrame = total_output(mode, writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l, \\\n",
    "        avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4, \\\n",
    "        avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "    )\n",
    "    \n",
    "    return avg_rouge_l, outFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "model = None\n",
    "# model    \n",
    "train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "logger.info('-----------------------------------------------------------')\n",
    "test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc))\n",
    "\n",
    "# !ipython nbconvert --to script Pointer_generator.ipynb\n",
    "train_outFrame.head()\n",
    "test_outFrame.head()\n",
    "removeLogger(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid\n",
    "# textrank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
