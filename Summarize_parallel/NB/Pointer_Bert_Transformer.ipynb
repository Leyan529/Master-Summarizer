{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0518 23:56:52.026219 140541676476224 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-05-18 23:56:52 - Pointer_Sep_BertEnc_Transformer_BertEmb - INFO: - logger已啟動\n",
      "I0518 23:56:52.959589 140541676476224 train_util.py:119] logger已啟動\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loggerName Pointer_Sep_BertEnc_Transformer_BertEmb\n",
      "writerPath runs/Pure_Cloth/Pointer_Sep_BertEnc_Transformer/BertEmb/exp\n"
     ]
    }
   ],
   "source": [
    "from utils import config\n",
    "from utils.bert import data\n",
    "\n",
    "from utils.bert.batcher import *\n",
    "from utils.bert.train_util import *\n",
    "from utils.bert.initialize import loadCheckpoint, save_model\n",
    "from utils.bert.write_result import *\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_type', type=str, default='transformer', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--copy', type=bool, default=True, choices=[True, False])\n",
    "parser.add_argument(\"-encoder\", default='bert', type=str, choices=['bert', 'Transformer'])\n",
    "parser.add_argument(\"-max_pos\", default=500, type=int)\n",
    "parser.add_argument(\"-use_bert_emb\", type=str2bool, nargs='?',const=True,default=True, choices=[False, True])\n",
    "\n",
    "parser.add_argument(\"-lr_bert\", default=2e-2, type=float, help='2e-3')\n",
    "parser.add_argument(\"-lr_dec\", default=2e-2, type=float, help='2e-3')\n",
    "parser.add_argument(\"-share_emb\", type=str2bool, nargs='?', const=True, default=False)\n",
    "parser.add_argument(\"-finetune_bert\", type=bool, default=True)\n",
    "    \n",
    "'''\n",
    "原 Bert Base paper核心參數\n",
    "dropout = 0.1\n",
    "num_layers = 12\n",
    "num_heads = 8\n",
    "emb_dim(d_model) : 768\n",
    "ff_embed_dim = 4*emb_dim = 3072\n",
    "\n",
    "bert_config = BertConfig(self.encoder.model.config.vocab_size, hidden_size=768,\n",
    "                                     num_hidden_layers=12, num_attention_heads=8,\n",
    "                                     intermediate_size= 3072,\n",
    "                                     hidden_dropout_prob=0.1,\n",
    "                                     attention_probs_dropout_prob=0.1)\n",
    "'''\n",
    "parser.add_argument(\"-enc_dropout\", default=0.1, type=float)\n",
    "parser.add_argument(\"-enc_layers\", default=12, type=int)\n",
    "parser.add_argument(\"-enc_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-enc_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-enc_ff_size\", default=3072, type=int)\n",
    "\n",
    "parser.add_argument(\"-dec_dropout\", default=0.1, type=float)\n",
    "parser.add_argument(\"-dec_layers\", default=12, type=int)\n",
    "parser.add_argument(\"-dec_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-dec_heads\", default=8, type=int)\n",
    "parser.add_argument(\"-dec_ff_size\", default=2048, type=int)\n",
    "parser.add_argument(\"-sep_optim\", type=str2bool, nargs='?',const=True,default=True, choices=[False, True])\n",
    "\n",
    "parser.add_argument(\"-param_init\", default=0, type=float)\n",
    "parser.add_argument(\"-param_init_glorot\", type=str2bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-optim\", default='adam', type=str)\n",
    "parser.add_argument(\"-lr\", default=1, type=float)\n",
    "parser.add_argument(\"-beta1\", default= 0.9, type=float)\n",
    "parser.add_argument(\"-beta2\", default=0.999, type=float)\n",
    "parser.add_argument(\"-warmup_steps\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_bert\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_dec\", default=8000, type=int)\n",
    "parser.add_argument(\"-max_grad_norm\", default=0, type=float)\n",
    "\n",
    "parser.add_argument(\"-block_trigram\", type=str2bool, nargs='?', const=True, default=True)\n",
    "\n",
    "\n",
    "parser.add_argument('--train_rl', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_keys', \n",
    "                    help = 'POS_keys / DEP_keys / Noun_adj_keys / TextRank_keys')\n",
    "\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument(\"-label_smoothing\", default=0.0, type=float)\n",
    "parser.add_argument(\"-generator_shard_size\", default=32, type=int)\n",
    "parser.add_argument(\"-alpha\",  default=0.6, type=float)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=500)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=40)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=10)\n",
    "parser.add_argument('--max_epochs', type=int, default=15)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=5)\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "\n",
    "# parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "# parser.add_argument('--emb_dim', type=int, default=512)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default='0000010', help='0000010')\n",
    "# parser.add_argument('--word_emb_type', type=str, default='glove', help='word2Vec/glove/FastText')\n",
    "# parser.add_argument('--pre_train_emb', type=bool, default=False, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0518 23:56:54.005227 140541676476224 tokenization.py:157] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "{'BOS': 1, 'EOS': 2, 'PAD': 0, 'EOQ': 3, 'SEP': 102, 'CLS': 101, 'UNK': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0518 23:56:59.022132 140541676476224 ultratb.py:149] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "I0518 23:56:59.028973 140541676476224 ultratb.py:1111] \n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-1b9bb1c260d5>\", line 1, in <module>\n",
      "    train_loader, validate_loader, vocab, symbols = getDataLoader(logger, config)\n",
      "  File \"/home/eagleuser/Users/leyan/Summarize_parallel/utils/bert/batcher.py\", line 208, in getDataLoader\n",
      "    total_df = pd.read_excel(config.xls_path)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/pandas/util/_decorators.py\", line 178, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/pandas/util/_decorators.py\", line 178, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/pandas/io/excel.py\", line 307, in read_excel\n",
      "    io = ExcelFile(io, engine=engine)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/pandas/io/excel.py\", line 394, in __init__\n",
      "    self.book = xlrd.open_workbook(self._io)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/xlrd/__init__.py\", line 138, in open_workbook\n",
      "    ragged_rows=ragged_rows,\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/xlrd/xlsx.py\", line 832, in open_workbook_2007_xml\n",
      "    x12sst.process_stream(zflo, 'SST')\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/xlrd/xlsx.py\", line 438, in process_stream_iterparse\n",
      "    for event, elem in ET.iterparse(stream):\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/xml/etree/ElementTree.py\", line 1226, in iterator\n",
      "    pullparser.feed(data)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/xml/etree/ElementTree.py\", line 1268, in feed\n",
      "    self._parser.feed(data)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/xml/etree/ElementTree.py\", line 1624, in feed\n",
      "    self.parser.Parse(data, 0)\n",
      "  File \"/tmp/build/80754af9/python_1540319457073/work/Modules/pyexpat.c\", line 414, in StartElement\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/xml/etree/ElementTree.py\", line 1548, in _start\n",
      "    return self.target.start(tag, attrib)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/inspect.py\", line 1488, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/inspect.py\", line 1446, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab, symbols = getDataLoader(logger, config)\n",
    "tokenizer = vocab.tokenizer\n",
    "train_batches = len(iter(train_loader))\n",
    "test_batches = len(iter(validate_loader))\n",
    "save_steps = int(train_batches/1000)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.transformer.loss import *\n",
    "from utils.transformer.optimizers import Optimizer\n",
    "from transformer import *\n",
    "from utils.transformer.predictor import build_predictor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "model = AbsSummarizer(config)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(config, logger, load_model_path, model)\n",
    "else:    \n",
    "    if (config.sep_optim):\n",
    "        optim_bert = Optimizer(\n",
    "            config.optim, config.lr_bert, config.max_grad_norm,\n",
    "            beta1=config.beta1, beta2=config.beta2,\n",
    "            decay_method='noam',\n",
    "            warmup_steps=config.warmup_steps_bert)\n",
    "\n",
    "        optim_dec = Optimizer(\n",
    "            config.optim, config.lr_dec, config.max_grad_norm,\n",
    "            beta1=config.beta1, beta2=config.beta2,\n",
    "            decay_method='noam',\n",
    "            warmup_steps=config.warmup_steps_dec)\n",
    "        \n",
    "        params = [(n, p) for n, p in list(model.named_parameters()) if n.startswith('encoder.model')]\n",
    "        optim_bert.set_parameters(params)\n",
    "\n",
    "        params = [(n, p) for n, p in list(model.named_parameters()) if not n.startswith('encoder.model')]\n",
    "        optim_dec.set_parameters(params)\n",
    "\n",
    "        optimizer = [optim_bert, optim_dec]\n",
    "    else:\n",
    "        optimizer = Optimizer(\n",
    "            config.optim, config.lr, config.max_grad_norm,\n",
    "            beta1=config.beta1, beta2=config.beta2,\n",
    "            decay_method='noam',\n",
    "            warmup_steps=config.warmup_steps)\n",
    "        optimizer.set_parameters(list(model.named_parameters()))\n",
    "        optimizer = [optimizer]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "setattr(config, 'device_ids', [0])\n",
    "\n",
    "model = get_cuda(model)\n",
    "net = nn.DataParallel(model, device_ids=config.device_ids)\n",
    "# model = nn.DataParallel(model).cuda()\n",
    "model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(net, model, config, batch):\n",
    "    normalization = 0\n",
    "    'Encoder data'\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, _, _, _, _, _, enc_seg, enc_cls, enc_cls_mask = get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "    'Decoder data'\n",
    "    dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = get_output_from_batch(batch, config, batch_first = True) # Get input and target \n",
    "\n",
    "    \n",
    "    num_tokens = dec_batch[:, 1:].ne(0).sum()\n",
    "    normalization += num_tokens.item()    \n",
    "\n",
    "#     pred, state = model(enc_batch, dec_batch, enc_seg, \n",
    "#         enc_cls, enc_padding_mask, dec_padding_mask, enc_cls_mask, \n",
    "#         extra_zeros, enc_batch_extend_vocab)\n",
    "    pred, state = net(enc_batch, dec_batch, enc_seg, \n",
    "        enc_cls, enc_padding_mask, dec_padding_mask, enc_cls_mask, \n",
    "        extra_zeros, enc_batch_extend_vocab)\n",
    "    \n",
    "    criterion = choose_criterion(config, model.vocab_size)\n",
    "    loss, num_correct, target = compute_loss(None, criterion, pred, dec_batch[:,1:], num_tokens, tokenizer)\n",
    "    # loss = loss / normalization  # Normalized losses; (batch_size)\n",
    "    # --------------------------------------------------------------------------------\n",
    "    acc = accuracy(num_correct, num_tokens)\n",
    "    cross_entropy = xent(loss, num_tokens)\n",
    "    perplexity = ppl(loss, num_tokens)\n",
    "\n",
    "#     print(\"num_tokens:%s; acc: %6.2f; perplexity: %5.2f; cross entropy loss: %4.2f\" \n",
    "#                             % (num_tokens,\n",
    "#                             acc,\n",
    "#                             perplexity,\n",
    "#                             cross_entropy\n",
    "#                             ))\n",
    "    # >>>>>>>> DEBUG Session <<<<<<<<<\n",
    "    # print('------------------------------------')\n",
    "    # print(\"ENC\\n\")\n",
    "    # print(enc_batch.shape)\n",
    "    # print(\"DEC\\n\")\n",
    "    # print(dec_batch.shape)\n",
    "    # print(\"TGT\\n\")\n",
    "    # print(target_batch.shape)\n",
    "    # print(\"ENCP\\n\")\n",
    "    # print(enc_padding_mask.shape)\n",
    "    # print(\"DECP\\n\")\n",
    "    # print(dec_padding_mask.shape)\n",
    "    # print(\"enc_seg\\n\")\n",
    "    # print(enc_seg.shape)\n",
    "    # print(\"enc_cls\\n\")\n",
    "    # print(enc_cls.shape)\n",
    "    # print(\"enc_cls_mask\\n\")\n",
    "    # print(enc_cls_mask.shape)\n",
    "    return loss.div(float(normalization))    \n",
    "#     return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, net, model):\n",
    "    losses = []\n",
    "    # batch = next(iter(validate_loader))\n",
    "    val_num = len(iter(validate_loader))\n",
    "    for idx, batch in enumerate(validate_loader):\n",
    "        loss = train_one(net, model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "        if idx>= val_num/10: break\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "@torch.autograd.no_grad()\n",
    "def decode(writer, logger, step, config, model, batch, mode):\n",
    "    # 動態取batch\n",
    "    if mode == 'test':\n",
    "        # num = len(iter(batch))\n",
    "        # select_batch = None\n",
    "        # rand_b_id = randint(0,num-1)\n",
    "        # logger.info('test_batch : ' + str(num)+ ' ' + str(rand_b_id))\n",
    "        # for idx, b in enumerate(batch):\n",
    "        #     if idx == rand_b_id:\n",
    "        #         select_batch = b\n",
    "        #         break\n",
    "        select_batch = next(iter(batch))\n",
    "        batch = select_batch\n",
    "        if type(batch) == torch.utils.data.dataloader.DataLoader:\n",
    "            batch = next(iter(batch))\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    '''\n",
    "    batch_data = self.translate_batch(batch)\n",
    "    translations = self.from_batch(batch_data)\n",
    "    '''\n",
    "    gold_tgt_len = batch.dec_tgt.size(1)\n",
    "    setattr(config, 'min_length',gold_tgt_len + 20)\n",
    "    setattr(config, 'max_length',gold_tgt_len + 60)\n",
    "    predictor = build_predictor(config, tokenizer, symbols, model, logger)\n",
    "\n",
    "    # 'Encoder data'\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, \\\n",
    "        _, _, _, _, _, enc_seg, enc_cls, enc_cls_mask = get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "    # 'Decoder data'\n",
    "    dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = get_output_from_batch(batch, config, batch_first = True) # Get input and target \n",
    "\n",
    "    setattr(batch, 'src',enc_batch)\n",
    "    setattr(batch, 'segs',enc_seg)\n",
    "    setattr(batch, 'mask_src',enc_padding_mask)\n",
    "\n",
    "    batch_data = predictor.translate_batch(batch)\n",
    "    translations = predictor.from_batch(batch_data) # translation = (pred_sents, gold_sent, raw_src)\n",
    "    article_sents = [t[2] for t in translations]\n",
    "    decoded_sents = [t[0] for t in translations]\n",
    "    ref_sents = [t[1] for t in translations]\n",
    "    keywords_list = [str(word_list) for word_list in batch.key_words]\n",
    "#     print('decoded_sents',decoded_sents)\n",
    "    # ---------------------------------------------------------------------------\n",
    "    rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents,                     keywords_list, ref_sents, 0, write = False)\n",
    "    write_bleu(writer, step, mode, article_sents, decoded_sents, keywords_list, ref_sents, 0)\n",
    "\n",
    "    write_group(writer, step, mode, article_sents, decoded_sents, keywords_list, ref_sents, 0)\n",
    "\n",
    "    return rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import time\n",
    "@torch.autograd.no_grad()\n",
    "def avg_acc(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(iter(dataloader))\n",
    "    avg_rouge_l = []\n",
    "    acc_st, acc_cost = 0, 0\n",
    "    avg_acc_cost = []\n",
    "    for idx, batch in enumerate(dataloader): \n",
    "        if idx >= num/10000: break\n",
    "        acc_st = time.time()\n",
    "        # ---------------------------------------------------------------------------\n",
    "        '''\n",
    "        batch_data = self.translate_batch(batch)\n",
    "        translations = self.from_batch(batch_data)\n",
    "        '''\n",
    "        gold_tgt_len = batch.dec_tgt.size(1)\n",
    "        setattr(config, 'min_length',gold_tgt_len + 20)\n",
    "        setattr(config, 'max_length',gold_tgt_len + 60)\n",
    "        predictor = build_predictor(config, tokenizer, symbols, model, logger)\n",
    "\n",
    "        # 'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, \\\n",
    "        _, _, _, _, _, enc_seg, enc_cls, enc_cls_mask = get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        # 'Decoder data'\n",
    "        dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = get_output_from_batch(batch, config, batch_first = True) # Get input and target \n",
    "\n",
    "        setattr(batch, 'src',enc_batch)\n",
    "        setattr(batch, 'segs',enc_seg)\n",
    "        setattr(batch, 'mask_src',enc_padding_mask)\n",
    "\n",
    "        batch_data = predictor.translate_batch(batch)\n",
    "        translations = predictor.from_batch(batch_data) # translation = (pred_sents, gold_sent, raw_src)\n",
    "        article_sents = [t[2] for t in translations]\n",
    "        decoded_sents = [t[0] for t in translations]\n",
    "        ref_sents = [t[1] for t in translations]\n",
    "        keywords_list = [str(word_list) for word_list in batch.key_words]\n",
    "\n",
    "\n",
    "        rouge_1, rouge_2, rouge_l = \\\n",
    "        write_rouge(writer, None, None, article_sents, decoded_sents, keywords_list, ref_sents, 0, write = False)\n",
    "        # ---------------------------------------------------------------------------\n",
    "        avg_rouge_l.append(rouge_l)\n",
    "        acc_cost = time.time() - acc_st\n",
    "        avg_acc_cost.append(acc_cost)\n",
    "\n",
    "\n",
    "    avg_rouge_l = sum(avg_rouge_l) / len(avg_rouge_l)\n",
    "    writer.add_scalars('scalar_avg/acc',  \n",
    "                   {'%sing_avg_acc'%(mode): avg_rouge_l\n",
    "                   }, epoch)\n",
    "    avg_acc_cost = sum(avg_acc_cost) / len(avg_acc_cost)\n",
    "    return avg_rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(dataloader)\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "    avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    avg_meteor = []\n",
    "    outFrame = None\n",
    "    avg_time = 0\n",
    "    \n",
    "    rouge = Rouge()  \n",
    "    \n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        start = time.time() \n",
    "        gold_tgt_len = batch.dec_tgt.size(1)\n",
    "        setattr(config, 'min_length',gold_tgt_len + 20)\n",
    "        setattr(config, 'max_length',gold_tgt_len + 60)\n",
    "        predictor = build_predictor(config, tokenizer, symbols, model, logger)\n",
    "\n",
    "        # 'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, _, \\\n",
    "        _, _, _, _, enc_seg, enc_cls, enc_cls_mask = \\\n",
    "            get_input_from_batch(batch, config, batch_first = True)\n",
    "\n",
    "        # 'Decoder data'\n",
    "        dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(batch, config, batch_first = True) # Get input and target \n",
    "\n",
    "        setattr(batch, 'src',enc_batch)\n",
    "        setattr(batch, 'segs',enc_seg)\n",
    "        setattr(batch, 'mask_src',enc_padding_mask)\n",
    "\n",
    "        batch_data = predictor.translate_batch(batch)\n",
    "        translations = predictor.from_batch(batch_data) # translation = (pred_sents, gold_sent, raw_src)\n",
    "        article_sents = [t[2] for t in translations]\n",
    "        decoded_sents = [t[0] for t in translations]\n",
    "        ref_sents = [t[1] for t in translations]\n",
    "        keywords_list = [str(word_list) for word_list in batch.key_words]\n",
    "        cost = (time.time() - start)\n",
    "        avg_time += cost        \n",
    "\n",
    "        rouge_1, rouge_2, rouge_l, self_Bleu_1, self_Bleu_2, self_Bleu_3, self_Bleu_4, \\\n",
    "            Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "\n",
    "        if idx %1000 ==0 and idx >0 : print(idx)\n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "        # avg_self_bleu1.extend(self_Bleu_1)\n",
    "        # avg_self_bleu2.extend(self_Bleu_2)\n",
    "        # avg_self_bleu3.extend(self_Bleu_3)\n",
    "        # avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "        avg_bleu1.extend(Bleu_1)\n",
    "        avg_bleu2.extend(Bleu_2)\n",
    "        avg_bleu3.extend(Bleu_3)\n",
    "        avg_bleu4.extend(Bleu_4)\n",
    "        avg_meteor.extend(Meteor)\n",
    "        # ----------------------------------------------------    \n",
    "    avg_time = avg_time / (num * config.batch_size) \n",
    "    \n",
    "    avg_rouge_l, outFrame = total_output(mode, writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l, \\\n",
    "        avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4, \\\n",
    "        avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "    )\n",
    "    \n",
    "    return avg_rouge_l, outFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "loss_st, loss_cost = 0,0\n",
    "decode_st, decode_cost = 0,0\n",
    "\n",
    "write_train_para(writer, config)\n",
    "logger.info('------Training START--------')\n",
    "running_avg_loss, running_avg_rl_loss = 0, 0\n",
    "sum_total_reward = 0\n",
    "step = 0\n",
    "# save_steps = 10\n",
    "# try:\n",
    "for epoch in range(config.max_epochs):\n",
    "    for batch in train_loader:\n",
    "        step += 1            \n",
    "        loss_st = time.time()\n",
    "        mle_loss = train_one(net, model, config, batch)\n",
    "        print(step, mle_loss)\n",
    "        if config.train_rl:\n",
    "            rl_loss, batch_reward = train_one_RL(net, config, batch)             \n",
    "        else:\n",
    "            rl_loss = T.FloatTensor([0]).cuda()\n",
    "        (config.mle_weight * mle_loss + config.rl_weight * rl_loss).backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "        model.zero_grad() # 清空过往梯度\n",
    "        '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "        if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "                # clip_grad_norm_(model.parameters(), 5.0)                     \n",
    "            for opt_idx, o in enumerate(optimizer):\n",
    "                o.step() # 根据累计的梯度更新网络参数\n",
    "                if opt_idx == 0: opt_name = 'lr_bert'\n",
    "                else: opt_name = 'lr_dec'\n",
    "                writer.add_scalars('scalar/%s' % opt_name,  \n",
    "                       {'lr': o.learning_rate\n",
    "                       }, step)\n",
    "\n",
    "\n",
    "        if step%1000 == 0 :\n",
    "            with T.autograd.no_grad():\n",
    "                train_batch_loss = mle_loss.item()\n",
    "                train_batch_rl_loss = rl_loss.item()\n",
    "                val_avg_loss = validate(validate_loader, config, net, model) # call batch by validate_loader\n",
    "                running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "                running_avg_rl_loss = calc_running_avg_loss(train_batch_rl_loss, running_avg_rl_loss)\n",
    "                running_avg_reward = sum_total_reward / step\n",
    "                if step % save_steps == 0:\n",
    "                    logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "                                % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                writer.add_scalars('scalar/Loss',  \n",
    "                   {'train_batch_loss': train_batch_loss\n",
    "                   }, step)\n",
    "                writer.add_scalars('scalar_avg/loss',  \n",
    "                   {'train_avg_loss': running_avg_loss,\n",
    "                    'test_avg_loss': val_avg_loss\n",
    "                   }, step)\n",
    "                if running_avg_reward > 0:\n",
    "#                         logger.info('epoch %d: %d, running_avg_reward = %f'\n",
    "#                                 % (epoch, step, running_avg_reward))\n",
    "                    writer.add_scalars('scalar_avg/Reward',  \n",
    "                       {'running_avg_reward': running_avg_reward\n",
    "                       }, step)\n",
    "                if running_avg_rl_loss != 0:\n",
    "#                         logger.info('epoch %d: %d, running_avg_rl_loss = %f'\n",
    "#                                 % (epoch, step, running_avg_rl_loss))\n",
    "                    writer.add_scalars('scalar_avg/RL_Loss',  \n",
    "                       {'running_avg_rl_loss': running_avg_rl_loss\n",
    "                       }, step)\n",
    "                loss_cost = time.time() - loss_st\n",
    "                if step % save_steps == 0: logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "                            % (epoch, step, loss_cost))\n",
    "        if step % save_steps == 0:\n",
    "            save_model(config, logger, model, optimizer, step, vocab, running_avg_loss, \\\n",
    "                       r_loss=0, title = loggerName)\n",
    "        if step%1000 == 0 and step > 0:\n",
    "            decode_st = time.time()\n",
    "            train_rouge_l_f = decode(writer, logger, step, config, model, batch, mode = 'train') # call batch by validate_loader\n",
    "            test_rouge_l_f = decode(writer, logger, step, config, model, validate_loader, mode = 'test') # call batch by validate_loader\n",
    "            decode_cost = time.time() - decode_st\n",
    "            if step%save_steps == 0: logger.info('epoch %d|step %d| decode cost = %f ms'% (epoch, step, decode_cost))\n",
    "\n",
    "            writer.add_scalars('scalar/Rouge-L',  \n",
    "               {'train_rouge_l_f': train_rouge_l_f,\n",
    "                'test_rouge_l_f': test_rouge_l_f\n",
    "               }, step)\n",
    "            if step%save_steps == 0:\n",
    "                logger.info('epoch %d: %d, train_rouge_l_f = %f, test_rouge_l_f = %f'\n",
    "                            % (epoch, step, train_rouge_l_f, test_rouge_l_f))\n",
    "#         break\n",
    "    logger.info('-------------------------------------------------------------')\n",
    "    train_avg_acc = avg_acc(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "    test_avg_acc = avg_acc(writer, logger, epoch, config, model, validate_loader, mode = 'test')                   \n",
    "    logger.info('epoch %d|step %d| train_avg_acc = %f, test_avg_acc = %f' % (epoch, step, train_avg_acc, test_avg_acc))\n",
    "    if running_avg_reward > 0:\n",
    "        logger.info('epoch %d|step %d| running_avg_reward = %f'% (epoch, step, running_avg_reward))\n",
    "    if running_avg_rl_loss != 0:\n",
    "        logger.info('epoch %d|step %d| running_avg_rl_loss = %f'% (epoch, step, running_avg_rl_loss))\n",
    "    logger.info('-------------------------------------------------------------')\n",
    "\n",
    "# except Excepation as e:\n",
    "#         print(e)\n",
    "# else:\n",
    "#     logger.info(u'------Training SUCCESS--------')  \n",
    "# finally:\n",
    "#     logger.info(u'------Training END--------')    \n",
    "#     train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "#     test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "#     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc))\n",
    "#     removeLogger(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_outFrame.head()\n",
    "# test_outFrame.head()\n",
    "\n",
    "# !ipython nbconvert --to script Pointer_Bert_Transformer.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
