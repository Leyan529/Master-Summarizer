{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0529 18:11:33.337913 139632626456384 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-05-29 18:11:34 - Pointer_generator_word2Vec_Intra_Atten - INFO: - logger已啟動\n",
      "I0529 18:11:34.233987 139632626456384 train_util.py:106] logger已啟動\n"
     ]
    }
   ],
   "source": [
    "from utils import config\n",
    "from utils.seq2seq import data\n",
    "\n",
    "from utils.seq2seq.batcher import *\n",
    "\n",
    "from utils.seq2seq.train_util import *\n",
    "from utils.seq2seq.rl_util import *\n",
    "from utils.seq2seq.initialize import loadCheckpoint, save_model\n",
    "from utils.seq2seq.write_result import *\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from translate.seq2seq_beam import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from utils.seq2seq.rl_util import *\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "eval_gpu = 0\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--key_attention', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_encoder', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--intra_decoder', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--copy', type=bool, default=True, help = 'True/False') # for transformer\n",
    "\n",
    "\n",
    "parser.add_argument('--model_type', type=str, default='seq2seq', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--train_rl', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_keys', \n",
    "                    help = 'POS_keys / DEP_keys / Noun_adj_keys / TextRank_keys')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--rand_unif_init_mag', type=float, default=0.02)\n",
    "parser.add_argument('--trunc_norm_init_std', type=float, default=0.001)\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument('--gound_truth_prob', type=float, default=0.5)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=500)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=20)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=5)\n",
    "parser.add_argument('--max_epochs', type=int, default=15)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=5)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "\n",
    "parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "parser.add_argument('--emb_dim', type=int, default=300)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default='', help='0002000')\n",
    "parser.add_argument('--word_emb_type', type=str, default='word2Vec', help='word2Vec/glove/FastText')\n",
    "parser.add_argument('--pre_train_emb', type=bool, default=True, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)\n",
    "\n",
    "eval_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-29 18:12:47 - Pointer_generator_word2Vec_Intra_Atten - INFO: - train : 504075, test : 56009\n",
      "I0529 18:12:47.250935 139632626456384 batcher.py:186] train : 504075, test : 56009\n",
      "2020-05-29 18:12:47 - Pointer_generator_word2Vec_Intra_Atten - INFO: - train batches : 15752, test batches : 1750\n",
      "I0529 18:12:47.730495 139632626456384 batcher.py:210] train batches : 15752, test batches : 1750\n"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "train_batches = len(iter(train_loader))\n",
    "test_batches = len(iter(validate_loader))\n",
    "save_steps = int(train_batches/250)*250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0529 18:12:48.332139 139632626456384 utils_any2vec.py:341] loading projection weights from ../Train-Data/Mix6_mainCat_best/Embedding/word2Vec/word2Vec.300d.txt\n",
      "I0529 18:12:59.723284 139632626456384 utils_any2vec.py:405] loaded (49676, 300) matrix from ../Train-Data/Mix6_mainCat_best/Embedding/word2Vec/word2Vec.300d.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/saved_models/Pointer_generator_word2Vec_Intra_Atten/0204750.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-29 18:13:02 - Pointer_generator_word2Vec_Intra_Atten - INFO: - Loaded model at model/saved_models/Pointer_generator_word2Vec_Intra_Atten/0204750.tar\n",
      "I0529 18:13:02.745857 139632626456384 initialize.py:212] Loaded model at model/saved_models/Pointer_generator_word2Vec_Intra_Atten/0204750.tar\n",
      "2020-05-29 18:13:02 - Pointer_generator_word2Vec_Intra_Atten - INFO: - Loaded model step = 204750, loss = 2.32, r_loss = 0.00 \n",
      "I0529 18:13:02.747702 139632626456384 initialize.py:213] Loaded model step = 204750, loss = 2.32, r_loss = 0.00 \n"
     ]
    }
   ],
   "source": [
    "from seq2seq import Model\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.distributed as dist\n",
    "\n",
    "from parallel import DataParallelModel, DataParallelCriterion\n",
    "# https://gist.github.com/thomwolf/7e2407fbd5945f07821adae3d9fd1312\n",
    "\n",
    "\n",
    "load_step = None\n",
    "model = Model(pre_train_emb=config.pre_train_emb, \n",
    "              word_emb_type = config.word_emb_type, \n",
    "              vocab = vocab)\n",
    "\n",
    "# model = model.cuda()\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=config.lr)   \n",
    "# optimizer = T.optim.Adagrad(model.parameters(),lr=config.lr, initial_accumulator_value=0.1)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(logger, load_model_path, model, optimizer)\n",
    "    # 若偵測到model切換成eval\n",
    "    eval_model = True\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(eval_gpu)\n",
    "    \n",
    "else:    \n",
    "    model.to('cuda:%s' % 0) #BCW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        With label smoothing,\n",
    "        KL-divergence between q_{smoothed ground truth prob.}(w)\n",
    "        and p_{prob. computed by model}(w) is minimized.\n",
    "        \"\"\"\n",
    "        def __init__(self, ignore_index):\n",
    "            super(NLLLoss, self).__init__()\n",
    "#             step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=PAD)\n",
    "            self.NLL = nn.NLLLoss(ignore_index=ignore_index, reduction='sum')\n",
    "\n",
    "        def forward(self, out, tar):  \n",
    "            # target dimension[0] / 2\n",
    "            # tar = target.contiguous().view(-1) \n",
    "            # out = output.contiguous().view(target.size(0),-1)\n",
    "\n",
    "            target = tar.contiguous().view(-1)\n",
    "            output = out[:tar.size(0)]\n",
    "            normalize = output.size(0) * output.size(1)\n",
    "            output = output.contiguous().view(target.size(0),-1)\n",
    "            loss = self.NLL(output, target) / normalize\n",
    "            \n",
    "            return loss\n",
    "\n",
    "if not eval_model:\n",
    "    criterion = NLLLoss(ignore_index=PAD)\n",
    "    parallel_model = DataParallelModel(model) # Encapsulate the model\n",
    "    parallel_loss = DataParallelCriterion(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sents(enc_out, inds, vocab, art_oovs):\n",
    "    decoded_strs = []\n",
    "    for i in range(len(enc_out)):\n",
    "        id_list = inds[i].tolist() # 取出每個sample sentence 的word id list\n",
    "        S = output2words(id_list, vocab, art_oovs[i]) #Generate sentence corresponding to sampled words\n",
    "        try:\n",
    "            end_idx = S.index(data.STOP_DECODING)\n",
    "            S = S[:end_idx]\n",
    "        except ValueError:\n",
    "            S = S\n",
    "        if len(S) < 2:          #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "            S = [\"xxx\"]\n",
    "        S = \" \".join(S)\n",
    "        decoded_strs.append(S)\n",
    "    return decoded_strs\n",
    "\n",
    "def merge_res(res):\n",
    "    ((inds1, log_probs1, enc_out1),(inds2, log_probs2, enc_out2)) = res\n",
    "    inds = T.cat([inds1, inds2], dim = 0).cpu()\n",
    "    log_probs = T.cat([log_probs1, log_probs2], dim = 0)\n",
    "    enc_out = T.cat([enc_out1, enc_out2], dim = 0).cpu()\n",
    "\n",
    "#     inds, log_probs, enc_out = res\n",
    "#     inds = inds.cpu()\n",
    "#     enc_out = enc_out.cpu()\n",
    "    return inds, log_probs, enc_out\n",
    "\n",
    "def train_one_rl(package, inputs):\n",
    "    config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e,                                 max_dec_len, dec_batch, target_batch = package\n",
    "    \n",
    "    rl_loss, batch_reward = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e,                                 max_dec_len, dec_batch, target_batch, train_rl = True, art_oovs = inputs.art_oovs, original_abstract = inputs.original_abstract, vocab = vocab)\n",
    "    rl_loss = nn.parallel.gather(rl_loss, 0).mean() \n",
    "    return rl_loss, batch_reward\n",
    "\n",
    "def train_one(package):\n",
    "    model.train()\n",
    "    config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch = package   \n",
    "\n",
    "    pred_probs = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch)\n",
    "    target = target_batch\n",
    "    loss = parallel_loss(config.mle_weight, pred_probs, target)\n",
    "    \n",
    "    return loss, pred_probs\n",
    "\n",
    "def write_res(inputs, batch_probs):\n",
    "    decoded_sents = []\n",
    "    for i, probs in enurmerate(batch_probs):\n",
    "        sents = []\n",
    "        for prob in probs:\n",
    "            _id = T.max(probs, dim=1)[1]\n",
    "            _id = _id.detach()\n",
    "            sents.append(_id)\n",
    "        decoded_sents.append(seq)\n",
    "            \n",
    "    output2words()        \n",
    "    article_sents = [article for article in inputs.original_article]\n",
    "    ref_sents = [ref for ref in inputs.original_abstract]\n",
    "#     decoded_sents = [summarize(article, words=30) for article in article_sents]\n",
    "#     decoded_sents = [sent if len(sent) > 5 else \"xxx xxx xxx xxx xxx\" for sent in decoded_sents]\n",
    "        \n",
    "#     article_sents, decoded_sents, keywords_list, \\\n",
    "#     ref_sents, long_seq_index = prepare_result(vocab, batch, pred_ids)\n",
    "\n",
    "#     rouge_1, rouge_2, rouge_l = write_rouge(writer, None, None, article_sents, decoded_sents, \\\n",
    "#                 keywords_list, ref_sents, long_seq_index, write = False)\n",
    "#     avg_rouge_l.append(rouge_l)\n",
    "#     acc_cost = time.time() - acc_st\n",
    "#     avg_acc_cost.append(acc_cost)\n",
    "    \n",
    "    return seq_sents\n",
    "\n",
    "def get_package(inputs):\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "        ct_e, enc_key_batch, enc_key_mask, enc_key_lens= \\\n",
    "            get_input_from_batch(inputs, config, batch_first = True)\n",
    "\n",
    "    dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(inputs, config, batch_first = True) # Get input and target batchs for training decoder            \n",
    "\n",
    "    max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0]    \n",
    "    # ----------------------------------------------------\n",
    "    package = (config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "                                max_dec_len, dec_batch, target_batch)\n",
    "    \n",
    "    inner_c = package[1] != max(package[4].tolist())[0]\n",
    "\n",
    "    return inner_c, package\n",
    "\n",
    "\n",
    "# for inputs in train_loader:  \n",
    "#     # MLE test\n",
    "#     # ----------------------------------------------------\n",
    "#     # pred_probs = parallel_model(config, max_enc_len, enc_batch, enc_key_batch, enc_lens, enc_padding_mask, enc_key_mask, extra_zeros, enc_batch_extend_vocab, ct_e, \\\n",
    "#     #                             max_dec_len, dec_batch, target_batch)\n",
    "#     # # pass\n",
    "#     # target = target_batch\n",
    "#     # loss = parallel_loss(config.mle_weight, pred_probs, target)\n",
    "#     loss = train_one(package)\n",
    "# #     loss.backward() # Backward pass \n",
    "# #     optimizer.step() # Optimizer step\n",
    "#     print('loss : ',loss)\n",
    "#     # pass\n",
    "#     # ----------------------------------------------------   \n",
    "#     if config.train_rl:\n",
    "#         rl_loss, batch_reward = train_one_rl(package)\n",
    "#         print('rl_loss : ',rl_loss, 'batch_reward : ',batch_reward)\n",
    "#     else:\n",
    "#         rl_loss = T.FloatTensor([0]).cuda()        \n",
    "    \n",
    "#     (config.mle_weight * loss + config.rl_weight * rl_loss).backward() # Backward pass   \n",
    "#     optimizer.step() # Optimizer step\n",
    "#     optimizer.zero_grad() # 清空过往梯度 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "#     batch = next(iter(validate_loader))\n",
    "    val_num = len(iter(validate_loader))\n",
    "    for idx, batch in enumerate(validate_loader):\n",
    "        inner_c, package = get_package(batch)\n",
    "        if inner_c: continue\n",
    "        loss, _ = train_one(package)\n",
    "#         loss = train_one(model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "#         if idx>= val_num/40: break\n",
    "#     model.train()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # del parallel_model, parallel_loss\n",
    "\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "# @torch.autograd.no_grad()\n",
    "# def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "#     # 動態取batch\n",
    "#     num = len(dataloader)\n",
    "#     avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "#     avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "#     avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "#     avg_meteor = []\n",
    "#     outFrame = None\n",
    "#     avg_time = 0\n",
    "        \n",
    "#     for idx, inputs in enumerate(dataloader):\n",
    "#         start = time.time() \n",
    "# #         'Encoder data'\n",
    "#         enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "#             ct_e, enc_key_batch, enc_key_mask, enc_key_lens = get_input_from_batch(inputs, config, batch_first = True)\n",
    "#         max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0] \n",
    "        \n",
    "#         if (max_enc_len != max(enc_lens.tolist())[0]): continue\n",
    "\n",
    "#         enc_batch = model.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "#         enc_key_batch = model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "#         enc_out, enc_hidden = model.encoder(enc_batch, enc_lens, max_enc_len)\n",
    "        \n",
    "# #         'Feed encoder data to predict'\n",
    "#         pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "#                                 enc_batch_extend_vocab, enc_key_batch, enc_key_mask, model, \n",
    "#                                 START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "#         article_sents, decoded_sents, keywords_list, ref_sents, long_seq_index = prepare_result(vocab, inputs, pred_ids)\n",
    "#         cost = (time.time() - start)\n",
    "#         avg_time += cost        \n",
    "\n",
    "        \n",
    "#         rouge_1, rouge_2, rouge_l,             Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        \n",
    "#         if idx %1000 ==0 and idx >0 : print(idx)\n",
    "#         if idx == 0: outFrame = batch_frame\n",
    "#         else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "#         # ----------------------------------------------------\n",
    "#         avg_rouge_1.extend(rouge_1)\n",
    "#         avg_rouge_2.extend(rouge_2)\n",
    "#         avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "#         # avg_self_bleu1.extend(self_Bleu_1)\n",
    "#         # avg_self_bleu2.extend(self_Bleu_2)\n",
    "#         # avg_self_bleu3.extend(self_Bleu_3)\n",
    "#         # avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "#         avg_bleu1.extend(Bleu_1)\n",
    "#         avg_bleu2.extend(Bleu_2)\n",
    "#         avg_bleu3.extend(Bleu_3)\n",
    "#         avg_bleu4.extend(Bleu_4)\n",
    "#         avg_meteor.extend(Meteor)\n",
    "#         # ----------------------------------------------------    \n",
    "#     avg_time = avg_time / (num * config.batch_size) \n",
    "    \n",
    "#     avg_rouge_l, outFrame = total_output(mode, writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l,         avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4,         avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "#     )\n",
    "    \n",
    "#     return avg_rouge_l, outFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del parallel_model, parallel_loss\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode(writer, dataloader, epoch):\n",
    "    # 動態取batch\n",
    "    num = len(dataloader)\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "    avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    avg_meteor = []\n",
    "    outFrame = None\n",
    "    avg_time = 0\n",
    "        \n",
    "    for idx, inputs in enumerate(dataloader):\n",
    "        start = time.time() \n",
    "#         'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, coverage, \\\n",
    "            ct_e, enc_key_batch, enc_key_mask, enc_key_lens = get_input_from_batch(inputs, config, batch_first = True)\n",
    "        max_enc_len = max(T.max(enc_lens,dim=0)).tolist()[0] \n",
    "        \n",
    "        if (max_enc_len != max(enc_lens.tolist())[0]): continue\n",
    "\n",
    "        enc_batch = parallel_model.module.embeds(enc_batch)  # Get embeddings for encoder input    \n",
    "        enc_key_batch = parallel_model.module.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "\n",
    "        enc_out, enc_hidden = parallel_model.module.encoder(enc_batch, enc_lens, max_enc_len)\n",
    "        \n",
    "#         'Feed encoder data to predict'\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, \n",
    "                                enc_batch_extend_vocab, enc_key_batch, enc_key_mask, parallel_model.module, \n",
    "                                START, END, UNKNOWN_TOKEN)\n",
    "\n",
    "        article_sents, decoded_sents, keywords_list, ref_sents, long_seq_index = prepare_result(vocab, inputs, pred_ids)\n",
    "        cost = (time.time() - start)\n",
    "        avg_time += cost        \n",
    "\n",
    "        \n",
    "        rouge_1, rouge_2, rouge_l,             Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        \n",
    "        if idx %1000 ==0 and idx >0 : print(idx); \n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "        # avg_self_bleu1.extend(self_Bleu_1)\n",
    "        # avg_self_bleu2.extend(self_Bleu_2)\n",
    "        # avg_self_bleu3.extend(self_Bleu_3)\n",
    "        # avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "        avg_bleu1.extend(Bleu_1)\n",
    "        avg_bleu2.extend(Bleu_2)\n",
    "        avg_bleu3.extend(Bleu_3)\n",
    "        avg_bleu4.extend(Bleu_4)\n",
    "        avg_meteor.extend(Meteor)\n",
    "        # ----------------------------------------------------    \n",
    "    avg_time = avg_time / (num * config.batch_size)    \n",
    "    \n",
    "    scalar_acc = {\n",
    "        'rouge_1':sum(avg_rouge_1) / len(avg_rouge_1),\n",
    "        'rouge_2':sum(avg_rouge_2) / len(avg_rouge_2),\n",
    "        'rouge_l':sum(avg_rouge_l) / len(avg_rouge_l),\n",
    "        \n",
    "        'bleu1':sum(avg_bleu1) / len(avg_bleu1),\n",
    "        'bleu2':sum(avg_bleu2) / len(avg_bleu2),\n",
    "        'bleu3':sum(avg_bleu3) / len(avg_bleu3),\n",
    "        'bleu4':sum(avg_bleu4) / len(avg_bleu4),\n",
    "        \n",
    "        'meteor':sum(avg_meteor) / len(avg_meteor)\n",
    "    }\n",
    "    \n",
    "    for scalar_name, accuracy in scalar_acc.items():\n",
    "        if 'rouge' in scalar_name:\n",
    "            writer.add_scalars('scalar/rouge',  \n",
    "               {scalar_name: accuracy,\n",
    "               }, epoch)\n",
    "        elif 'bleu' in scalar_name:\n",
    "            writer.add_scalars('scalar/bleu',  \n",
    "               {scalar_name: accuracy,\n",
    "               }, epoch)\n",
    "        else:\n",
    "            writer.add_scalars('scalar/meteor',  \n",
    "               {scalar_name: accuracy,\n",
    "               }, epoch)\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    total_output(epoch, 'test', writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l,                  avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "    )\n",
    "    # -----------------------------------------------------------\n",
    "    outFrame = outFrame.sort_values(by=['rouge_l'], ascending=False)\n",
    "    big_frame = outFrame.head()\n",
    "    small_frame = outFrame.tail()    \n",
    "    # -----------------------------------------------------------\n",
    "    i = 0\n",
    "    for view_item in big_frame.to_dict('records'):\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### rouge_l : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + str(view_item['rouge_l']), epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### decoded : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['decoded'], epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### reference : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['reference'], epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### keywords : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['keywords'], epoch)\n",
    "        writer.add_text('BigTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### article : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['article'], epoch)\n",
    "\n",
    "        i += 1\n",
    "    # -----------------------------------------------------------\n",
    "    i = 0\n",
    "    for view_item in small_frame.to_dict('records'):\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### rouge_l : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + str(view_item['rouge_l']), epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### decoded : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['decoded'], epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### reference : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['reference'], epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### keywords : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['keywords'], epoch)\n",
    "        writer.add_text('SmallTest/epoch_%s/##%s' % (epoch, i),\n",
    "                        \"### article : &nbsp;&nbsp;&nbsp;\\\n",
    "                        \" + view_item['article'], epoch)\n",
    "        i += 1\n",
    "    return outFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-29 18:14:16 - Pointer_generator_word2Vec_Intra_Atten - INFO: - train : 504075, test : 56009\n",
      "I0529 18:14:16.662764 139632626456384 batcher.py:186] train : 504075, test : 56009\n",
      "2020-05-29 18:14:17 - Pointer_generator_word2Vec_Intra_Atten - INFO: - train batches : 15752, test batches : 1750\n",
      "I0529 18:14:17.151173 139632626456384 batcher.py:210] train batches : 15752, test batches : 1750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-29 18:58:15 - Pointer_generator_word2Vec_Intra_Atten - INFO: - epoch 13: test_avg_acc = 0.362045\n",
      "I0529 18:58:15.503714 139632626456384 <ipython-input-8-66864a79e1e8>:139] epoch 13: test_avg_acc = 0.362045\n",
      "2020-05-29 18:58:15 - Pointer_generator_word2Vec_Intra_Atten - INFO: - logger已關閉\n",
      "I0529 18:58:15.505617 139632626456384 train_util.py:110] logger已關閉\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test \n",
      " ['Accuracy result:\\n', '##-- Rouge --##\\n', 'testing_avg_rouge_1: 0.39368598043577396 \\n', 'testing_avg_rouge_2: 0.2419271438487536 \\n', 'testing_avg_rouge_l: 0.36204499768860643 \\n', '##-- SELF-BLEU --##\\n', 'testing_avg_self_bleu1: 0.33206844083343207 \\n', 'testing_avg_self_bleu2: 0.21999261541470824 \\n', 'testing_avg_self_bleu3: 0.16542728293701464 \\n', 'testing_avg_self_bleu4: 0.13080982651246115 \\n', '##-- BLEU --##\\n', 'testing_avg_bleu1: 0.33206844083343207 \\n', 'testing_avg_bleu2: 0.2465073157615331 \\n', 'testing_avg_bleu3: 0.19985801329675718 \\n', 'testing_avg_bleu4: 0.16429857443351215 \\n', '##-- Meteor --##\\n', 'testing_avg_meteor: 0.32968444188539375 \\n', 'Num : 51712 Execute Time: 0.04466082089287894 \\n']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "loss_st, loss_cost = 0,0\n",
    "decode_st, decode_cost = 0,0\n",
    "last_save_step = 0\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "print_step = 250\n",
    "# save_steps = print_step\n",
    "if not eval_model:\n",
    "\n",
    "    write_train_para(writer, config)\n",
    "    logger.info('------Training START--------')\n",
    "    running_avg_loss, running_avg_rl_loss = 0, 0\n",
    "    sum_total_reward = 0\n",
    "    step = 0\n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(config, logger, vocab, loggerName, patience=3, verbose=True)\n",
    "    try:\n",
    "        for epoch in range(1, config.max_epochs+1):\n",
    "            for batch in train_loader:\n",
    "                step += 1; \n",
    "                loss_st = time.time()\n",
    "                inner_c, package = get_package(batch)\n",
    "                if inner_c: continue\n",
    "                parallel_model.module.train()\n",
    "                mle_loss, pred_probs = train_one(package)\n",
    "                if config.train_rl:\n",
    "                    rl_loss, batch_reward = train_one_rl(package, batch)             \n",
    "\n",
    "                    if step%print_step == 0 :\n",
    "                        writer.add_scalars('scalar/RL_Loss',  \n",
    "                           {'rl_loss': rl_loss\n",
    "                           }, step)\n",
    "                        writer.add_scalars('scalar/Reward',  \n",
    "                           {'batch_reward': batch_reward\n",
    "                           }, step)\n",
    "    #                     logger.info('epoch %d: %d, RL_Loss = %f, batch_reward = %f'\n",
    "    #                                     % (epoch, step, rl_loss, batch_reward))\n",
    "                    sum_total_reward += batch_reward\n",
    "                else:\n",
    "                    rl_loss = T.FloatTensor([0]).cuda()\n",
    "\n",
    "                (config.mle_weight * mle_loss + config.rl_weight * rl_loss).backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "                '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "                if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "        #             clip_grad_norm_(model.parameters(), 5.0)                      \n",
    "                    optimizer.step() # 根据累计的梯度更新网络参数\n",
    "                    optimizer.zero_grad() # 清空过往梯度 \n",
    "                if step%print_step == 0 :\n",
    "                    with T.autograd.no_grad():\n",
    "                        train_batch_loss = mle_loss.item()\n",
    "                        train_batch_rl_loss = rl_loss.item()\n",
    "#                         val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                        running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "                        running_avg_rl_loss = calc_running_avg_loss(train_batch_rl_loss, running_avg_rl_loss)\n",
    "                        running_avg_reward = sum_total_reward / step\n",
    "#                         if step % save_steps == 0:\n",
    "#                             logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "#                                         % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                        writer.add_scalars('scalar/Loss',  \n",
    "                           {'train_batch_loss': train_batch_loss\n",
    "                           }, step)\n",
    "                        writer.add_scalars('scalar_avg/loss',  \n",
    "                           {'train_avg_loss': running_avg_loss\n",
    "#                             'test_avg_loss': val_avg_loss\n",
    "                           }, step)\n",
    "                        if running_avg_reward > 0:\n",
    "    #                         logger.info('epoch %d: %d, running_avg_reward = %f'\n",
    "    #                                 % (epoch, step, running_avg_reward))\n",
    "                            writer.add_scalars('scalar_avg/Reward',  \n",
    "                               {'running_avg_reward': running_avg_reward\n",
    "                               }, step)\n",
    "                        if running_avg_rl_loss != 0:\n",
    "    #                         logger.info('epoch %d: %d, running_avg_rl_loss = %f'\n",
    "    #                                 % (epoch, step, running_avg_rl_loss))\n",
    "                            writer.add_scalars('scalar_avg/RL_Loss',  \n",
    "                               {'running_avg_rl_loss': running_avg_rl_loss\n",
    "                               }, step)\n",
    "                                                    \n",
    "                \n",
    "                if step % save_steps == 0:\n",
    "                    parallel_model.module.eval()\n",
    "                    logger.info('epoch : %s' % epoch)\n",
    "                    val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                    logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "                                        % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                    writer.add_scalars('scalar_avg/loss',  \n",
    "                           {'train_avg_loss': running_avg_loss,\n",
    "                            'test_avg_loss': val_avg_loss\n",
    "                           }, step)\n",
    "                    '''（讀取所儲存模型引數後，再進行並行化操作，否則無法利用之前的程式碼進行讀取）'''\n",
    "                    save_model(config, logger, parallel_model, optimizer, step, vocab, val_avg_loss, \\\n",
    "                               r_loss=0, title = loggerName)\n",
    "                    loss_cost = time.time() - loss_st\n",
    "                    logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "                                    % (epoch, step, loss_cost))\n",
    "                    writer.add_scalars('scalar_avg/epoch_loss',  \n",
    "                       {'train_avg_loss': running_avg_loss,\n",
    "                        'test_avg_loss': val_avg_loss\n",
    "                       }, epoch)\n",
    "                    last_save_step = step\n",
    "                    test_outFrame = decode(writer, validate_loader, epoch)                   \n",
    "\n",
    "            logger.info('-------------------------------------------------------------')\n",
    "\n",
    "            if running_avg_reward > 0:\n",
    "                logger.info('epoch %d|step %d| running_avg_reward = %f'% (epoch, step, running_avg_reward))\n",
    "            if running_avg_rl_loss != 0:\n",
    "                logger.info('epoch %d|step %d| running_avg_rl_loss = %f'% (epoch, step, running_avg_rl_loss))\n",
    "            logger.info('-------------------------------------------------------------')\n",
    "\n",
    "            early_stopping(parallel_model, optimizer, step, val_avg_loss) # update patience\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(\"Early stopping epoch %s\"%(epoch))\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        logger.info(u'------Training SUCCESS--------')  \n",
    "    finally:\n",
    "        logger.info(u'------Training END--------')   \n",
    "        logger.info(\"stopping epoch %s\"%(epoch))        \n",
    "        logger.info(\"last_save_step %s\"%(last_save_step))  \n",
    "        '''先將test_avg_acc調起來再decode train_'''\n",
    "    #     train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "#         test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, parallel_model.module, validate_loader, mode = 'test')\n",
    "    #     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc)) \n",
    "#         logger.info('epoch %d: test_avg_acc = %f' % (load_ep, test_avg_acc)) \n",
    "        removeLogger(logger)\n",
    "\n",
    "# else: # EVAL\n",
    "#     load_ep = float(config.load_ckpt) / float(save_steps)\n",
    "#     config.batch_size = 32\n",
    "#     train_loader, validate_loader, vocab = getDataLoader(logger, config)\n",
    "#     train_batches = len(iter(train_loader))\n",
    "#     test_batches = len(iter(validate_loader))\n",
    "# #     save_steps = int(train_batches/250)*250\n",
    "#     model.cuda(eval_gpu) \n",
    "#     model.eval()\n",
    "#     '''先將test_avg_acc調起來再decode train_'''\n",
    "# #     train_avg_acc, train_outFrame = decode_write_all(writer, logger, load_ep, config, model, train_loader, mode = 'train')\n",
    "#     test_avg_acc, test_outFrame = decode_write_all(writer, logger, load_ep, config, model, validate_loader, mode = 'test')\n",
    "# #     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (load_ep, train_avg_acc, test_avg_acc)) \n",
    "#     logger.info('epoch %d: test_avg_acc = %f' % (load_ep, test_avg_acc)) \n",
    "#     removeLogger(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article', 'keywords', 'reference', 'decoded', 'rouge_1', 'rouge_2',\n",
       "       'rouge_l', 'self_Bleu_1', 'self_Bleu_2', 'self_Bleu_3', 'self_Bleu_4',\n",
       "       'Bleu_1', 'Bleu_2', 'Bleu_3', 'Bleu_4', 'Meteor', 'article_lens',\n",
       "       'ref_lens', 'overlap', 'overlap_percent', 'gen_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge_1</th>\n",
       "      <th>article</th>\n",
       "      <th>reference</th>\n",
       "      <th>decoded</th>\n",
       "      <th>gen_type</th>\n",
       "      <th>overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51704</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>update may 2016 . after have this collar for y...</td>\n",
       "      <td>this collar is great tool and communication de...</td>\n",
       "      <td>great collar for the price great customer service</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51702</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>few year ago had the chance evaluate the leupo...</td>\n",
       "      <td>leupold rx tbr laser rangefinder the best</td>\n",
       "      <td>one of the best rangefinder rangefinder rangef...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51678</th>\n",
       "      <td>0.526316</td>\n",
       "      <td>this was the best investment make for our seni...</td>\n",
       "      <td>our senior puppy love it best purchase we make</td>\n",
       "      <td>this was the best investment make for our seni...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51674</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>first opinion that the olympus e520 absolutely...</td>\n",
       "      <td>excellent camera best value for the money</td>\n",
       "      <td>the best dslr camera on the market</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51672</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>okay here are initial impression about the pro...</td>\n",
       "      <td>superb tv for the price excellent picture but ...</td>\n",
       "      <td>great tv for the price but</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51670</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>simply amazing all have say about this samsung...</td>\n",
       "      <td>the best bang for your buck hdtv on the market...</td>\n",
       "      <td>best tv for the money on the market</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51666</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>purchase this about month ago office depot and...</td>\n",
       "      <td>memorex mlt tv the good bad and the ugly</td>\n",
       "      <td>good good tv for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51664</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>update happy say that have order case food sin...</td>\n",
       "      <td>super healthy dog food at great price</td>\n",
       "      <td>great food food for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51650</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>receive this printer january and get set succe...</td>\n",
       "      <td>good printer customer support has pro and con</td>\n",
       "      <td>great printer but poor customer support</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51618</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>little background currently have film camera f...</td>\n",
       "      <td>great camera combine with the easyshare printer</td>\n",
       "      <td>great little camera for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51605</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>old eye love lot light this unit kill outdoors...</td>\n",
       "      <td>my old eye love lot of light this unit kill it</td>\n",
       "      <td>my old eye love lot of light</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51601</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>aesthetic out beauty obviously subjective but ...</td>\n",
       "      <td>the best android wear watch but wait for price...</td>\n",
       "      <td>one of the best looking watch for the market</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51598</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>aesthetic out beauty obviously subjective but ...</td>\n",
       "      <td>the best android wear watch but wait for price...</td>\n",
       "      <td>one of the best looking watch for the market</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51597</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>for someone look for simple camera that can ea...</td>\n",
       "      <td>great camera if you want it for the right feature</td>\n",
       "      <td>great camera camera for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51571</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>have been fan logitech mouse for quite sometim...</td>\n",
       "      <td>great mouse for the hard core gamer</td>\n",
       "      <td>the best mouse for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51560</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>purchase the black tanto and the cold steel le...</td>\n",
       "      <td>compare favorably to the cold steel tanto</td>\n",
       "      <td>the cold steel steel steel steel knife</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51532</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>short answer this has become new photography b...</td>\n",
       "      <td>manfrotto befree carbon fiber tripod my new ph...</td>\n",
       "      <td>this has become my new photography best friend...</td>\n",
       "      <td>Abs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51531</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>had been look for battery case for iphone and ...</td>\n",
       "      <td>great iphone battery case for the price</td>\n",
       "      <td>great battery case for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51524</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>ok . will start off say for the price this goo...</td>\n",
       "      <td>good phone for price but has downfall</td>\n",
       "      <td>great beginner phone for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51516</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>just receive new canon 50d . have say that lov...</td>\n",
       "      <td>great camera for the price and feature</td>\n",
       "      <td>very good camera for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51499</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>overall good product and fairly easy setup als...</td>\n",
       "      <td>overall good product and fairly easy to set up</td>\n",
       "      <td>overall good product and fairly easy to setup ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51492</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>this little sharrk contender the portable powe...</td>\n",
       "      <td>sharrk portable power bank power pack mah</td>\n",
       "      <td>great portable portable power power pack power...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51489</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>spend lot time research wireless printer befor...</td>\n",
       "      <td>great printer but airprint may not work with y...</td>\n",
       "      <td>great printer work well with mac printer</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51432</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>take the wonderful canon s120 . that opening s...</td>\n",
       "      <td>worth the wait truly great pocket camera</td>\n",
       "      <td>great camera camera for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51427</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>well was stick between pay for the note and be...</td>\n",
       "      <td>consider the price this phone is awesome</td>\n",
       "      <td>the best phone for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51409</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>overall love the otis flexible memory cable cl...</td>\n",
       "      <td>review of the otis flexible memory cable clean...</td>\n",
       "      <td>love the otis flexible memory cable system</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51404</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>this review compare the irobot roomba 880 the ...</td>\n",
       "      <td>irobot roomba the neato</td>\n",
       "      <td>the roomba roomba roomba roomba roomba</td>\n",
       "      <td>Ext</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51397</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>overall will say that contempt with purchase ....</td>\n",
       "      <td>there are several good thing and several bad t...</td>\n",
       "      <td>there are several good thing about this produc...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51396</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>this first time buying selfie stick . not fan ...</td>\n",
       "      <td>first time selfie stick buyer impress so far</td>\n",
       "      <td>this first time buying selfie selfie stick</td>\n",
       "      <td>Abs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51372</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>own this for couple week now and while claim t...</td>\n",
       "      <td>best high definition tv for the price</td>\n",
       "      <td>great tv tv for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.842105</td>\n",
       "      <td>this great product . come with mfi certify cab...</td>\n",
       "      <td>this is great product it come with ft mfi certify</td>\n",
       "      <td>this is great product come come with mfi certi...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>get this watch for kid look great timing was a...</td>\n",
       "      <td>get this watch for my kid it look great timing...</td>\n",
       "      <td>get this watch for my kid it look great timing...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>surprisingly this local dim tv . advertise ein...</td>\n",
       "      <td>surprisingly this tv is full array local dim tv</td>\n",
       "      <td>surprisingly this local dim tv tv advertise</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>these are great elbow pad that stay place but ...</td>\n",
       "      <td>great elbow pad but not for large arm</td>\n",
       "      <td>these are great elbow pad that stay stay place...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>nice hat make vietnam and with non functional ...</td>\n",
       "      <td>nice hat make in vietnam and with non functional</td>\n",
       "      <td>nice hat make vietnam vietnam and with non fun...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>dog treat arrive schedule and the dog enjoy th...</td>\n",
       "      <td>was satisfied with the ordering process and wi...</td>\n",
       "      <td>dog was satisfied with the ordering and with t...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>wow amazing product although its not brand the...</td>\n",
       "      <td>wow amazing product although its not brand the...</td>\n",
       "      <td>wow amazing product although its its not the c...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.842105</td>\n",
       "      <td>good bag but close with the sun shade attach ....</td>\n",
       "      <td>good bag but will not close with the sun shade...</td>\n",
       "      <td>good bag but close close with the sun shade</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.814815</td>\n",
       "      <td>okay thought would more sturdy . but for cocka...</td>\n",
       "      <td>it is okay think it would be more sturdy but f...</td>\n",
       "      <td>okay thought would be more sturdy but for cock...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>get puncture with minute ride . tiny shard gla...</td>\n",
       "      <td>get puncture right away with in minute of star...</td>\n",
       "      <td>get puncture with with minute ride tiny</td>\n",
       "      <td>Abs</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>definitely recommend you like neat little addi...</td>\n",
       "      <td>great addition to your gun tool kit</td>\n",
       "      <td>definitely recommend if you like neat little a...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>the gold piece around the phone case break ver...</td>\n",
       "      <td>the gold piece around the phone case break in</td>\n",
       "      <td>the gold piece of around the phone case break</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>total body workout you also learn how pace you...</td>\n",
       "      <td>total body workout you also learn how to pace</td>\n",
       "      <td>total total body workout workout you also also...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>all our camping friend have buy when our old o...</td>\n",
       "      <td>so we buy it when our old one die and it is gr...</td>\n",
       "      <td>we buy when when our old one die and great eas...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>use with the standard cover and although you c...</td>\n",
       "      <td>use with the standard cover and although you c...</td>\n",
       "      <td>use with the standard cover and you can feel w...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>very bad product this the second one buy first...</td>\n",
       "      <td>very bad product this is the second one buy first</td>\n",
       "      <td>very bad product this is the second one one buy</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>buy 200 dollar phone and come with power cord ...</td>\n",
       "      <td>buy dollar phone and it does not come</td>\n",
       "      <td>buy dollar phone and come with power cord</td>\n",
       "      <td>Ext</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>decent bag but rip pretty easily canyoneere tr...</td>\n",
       "      <td>it is decent bag but it rip pretty easily on c...</td>\n",
       "      <td>decent bag but it rip pretty pretty easily can...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>will last long time . cat like it . like how s...</td>\n",
       "      <td>my cat like it did not like how strong the smell</td>\n",
       "      <td>cat like it like how strong the smell was</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>thank you yes did receive mini camera and the ...</td>\n",
       "      <td>did receive my mini camera and the timing was ...</td>\n",
       "      <td>yes the mini camera and the timing was good an...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>solid beginner lens for anyone who want get ou...</td>\n",
       "      <td>solid beginner lens for anyone who want to get...</td>\n",
       "      <td>solid beginner lens for anyone who want to get...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>conure love these and not big fan pellet . all...</td>\n",
       "      <td>my conure love these and she is not big fan</td>\n",
       "      <td>my conure love these and not big fan</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>very nice and great quality for the price . st...</td>\n",
       "      <td>very nice and great quality for the price</td>\n",
       "      <td>very nice and great quality for the price</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>dog love and love all the natural ingredient ....</td>\n",
       "      <td>my dog love it and love all the natural ingred...</td>\n",
       "      <td>my dog love it and love all the natural ingred...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.782609</td>\n",
       "      <td>star the fisheye lens the best not like other ...</td>\n",
       "      <td>the fisheye lens is the best it is not like ot...</td>\n",
       "      <td>star is the best not like like other with huge...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.758621</td>\n",
       "      <td>buy this for husband galaxy and work great . l...</td>\n",
       "      <td>for my husband galaxy and it is work great he ...</td>\n",
       "      <td>buy this for my husband and it work great love...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>very solid . more like solid chrome than cheap...</td>\n",
       "      <td>more like solid chrome than cheap pot metal</td>\n",
       "      <td>very solid chrome chrome cheap very nice finish</td>\n",
       "      <td>Ext</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>this excellent case love the kickstand and its...</td>\n",
       "      <td>this is an excellent case love the kickstand a...</td>\n",
       "      <td>this is excellent excellent case love the kick...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>yes dog love and has richness however look the...</td>\n",
       "      <td>yes my dog love and it has richness however</td>\n",
       "      <td>my dog love it and has richness</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>this great quality especially consider the pri...</td>\n",
       "      <td>this is great quality especially consider the ...</td>\n",
       "      <td>this is great quality especially consider the ...</td>\n",
       "      <td>Ext</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20185 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rouge_1                                            article  \\\n",
       "51704  0.421053  update may 2016 . after have this collar for y...   \n",
       "51702  0.500000  few year ago had the chance evaluate the leupo...   \n",
       "51678  0.526316  this was the best investment make for our seni...   \n",
       "51674  0.461538  first opinion that the olympus e520 absolutely...   \n",
       "51672  0.555556  okay here are initial impression about the pro...   \n",
       "51670  0.588235  simply amazing all have say about this samsung...   \n",
       "51666  0.461538  purchase this about month ago office depot and...   \n",
       "51664  0.500000  update happy say that have order case food sin...   \n",
       "51650  0.428571  receive this printer january and get set succe...   \n",
       "51618  0.461538  little background currently have film camera f...   \n",
       "51605  0.777778  old eye love lot light this unit kill outdoors...   \n",
       "51601  0.444444  aesthetic out beauty obviously subjective but ...   \n",
       "51598  0.444444  aesthetic out beauty obviously subjective but ...   \n",
       "51597  0.533333  for someone look for simple camera that can ea...   \n",
       "51571  0.500000  have been fan logitech mouse for quite sometim...   \n",
       "51560  0.545455  purchase the black tanto and the cold steel le...   \n",
       "51532  0.555556  short answer this has become new photography b...   \n",
       "51531  0.923077  had been look for battery case for iphone and ...   \n",
       "51524  0.461538  ok . will start off say for the price this goo...   \n",
       "51516  0.615385  just receive new canon 50d . have say that lov...   \n",
       "51499  0.700000  overall good product and fairly easy setup als...   \n",
       "51492  0.600000  this little sharrk contender the portable powe...   \n",
       "51489  0.500000  spend lot time research wireless printer befor...   \n",
       "51432  0.500000  take the wonderful canon s120 . that opening s...   \n",
       "51427  0.500000  well was stick between pay for the note and be...   \n",
       "51409  0.750000  overall love the otis flexible memory cable cl...   \n",
       "51404  0.666667  this review compare the irobot roomba 880 the ...   \n",
       "51397  0.588235  overall will say that contempt with purchase ....   \n",
       "51396  0.571429  this first time buying selfie stick . not fan ...   \n",
       "51372  0.666667  own this for couple week now and while claim t...   \n",
       "...         ...                                                ...   \n",
       "52     0.842105  this great product . come with mfi certify cab...   \n",
       "8      0.909091  get this watch for kid look great timing was a...   \n",
       "14     0.714286  surprisingly this local dim tv . advertise ein...   \n",
       "13     0.421053  these are great elbow pad that stay place but ...   \n",
       "12     0.941176  nice hat make vietnam and with non functional ...   \n",
       "11     0.875000  dog treat arrive schedule and the dog enjoy th...   \n",
       "10     0.947368  wow amazing product although its not brand the...   \n",
       "9      0.842105  good bag but close with the sun shade attach ....   \n",
       "4      0.814815  okay thought would more sturdy . but for cocka...   \n",
       "6      0.588235  get puncture with minute ride . tiny shard gla...   \n",
       "5      0.444444  definitely recommend you like neat little addi...   \n",
       "3      0.875000  the gold piece around the phone case break ver...   \n",
       "2      0.800000  total body workout you also learn how pace you...   \n",
       "16     0.914286  all our camping friend have buy when our old o...   \n",
       "15     0.857143  use with the standard cover and although you c...   \n",
       "17     0.947368  very bad product this the second one buy first...   \n",
       "18     0.625000  buy 200 dollar phone and come with power cord ...   \n",
       "19     0.818182  decent bag but rip pretty easily canyoneere tr...   \n",
       "20     0.777778  will last long time . cat like it . like how s...   \n",
       "0      0.720000  thank you yes did receive mini camera and the ...   \n",
       "21     1.000000  solid beginner lens for anyone who want get ou...   \n",
       "22     0.888889  conure love these and not big fan pellet . all...   \n",
       "23     1.000000  very nice and great quality for the price . st...   \n",
       "24     1.000000  dog love and love all the natural ingredient ....   \n",
       "25     0.782609  star the fisheye lens the best not like other ...   \n",
       "26     0.758621  buy this for husband galaxy and work great . l...   \n",
       "27     0.428571  very solid . more like solid chrome than cheap...   \n",
       "28     0.952381  this excellent case love the kickstand and its...   \n",
       "29     0.875000  yes dog love and has richness however look the...   \n",
       "31     1.000000  this great quality especially consider the pri...   \n",
       "\n",
       "                                               reference  \\\n",
       "51704  this collar is great tool and communication de...   \n",
       "51702          leupold rx tbr laser rangefinder the best   \n",
       "51678     our senior puppy love it best purchase we make   \n",
       "51674          excellent camera best value for the money   \n",
       "51672  superb tv for the price excellent picture but ...   \n",
       "51670  the best bang for your buck hdtv on the market...   \n",
       "51666           memorex mlt tv the good bad and the ugly   \n",
       "51664              super healthy dog food at great price   \n",
       "51650      good printer customer support has pro and con   \n",
       "51618    great camera combine with the easyshare printer   \n",
       "51605     my old eye love lot of light this unit kill it   \n",
       "51601  the best android wear watch but wait for price...   \n",
       "51598  the best android wear watch but wait for price...   \n",
       "51597  great camera if you want it for the right feature   \n",
       "51571                great mouse for the hard core gamer   \n",
       "51560          compare favorably to the cold steel tanto   \n",
       "51532  manfrotto befree carbon fiber tripod my new ph...   \n",
       "51531            great iphone battery case for the price   \n",
       "51524              good phone for price but has downfall   \n",
       "51516             great camera for the price and feature   \n",
       "51499     overall good product and fairly easy to set up   \n",
       "51492          sharrk portable power bank power pack mah   \n",
       "51489  great printer but airprint may not work with y...   \n",
       "51432           worth the wait truly great pocket camera   \n",
       "51427           consider the price this phone is awesome   \n",
       "51409  review of the otis flexible memory cable clean...   \n",
       "51404                            irobot roomba the neato   \n",
       "51397  there are several good thing and several bad t...   \n",
       "51396       first time selfie stick buyer impress so far   \n",
       "51372              best high definition tv for the price   \n",
       "...                                                  ...   \n",
       "52     this is great product it come with ft mfi certify   \n",
       "8      get this watch for my kid it look great timing...   \n",
       "14       surprisingly this tv is full array local dim tv   \n",
       "13                 great elbow pad but not for large arm   \n",
       "12      nice hat make in vietnam and with non functional   \n",
       "11     was satisfied with the ordering process and wi...   \n",
       "10     wow amazing product although its not brand the...   \n",
       "9      good bag but will not close with the sun shade...   \n",
       "4      it is okay think it would be more sturdy but f...   \n",
       "6      get puncture right away with in minute of star...   \n",
       "5                    great addition to your gun tool kit   \n",
       "3          the gold piece around the phone case break in   \n",
       "2          total body workout you also learn how to pace   \n",
       "16     so we buy it when our old one die and it is gr...   \n",
       "15     use with the standard cover and although you c...   \n",
       "17     very bad product this is the second one buy first   \n",
       "18                 buy dollar phone and it does not come   \n",
       "19     it is decent bag but it rip pretty easily on c...   \n",
       "20      my cat like it did not like how strong the smell   \n",
       "0      did receive my mini camera and the timing was ...   \n",
       "21     solid beginner lens for anyone who want to get...   \n",
       "22           my conure love these and she is not big fan   \n",
       "23             very nice and great quality for the price   \n",
       "24     my dog love it and love all the natural ingred...   \n",
       "25     the fisheye lens is the best it is not like ot...   \n",
       "26     for my husband galaxy and it is work great he ...   \n",
       "27           more like solid chrome than cheap pot metal   \n",
       "28     this is an excellent case love the kickstand a...   \n",
       "29           yes my dog love and it has richness however   \n",
       "31     this is great quality especially consider the ...   \n",
       "\n",
       "                                                 decoded gen_type  overlap  \n",
       "51704  great collar for the price great customer service      Ext        8  \n",
       "51702  one of the best rangefinder rangefinder rangef...      Ext        6  \n",
       "51678  this was the best investment make for our seni...      Ext        5  \n",
       "51674                 the best dslr camera on the market      Ext        5  \n",
       "51672                         great tv for the price but      Ext        7  \n",
       "51670                best tv for the money on the market      Ext        8  \n",
       "51666                         good good tv for the price      Ext        7  \n",
       "51664                      great food food for the price      Ext        4  \n",
       "51650            great printer but poor customer support      Ext        7  \n",
       "51618                  great little camera for the price      Ext        6  \n",
       "51605                       my old eye love lot of light      Ext        9  \n",
       "51601       one of the best looking watch for the market      Ext        9  \n",
       "51598       one of the best looking watch for the market      Ext        9  \n",
       "51597                  great camera camera for the price      Ext        8  \n",
       "51571                       the best mouse for the price      Ext        7  \n",
       "51560             the cold steel steel steel steel knife      Ext        5  \n",
       "51532  this has become my new photography best friend...      Abs        4  \n",
       "51531                   great battery case for the price      Ext        6  \n",
       "51524                 great beginner phone for the price      Ext        7  \n",
       "51516                     very good camera for the price      Ext        6  \n",
       "51499  overall good product and fairly easy to setup ...      Ext        7  \n",
       "51492  great portable portable power power pack power...      Ext        5  \n",
       "51489           great printer work well with mac printer      Ext        8  \n",
       "51432                  great camera camera for the price      Ext        5  \n",
       "51427                       the best phone for the price      Ext        4  \n",
       "51409         love the otis flexible memory cable system      Ext        8  \n",
       "51404             the roomba roomba roomba roomba roomba      Ext        4  \n",
       "51397  there are several good thing about this produc...      Ext        8  \n",
       "51396         this first time buying selfie selfie stick      Abs        4  \n",
       "51372                          great tv tv for the price      Ext        5  \n",
       "...                                                  ...      ...      ...  \n",
       "52     this is great product come come with mfi certi...      Ext        7  \n",
       "8      get this watch for my kid it look great timing...      Ext       16  \n",
       "14           surprisingly this local dim tv tv advertise      Ext        5  \n",
       "13     these are great elbow pad that stay stay place...      Ext        5  \n",
       "12     nice hat make vietnam vietnam and with non fun...      Ext        8  \n",
       "11     dog was satisfied with the ordering and with t...      Ext       10  \n",
       "10     wow amazing product although its its not the c...      Ext        9  \n",
       "9            good bag but close close with the sun shade      Ext        9  \n",
       "4      okay thought would be more sturdy but for cock...      Ext       10  \n",
       "6                get puncture with with minute ride tiny      Abs        5  \n",
       "5      definitely recommend if you like neat little a...      Ext        4  \n",
       "3          the gold piece of around the phone case break      Ext        8  \n",
       "2      total total body workout workout you also also...      Ext        8  \n",
       "16     we buy when when our old one die and great eas...      Ext       15  \n",
       "15     use with the standard cover and you can feel w...      Ext       19  \n",
       "17       very bad product this is the second one one buy      Ext        9  \n",
       "18             buy dollar phone and come with power cord      Ext        5  \n",
       "19     decent bag but it rip pretty pretty easily can...      Ext       10  \n",
       "20             cat like it like how strong the smell was      Ext        8  \n",
       "0      yes the mini camera and the timing was good an...      Ext       16  \n",
       "21     solid beginner lens for anyone who want to get...      Ext        9  \n",
       "22                  my conure love these and not big fan      Ext        7  \n",
       "23             very nice and great quality for the price      Ext        8  \n",
       "24     my dog love it and love all the natural ingred...      Ext        8  \n",
       "25     star is the best not like like other with huge...      Ext       11  \n",
       "26     buy this for my husband and it work great love...      Ext       11  \n",
       "27       very solid chrome chrome cheap very nice finish      Ext        6  \n",
       "28     this is excellent excellent case love the kick...      Ext        9  \n",
       "29                       my dog love it and has richness      Ext        7  \n",
       "31     this is great quality especially consider the ...      Ext        7  \n",
       "\n",
       "[20185 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outFrame[test_outFrame[\"rouge_1\"]>=0.4][['rouge_1','article', 'reference', 'decoded', 'gen_type','overlap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_16 epoch_6\n",
    "# testing_avg_rouge_1: 0.3873426628114616 \\n', \n",
    "# 'testing_avg_rouge_2: 0.25943944916828854 \\n', \n",
    "# 'testing_avg_rouge_l: 0.3614074094052472 \\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('Leyan': conda)",
   "language": "python",
   "name": "python36764bitleyancondaa378f3cedbcc4b3f906a2276b3eef765"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
