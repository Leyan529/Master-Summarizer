{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0519 16:08:43.188616 140527635740480 file_utils.py:35] PyTorch version 1.4.0 available.\n",
      "2020-05-19 16:08:44 - Pointer_Sep_BertEnc_Transformer_BertEmb - INFO: - logger已啟動\n",
      "I0519 16:08:44.422532 140527635740480 train_util.py:119] logger已啟動\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loggerName Pointer_Sep_BertEnc_Transformer_BertEmb\n",
      "writerPath runs/Mix6_mainCat_20/Pointer_Sep_BertEnc_Transformer/BertEmb/exp\n"
     ]
    }
   ],
   "source": [
    "from utils import config\n",
    "from utils.bert import data\n",
    "\n",
    "from utils.bert.batcher import *\n",
    "from utils.bert.train_util import *\n",
    "from utils.bert.initialize import loadCheckpoint, save_model\n",
    "from utils.bert.write_result import *\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_type', type=str, default='transformer', choices=['seq2seq', 'transformer'])\n",
    "parser.add_argument('--copy', type=bool, default=True, choices=[True, False])\n",
    "parser.add_argument(\"-encoder\", default='bert', type=str, choices=['bert', 'Transformer'])\n",
    "parser.add_argument(\"-max_pos\", default=800, type=int)\n",
    "parser.add_argument(\"-use_bert_emb\", type=str2bool, nargs='?',const=True,default=True, choices=[False, True])\n",
    "\n",
    "parser.add_argument(\"-lr_bert\", default=2e-2, type=float, help='2e-3')\n",
    "parser.add_argument(\"-lr_dec\", default=2e-2, type=float, help='2e-3')\n",
    "parser.add_argument(\"-share_emb\", type=str2bool, nargs='?', const=True, default=False)\n",
    "parser.add_argument(\"-finetune_bert\", type=bool, default=True)\n",
    "    \n",
    "'''\n",
    "原 Bert Base paper核心參數\n",
    "dropout = 0.1\n",
    "num_layers = 12\n",
    "num_heads = 8\n",
    "emb_dim(d_model) : 768\n",
    "ff_embed_dim = 4*emb_dim = 3072\n",
    "\n",
    "bert_config = BertConfig(self.encoder.model.config.vocab_size, hidden_size=768,\n",
    "                                     num_hidden_layers=12, num_attention_heads=8,\n",
    "                                     intermediate_size= 3072,\n",
    "                                     hidden_dropout_prob=0.1,\n",
    "                                     attention_probs_dropout_prob=0.1)\n",
    "'''\n",
    "parser.add_argument(\"-enc_dropout\", default=0.1, type=float)\n",
    "parser.add_argument(\"-enc_layers\", default=8, type=int)\n",
    "parser.add_argument(\"-enc_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-enc_heads\", default=4, type=int)\n",
    "parser.add_argument(\"-enc_ff_size\", default=3072, type=int)\n",
    "\n",
    "parser.add_argument(\"-dec_dropout\", default=0.1, type=float)\n",
    "parser.add_argument(\"-dec_layers\", default=8, type=int)\n",
    "parser.add_argument(\"-dec_hidden_size\", default=768, type=int)\n",
    "parser.add_argument(\"-dec_heads\", default=4, type=int)\n",
    "parser.add_argument(\"-dec_ff_size\", default=2048, type=int)\n",
    "parser.add_argument(\"-sep_optim\", type=str2bool, nargs='?',const=True,default=True, choices=[False, True])\n",
    "\n",
    "parser.add_argument(\"-param_init\", default=0, type=float)\n",
    "parser.add_argument(\"-param_init_glorot\", type=str2bool, nargs='?',const=True,default=True)\n",
    "parser.add_argument(\"-optim\", default='adam', type=str)\n",
    "parser.add_argument(\"-lr\", default=1, type=float)\n",
    "parser.add_argument(\"-beta1\", default= 0.9, type=float)\n",
    "parser.add_argument(\"-beta2\", default=0.999, type=float)\n",
    "parser.add_argument(\"-warmup_steps\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_bert\", default=8000, type=int)\n",
    "parser.add_argument(\"-warmup_steps_dec\", default=8000, type=int)\n",
    "parser.add_argument(\"-max_grad_norm\", default=0, type=float)\n",
    "\n",
    "parser.add_argument(\"-block_trigram\", type=str2bool, nargs='?', const=True, default=True)\n",
    "\n",
    "\n",
    "parser.add_argument('--train_rl', type=bool, default=False, help = 'True/False')\n",
    "parser.add_argument('--keywords', type=str, default='POS_keys', \n",
    "                    help = 'POS_keys / DEP_keys / Noun_adj_keys / TextRank_keys')\n",
    "\n",
    "parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "parser.add_argument(\"-label_smoothing\", default=0.0, type=float)\n",
    "parser.add_argument(\"-generator_shard_size\", default=32, type=int)\n",
    "parser.add_argument(\"-alpha\",  default=0.6, type=float)\n",
    "\n",
    "parser.add_argument('--max_enc_steps', type=int, default=800)\n",
    "parser.add_argument('--max_dec_steps', type=int, default=30)\n",
    "parser.add_argument('--min_dec_steps', type=int, default=10)\n",
    "parser.add_argument('--max_epochs', type=int, default=15)\n",
    "parser.add_argument('--vocab_size', type=int, default=50000)\n",
    "parser.add_argument('--beam_size', type=int, default=16)\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "\n",
    "# parser.add_argument('--hidden_dim', type=int, default=512)\n",
    "# parser.add_argument('--emb_dim', type=int, default=512)\n",
    "parser.add_argument('--gradient_accum', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--load_ckpt', type=str, default='0000010', help='0000010')\n",
    "# parser.add_argument('--word_emb_type', type=str, default='glove', help='word2Vec/glove/FastText')\n",
    "# parser.add_argument('--pre_train_emb', type=bool, default=False, help = 'True/False') # 若pre_train_emb為false, 則emb type為NoPretrain\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "config = re_config(opt)\n",
    "\n",
    "loggerName, writerPath = getName(config)    \n",
    "logger = getLogger(loggerName)\n",
    "writer = SummaryWriter(writerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0519 16:08:45.467981 140527635740480 tokenization.py:157] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "{'BOS': 1, 'EOS': 2, 'PAD': 0, 'EOQ': 3, 'SEP': 102, 'CLS': 101, 'UNK': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0519 16:09:05.190528 140527635740480 ultratb.py:149] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "I0519 16:09:05.195516 140527635740480 ultratb.py:1111] \n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-1b9bb1c260d5>\", line 1, in <module>\n",
      "    train_loader, validate_loader, vocab, symbols = getDataLoader(logger, config)\n",
      "  File \"/home/eagleuser/Users/leyan/Summarize_parallel/utils/bert/batcher.py\", line 208, in getDataLoader\n",
      "    total_df = pd.read_excel(config.xls_path)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/pandas/util/_decorators.py\", line 178, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/pandas/util/_decorators.py\", line 178, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/pandas/io/excel.py\", line 307, in read_excel\n",
      "    io = ExcelFile(io, engine=engine)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/pandas/io/excel.py\", line 394, in __init__\n",
      "    self.book = xlrd.open_workbook(self._io)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/xlrd/__init__.py\", line 138, in open_workbook\n",
      "    ragged_rows=ragged_rows,\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/xlrd/xlsx.py\", line 832, in open_workbook_2007_xml\n",
      "    x12sst.process_stream(zflo, 'SST')\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/xlrd/xlsx.py\", line 438, in process_stream_iterparse\n",
      "    for event, elem in ET.iterparse(stream):\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/xml/etree/ElementTree.py\", line 1223, in iterator\n",
      "    data = source.read(16 * 1024)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/zipfile.py\", line 872, in read\n",
      "    data = self._read1(n)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/zipfile.py\", line 940, in _read1\n",
      "    data += self._read2(n - len(data))\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/zipfile.py\", line 972, in _read2\n",
      "    data = self._fileobj.read(n)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/zipfile.py\", line 729, in read\n",
      "    data = self._file.read(n)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/inspect.py\", line 1488, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/inspect.py\", line 1446, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/posixpath.py\", line 388, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/posixpath.py\", line 422, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/eagleuser/.conda/envs/Leyan/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "train_loader, validate_loader, vocab, symbols = getDataLoader(logger, config)\n",
    "tokenizer = vocab.tokenizer\n",
    "train_batches = len(iter(train_loader))\n",
    "test_batches = len(iter(validate_loader))\n",
    "save_steps = int(train_batches/1000)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.transformer.loss import *\n",
    "from utils.transformer.optimizers import Optimizer\n",
    "from transformer import *\n",
    "from utils.transformer.predictor import build_predictor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from parallel import DataParallelModel, DataParallelCriterion\n",
    "# https://gist.github.com/thomwolf/7e2407fbd5945f07821adae3d9fd1312\n",
    "\n",
    "model = AbsSummarizer(config)\n",
    "\n",
    "load_model_path = config.save_model_path + '/%s/%s.tar' % (loggerName, config.load_ckpt)\n",
    "if os.path.exists(load_model_path):\n",
    "    model, optimizer, load_step = loadCheckpoint(config, logger, load_model_path, model)\n",
    "else:    \n",
    "    if (config.sep_optim):\n",
    "        optim_bert = Optimizer(\n",
    "            config.optim, config.lr_bert, config.max_grad_norm,\n",
    "            beta1=config.beta1, beta2=config.beta2,\n",
    "            decay_method='noam',\n",
    "            warmup_steps=config.warmup_steps_bert)\n",
    "\n",
    "        optim_dec = Optimizer(\n",
    "            config.optim, config.lr_dec, config.max_grad_norm,\n",
    "            beta1=config.beta1, beta2=config.beta2,\n",
    "            decay_method='noam',\n",
    "            warmup_steps=config.warmup_steps_dec)\n",
    "        \n",
    "        params = [(n, p) for n, p in list(model.named_parameters()) if n.startswith('encoder.model')]\n",
    "        optim_bert.set_parameters(params)\n",
    "\n",
    "        params = [(n, p) for n, p in list(model.named_parameters()) if not n.startswith('encoder.model')]\n",
    "        optim_dec.set_parameters(params)\n",
    "\n",
    "        optimizer = [optim_bert, optim_dec]\n",
    "    else:\n",
    "        optimizer = Optimizer(\n",
    "            config.optim, config.lr, config.max_grad_norm,\n",
    "            beta1=config.beta1, beta2=config.beta2,\n",
    "            decay_method='noam',\n",
    "            warmup_steps=config.warmup_steps)\n",
    "        optimizer.set_parameters(list(model.named_parameters()))\n",
    "        optimizer = [optimizer]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "setattr(config, 'device_ids', [0])\n",
    "\n",
    "model = get_cuda(model)\n",
    "# net = nn.DataParallel(model, device_ids=config.device_ids)\n",
    "# model = nn.DataParallel(model).cuda()\n",
    "model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model = DataParallelModel(model) # Encapsulate the model\n",
    "\n",
    "criterion = choose_criterion(config, model.vocab_size)\n",
    "parallel_loss = DataParallelCriterion(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss, num_correct, target = compute_loss(None, criterion, pred, dec_batch[:,1:], num_tokens, tokenizer)\n",
    "# # --------------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_res(res):\n",
    "    ((pred1, attn1),(pred2, attn2)) = res\n",
    "    merge_pred = torch.cat([pred1, pred2], dim = 0).cpu()\n",
    "    attn = torch.cat([attn1, attn2], dim = 0).cpu()\n",
    "    return (pred1, pred2), attn, merge_pred\n",
    "\n",
    "def compute_loss(preds, target, merge_pred, num_tokens, tokenizer):\n",
    "    gtruth = target   \n",
    "    loss = parallel_loss(config.mle_weight , preds, gtruth) \n",
    "    num_correct = compute_correct(merge_pred, target, num_tokens, tokenizer)  \n",
    "    return loss, num_correct, target\n",
    "\n",
    "def get_package(inputs):    \n",
    "    # ----------------------------------------------------\n",
    "    normalization = 0\n",
    "    'Encoder data'\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, _, _, _, _, _, enc_seg, enc_cls, enc_cls_mask = get_input_from_batch(inputs, config, batch_first = True)\n",
    "    # ----------------------------------------------------\n",
    "    'Decoder data'\n",
    "    dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = get_output_from_batch(inputs, config, batch_first = True) # Get input and target \n",
    "    num_tokens = dec_batch[:, 1:].ne(0).sum()\n",
    "    normalization += num_tokens.item() \n",
    "    return (enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, enc_seg, enc_cls, enc_cls_mask,            dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch,             num_tokens, normalization)\n",
    "\n",
    "def train_one(package):\n",
    "    enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, enc_seg, enc_cls, enc_cls_mask,            dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch,             num_tokens, normalization = package\n",
    "    # ----------------------------------------------------    \n",
    "    parallel_res = parallel_model(enc_batch, dec_batch, enc_seg, enc_cls, enc_padding_mask, dec_padding_mask, enc_cls_mask, extra_zeros, enc_batch_extend_vocab)\n",
    "    preds, attn, merge_pred = merge_res(parallel_res)\n",
    "    \n",
    "#     gtruth = target   \n",
    "    loss = parallel_loss(config.mle_weight, preds, dec_batch[:,1:]) \n",
    "    \n",
    "#     loss, num_correct, target = compute_loss(preds, dec_batch[:,1:], merge_pred, num_tokens, tokenizer)\n",
    "#     acc = accuracy(num_correct, num_tokens)\n",
    "#     cross_entropy = xent(loss, num_tokens)\n",
    "#     perplexity = ppl(loss, num_tokens)    \n",
    "    loss = loss / normalization\n",
    "    \n",
    "#     print(\"num_tokens:%s; acc: %6.2f; perplexity: %5.2f; cross entropy loss: %4.2f\" \n",
    "#                         % (num_tokens,\n",
    "#                         acc,\n",
    "#                         perplexity,\n",
    "#                         cross_entropy\n",
    "#                         ))\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from utils.seq2seq.write_result import total_evaulate, total_output\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def decode_write_all(writer, logger, epoch, config, model, dataloader, mode):\n",
    "    # 動態取batch\n",
    "    num = len(dataloader)\n",
    "    avg_rouge_1, avg_rouge_2, avg_rouge_l  = [], [], []\n",
    "    avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4 = [], [], [], []\n",
    "    avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4 = [], [], [], []\n",
    "    avg_meteor = []\n",
    "    outFrame = None\n",
    "    avg_time = 0\n",
    "    \n",
    "    rouge = Rouge()  \n",
    "    \n",
    "    for idx, inputs in enumerate(dataloader):    \n",
    "        start = time.time() \n",
    "        gold_tgt_len = inputs.dec_tgt.size(1)\n",
    "        setattr(config, 'min_length',gold_tgt_len + 20)\n",
    "        setattr(config, 'max_length',gold_tgt_len + 60)\n",
    "        predictor = build_predictor(config, tokenizer, symbols, model, logger)\n",
    "\n",
    "        # 'Encoder data'\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, _, \\\n",
    "        _, _, _, _, enc_seg, enc_cls, enc_cls_mask = \\\n",
    "            get_input_from_batch(inputs, config, batch_first = True)\n",
    "\n",
    "        # 'Decoder data'\n",
    "        dec_batch, dec_padding_mask, dec_lens, max_dec_len, target_batch = \\\n",
    "        get_output_from_batch(inputs, config, batch_first = True) # Get input and target\n",
    "\n",
    "        setattr(inputs, 'src',enc_batch)\n",
    "        setattr(inputs, 'segs',enc_seg)\n",
    "        setattr(inputs, 'mask_src',enc_padding_mask)\n",
    "\n",
    "        inputs_data = predictor.translate_batch(inputs)\n",
    "        translations = predictor.from_batch(inputs_data) # translation = (pred_sents, gold_sent, raw_src)\n",
    "        article_sents = [t[2] for t in translations]\n",
    "        decoded_sents = [t[0] for t in translations]\n",
    "        ref_sents = [t[1] for t in translations]\n",
    "        \n",
    "        keywords_list = [str(word_list) for word_list in inputs.key_words]\n",
    "        cost = (time.time() - start)\n",
    "        avg_time += cost        \n",
    "\n",
    "        rouge_1, rouge_2, rouge_l, self_Bleu_1, self_Bleu_2, self_Bleu_3, self_Bleu_4, \\\n",
    "            Bleu_1, Bleu_2, Bleu_3, Bleu_4, Meteor, batch_frame = total_evaulate(article_sents, keywords_list, decoded_sents, ref_sents)\n",
    "        print(idx)\n",
    "        if idx %1000 ==0 and idx >0 : print(idx)\n",
    "        if idx == 0: outFrame = batch_frame\n",
    "        else: outFrame = pd.concat([outFrame, batch_frame], axis=0, ignore_index=True) \n",
    "        # ----------------------------------------------------\n",
    "        avg_rouge_1.extend(rouge_1)\n",
    "        avg_rouge_2.extend(rouge_2)\n",
    "        avg_rouge_l.extend(rouge_l)   \n",
    "        \n",
    "        avg_self_bleu1.extend(self_Bleu_1)\n",
    "        avg_self_bleu2.extend(self_Bleu_2)\n",
    "        avg_self_bleu3.extend(self_Bleu_3)\n",
    "        avg_self_bleu4.extend(self_Bleu_4)\n",
    "        \n",
    "        avg_bleu1.extend(Bleu_1)\n",
    "        avg_bleu2.extend(Bleu_2)\n",
    "        avg_bleu3.extend(Bleu_3)\n",
    "        avg_bleu4.extend(Bleu_4)\n",
    "        avg_meteor.extend(Meteor)\n",
    "        # ----------------------------------------------------    \n",
    "    avg_time = avg_time / (num * config.batch_size) \n",
    "    \n",
    "    avg_rouge_l, outFrame = total_output(mode, writerPath, outFrame, avg_time, avg_rouge_1, avg_rouge_2, avg_rouge_l, \\\n",
    "        avg_self_bleu1, avg_self_bleu2, avg_self_bleu3, avg_self_bleu4, \\\n",
    "        avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4, avg_meteor\n",
    "    )\n",
    "    \n",
    "    return avg_rouge_l, outFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "@torch.autograd.no_grad()\n",
    "def validate(validate_loader, config, model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "#     batch = next(iter(validate_loader))\n",
    "    val_num = len(iter(validate_loader))\n",
    "    for idx, batch in enumerate(validate_loader):\n",
    "#         package = get_package(batch)\n",
    "        loss = train_one(get_package(batch))\n",
    "#         loss = train_one(model, config, batch)\n",
    "        losses.append(loss.item())\n",
    "        if idx>= val_num/40: break\n",
    "#     model.train()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss\n",
    "\n",
    "@torch.autograd.no_grad()\n",
    "def calc_running_avg_loss(loss, running_avg_loss, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step = 0\n",
    "# for epoch in range(config.max_epochs):\n",
    "#     for inputs in train_loader:\n",
    "#         step += 1            \n",
    "#         loss_st = time.time()\n",
    "#         package = get_package(inputs)\n",
    "#         mle_loss = train_one(package)\n",
    "# #         print(step, mle_loss)\n",
    "#         mle_loss.backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "#         model.zero_grad() # 清空过往梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.batch_size = 8\n",
    "# train_loader, validate_loader, vocab, symbols = getDataLoader(logger, config)\n",
    "# tokenizer = vocab.tokenizer\n",
    "# train_batches = len(iter(train_loader))\n",
    "# test_batches = len(iter(validate_loader))\n",
    "\n",
    "\n",
    "# train_avg_acc, train_outFrame = decode_write_all(writer, logger, 10, config, model, train_loader, mode = 'train')\n",
    "# test_avg_acc, test_outFrame = decode_write_all(writer, logger, 10, config, model, validate_loader, mode = 'test')\n",
    "# logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (10, train_avg_acc, test_avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "loss_st, loss_cost = 0,0\n",
    "decode_st, decode_cost = 0,0\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "write_train_para(writer, config)\n",
    "logger.info('------Training START--------')\n",
    "running_avg_loss, running_avg_rl_loss = 0, 0\n",
    "sum_total_reward = 0\n",
    "step = 0\n",
    "# save_steps = 10\n",
    "print_step = 2000\n",
    "early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "try:\n",
    "    for epoch in range(config.max_epochs):\n",
    "        for inputs in train_loader:\n",
    "            step += 1            \n",
    "            loss_st = time.time()\n",
    "            package = get_package(inputs)\n",
    "            parallel_model.module.train()\n",
    "            mle_loss = train_one(package)\n",
    "    #         print(step, mle_loss)\n",
    "            mle_loss.backward()  # 反向传播，计算当前梯度\n",
    "\n",
    "            model.zero_grad() # 清空过往梯度\n",
    "            '''梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空'''\n",
    "            if step % (config.gradient_accum) == 0: # gradient accumulation\n",
    "                    # clip_grad_norm_(model.parameters(), 5.0)                     \n",
    "                for opt_idx, o in enumerate(optimizer):\n",
    "                    o.step() # 根据累计的梯度更新网络参数\n",
    "                    if opt_idx == 0: opt_name = 'lr_bert'\n",
    "                    else: opt_name = 'lr_dec'\n",
    "                    writer.add_scalars('scalar/%s' % opt_name,  \n",
    "                           {'lr': o.learning_rate\n",
    "                           }, step)\n",
    "            if step%print_step == 0 :\n",
    "                with T.autograd.no_grad():\n",
    "                    train_batch_loss = mle_loss.item()\n",
    "#                     val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                    running_avg_loss = calc_running_avg_loss(train_batch_loss, running_avg_loss)\n",
    "                    running_avg_reward = sum_total_reward / step\n",
    "#                     if step % print_step == 0:\n",
    "#                         logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "#                                     % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                    writer.add_scalars('scalar/Loss',  \n",
    "                       {'train_batch_loss': train_batch_loss\n",
    "                       }, step)\n",
    "                    writer.add_scalars('scalar_avg/loss',  \n",
    "                       {'train_avg_loss': running_avg_loss\n",
    "#                         'test_avg_loss': val_avg_loss\n",
    "                       }, step)\n",
    "#                     loss_cost = time.time() - loss_st\n",
    "#                     if step % save_steps == 0: logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "#                                 % (epoch, step, loss_cost))\n",
    "            if step % save_steps == 0:\n",
    "                parallel_model.module.eval()\n",
    "                logger.info('epoch : %s' % epoch)\n",
    "                val_avg_loss = validate(validate_loader, config, model) # call batch by validate_loader\n",
    "                logger.info('epoch %d: %d, training batch loss = %f, running_avg_loss loss = %f, validation loss = %f'\n",
    "                                    % (epoch, step, train_batch_loss, running_avg_loss, val_avg_loss))\n",
    "                writer.add_scalars('scalar_avg/loss',  \n",
    "                       {'train_avg_loss': running_avg_loss,\n",
    "                        'test_avg_loss': val_avg_loss\n",
    "                       }, step)\n",
    "                '''（讀取所儲存模型引數後，再進行並行化操作，否則無法利用之前的程式碼進行讀取）'''\n",
    "                save_model(config, logger, parallel_model, optimizer, step, vocab, running_avg_loss, \\\n",
    "                           r_loss=0, title = loggerName)\n",
    "                loss_cost = time.time() - loss_st\n",
    "                logger.info('epoch %d|step %d| compute loss cost = %f ms'\n",
    "                                % (epoch, step, loss_cost))\n",
    "                writer.add_scalars('scalar_avg/epoch_loss',  \n",
    "                   {'train_avg_loss': running_avg_loss,\n",
    "                    'test_avg_loss': val_avg_loss\n",
    "                   }, epoch)\n",
    "                last_save_step = step\n",
    "        logger.info('-------------------------------------------------------------')\n",
    "\n",
    "        early_stopping(val_avg_loss) # update patience\n",
    "        if early_stopping.early_stop:\n",
    "            logger.info(\"Early stopping epoch %s\"%(epoch))\n",
    "            break\n",
    "    \n",
    "except Excepation as e:\n",
    "        print(e)\n",
    "else:\n",
    "    logger.info(u'------Training SUCCESS--------')  \n",
    "finally:\n",
    "    logger.info(u'------Training END--------')    \n",
    "    logger.info(\"stopping epoch %s\"%(epoch))\n",
    "    config.batch_size = 8\n",
    "    train_loader, validate_loader, vocab, symbols = getDataLoader(logger, config)\n",
    "    tokenizer = vocab.tokenizer\n",
    "    train_batches = len(iter(train_loader))\n",
    "    test_batches = len(iter(validate_loader))\n",
    "\n",
    "    \n",
    "#     train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "    test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "#     logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc))\n",
    "    logger.info('epoch %d: test_avg_acc = %f' % (epoch, test_avg_acc))\n",
    "    removeLogger(logger)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_avg_acc, train_outFrame = decode_write_all(writer, logger, epoch, config, model, train_loader, mode = 'train')\n",
    "# test_avg_acc, test_outFrame = decode_write_all(writer, logger, epoch, config, model, validate_loader, mode = 'test')\n",
    "# logger.info('epoch %d: train_avg_acc = %f, test_avg_acc = %f' % (epoch, train_avg_acc, test_avg_acc)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('Leyan': conda)",
   "language": "python",
   "name": "python36764bitleyancondaa378f3cedbcc4b3f906a2276b3eef765"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
