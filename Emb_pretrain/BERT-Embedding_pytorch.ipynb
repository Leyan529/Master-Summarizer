{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM , BertForNextSentencePrediction\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "# config = BertConfig(max_position_embeddings=512)\n",
    "\n",
    "# bert_config = BertConfig(vocab_size_or_config_json_file=30522,\n",
    "#                          type_vocab_size=2,\n",
    "#                          num_labels=1000,\n",
    "#                          hidden_size=128,\n",
    "#                          num_hidden_layers=2,\n",
    "#                          num_attention_heads=8,\n",
    "#                          intermediate_size=256,\n",
    "#                          hidden_dropout_prob=0.01,\n",
    "#                          max_position_embeddings=1000,\n",
    "#                          attention_probs_dropout_prob=0.01\n",
    "#                          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bert_embed_matrix():\n",
    "    bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_embeddings = list(bert.children())[0]\n",
    "    bert_word_embeddings = list(bert_embeddings.children())[0]\n",
    "    mat = bert_word_embeddings.weight.data.numpy()\n",
    "    return mat\n",
    "\n",
    "embedding_matrix = get_bert_embed_matrix() # Bert word embedding weights\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer 裡頭的字典資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小： 30522 True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "vocab = tokenizer.vocab # word_to_id\n",
    "print(\"字典大小：\", len(vocab),'june' in vocab)\n",
    "\n",
    "vocab['june'] # word2id\n",
    "\n",
    "tokenizer.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(vocab)\n",
    "list(vocab.items())[1012][0]\n",
    "# list(vocab.items())\n",
    "\n",
    "vocab_dict = {}\n",
    "for v , k in vocab.items():\n",
    "    vocab_dict[k] = v\n",
    "    \n",
    "vocab_dict[1012]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token               index          \n",
      "-------------------------\n",
      "sings               10955\n",
      "##ssar              25556\n",
      "tactical             8608\n",
      "condensed           25011\n",
      "blew                 8682\n",
      "語                    1950\n",
      "##lf                10270\n",
      "silently             8601\n",
      "##hering            22658\n",
      "locomotives          7830\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids):\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertModel embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): BertLayerNorm()\n",
       "  (dropout): Dropout(p=0.1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "# BERT\n",
    "# model = BertModel.from_pretrained('bert-base-uncased',max_position_embeddings = 1024)\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels = 3)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model # 主任務模型\n",
    "'''\n",
    "Bert的詞向量主要是由三個向量相加組合而成，\n",
    "1.分別是單詞本身的向量，\n",
    "2.單詞所在句子中位置的向量\n",
    "3.句子所在單個訓練文本中位置的向量。\n",
    "'''\n",
    "model.embeddings # 由主任務模型接出的 bert embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(BertModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert 句子斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有 ## 前綴的 tokens 即為 wordpieces。\n",
    "# 以詞彙 fragment 來說，其可以被拆成 frag 與 ##ment 兩個 pieces，\n",
    "# 而一個 word 也可以獨自形成一個 wordpiece。wordpieces 可以由蒐集大量文本並找出其中常見的 pattern 取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 除了一般的 wordpieces 以外，BERT 裡頭有 5 個特殊 tokens 各司其職：\n",
    "\n",
    "# [CLS]：在做分類任務時其最後一層的 repr. 會被視為整個輸入序列的 repr.\n",
    "# [SEP]：有兩個句子的文本會被串接成一個輸入序列，並在兩句之間插入這個 token 以做區隔\n",
    "# [UNK]：沒出現在 BERT 字典裡頭的字會被這個 token 取代\n",
    "# [PAD]：zero padding 遮罩，將長度不一的輸入序列補齊方便做 batch 運算\n",
    "# [MASK]：未知遮罩，僅在預訓練階段會用到\n",
    "\n",
    "text = '''\n",
    "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
    "every thing work great except the bag . \n",
    "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
    "every thing work great except the bag . \n",
    "one of the front buckle was defective and would not close . \n",
    "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
    "you will basically need to buy a new camera bag anyways .\n",
    "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
    "every thing work great except the bag . \n",
    "one of the front buckle was defective and would not close . \n",
    "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
    "you will basically need to buy a new camera bag anyways .\n",
    "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
    "every thing work great except the bag . \n",
    "one of the front buckle was defective and would not close . \n",
    "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
    "you will basically need to buy a new camera bag anyways .\n",
    "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
    "every thing work great except the bag . \n",
    "one of the front buckle was defective and would not close . \n",
    "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
    "you will basically need to buy a new camera bag anyways .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
      "every thing work great except the bag . \n",
      "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
      "every thing work great except the bag . \n",
      "one of the front buckle was defective and would not close . \n",
      "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
      "you will basically need to buy a new camera bag anyways .\n",
      "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
      "every thing work great except the bag . \n",
      "one of the front buckle was defective and would not close . \n",
      "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
      "you will basically need to buy a new camera bag anyways .\n",
      "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
      "every thing work great except the bag . \n",
      "one of the front buckle was defective and would not close . \n",
      "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
      "you will basically need to buy a new camera bag anyways .\n",
      "i purchase the bundle package so i would not need to buy a second lens or camera bag to start out . \n",
      "every thing work great except the bag . \n",
      "one of the front buckle was defective and would not close . \n",
      "so you may want to consider buy the camera lense and bag a la cart so you can get exactly what you want . \n",
      "you will basically need to buy a new camera bag anyways .\n",
      "\n",
      "['i', 'purchase', 'the', 'bundle', 'package', 'so', 'i', 'would', 'not', 'need', 'to', 'buy', 'a', 'second', 'lens', 'or', 'camera', 'bag', 'to', 'start', 'out', '.', 'every', 'thing', 'work', 'great', 'except', 'the', 'bag', '.', 'i', 'purchase', 'the', 'bundle', 'package', 'so', 'i', 'would', 'not', 'need', 'to', 'buy', 'a', 'second', 'lens', 'or', 'camera', 'bag', 'to', 'start', 'out', '.', 'every', 'thing', 'work', 'great', 'except', 'the', 'bag', '.', 'one', 'of', 'the', 'front', 'buckle', 'was', 'defective', 'and', 'would', 'not', 'close', '.', 'so', 'you', 'may', 'want', 'to', 'consider', 'buy', 'the', 'camera', 'lens', '##e', 'and', 'bag', 'a', 'la', 'cart', 'so', 'you', 'can', 'get', 'exactly', 'what', 'you', 'want', '.', 'you', 'will', 'basically', 'need', 'to', 'buy', 'a', 'new', 'camera', 'bag', 'anyway', '##s', '.', 'i', 'purchase', 'the', 'bundle', 'package', 'so', 'i', 'would', 'not', 'need', 'to', 'buy', 'a', 'second', 'lens', 'or', 'camera', 'bag', 'to', 'start', 'out', '.', 'every', 'thing', 'work', 'great', 'except', 'the', 'bag', '.', 'one', 'of', 'the', 'front', 'buckle', 'was', 'defective', 'and', 'would', 'not', 'close', '.', 'so', 'you', 'may', 'want', 'to', 'consider', 'buy', 'the', 'camera', 'lens', '##e', 'and', 'bag', 'a', 'la', 'cart', 'so', 'you', 'can', 'get', 'exactly', 'what', 'you', 'want', '.', 'you', 'will', 'basically', 'need', 'to', 'buy', 'a', 'new', 'camera', 'bag', 'anyway', '##s', '.', 'i', 'purchase', 'the', 'bundle', 'package', 'so', 'i', 'would', 'not', 'need', 'to', 'buy', 'a', 'second', 'lens', 'or', 'camera', 'bag', 'to', 'start', 'out', '.', 'every', 'thing', 'work', 'great', 'except', 'the', 'bag', '.', 'one', 'of', 'the', 'front', 'buckle', 'was', 'defective', 'and', 'would', 'not', 'close', '.', 'so', 'you', 'may', 'want', 'to', 'consider', 'buy', 'the', 'camera', 'lens', '##e', 'and', 'bag', 'a', 'la', 'cart', 'so', 'you', 'can', 'get', 'exactly', 'what', 'you', 'want', '.', 'you', 'will', 'basically', 'need', 'to', 'buy', 'a', 'new', 'camera', 'bag', 'anyway', '##s', '.', 'i', 'purchase', 'the', 'bundle', 'package', 'so', 'i', 'would', 'not', 'need', 'to', 'buy', 'a', 'second', 'lens', 'or', 'camera', 'bag', 'to', 'start', 'out', '.', 'every', 'thing', 'work', 'great', 'except', 'the', 'bag', '.', 'one', 'of', 'the', 'front', 'buckle', 'was', 'defective', 'and', 'would', 'not', 'close', '.', 'so', 'you', 'may', 'want', 'to', 'consider', 'buy', 'the', 'camera', 'lens', '##e', 'and', 'bag', 'a', 'la', 'cart', 'so', 'you', 'can', 'get', 'exactly', 'what', 'you', 'want', '.', 'you', 'will', 'basically', 'need', 'to', 'buy', 'a', 'new', 'camera', 'bag', 'anyway', '##s', '.'] ...\n",
      "[1045, 5309, 1996, 14012, 7427, 2061, 1045, 2052, 2025, 2342, 2000, 4965, 1037, 2117, 10014, 2030, 4950, 4524, 2000, 2707, 2041, 1012, 2296, 2518, 2147, 2307, 3272, 1996, 4524, 1012, 1045, 5309, 1996, 14012, 7427, 2061, 1045, 2052, 2025, 2342, 2000, 4965, 1037, 2117, 10014, 2030, 4950, 4524, 2000, 2707, 2041, 1012, 2296, 2518, 2147, 2307, 3272, 1996, 4524, 1012, 2028, 1997, 1996, 2392, 22853, 2001, 28829, 1998, 2052, 2025, 2485, 1012, 2061, 2017, 2089, 2215, 2000, 5136, 4965, 1996, 4950, 10014, 2063, 1998, 4524, 1037, 2474, 11122, 2061, 2017, 2064, 2131, 3599, 2054, 2017, 2215, 1012, 2017, 2097, 10468, 2342, 2000, 4965, 1037, 2047, 4950, 4524, 4312, 2015, 1012, 1045, 5309, 1996, 14012, 7427, 2061, 1045, 2052, 2025, 2342, 2000, 4965, 1037, 2117, 10014, 2030, 4950, 4524, 2000, 2707, 2041, 1012, 2296, 2518, 2147, 2307, 3272, 1996, 4524, 1012, 2028, 1997, 1996, 2392, 22853, 2001, 28829, 1998, 2052, 2025, 2485, 1012, 2061, 2017, 2089, 2215, 2000, 5136, 4965, 1996, 4950, 10014, 2063, 1998, 4524, 1037, 2474, 11122, 2061, 2017, 2064, 2131, 3599, 2054, 2017, 2215, 1012, 2017, 2097, 10468, 2342, 2000, 4965, 1037, 2047, 4950, 4524, 4312, 2015, 1012, 1045, 5309, 1996, 14012, 7427, 2061, 1045, 2052, 2025, 2342, 2000, 4965, 1037, 2117, 10014, 2030, 4950, 4524, 2000, 2707, 2041, 1012, 2296, 2518, 2147, 2307, 3272, 1996, 4524, 1012, 2028, 1997, 1996, 2392, 22853, 2001, 28829, 1998, 2052, 2025, 2485, 1012, 2061, 2017, 2089, 2215, 2000, 5136, 4965, 1996, 4950, 10014, 2063, 1998, 4524, 1037, 2474, 11122, 2061, 2017, 2064, 2131, 3599, 2054, 2017, 2215, 1012, 2017, 2097, 10468, 2342, 2000, 4965, 1037, 2047, 4950, 4524, 4312, 2015, 1012, 1045, 5309, 1996, 14012, 7427, 2061, 1045, 2052, 2025, 2342, 2000, 4965, 1037, 2117, 10014, 2030, 4950, 4524, 2000, 2707, 2041, 1012, 2296, 2518, 2147, 2307, 3272, 1996, 4524, 1012, 2028, 1997, 1996, 2392, 22853, 2001, 28829, 1998, 2052, 2025, 2485, 1012, 2061, 2017, 2089, 2215, 2000, 5136, 4965, 1996, 4950, 10014, 2063, 1998, 4524, 1037, 2474, 11122, 2061, 2017, 2064, 2131, 3599, 2054, 2017, 2215, 1012, 2017, 2097, 10468, 2342, 2000, 4965, 1037, 2047, 4950, 4524, 4312, 2015, 1012] ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "\n",
    "# text = 'Terrific Book for Learning the Art of Crochet'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(text)\n",
    "print(tokens, '...')\n",
    "print(ids, '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create an instance of BertModel initialized with pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "# returns the base model (the one with 12 layers) pre-trained on uncased sequences\n",
    "BertModel = BertModel.from_pretrained('bert-base-uncased') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Tokenize the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['terrific', 'book', 'for', 'learning', 'the', 'art', 'of', 'cr', '##oche', '##t']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Terrific Book for Learning the Art of Crochet'\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Add [CLS] and [SEP] tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'terrific', 'book', 'for', 'learning', 'the', 'art', 'of', 'cr', '##oche', '##t', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Padding the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'terrific', 'book', 'for', 'learning', 'the', 'art', 'of', 'cr', '##oche', '##t', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "T = 15 # maximum length\n",
    "padded_tokens = tokens + ['[PAD]' for _ in range(T - len(tokens))]\n",
    "print(padded_tokens)\n",
    "# Out: ['[CLS]', 'i', 'really', 'enjoyed', 'this', 'movie', 'a', 'lot', '.', '[SEP]', '[PAD]', '[PAD]']\n",
    "attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "print(attn_mask)\n",
    "# Out: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Maintain a list of segment tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_ids = [0 for _ in range(len(padded_tokens))] #Since we only have a single sequence as input\n",
    "seg_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Obtaining indices of the tokens in BERT’s vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 27547, 2338, 2005, 4083, 1996, 2396, 1997, 13675, 23555, 2102, 102, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Obtaining indices for each token\n",
    "sent_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "print(sent_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all these steps together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of input sequence tokens in the vocabulary. To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n",
    "\n",
    "# For sequence pairs:\n",
    "# tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "\n",
    "# For single sequences:\n",
    "# tokens:         [CLS] the dog is hairy . [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "\n",
    "\n",
    "SENTENCE_A = \"Perfect for beginners and already knowledgeable crocheters alike!\"\n",
    "# SENTENCE_A = SENTENCE_A.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded_tokens: ['[CLS]', 'terrific', 'book', 'for', 'learning', 'the', 'art', 'of', 'cr', '##oche', '##t', '.', '[SEP]', 'terrific', 'book', 'for', 'learning', 'the', 'art', 'of', 'cr', '##oche', '##t', '.', '[SEP]']\n",
      "seg_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "torch.Size([1, 25, 768])\n",
      "torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sentence = 'Terrific Book for Learning the Art of Crochet .'\n",
    "#Step 1: Tokenize\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "# print('tokens:',tokens)\n",
    "#Step 2: Add [CLS] and [SEP]\n",
    "# [CLS]：在做任務時其最後一層的 repr. 會被視為整個輸入序列的 repr.\n",
    "# [SEP]：有兩個句子的文本會被串接成一個輸入序列，並在兩句之間插入這個 token 以做區隔\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]'] + tokens + ['[SEP]']\n",
    "#Step 3: Pad tokens\n",
    "T = 20 # maximum length\n",
    "padded_tokens = tokens + ['[PAD]' for _ in range(T - len(tokens))]\n",
    "attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "print('padded_tokens:',padded_tokens)\n",
    "#Step 4: Segment ids\n",
    "# seg_ids = [0 for _ in range(len(padded_tokens))] #Optional!\n",
    "seg_ids = []\n",
    "ap_mode = 0\n",
    "for token in tokens:\n",
    "    if token == '[SEP]':\n",
    "        if ap_mode == 1: seg_ids.append(0) ; ap_mode = 0 \n",
    "        elif ap_mode == 0: seg_ids.append(0) ; ap_mode = 1    \n",
    "    elif token == '[CLS]': seg_ids.append(0)\n",
    "    else:  seg_ids.append(ap_mode)\n",
    "        \n",
    "#Step 5: Get BERT vocabulary index for each token\n",
    "token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "print('seg_ids',seg_ids)\n",
    "\n",
    "#Converting everything to torch tensors before feeding them to bert_model\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "attn_mask = torch.tensor(attn_mask).unsqueeze(0) #Shape : [1, 12]\n",
    "seg_ids   = torch.tensor(seg_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "\n",
    "#Feed them to bert\n",
    "# bert_config = BertConfig(vocab_size_or_config_json_file=30522,\n",
    "#                          type_vocab_size=2,\n",
    "#                          num_labels=len(label_list),\n",
    "#                          hidden_size=128,\n",
    "#                          num_hidden_layers=2,\n",
    "#                          num_attention_heads=8,\n",
    "#                          intermediate_size=256,\n",
    "#                          hidden_dropout_prob=0.01,\n",
    "#                          max_position_embeddings=128,\n",
    "#                          attention_probs_dropout_prob=0.01\n",
    "#                          )\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# hidden_reps, cls_head = bert_model(token_ids, attention_mask = attn_mask,\\\n",
    "#                                   token_type_ids = seg_ids, \\\n",
    "#                                    output_all_encoded_layers=True)\n",
    "\n",
    "hidden_reps, cls_head = bert_model(token_ids)\n",
    "\n",
    "# hidden_reps, cls_head = bert_model(token_ids, attention_mask = attn_mask)\n",
    "print(hidden_reps[0].shape)\n",
    "#Out: torch.Size([1, 12, 768])\n",
    "print(cls_head[0].shape)\n",
    "#Out: torch.Size([1, 768])\n",
    "\n",
    "'''\n",
    "1.(hidden_reps) contains the hidden states of each token in the input sequence after feeding them \n",
    "through a series of self-attention layers. \n",
    "\n",
    "\n",
    "2.(cls_head) contains the hidden representation of just the ‘[CLS]’ token after additionally being \n",
    "passed to a fully connected layer with tanh activation function.\n",
    "'''\n",
    "\n",
    "'''\n",
    "class BertModel(BertPreTrainedModel)\n",
    " |  BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
    " |  \n",
    " |  Params:\n",
    " |      config: a BertConfig class instance with the configuration to build a new model\n",
    " |  \n",
    " |  Inputs:\n",
    " |      `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    " |          with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    " |          `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    " |      `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    " |          types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    " |          a `sentence B` token (see BERT paper for more details).\n",
    " |      `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    " |          selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    " |          input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    " |          a batch has varying length sentences.\n",
    " |      `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
    " |  \n",
    " |  Outputs: Tuple of (encoded_layers, pooled_output)\n",
    " |      `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
    " |          - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
    " |              of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
    " |              encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
    " |          - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
    " |              to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
    " |      `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
    " |          classifier pretrained on top of the hidden state associated to the first character of the\n",
    " |          input (`CLS`) to train on the Next-Sentence task (see BERT's paper).\n",
    "'''\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://medium.com/swlh/painless-fine-tuning-of-bert-in-pytorch-b91c14912caa\n",
    "# 拜讀改code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0227 01:53:09.124823 18764 file_utils.py:35] PyTorch version 0.4.1 available.\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I0227 01:53:14.063767 18764 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-vocab.bin from cache at C:\\Users\\user\\.cache\\torch\\transformers\\b24cb708726fd43cbf1a382da9ed3908263e4fb8a156f9e0a4f45b7540c69caa.a6a9c41b856e5c31c9f125dd6a7ed4b833fbcefda148b627871d4171b25cffd1\n",
      "I0227 01:53:15.302678 18764 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-config.json from cache at C:\\Users\\user\\.cache\\torch\\transformers\\a6dfd6a3896b3ae4c1a3c5f26ff1f1827c26c15b679de9212a04060eaf1237df.4f34ea1bcf7fb8fa015d300b4847b2030f36e7d2c6e4c92075a244afa0cc3d67\n",
      "I0227 01:53:15.304637 18764 configuration_utils.py:199] Model config {\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"TransfoXLLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": 0,\n",
      "  \"clamp_len\": 1000,\n",
      "  \"cutoffs\": [\n",
      "    20000,\n",
      "    40000,\n",
      "    200000\n",
      "  ],\n",
      "  \"d_embed\": 1024,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 4096,\n",
      "  \"d_model\": 1024,\n",
      "  \"div_val\": 4,\n",
      "  \"dropatt\": 0.0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"ext_len\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"init\": \"normal\",\n",
      "  \"init_range\": 0.01,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"mem_len\": 1600,\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 18,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pre_lnorm\": false,\n",
      "  \"proj_init_std\": 0.01,\n",
      "  \"pruned_heads\": {},\n",
      "  \"same_length\": true,\n",
      "  \"sample_softmax\": -1,\n",
      "  \"tgt_len\": 128,\n",
      "  \"tie_projs\": [\n",
      "    false,\n",
      "    true,\n",
      "    true,\n",
      "    true\n",
      "  ],\n",
      "  \"tie_weight\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"untie_r\": true,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 267735\n",
      "}\n",
      "\n",
      "I0227 01:53:16.228561 18764 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin from cache at C:\\Users\\user\\.cache\\torch\\transformers\\12642ff7d0279757d8356bfd86a729d9697018a0c93ad042de1d0d2cc17fd57b.e9704971f27275ec067a00a67e6a5f0b05b4306b3f714a96e9f763d8fb612671\n"
     ]
    }
   ],
   "source": [
    "from transformers import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
    "model = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method_descriptor:\n",
      "\n",
      "detach(...)\n",
      "    Returns a new Tensor, detached from the current graph.\n",
      "    \n",
      "    The result will never require gradient.\n",
      "    \n",
      "    .. note::\n",
      "    \n",
      "      Returned Tensor uses the same data tensor as the original one.\n",
      "      In-place modifications on either of them will be seen, and may trigger\n",
      "      errors in correctness checks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# help(torch.argmax)\n",
    "\n",
    "help(torch.Tensor.detach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function scatter_add in module torch.tensor:\n",
      "\n",
      "scatter_add(self, dim, index, source)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.Tensor.scatter_add)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
