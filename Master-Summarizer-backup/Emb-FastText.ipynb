{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from model import Model\n",
    "\n",
    "\n",
    "from data_util import config, data\n",
    "from data_util.batcher import Batcher\n",
    "from data_util.data import Vocab\n",
    "from write_result import *\n",
    "\n",
    "from train_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse\n",
    "import torchsnooper\n",
    "import logging\n",
    "\n",
    "# -------- Test Packages -------\n",
    "from beam_search import *\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "# _6\n",
    "config.lr = 0.0001\n",
    "config.eps = 1e-12\n",
    "config.min_dec_steps = 4\n",
    "config.vocab_size = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## batch_size : 8\n",
      "## beam_size : 16\n",
      "## ber_layer : 11\n",
      "## data_type : Cameras_new8\n",
      "## emb_dim : 300\n",
      "## emb_grad : False\n",
      "## eps : 1e-12\n",
      "## gound_truth_prob : 0.1\n",
      "## hidden_dim : 512\n",
      "## intra_decoder : True\n",
      "## intra_encoder : True\n",
      "## key_attention : False\n",
      "## keywords : POS_FOP_keywords\n",
      "## loggerName : Text-Summary\n",
      "## lr : 0.0001\n",
      "## max_dec_steps : 50\n",
      "## max_enc_steps : 1000\n",
      "## max_epochs : 100\n",
      "## max_iterations : 500000\n",
      "## max_key_num : 8\n",
      "## min_dec_steps : 4\n",
      "## rand_unif_init_mag : 0.02\n",
      "## trunc_norm_init_std : 0.0001\n",
      "## vocab_size : 60000\n",
      "## word_emb_type : word2Vec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_str = ''\n",
    "for a in dir(config):\n",
    "    if type(getattr(config, a)) in [str,int,float,bool] \\\n",
    "    and 'path' not in str(a) \\\n",
    "    and '__' not in str(a) \\\n",
    "    and 'info' not in str(a):\n",
    "\n",
    "        info_str += '## %s : %s\\n'%(a,getattr(config, a))\n",
    "\n",
    "# [print(a,getattr(config, a)) for a in dir(config)\n",
    "# if type(getattr(config, a)) in [str,int,float]\n",
    "#  and 'path' not in str(a)\n",
    "#  and '__' not in str(a)\n",
    "#  and 'info' not in str(a)\n",
    "# ]\n",
    "print(info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def getLogger(loggerName, loggerPath):\n",
    "    # 設置logger\n",
    "    logger = logging.getLogger(loggerName)  # 不加名稱設置root logger\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s: - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    logging.Filter(loggerName)\n",
    "\n",
    "    # 使用FileHandler輸出到文件\n",
    "    directory = os.path.dirname(loggerPath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fh = logging.FileHandler(loggerPath)\n",
    "\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # 使用StreamHandler輸出到屏幕\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    ch.setFormatter(formatter)\n",
    "    # 添加兩個Handler\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    # Handler只啟動一次\n",
    "    # 設置logger\n",
    "    logger.info(u'logger已啟動')\n",
    "    return logger\n",
    "\n",
    "def removeLogger(logger):\n",
    "    logger.info(u'logger已關閉')\n",
    "    handlers = logger.handlers[:]\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_summary_sents :  <s> great camera read review if your try to decide on this or some other similarly price camera </s>\n",
      "key_words :  ['camera', 'more', 'electronic']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> upgrade from my canon rebel xti and am so glad did </s>\n",
      "key_words :  ['mode', 'not', 'excellent']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> good little camera for the price </s>\n",
      "key_words :  ['image', 'most', 'small']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> nice canon good feature </s>\n",
      "key_words :  ['feature', 'external']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> zoom is amazing low light night day sunshine or not its great </s>\n",
      "key_words :  ['camera', 'direct']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> spend the extra and get this camera </s>\n",
      "key_words :  ['money', 'unbelievable']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> this is an ideal camera and lens for non professional </s>\n",
      "key_words :  ['camera', 'extra']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> xsi for about year and this is just wonderful it bring new depth to old lense and </s>\n",
      "key_words :  ['display', 'odd']\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_batch():\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    # with torchsnooper.snoop():\n",
    "    while batch is not None:\n",
    "        example_list = batch.example_list\n",
    "        for ex in example_list:\n",
    "            r = str(ex.original_review)\n",
    "            s = str(ex.original_summary)\n",
    "            k = str(ex.key_words)\n",
    "            sent = ex.original_summary_sents\n",
    "#             print(\"original_review_sents:\", r)\n",
    "            print(\"original_summary_sents : \", s)\n",
    "            print(\"key_words : \", k)\n",
    "            print('------------------------------------------------------------\\n')            \n",
    "        batch = batcher.next_batch()      \n",
    "        break\n",
    "test_batch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bin Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 33574\n",
      "\n",
      "test : 4196\n",
      "\n",
      "valid : 4196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(config.bin_info,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    [print(line) for line in lines]\n",
    "    train_num = int(lines[0].split(\":\")[1])\n",
    "    test_num = int(lines[1].split(\":\")[1])\n",
    "    val_num = int(lines[2].split(\":\")[1])\n",
    "    # f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    # f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    # f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "           Kernel Shape  Output Shape   Params  Mult-Adds\n",
      "Layer                                                    \n",
      "0_lstm                -  [1631, 1024]  3334144    3325952\n",
      "1_reduce_h  [1024, 512]      [8, 512]   524800     524288\n",
      "2_reduce_c  [1024, 512]      [8, 512]   524800     524288\n",
      "---------------------------------------------------------\n",
      "                       Totals\n",
      "Total params          4383744\n",
      "Trainable params      4383744\n",
      "Non-trainable params        0\n",
      "Mult-Adds             4374528\n",
      "=========================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[1631, 1024]</td>\n",
       "      <td>3334144</td>\n",
       "      <td>3325952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_reduce_h</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_reduce_c</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kernel Shape  Output Shape   Params  Mult-Adds\n",
       "Layer                                                    \n",
       "0_lstm                -  [1631, 1024]  3334144    3325952\n",
       "1_reduce_h  [1024, 512]      [8, 512]   524800     524288\n",
       "2_reduce_c  [1024, 512]      [8, 512]   524800     524288"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Encoder,Model\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "encoder = Encoder().to(device)    \n",
    "\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = Model(False,'glove',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "summary(encoder, enc_batch, enc_lens) # encoder summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                               Kernel Shape    Output Shape    Params  \\\n",
      "Layer                                                                   \n",
      "0_x_context                     [1324, 300]        [8, 300]    397500   \n",
      "1_lstm                                    -        [8, 512]   1667072   \n",
      "2_enc_attention.Linear_W_h     [1024, 1024]  [8, 330, 1024]   1048576   \n",
      "3_enc_attention.Linear_W_s     [1024, 1024]       [8, 1024]   1049600   \n",
      "4_enc_attention.Linear_v          [1024, 1]     [8, 330, 1]      1024   \n",
      "5_dec_attention.Linear_W_prev    [512, 512]     [8, 1, 512]    262144   \n",
      "6_dec_attention.Linear_W_s       [512, 512]        [8, 512]    262656   \n",
      "7_dec_attention.Linear_v           [512, 1]       [8, 1, 1]       512   \n",
      "8_p_gen_linear                    [2860, 1]          [8, 1]      2861   \n",
      "9_V                             [2048, 512]        [8, 512]   1049088   \n",
      "10_V1                          [512, 60000]      [8, 60000]  30780000   \n",
      "\n",
      "                               Mult-Adds  \n",
      "Layer                                     \n",
      "0_x_context                       397200  \n",
      "1_lstm                           1662976  \n",
      "2_enc_attention.Linear_W_h       1048576  \n",
      "3_enc_attention.Linear_W_s       1048576  \n",
      "4_enc_attention.Linear_v            1024  \n",
      "5_dec_attention.Linear_W_prev     262144  \n",
      "6_dec_attention.Linear_W_s        262144  \n",
      "7_dec_attention.Linear_v             512  \n",
      "8_p_gen_linear                      2860  \n",
      "9_V                              1048576  \n",
      "10_V1                           30720000  \n",
      "--------------------------------------------------------------------------------\n",
      "                        Totals\n",
      "Total params          36521033\n",
      "Trainable params      36521033\n",
      "Non-trainable params         0\n",
      "Mult-Adds             36454588\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_x_context</th>\n",
       "      <td>[1324, 300]</td>\n",
       "      <td>[8, 300]</td>\n",
       "      <td>397500</td>\n",
       "      <td>397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>1667072</td>\n",
       "      <td>1662976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_enc_attention.Linear_W_h</th>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>[8, 330, 1024]</td>\n",
       "      <td>1048576</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_enc_attention.Linear_W_s</th>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>[8, 1024]</td>\n",
       "      <td>1049600</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_enc_attention.Linear_v</th>\n",
       "      <td>[1024, 1]</td>\n",
       "      <td>[8, 330, 1]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_dec_attention.Linear_W_prev</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>[8, 1, 512]</td>\n",
       "      <td>262144</td>\n",
       "      <td>262144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_dec_attention.Linear_W_s</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>262656</td>\n",
       "      <td>262144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_dec_attention.Linear_v</th>\n",
       "      <td>[512, 1]</td>\n",
       "      <td>[8, 1, 1]</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_p_gen_linear</th>\n",
       "      <td>[2860, 1]</td>\n",
       "      <td>[8, 1]</td>\n",
       "      <td>2861</td>\n",
       "      <td>2860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9_V</th>\n",
       "      <td>[2048, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>1049088</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_V1</th>\n",
       "      <td>[512, 60000]</td>\n",
       "      <td>[8, 60000]</td>\n",
       "      <td>30780000</td>\n",
       "      <td>30720000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Kernel Shape    Output Shape    Params  \\\n",
       "Layer                                                                   \n",
       "0_x_context                     [1324, 300]        [8, 300]    397500   \n",
       "1_lstm                                    -        [8, 512]   1667072   \n",
       "2_enc_attention.Linear_W_h     [1024, 1024]  [8, 330, 1024]   1048576   \n",
       "3_enc_attention.Linear_W_s     [1024, 1024]       [8, 1024]   1049600   \n",
       "4_enc_attention.Linear_v          [1024, 1]     [8, 330, 1]      1024   \n",
       "5_dec_attention.Linear_W_prev    [512, 512]     [8, 1, 512]    262144   \n",
       "6_dec_attention.Linear_W_s       [512, 512]        [8, 512]    262656   \n",
       "7_dec_attention.Linear_v           [512, 1]       [8, 1, 1]       512   \n",
       "8_p_gen_linear                    [2860, 1]          [8, 1]      2861   \n",
       "9_V                             [2048, 512]        [8, 512]   1049088   \n",
       "10_V1                          [512, 60000]      [8, 60000]  30780000   \n",
       "\n",
       "                               Mult-Adds  \n",
       "Layer                                     \n",
       "0_x_context                       397200  \n",
       "1_lstm                           1662976  \n",
       "2_enc_attention.Linear_W_h       1048576  \n",
       "3_enc_attention.Linear_W_s       1048576  \n",
       "4_enc_attention.Linear_v            1024  \n",
       "5_dec_attention.Linear_W_prev     262144  \n",
       "6_dec_attention.Linear_W_s        262144  \n",
       "7_dec_attention.Linear_v             512  \n",
       "8_p_gen_linear                      2860  \n",
       "9_V                              1048576  \n",
       "10_V1                           30720000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Decoder,Model\n",
    "from train_util import *\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# decoder = Decoder().to(device)    \n",
    "\n",
    "model = Model(False,'glove',vocab)\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "# train_batch_MLE\n",
    "dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "step_losses = []\n",
    "s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "# x_t 為decoder每一個time step 的batch input\n",
    "x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "sum_temporal_srcs = None     \n",
    "             \n",
    "    \n",
    "for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "    use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "    # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "    # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "    x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "    x_t = model.embeds(x_t)\n",
    "    enc_key_batch = model.embeds(enc_key_batch)\n",
    "#     final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "    final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "    x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "    extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "    enc_key_batch, enc_key_lens)\n",
    "#     print('x_t2',x_t)\n",
    "#     print(type(enc_batch_extend_vocab))\n",
    "#     print(enc_batch_extend_vocab.shape)\n",
    "#     print(enc_batch_extend_vocab)\n",
    "    decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "    break\n",
    "decoder_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "def write_enc_graph():\n",
    "    encoder_writer = SummaryWriter('runs/Pointer-Generator/glove/Encoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    encoder = Encoder().to(device) \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = Model(False,'glove',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "#     enc_batch = Variable(torch.rand(enc_batch.shape)).to(device) \n",
    "    enc_lens = torch.from_numpy(enc_lens).to(device) \n",
    "\n",
    "    encoder_writer.add_graph(encoder, (enc_batch, enc_lens), verbose=True)\n",
    "    encoder_writer.close()\n",
    "\n",
    "def write_dec_graph():\n",
    "    decoder_writer = SummaryWriter('runs/Pointer-Generator/glove/Decoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    # decoder = Decoder().to(device)    \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    model = Model(False,'glove',vocab)\n",
    "    \n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "    # train_batch_MLE\n",
    "    dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "    step_losses = []\n",
    "    s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "    # x_t 為decoder每一個time step 的batch input\n",
    "    x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "    prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    sum_temporal_srcs = None     \n",
    "\n",
    "\n",
    "    for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "        use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "        # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "        # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "        if t == 0 :temp_batch = dec_batch[:, t]\n",
    "        x_t = use_gound_truth * temp_batch + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "        x_t = model.embeds(x_t)\n",
    "    #     final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "        final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "        x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "        extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "        enc_key_batch, enc_key_lens)        \n",
    "\n",
    "\n",
    "        #         decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "#         x_t = Variable(torch.rand(x_t.shape)).to(device) \n",
    "        #             s_t = Variable(torch.rand(s_t.shape)).to(device)\n",
    "#         enc_out = Variable(torch.rand(enc_out.shape)).to(device)\n",
    "#         enc_padding_mask = Variable(torch.rand(enc_padding_mask.shape)).to(device,dtype=torch.long)\n",
    "#         context = Variable(torch.rand(context.shape)).to(device)\n",
    "#         extra_zeros = Variable(torch.rand(extra_zeros.shape)).to(device)\n",
    "#         enc_batch_extend_vocab = Variable(torch.rand(enc_batch_extend_vocab.shape)).to(device)\n",
    "        #             sum_temporal_srcs = Variable(torch.rand(sum_temporal_srcs.shape)).to(device)\n",
    "        #             prev_s = Variable(torch.rand(prev_s.shape)).to(device)\n",
    "#         enc_key_batch = Variable(torch.rand(enc_key_batch.shape)).to(device)\n",
    "        enc_key_lens = torch.from_numpy(enc_key_lens).to(device) \n",
    "        \n",
    "        decoder_writer.add_graph(model.decoder, \n",
    "                         (x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens), verbose=True)\n",
    "        decoder_writer.close()\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-19 11:10:45 - Text-Summary - INFO: - logger已啟動\n",
      "2020-03-19 11:10:45 - Text-Summary - INFO: - ------Training Setting--------\n",
      "2020-03-19 11:10:45 - Text-Summary - INFO: - Traing Type :Cameras_new8\n",
      "2020-03-19 11:10:45 - Text-Summary - INFO: - Training mle: True, mle weight: 1.00\n",
      "2020-03-19 11:10:45 - Text-Summary - INFO: - use pre_train_FastText vocab_size 60000 \n",
      "\n",
      "2020-03-19 11:10:45 - Text-Summary - INFO: - intra_encoder: True intra_decoder: True \n",
      "\n",
      "2020-03-19 11:11:02 - Text-Summary - INFO: - Model(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
      "    (reduce_h): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (reduce_c): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (enc_attention): encoder_attention(\n",
      "      (W_h): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (W_s): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (W_t): Linear(in_features=300, out_features=1024, bias=True)\n",
      "      (v): Linear(in_features=1024, out_features=1, bias=False)\n",
      "    )\n",
      "    (dec_attention): decoder_attention(\n",
      "      (W_prev): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (W_s): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (W_t): Linear(in_features=300, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (x_context): Linear(in_features=1324, out_features=300, bias=True)\n",
      "    (x_key_context): Linear(in_features=1624, out_features=300, bias=True)\n",
      "    (lstm): LSTMCell(300, 512)\n",
      "    (p_gen_linear): Linear(in_features=2860, out_features=1, bias=True)\n",
      "    (V): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (V1): Linear(in_features=512, out_features=60000, bias=True)\n",
      "  )\n",
      "  (embeds): Embedding(60000, 300)\n",
      ")\n",
      "2020-03-19 11:11:02 - Text-Summary - INFO: - ------Training START--------\n",
      "2020-03-19 11:11:11 - Text-Summary - INFO: - iter: 0 train_rouge_l_f: 0.014 test_rouge_l_f: 0.029 \n",
      "\n",
      "2020-03-19 11:27:51 - Text-Summary - INFO: - iter: 1000 train_rouge_l_f: 0.151 test_rouge_l_f: 0.302 \n",
      "\n",
      "2020-03-19 11:44:51 - Text-Summary - INFO: - iter: 2000 train_rouge_l_f: 0.145 test_rouge_l_f: 0.247 \n",
      "\n",
      "2020-03-19 12:01:58 - Text-Summary - INFO: - iter: 3000 train_rouge_l_f: 0.341 test_rouge_l_f: 0.438 \n",
      "\n",
      "2020-03-19 12:19:16 - Text-Summary - INFO: - iter: 4000 train_rouge_l_f: 0.270 test_rouge_l_f: 0.293 \n",
      "\n",
      "2020-03-19 12:36:47 - Text-Summary - INFO: - iter: 5000 train_rouge_l_f: 0.304 test_rouge_l_f: 0.449 \n",
      "\n",
      "2020-03-19 12:53:14 - Text-Summary - INFO: - iter: 6000 train_rouge_l_f: 0.404 test_rouge_l_f: 0.470 \n",
      "\n",
      "2020-03-19 13:10:39 - Text-Summary - INFO: - iter: 7000 train_rouge_l_f: 0.368 test_rouge_l_f: 0.372 \n",
      "\n",
      "2020-03-19 13:27:30 - Text-Summary - INFO: - iter: 8000 train_rouge_l_f: 0.084 test_rouge_l_f: 0.404 \n",
      "\n",
      "2020-03-19 13:44:31 - Text-Summary - INFO: - iter: 9000 train_rouge_l_f: 0.285 test_rouge_l_f: 0.610 \n",
      "\n",
      "2020-03-19 14:01:42 - Text-Summary - INFO: - iter: 10000 train_rouge_l_f: 0.363 test_rouge_l_f: 0.295 \n",
      "\n",
      "2020-03-19 14:18:21 - Text-Summary - INFO: - iter: 11000 train_rouge_l_f: 0.332 test_rouge_l_f: 0.336 \n",
      "\n",
      "2020-03-19 14:35:16 - Text-Summary - INFO: - iter: 12000 train_rouge_l_f: 0.391 test_rouge_l_f: 0.644 \n",
      "\n",
      "2020-03-19 14:52:23 - Text-Summary - INFO: - iter: 13000 train_rouge_l_f: 0.291 test_rouge_l_f: 0.494 \n",
      "\n",
      "2020-03-19 15:09:49 - Text-Summary - INFO: - iter: 14000 train_rouge_l_f: 0.583 test_rouge_l_f: 0.597 \n",
      "\n",
      "2020-03-19 15:26:37 - Text-Summary - INFO: - iter: 15000 train_rouge_l_f: 0.285 test_rouge_l_f: 0.448 \n",
      "\n",
      "2020-03-19 15:44:23 - Text-Summary - INFO: - iter: 16000 train_rouge_l_f: 0.652 test_rouge_l_f: 0.569 \n",
      "\n",
      "2020-03-19 16:01:14 - Text-Summary - INFO: - iter: 17000 train_rouge_l_f: 0.357 test_rouge_l_f: 0.403 \n",
      "\n",
      "2020-03-19 16:18:32 - Text-Summary - INFO: - iter: 18000 train_rouge_l_f: 0.374 test_rouge_l_f: 0.557 \n",
      "\n",
      "2020-03-19 16:35:20 - Text-Summary - INFO: - iter: 19000 train_rouge_l_f: 0.449 test_rouge_l_f: 0.568 \n",
      "\n",
      "2020-03-19 16:52:22 - Text-Summary - INFO: - iter: 20000 train_rouge_l_f: 0.204 test_rouge_l_f: 0.524 \n",
      "\n",
      "2020-03-19 17:09:15 - Text-Summary - INFO: - iter: 21000 train_rouge_l_f: 0.456 test_rouge_l_f: 0.522 \n",
      "\n",
      "2020-03-19 17:26:32 - Text-Summary - INFO: - iter: 22000 train_rouge_l_f: 0.216 test_rouge_l_f: 0.425 \n",
      "\n",
      "2020-03-19 17:43:47 - Text-Summary - INFO: - iter: 23000 train_rouge_l_f: 0.235 test_rouge_l_f: 0.530 \n",
      "\n",
      "2020-03-19 18:01:02 - Text-Summary - INFO: - iter: 24000 train_rouge_l_f: 0.119 test_rouge_l_f: 0.590 \n",
      "\n",
      "2020-03-19 18:18:04 - Text-Summary - INFO: - iter: 25000 train_rouge_l_f: 0.408 test_rouge_l_f: 0.401 \n",
      "\n",
      "2020-03-19 18:35:21 - Text-Summary - INFO: - iter: 26000 train_rouge_l_f: 0.526 test_rouge_l_f: 0.644 \n",
      "\n",
      "2020-03-19 18:52:32 - Text-Summary - INFO: - iter: 27000 train_rouge_l_f: 0.743 test_rouge_l_f: 0.644 \n",
      "\n",
      "2020-03-19 19:09:37 - Text-Summary - INFO: - iter: 28000 train_rouge_l_f: 0.316 test_rouge_l_f: 0.502 \n",
      "\n",
      "2020-03-19 19:26:48 - Text-Summary - INFO: - iter: 29000 train_rouge_l_f: 0.207 test_rouge_l_f: 0.605 \n",
      "\n",
      "2020-03-19 19:44:20 - Text-Summary - INFO: - iter: 30000 train_rouge_l_f: 0.178 test_rouge_l_f: 0.507 \n",
      "\n",
      "2020-03-19 20:01:44 - Text-Summary - INFO: - iter: 31000 train_rouge_l_f: 0.278 test_rouge_l_f: 0.631 \n",
      "\n",
      "2020-03-19 20:18:55 - Text-Summary - INFO: - iter: 32000 train_rouge_l_f: 0.693 test_rouge_l_f: 0.631 \n",
      "\n",
      "2020-03-19 20:36:42 - Text-Summary - INFO: - iter: 33000 train_rouge_l_f: 0.320 test_rouge_l_f: 0.631 \n",
      "\n",
      "2020-03-19 20:53:37 - Text-Summary - INFO: - iter: 34000 train_rouge_l_f: 0.495 test_rouge_l_f: 0.507 \n",
      "\n",
      "2020-03-19 21:10:16 - Text-Summary - INFO: - iter: 35000 train_rouge_l_f: 0.362 test_rouge_l_f: 0.508 \n",
      "\n",
      "2020-03-19 21:27:02 - Text-Summary - INFO: - iter: 36000 train_rouge_l_f: 0.513 test_rouge_l_f: 0.548 \n",
      "\n",
      "2020-03-19 21:44:44 - Text-Summary - INFO: - iter: 37000 train_rouge_l_f: 0.591 test_rouge_l_f: 0.547 \n",
      "\n",
      "2020-03-19 22:02:28 - Text-Summary - INFO: - iter: 38000 train_rouge_l_f: 0.579 test_rouge_l_f: 0.644 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/u012869752/article/details/72513141\n",
    "# 由于在jupyter notebook中，args不为空\n",
    "from run import *\n",
    "from glob import glob\n",
    "# nvidia-smi -pm 1\n",
    "if __name__ == \"__main__\":   \n",
    "    try:\n",
    "        # --------------------------Training ----------------------------------\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--train_mle', type=bool, default=True)\n",
    "        parser.add_argument('--train_rl', type=bool, default=False)\n",
    "        parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "#         parser.add_argument('--load_model', type=str, default='/0045000_1.05_0.00.tar')\n",
    "        parser.add_argument('--load_model', type=str, default=None)\n",
    "        parser.add_argument('--new_lr', type=float, default=None)\n",
    "        parser.add_argument('--multi_device', type=bool, default=True)\n",
    "        parser.add_argument('--view', type=bool, default=True)\n",
    "        parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "        parser.add_argument('--word_emb_type', type=str, default='FastText')\n",
    "        parser.add_argument('--train_action', type=bool, default=True)\n",
    "        opt = parser.parse_args(args=[])\n",
    "        \n",
    "        today = dt.now()\n",
    "        loggerPath = \"LOG/%s-(%s_%s_%s)-(%s:%s:%s)\"%(opt.word_emb_type,\n",
    "                  today.year,today.month,today.day,\n",
    "                  today.hour,today.minute,today.second)\n",
    "\n",
    "        logger = getLogger(config.loggerName,loggerPath)   \n",
    "        \n",
    "        if opt.load_model == None:\n",
    "            shutil.rmtree('runs/Pointer-Generator/FastText', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/FastText/exp-4', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/FastText/Eecoder', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/FastText/Decoder', ignore_errors=True) # clear previous \n",
    "\n",
    "        writer = SummaryWriter('runs/Pointer-Generator/FastText/exp-4')\n",
    "#         writer = SummaryWriter('runs/Pointer-Generator/FastText')\n",
    "        writer.add_text('Train_Para/',info_str,0)\n",
    "#         write_enc_graph()\n",
    "#         write_dec_graph()\n",
    "        if opt.train_action: train_action(opt, logger, writer, train_num)\n",
    "\n",
    "    except KeyError as e:\n",
    "        traceback = sys.exc_info()[2]\n",
    "        print(sys.exc_info())\n",
    "        print(traceback.tb_lineno)\n",
    "        print(e)\n",
    "    finally:\n",
    "        removeLogger(logger)\n",
    "        \n",
    "        # export scalar data to JSON for external processing\n",
    "        # tensorboard --logdir /home/eagleuser/Users/leyan/Text-Summarizer-FOP/TensorBoard\n",
    "#         tensorboard --logdir ./runs\n",
    "#         if not os.path.exists('TensorBoard'): os.makedirs('TensorBoard')\n",
    "#         writer.export_scalars_to_json(\"TensorBoard/test.json\")\n",
    "        writer.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
