{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# from product import *\n",
    "from data_util.product import *\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "import nltk\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "# from product import *\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "CHUNK_SIZE = 1000  # num examples per chunk, for the chunked data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key word Attention DataSet 讀取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_util.config import *\n",
    "# _, category1, category2, _ = DVD_Player().getAttr()\n",
    "_,category1,category2,_ = Cameras().getAttr()\n",
    "# _,category1,category2,_ = Cell_Phones().getAttr()\n",
    "# _,category1,category2,_ = GPS().getAttr()\n",
    "# _,category1,category2,_ = Keyboards().getAttr()\n",
    "# category1,category2 = config.category1 , config.category2\n",
    "\n",
    "xlsx_path = \"XLSX/category/%s_%s_key.xlsx\"%(category1,category2)\n",
    "# df.to_csv(csv_path) #默认dt是DataFrame的一个实例，参数解释如下\n",
    "# key_train_df.to_excel(csv_path, encoding='utf8')\n",
    "orign_key_df = pd.read_excel(xlsx_path)\n",
    "print(xlsx_path + \" Read finished\")\n",
    "len(orign_key_df)\n",
    "\n",
    "orign_key_df['bert_review'] = '' \n",
    "orign_key_df['bert_summary'] = ''\n",
    "orign_key_df['bert_review_len'] = 0\n",
    "orign_key_df['bert_summary_len'] = 0\n",
    "\n",
    "orign_key_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key word load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'FOP-View/%s_%s_keywords2.txt' % (category1, category2)\n",
    "print('load %s keywords...' % (fn))\n",
    "total_keywords = set()\n",
    "with open(fn, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        k, v = line.split(\":\")\n",
    "        total_keywords.add(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_lexicon = {}\n",
    "for filename in os.listdir('opinion-lexicon-English/'):      \n",
    "    if \"txt\" not in filename: continue\n",
    "    print(filename)\n",
    "    with open('opinion-lexicon-English/'+filename,'r') as f_input:\n",
    "        lexion = []\n",
    "        for line in f_input:\n",
    "            if line.startswith(\";\"):\n",
    "                continue\n",
    "            word = line.replace(\"\\n\",\"\")\n",
    "            if word != \"\" : lexion.append(word)\n",
    "        pos = filename.replace(\".txt\",\"\")\n",
    "        opinion_lexicon[pos] = lexion\n",
    "\n",
    "opinion_lexicon[\"total-words\"] = opinion_lexicon[\"negative-words\"] + opinion_lexicon[\"positive-words\"]\n",
    "print(\"total-words 已取得\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 裡頭的字典資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "vocab = tokenizer.vocab # word_to_id\n",
    "print(\"字典大小：\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert-Summary 資料清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def isnumber(aString):\n",
    "    try:\n",
    "        float(aString)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def create_custom_tokenizer(nlp):\n",
    "    prefix_re = re.compile(r'[0-9]\\.')\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search)\n",
    "\n",
    "nlp.tokenizer = create_custom_tokenizer(nlp)\n",
    "\n",
    "alphbet_stopword = ['b','c','d','e','f','g','h','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_summary(x):\n",
    "    #x = x.replace(\"\\n\",\" \")\n",
    "    x = x.replace(\"\\n\",\"\").replace(\"</s>\",\"\").replace(\"<s>\",\"\")\n",
    "    x = \"<s>\" + x + \"</s>\"  \n",
    "    tokens = [str(token) for token in x.split(\" \") if (\" \" not in str(token)) and \\\n",
    "                   #(str(token).isalpha()) and \\\n",
    "                  (len(str(token)) > 1)   ]\n",
    "    # print(tokens)\n",
    "    #return \" \".join(tokens),tokens\n",
    "    \n",
    "    newtokens = []\n",
    "    for token in tokens:\n",
    "        if (isnumber(token) or len(token) == 1 or token == \".\") and (token not in alphbet_stopword):\n",
    "            newtokens.append(token)\n",
    "        else:\n",
    "            token = token.replace(\".\",\" . \")\n",
    "            sub_tokens = token.split(\" \")\n",
    "            sub_tokens = [t for t in sub_tokens if t != \"\" and t not in alphbet_stopword]\n",
    "            newtokens.extend(sub_tokens)\n",
    "    \n",
    "#     dot_tokens = [token for token in newtokens if (token[0] == \".\" or token[-1] == \".\") and (len(token)>1)]\n",
    "#     if len(dot_tokens) > 0 :print(dot_tokens)\n",
    "        \n",
    "    return \" \".join(newtokens).replace(' . . ',' . '),newtokens\n",
    "    \n",
    "\n",
    "def bert_compose_summary(newtokens):\n",
    "#     x = x.replace(\"\\n\",\"\").replace(\"</s>\",\"\").replace(\"<s>\",\"\")\n",
    "#     x = \"<s>\" + x + \"</s>\"  \n",
    "#     tokens = [str(token) for token in nlp(x) if (\" \" not in str(token)) and \\\n",
    "#                   (str(token).isalpha()) and \\\n",
    "#                   (len(str(token)) > 1)   ]\n",
    "    \n",
    "#     newtokens = []\n",
    "#     for token in tokens:\n",
    "#         if (isnumber(token) or len(token) == 1 or token == \".\") and (token not in alphbet_stopword):\n",
    "#             newtokens.append(token)\n",
    "#         else:\n",
    "#             token = token.replace(\".\",\" . \")\n",
    "#             sub_tokens = token.split(\" \")\n",
    "#             sub_tokens = [t for t in sub_tokens if t != \"\" and t not in alphbet_stopword]\n",
    "#             newtokens.extend(sub_tokens)\n",
    "\n",
    "#     dot_tokens = [t for t in newtokens if (\".\" in t) and (len(t) > 1) ]\n",
    "#     if len(dot_tokens) > 0 :print(dot_tokens)\n",
    "    newtokens = [t for t in newtokens if t not in [\"<s>\",\"</s>\"]]\n",
    "    newtokens = ['[CLS]'] + tokenizer.tokenize(\" \".join(newtokens)) + ['[SEP]']\n",
    "    return \" \".join(newtokens)\n",
    "\n",
    "'''\n",
    "def calc_summary_len(x):\n",
    "#     tokens = [token for token in nlp(x)]\n",
    "#     print(tokens)\n",
    "#     print([len(t) for t in tokens])\n",
    "#     return len(tokens)\n",
    "    return len(x.split(\" \"))\n",
    "\n",
    "nlp.tokenizer = create_custom_tokenizer(nlp)\n",
    "\n",
    "orign_key_df['lemm_summary'] = orign_key_df['lemm_summary'].apply(compose_summary)\n",
    "orign_key_df['lemm_summary_len'] = orign_key_df['lemm_summary'].apply(calc_summary_len)\n",
    "\n",
    "\n",
    "amount = len(orign_key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "\n",
    "orign_key_df.head()\n",
    "'''\n",
    "orign_key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 154488/154488 [1:45:58<00:00, 24.30it/s]Total data : 154488\n\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_ID</th>\n      <th>review</th>\n      <th>summary</th>\n      <th>big_categories</th>\n      <th>small_categories</th>\n      <th>lemm_review</th>\n      <th>lemm_summary</th>\n      <th>lemm_review_len</th>\n      <th>lemm_summary_len</th>\n      <th>overall</th>\n      <th>vote</th>\n      <th>total_keyword</th>\n      <th>FOP_sents</th>\n      <th>total_mention_features</th>\n      <th>bert_review</th>\n      <th>bert_summary</th>\n      <th>bert_review_len</th>\n      <th>bert_summary_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1356566400</td>\n      <td>this book was almost as small a pamphlet for 2...</td>\n      <td>too small</td>\n      <td>Electronics</td>\n      <td>Camera Cases</td>\n      <td>['this book was almost as small a pamphlet for...</td>\n      <td>&lt;s&gt; too small &lt;/s&gt;</td>\n      <td>44</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>intend small book pamphlet gift expect emphasi...</td>\n      <td></td>\n      <td>[CLS] too small [SEP]</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1325721600</td>\n      <td>this book is a collectors item and i love they...</td>\n      <td>wonderful book a keepsake</td>\n      <td>Electronics</td>\n      <td>Camera Cases</td>\n      <td>['this book is a collector item and i love the...</td>\n      <td>&lt;s&gt; wonderful book keepsake &lt;/s&gt;</td>\n      <td>53</td>\n      <td>5</td>\n      <td>5</td>\n      <td>9</td>\n      <td>item wonderful,quality item,nostalgia wonderfu...</td>\n      <td>great quality though and a wonderful nostalgia...</td>\n      <td>item collector book price copy high buy price ...</td>\n      <td></td>\n      <td>[CLS] wonderful book keeps ##ake [SEP]</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>951782400</td>\n      <td>i would like to say the the cannon elph37oz is...</td>\n      <td>elph 37oz is the best</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>['i would like to say the the cannon elph37 oz...</td>\n      <td>&lt;s&gt; elph 37 oz is the best &lt;/s&gt;</td>\n      <td>58</td>\n      <td>8</td>\n      <td>5</td>\n      <td>51</td>\n      <td>camera inferior,money extra,camera cheap</td>\n      <td>pay the extra money and get this one rather th...</td>\n      <td>cannon money camera buy decide camera picture ...</td>\n      <td></td>\n      <td>[CLS] el ##ph 37 oz is the best [SEP]</td>\n      <td>0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>985910400</td>\n      <td>i have had and used q 370z for over a year and...</td>\n      <td>a good all around aps camera</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>['i have have and use q 370z for over a year a...</td>\n      <td>&lt;s&gt; good all around aps camera &lt;/s&gt;</td>\n      <td>137</td>\n      <td>7</td>\n      <td>5</td>\n      <td>13</td>\n      <td>eye red,light low room increase lense mm,slrs ...</td>\n      <td>yes i sometimes get red eye in low light but a...</td>\n      <td>snapshot photographer year low lens red avoid ...</td>\n      <td></td>\n      <td>[CLS] good all around ap ##s camera [SEP]</td>\n      <td>0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>967075200</td>\n      <td>i used this camera once and it took relatively...</td>\n      <td>terrible camera worse customer service</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>['i use this camera once and it take relativel...</td>\n      <td>&lt;s&gt; terrible camera worse customer service &lt;/s&gt;</td>\n      <td>96</td>\n      <td>7</td>\n      <td>1</td>\n      <td>48</td>\n      <td>memory internal camera service camera send sty...</td>\n      <td>however on my next trip the camera s internal ...</td>\n      <td>camera picture camera trip memory service comp...</td>\n      <td></td>\n      <td>[CLS] terrible camera worse customer service [...</td>\n      <td>0</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    review_ID                                             review  \\\n0  1356566400  this book was almost as small a pamphlet for 2...   \n1  1325721600  this book is a collectors item and i love they...   \n2   951782400  i would like to say the the cannon elph37oz is...   \n3   985910400  i have had and used q 370z for over a year and...   \n4   967075200  i used this camera once and it took relatively...   \n\n                                  summary big_categories small_categories  \\\n0                               too small    Electronics     Camera Cases   \n1               wonderful book a keepsake    Electronics     Camera Cases   \n2                   elph 37oz is the best    Electronics      APS Cameras   \n3            a good all around aps camera    Electronics      APS Cameras   \n4  terrible camera worse customer service    Electronics      APS Cameras   \n\n                                         lemm_review  \\\n0  ['this book was almost as small a pamphlet for...   \n1  ['this book is a collector item and i love the...   \n2  ['i would like to say the the cannon elph37 oz...   \n3  ['i have have and use q 370z for over a year a...   \n4  ['i use this camera once and it take relativel...   \n\n                                      lemm_summary  lemm_review_len  \\\n0                               <s> too small </s>               44   \n1                 <s> wonderful book keepsake </s>               53   \n2                  <s> elph 37 oz is the best </s>               58   \n3              <s> good all around aps camera </s>              137   \n4  <s> terrible camera worse customer service </s>               96   \n\n   lemm_summary_len  overall  vote  \\\n0                 4        1     2   \n1                 5        5     9   \n2                 8        5    51   \n3                 7        5    13   \n4                 7        1    48   \n\n                                       total_keyword  \\\n0                                                NaN   \n1  item wonderful,quality item,nostalgia wonderfu...   \n2           camera inferior,money extra,camera cheap   \n3  eye red,light low room increase lense mm,slrs ...   \n4  memory internal camera service camera send sty...   \n\n                                           FOP_sents  \\\n0                                                NaN   \n1  great quality though and a wonderful nostalgia...   \n2  pay the extra money and get this one rather th...   \n3  yes i sometimes get red eye in low light but a...   \n4  however on my next trip the camera s internal ...   \n\n                              total_mention_features bert_review  \\\n0  intend small book pamphlet gift expect emphasi...               \n1  item collector book price copy high buy price ...               \n2  cannon money camera buy decide camera picture ...               \n3  snapshot photographer year low lens red avoid ...               \n4  camera picture camera trip memory service comp...               \n\n                                        bert_summary  bert_review_len  \\\n0                              [CLS] too small [SEP]                0   \n1             [CLS] wonderful book keeps ##ake [SEP]                0   \n2              [CLS] el ##ph 37 oz is the best [SEP]                0   \n3          [CLS] good all around ap ##s camera [SEP]                0   \n4  [CLS] terrible camera worse customer service [...                0   \n\n   bert_summary_len  \n0                 4  \n1                 6  \n2                 9  \n3                 8  \n4                 7  "
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 非符號alpha word重疊數\n",
    "with tqdm(total=len(orign_key_df)) as pbar:\n",
    "    for i ,row in orign_key_df.iterrows():       \n",
    "        lemm_summary,newtokens = compose_summary(row['lemm_summary'])\n",
    "        bert_summary = bert_compose_summary(newtokens)\n",
    "        \n",
    "        orign_key_df.loc[i,'lemm_summary'] = lemm_summary\n",
    "        orign_key_df.loc[i,'bert_summary'] = bert_summary\n",
    "        \n",
    "        orign_key_df.loc[i,'lemm_summary_len'] = len(lemm_summary.split(\" \"))       \n",
    "        orign_key_df.loc[i,'bert_summary_len'] = len(bert_summary.split(\" \"))\n",
    "\n",
    "        pbar.update(1)\n",
    "        \n",
    "amount = len(orign_key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "orign_key_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert-review 多句合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "def compose_review(x):\n",
    "    x = eval(x)\n",
    "    x = \"\\n\".join(x)\n",
    "    x = x.replace(\".\\n\",\" . \").replace(\"\\n.\",\" . \").replace(\"\\n\",\" \")\n",
    "#     x = x.replace(\"\\n\",\" \")    \n",
    "    tokens = [str(token) for token in x.split(\" \") if (\" \" not in str(token))and (str(token) == '.' or str(token).isalpha())]\n",
    "    #tokens = [str(token) for token in x.split(\" \") if (\" \" not in str(token)) ]\n",
    "\n",
    "    #return \" \".join(tokens)\n",
    "    \n",
    "    newtokens = []\n",
    "    for token in tokens:\n",
    "#         if (len(token) == 1 or token == \".\"):\n",
    "        if (isnumber(token) or len(token) == 1 or token == \".\")and (token not in alphbet_stopword):\n",
    "            newtokens.append(token)\n",
    "#             if (token not in ['a','i']) and (token != \".\"): print(token)\n",
    "        else:\n",
    "#             token = token.replace(\".\",\" . \")\n",
    "            token = token.replace(\".\",\" . \")\n",
    "            sub_tokens = token.split(\" \")\n",
    "            sub_tokens = [t for t in sub_tokens if t != \"\" and t not in alphbet_stopword]\n",
    "            if len(sub_tokens) == 0: continue\n",
    "            newtokens.extend(sub_tokens)\n",
    "\n",
    "    #dot_tokens = [token for token in newtokens if (token[0] == \".\" or token[-1] == \".\") and (len(token)>1)]\n",
    "    #if len(dot_tokens) > 0 :print(dot_tokens)\n",
    "    \n",
    "    return \" \".join(newtokens).replace(' . . ',' . ')\n",
    "\n",
    "\n",
    "'''\n",
    "def calc_review_len(x):\n",
    "#     tokens = [token for token in nlp(x)]\n",
    "#     print(tokens)\n",
    "#     print([len(t) for t in tokens])\n",
    "#     return len(tokens)\n",
    "    return len(x.split(\" \"))\n",
    "key_df = deepcopy(orign_key_df)\n",
    "key_df['lemm_review'] = key_df['lemm_review'].apply(compose_review)\n",
    "key_df['lemm_review_len'] = key_df['lemm_review'].apply(calc_review_len)\n",
    "key_df.head()\n",
    "'''\n",
    "\n",
    "def bert_compose_review(x):\n",
    "    x = eval(x)\n",
    "    review_sents = deepcopy(x)\n",
    "    total_tokens = []\n",
    "    for sent in review_sents:\n",
    "        sent = sent.replace('\\n','[SEP]')\n",
    "        tokens = [str(token) for token in sent.split(\" \") if (\" \" not in str(token)) ]\n",
    "        newtokens = []\n",
    "        for token in tokens:\n",
    "            if (isnumber(token) or len(token) == 1 or token == \".\")and (token not in alphbet_stopword):\n",
    "                newtokens.append(token)\n",
    "            else:\n",
    "                token = token.replace(\".\",\" . \")\n",
    "                sub_tokens = token.split(\" \")\n",
    "                sub_tokens = [t for t in sub_tokens if t != \"\" and t not in alphbet_stopword]\n",
    "                newtokens.extend(sub_tokens)\n",
    "        newtokens = newtokens + ['[SEP]']        \n",
    "        total_tokens.extend(newtokens)\n",
    "    total_tokens = ['[CLS]'] + tokenizer.tokenize(\" \".join(total_tokens))    \n",
    "    return \" \".join(total_tokens)\n",
    "'''\n",
    "# def compose_review(bert_review):\n",
    "#     return bert_review.replace('[CLS] ','').replace('[SEP] ','')\n",
    "'''\n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_ID</th>\n      <th>review</th>\n      <th>summary</th>\n      <th>big_categories</th>\n      <th>small_categories</th>\n      <th>lemm_review</th>\n      <th>lemm_summary</th>\n      <th>lemm_review_len</th>\n      <th>lemm_summary_len</th>\n      <th>overall</th>\n      <th>vote</th>\n      <th>total_keyword</th>\n      <th>FOP_sents</th>\n      <th>total_mention_features</th>\n      <th>bert_review</th>\n      <th>bert_summary</th>\n      <th>bert_review_len</th>\n      <th>bert_summary_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1356566400</td>\n      <td>this book was almost as small a pamphlet for 2...</td>\n      <td>too small</td>\n      <td>Electronics</td>\n      <td>Camera Cases</td>\n      <td>['this book was almost as small a pamphlet for...</td>\n      <td>&lt;s&gt; too small &lt;/s&gt;</td>\n      <td>44</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>intend small book pamphlet gift expect emphasi...</td>\n      <td></td>\n      <td>[CLS] too small [SEP]</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1325721600</td>\n      <td>this book is a collectors item and i love they...</td>\n      <td>wonderful book a keepsake</td>\n      <td>Electronics</td>\n      <td>Camera Cases</td>\n      <td>['this book is a collector item and i love the...</td>\n      <td>&lt;s&gt; wonderful book keepsake &lt;/s&gt;</td>\n      <td>53</td>\n      <td>5</td>\n      <td>5</td>\n      <td>9</td>\n      <td>item wonderful,quality item,nostalgia wonderfu...</td>\n      <td>great quality though and a wonderful nostalgia...</td>\n      <td>item collector book price copy high buy price ...</td>\n      <td></td>\n      <td>[CLS] wonderful book keeps ##ake [SEP]</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>951782400</td>\n      <td>i would like to say the the cannon elph37oz is...</td>\n      <td>elph 37oz is the best</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>['i would like to say the the cannon elph37 oz...</td>\n      <td>&lt;s&gt; elph 37 oz is the best &lt;/s&gt;</td>\n      <td>58</td>\n      <td>8</td>\n      <td>5</td>\n      <td>51</td>\n      <td>camera inferior,money extra,camera cheap</td>\n      <td>pay the extra money and get this one rather th...</td>\n      <td>cannon money camera buy decide camera picture ...</td>\n      <td></td>\n      <td>[CLS] el ##ph 37 oz is the best [SEP]</td>\n      <td>0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>985910400</td>\n      <td>i have had and used q 370z for over a year and...</td>\n      <td>a good all around aps camera</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>['i have have and use q 370z for over a year a...</td>\n      <td>&lt;s&gt; good all around aps camera &lt;/s&gt;</td>\n      <td>137</td>\n      <td>7</td>\n      <td>5</td>\n      <td>13</td>\n      <td>eye red,light low room increase lense mm,slrs ...</td>\n      <td>yes i sometimes get red eye in low light but a...</td>\n      <td>snapshot photographer year low lens red avoid ...</td>\n      <td></td>\n      <td>[CLS] good all around ap ##s camera [SEP]</td>\n      <td>0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>967075200</td>\n      <td>i used this camera once and it took relatively...</td>\n      <td>terrible camera worse customer service</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>['i use this camera once and it take relativel...</td>\n      <td>&lt;s&gt; terrible camera worse customer service &lt;/s&gt;</td>\n      <td>96</td>\n      <td>7</td>\n      <td>1</td>\n      <td>48</td>\n      <td>memory internal camera service camera send sty...</td>\n      <td>however on my next trip the camera s internal ...</td>\n      <td>camera picture camera trip memory service comp...</td>\n      <td></td>\n      <td>[CLS] terrible camera worse customer service [...</td>\n      <td>0</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    review_ID                                             review  \\\n0  1356566400  this book was almost as small a pamphlet for 2...   \n1  1325721600  this book is a collectors item and i love they...   \n2   951782400  i would like to say the the cannon elph37oz is...   \n3   985910400  i have had and used q 370z for over a year and...   \n4   967075200  i used this camera once and it took relatively...   \n\n                                  summary big_categories small_categories  \\\n0                               too small    Electronics     Camera Cases   \n1               wonderful book a keepsake    Electronics     Camera Cases   \n2                   elph 37oz is the best    Electronics      APS Cameras   \n3            a good all around aps camera    Electronics      APS Cameras   \n4  terrible camera worse customer service    Electronics      APS Cameras   \n\n                                         lemm_review  \\\n0  ['this book was almost as small a pamphlet for...   \n1  ['this book is a collector item and i love the...   \n2  ['i would like to say the the cannon elph37 oz...   \n3  ['i have have and use q 370z for over a year a...   \n4  ['i use this camera once and it take relativel...   \n\n                                      lemm_summary  lemm_review_len  \\\n0                               <s> too small </s>               44   \n1                 <s> wonderful book keepsake </s>               53   \n2                  <s> elph 37 oz is the best </s>               58   \n3              <s> good all around aps camera </s>              137   \n4  <s> terrible camera worse customer service </s>               96   \n\n   lemm_summary_len  overall  vote  \\\n0                 4        1     2   \n1                 5        5     9   \n2                 8        5    51   \n3                 7        5    13   \n4                 7        1    48   \n\n                                       total_keyword  \\\n0                                                NaN   \n1  item wonderful,quality item,nostalgia wonderfu...   \n2           camera inferior,money extra,camera cheap   \n3  eye red,light low room increase lense mm,slrs ...   \n4  memory internal camera service camera send sty...   \n\n                                           FOP_sents  \\\n0                                                NaN   \n1  great quality though and a wonderful nostalgia...   \n2  pay the extra money and get this one rather th...   \n3  yes i sometimes get red eye in low light but a...   \n4  however on my next trip the camera s internal ...   \n\n                              total_mention_features bert_review  \\\n0  intend small book pamphlet gift expect emphasi...               \n1  item collector book price copy high buy price ...               \n2  cannon money camera buy decide camera picture ...               \n3  snapshot photographer year low lens red avoid ...               \n4  camera picture camera trip memory service comp...               \n\n                                        bert_summary  bert_review_len  \\\n0                              [CLS] too small [SEP]                0   \n1             [CLS] wonderful book keeps ##ake [SEP]                0   \n2              [CLS] el ##ph 37 oz is the best [SEP]                0   \n3          [CLS] good all around ap ##s camera [SEP]                0   \n4  [CLS] terrible camera worse customer service [...                0   \n\n   bert_summary_len  \n0                 4  \n1                 6  \n2                 9  \n3                 8  \n4                 7  "
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orign_key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 154488/154488 [2:32:05<00:00, 16.93it/s]Total data : 154488\n\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_ID</th>\n      <th>review</th>\n      <th>summary</th>\n      <th>big_categories</th>\n      <th>small_categories</th>\n      <th>lemm_review</th>\n      <th>lemm_summary</th>\n      <th>lemm_review_len</th>\n      <th>lemm_summary_len</th>\n      <th>overall</th>\n      <th>vote</th>\n      <th>total_keyword</th>\n      <th>FOP_sents</th>\n      <th>total_mention_features</th>\n      <th>bert_review</th>\n      <th>bert_summary</th>\n      <th>bert_review_len</th>\n      <th>bert_summary_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1356566400</td>\n      <td>this book was almost as small a pamphlet for 2...</td>\n      <td>too small</td>\n      <td>Electronics</td>\n      <td>Camera Cases</td>\n      <td>this book was almost as small a pamphlet for i...</td>\n      <td>&lt;s&gt; too small &lt;/s&gt;</td>\n      <td>46</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>intend small book pamphlet gift expect emphasi...</td>\n      <td>[CLS] this book was almost as small a pamphlet...</td>\n      <td>[CLS] too small [SEP]</td>\n      <td>51</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1325721600</td>\n      <td>this book is a collectors item and i love they...</td>\n      <td>wonderful book a keepsake</td>\n      <td>Electronics</td>\n      <td>Camera Cases</td>\n      <td>this book is a collector item and i love they ...</td>\n      <td>&lt;s&gt; wonderful book keepsake &lt;/s&gt;</td>\n      <td>56</td>\n      <td>5</td>\n      <td>5</td>\n      <td>9</td>\n      <td>item wonderful,quality item,nostalgia wonderfu...</td>\n      <td>great quality though and a wonderful nostalgia...</td>\n      <td>item collector book price copy high buy price ...</td>\n      <td>[CLS] this book is a collector item and i love...</td>\n      <td>[CLS] wonderful book keeps ##ake [SEP]</td>\n      <td>64</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>951782400</td>\n      <td>i would like to say the the cannon elph37oz is...</td>\n      <td>elph 37oz is the best</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>i would like to say the the cannon oz is worth...</td>\n      <td>&lt;s&gt; elph 37 oz is the best &lt;/s&gt;</td>\n      <td>59</td>\n      <td>8</td>\n      <td>5</td>\n      <td>51</td>\n      <td>camera inferior,money extra,camera cheap</td>\n      <td>pay the extra money and get this one rather th...</td>\n      <td>cannon money camera buy decide camera picture ...</td>\n      <td>[CLS] i would like to say the the cannon el ##...</td>\n      <td>[CLS] el ##ph 37 oz is the best [SEP]</td>\n      <td>69</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>985910400</td>\n      <td>i have had and used q 370z for over a year and...</td>\n      <td>a good all around aps camera</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>i have have and use for over a year and i have...</td>\n      <td>&lt;s&gt; good all around aps camera &lt;/s&gt;</td>\n      <td>135</td>\n      <td>7</td>\n      <td>5</td>\n      <td>13</td>\n      <td>eye red,light low room increase lense mm,slrs ...</td>\n      <td>yes i sometimes get red eye in low light but a...</td>\n      <td>snapshot photographer year low lens red avoid ...</td>\n      <td>[CLS] i have have and use 370 ##z for over a y...</td>\n      <td>[CLS] good all around ap ##s camera [SEP]</td>\n      <td>160</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>967075200</td>\n      <td>i used this camera once and it took relatively...</td>\n      <td>terrible camera worse customer service</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>i use this camera once and it take relatively ...</td>\n      <td>&lt;s&gt; terrible camera worse customer service &lt;/s&gt;</td>\n      <td>99</td>\n      <td>7</td>\n      <td>1</td>\n      <td>48</td>\n      <td>memory internal camera service camera send sty...</td>\n      <td>however on my next trip the camera s internal ...</td>\n      <td>camera picture camera trip memory service comp...</td>\n      <td>[CLS] i use this camera once and it take relat...</td>\n      <td>[CLS] terrible camera worse customer service [...</td>\n      <td>110</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    review_ID                                             review  \\\n0  1356566400  this book was almost as small a pamphlet for 2...   \n1  1325721600  this book is a collectors item and i love they...   \n2   951782400  i would like to say the the cannon elph37oz is...   \n3   985910400  i have had and used q 370z for over a year and...   \n4   967075200  i used this camera once and it took relatively...   \n\n                                  summary big_categories small_categories  \\\n0                               too small    Electronics     Camera Cases   \n1               wonderful book a keepsake    Electronics     Camera Cases   \n2                   elph 37oz is the best    Electronics      APS Cameras   \n3            a good all around aps camera    Electronics      APS Cameras   \n4  terrible camera worse customer service    Electronics      APS Cameras   \n\n                                         lemm_review  \\\n0  this book was almost as small a pamphlet for i...   \n1  this book is a collector item and i love they ...   \n2  i would like to say the the cannon oz is worth...   \n3  i have have and use for over a year and i have...   \n4  i use this camera once and it take relatively ...   \n\n                                      lemm_summary  lemm_review_len  \\\n0                               <s> too small </s>               46   \n1                 <s> wonderful book keepsake </s>               56   \n2                  <s> elph 37 oz is the best </s>               59   \n3              <s> good all around aps camera </s>              135   \n4  <s> terrible camera worse customer service </s>               99   \n\n   lemm_summary_len  overall  vote  \\\n0                 4        1     2   \n1                 5        5     9   \n2                 8        5    51   \n3                 7        5    13   \n4                 7        1    48   \n\n                                       total_keyword  \\\n0                                                NaN   \n1  item wonderful,quality item,nostalgia wonderfu...   \n2           camera inferior,money extra,camera cheap   \n3  eye red,light low room increase lense mm,slrs ...   \n4  memory internal camera service camera send sty...   \n\n                                           FOP_sents  \\\n0                                                NaN   \n1  great quality though and a wonderful nostalgia...   \n2  pay the extra money and get this one rather th...   \n3  yes i sometimes get red eye in low light but a...   \n4  however on my next trip the camera s internal ...   \n\n                              total_mention_features  \\\n0  intend small book pamphlet gift expect emphasi...   \n1  item collector book price copy high buy price ...   \n2  cannon money camera buy decide camera picture ...   \n3  snapshot photographer year low lens red avoid ...   \n4  camera picture camera trip memory service comp...   \n\n                                         bert_review  \\\n0  [CLS] this book was almost as small a pamphlet...   \n1  [CLS] this book is a collector item and i love...   \n2  [CLS] i would like to say the the cannon el ##...   \n3  [CLS] i have have and use 370 ##z for over a y...   \n4  [CLS] i use this camera once and it take relat...   \n\n                                        bert_summary  bert_review_len  \\\n0                              [CLS] too small [SEP]               51   \n1             [CLS] wonderful book keeps ##ake [SEP]               64   \n2              [CLS] el ##ph 37 oz is the best [SEP]               69   \n3          [CLS] good all around ap ##s camera [SEP]              160   \n4  [CLS] terrible camera worse customer service [...              110   \n\n   bert_summary_len  \n0                 4  \n1                 6  \n2                 9  \n3                 8  \n4                 7  "
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_df = deepcopy(orign_key_df)\n",
    "\n",
    "from tqdm import tqdm\n",
    "# 非符號alpha word重疊數\n",
    "with tqdm(total=len(key_df)) as pbar:\n",
    "    for i ,row in key_df.iterrows():       \n",
    "        bert_review = bert_compose_review(row['lemm_review'])\n",
    "        lemm_review = compose_review(row['lemm_review'])\n",
    "        \n",
    "        key_df.loc[i,'bert_review'] = bert_review\n",
    "        key_df.loc[i,'lemm_review'] = lemm_review\n",
    "        \n",
    "        key_df.loc[i,'bert_review_len'] = len(bert_review.split(\" \"))\n",
    "        key_df.loc[i,'lemm_review_len'] = len(lemm_review.split(\" \"))\n",
    "        \n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "amount = len(key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "\n",
    "key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(text):\n",
    "    keywords = set()\n",
    "    for words in text.split(\",\"):\n",
    "        for word in words.split(\" \"):\n",
    "            keywords.add(word)\n",
    "    keywords = \" \".join(keywords)\n",
    "    return keywords\n",
    "\n",
    "def calc_keyword_num(x):\n",
    "    return len(x.split(\" \"))\n",
    "\n",
    "# and(key_df.lemm_review_len>20)\n",
    "flit_key_df = key_df[(key_df.lemm_summary_len>=4) ] # 過濾single word summary\n",
    "flit_key_df = flit_key_df[(flit_key_df.lemm_review_len <= 1000) ] # 過濾single word summary\n",
    "flit_key_df = flit_key_df[(flit_key_df.lemm_review_len >= 50) ] # 過濾single word summary\n",
    "\n",
    "flit_key_df = flit_key_df.dropna(\n",
    "    axis=0,     # 0: 对行进行操作; 1: 对列进行操作\n",
    "    how='any'   # 'any': 只要存在 NaN 就 drop 掉; 'all': 必须全部是 NaN 才 drop \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 過濾不合適的訓練資料"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOP_keywords 資料整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_ID</th>\n      <th>review</th>\n      <th>summary</th>\n      <th>big_categories</th>\n      <th>small_categories</th>\n      <th>lemm_review</th>\n      <th>lemm_summary</th>\n      <th>lemm_review_len</th>\n      <th>lemm_summary_len</th>\n      <th>overall</th>\n      <th>vote</th>\n      <th>total_keyword</th>\n      <th>FOP_sents</th>\n      <th>total_mention_features</th>\n      <th>bert_review</th>\n      <th>bert_summary</th>\n      <th>bert_review_len</th>\n      <th>bert_summary_len</th>\n      <th>FOP_keywords</th>\n      <th>FOP_keywords_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1325721600</td>\n      <td>this book is a collectors item and i love they...</td>\n      <td>wonderful book a keepsake</td>\n      <td>Electronics</td>\n      <td>Camera Cases</td>\n      <td>this book is a collector item and i love they ...</td>\n      <td>&lt;s&gt; wonderful book keepsake &lt;/s&gt;</td>\n      <td>56</td>\n      <td>5</td>\n      <td>5</td>\n      <td>9</td>\n      <td>item wonderful,quality item,nostalgia wonderfu...</td>\n      <td>great quality though and a wonderful nostalgia...</td>\n      <td>item collector book price copy high buy price ...</td>\n      <td>[CLS] this book is a collector item and i love...</td>\n      <td>[CLS] wonderful book keeps ##ake [SEP]</td>\n      <td>64</td>\n      <td>6</td>\n      <td>nostalgia wonderful quality great item</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>951782400</td>\n      <td>i would like to say the the cannon elph37oz is...</td>\n      <td>elph 37oz is the best</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>i would like to say the the cannon oz is worth...</td>\n      <td>&lt;s&gt; elph 37 oz is the best &lt;/s&gt;</td>\n      <td>59</td>\n      <td>8</td>\n      <td>5</td>\n      <td>51</td>\n      <td>camera inferior,money extra,camera cheap</td>\n      <td>pay the extra money and get this one rather th...</td>\n      <td>cannon money camera buy decide camera picture ...</td>\n      <td>[CLS] i would like to say the the cannon el ##...</td>\n      <td>[CLS] el ##ph 37 oz is the best [SEP]</td>\n      <td>69</td>\n      <td>9</td>\n      <td>inferior camera extra cheap money</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>985910400</td>\n      <td>i have had and used q 370z for over a year and...</td>\n      <td>a good all around aps camera</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>i have have and use for over a year and i have...</td>\n      <td>&lt;s&gt; good all around aps camera &lt;/s&gt;</td>\n      <td>135</td>\n      <td>7</td>\n      <td>5</td>\n      <td>13</td>\n      <td>eye red,light low room increase lense mm,slrs ...</td>\n      <td>yes i sometimes get red eye in low light but a...</td>\n      <td>snapshot photographer year low lens red avoid ...</td>\n      <td>[CLS] i have have and use 370 ##z for over a y...</td>\n      <td>[CLS] good all around ap ##s camera [SEP]</td>\n      <td>160</td>\n      <td>8</td>\n      <td>room eye increase light lense mm red slrs low</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>967075200</td>\n      <td>i used this camera once and it took relatively...</td>\n      <td>terrible camera worse customer service</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>i use this camera once and it take relatively ...</td>\n      <td>&lt;s&gt; terrible camera worse customer service &lt;/s&gt;</td>\n      <td>99</td>\n      <td>7</td>\n      <td>1</td>\n      <td>48</td>\n      <td>memory internal camera service camera send sty...</td>\n      <td>however on my next trip the camera s internal ...</td>\n      <td>camera picture camera trip memory service comp...</td>\n      <td>[CLS] i use this camera once and it take relat...</td>\n      <td>[CLS] terrible camera worse customer service [...</td>\n      <td>110</td>\n      <td>7</td>\n      <td>internal camera stylus olympus send service me...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>999388800</td>\n      <td>if you are considering a regular canon elph go...</td>\n      <td>great purchase</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>if you are consider a regular canon elph go wi...</td>\n      <td>&lt;s&gt; great purchase &lt;/s&gt;</td>\n      <td>133</td>\n      <td>4</td>\n      <td>5</td>\n      <td>15</td>\n      <td>elph regular,zooming powerful camera lightweig...</td>\n      <td>if you are consider a regular canon elph go wi...</td>\n      <td>canon zooming extra elph small print lightweig...</td>\n      <td>[CLS] if you are consider a regular canon el #...</td>\n      <td>[CLS] great purchase [SEP]</td>\n      <td>149</td>\n      <td>4</td>\n      <td>postcard professional camera elph small lightw...</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    review_ID                                             review  \\\n1  1325721600  this book is a collectors item and i love they...   \n2   951782400  i would like to say the the cannon elph37oz is...   \n3   985910400  i have had and used q 370z for over a year and...   \n4   967075200  i used this camera once and it took relatively...   \n5   999388800  if you are considering a regular canon elph go...   \n\n                                  summary big_categories small_categories  \\\n1               wonderful book a keepsake    Electronics     Camera Cases   \n2                   elph 37oz is the best    Electronics      APS Cameras   \n3            a good all around aps camera    Electronics      APS Cameras   \n4  terrible camera worse customer service    Electronics      APS Cameras   \n5                          great purchase    Electronics      APS Cameras   \n\n                                         lemm_review  \\\n1  this book is a collector item and i love they ...   \n2  i would like to say the the cannon oz is worth...   \n3  i have have and use for over a year and i have...   \n4  i use this camera once and it take relatively ...   \n5  if you are consider a regular canon elph go wi...   \n\n                                      lemm_summary  lemm_review_len  \\\n1                 <s> wonderful book keepsake </s>               56   \n2                  <s> elph 37 oz is the best </s>               59   \n3              <s> good all around aps camera </s>              135   \n4  <s> terrible camera worse customer service </s>               99   \n5                          <s> great purchase </s>              133   \n\n   lemm_summary_len  overall  vote  \\\n1                 5        5     9   \n2                 8        5    51   \n3                 7        5    13   \n4                 7        1    48   \n5                 4        5    15   \n\n                                       total_keyword  \\\n1  item wonderful,quality item,nostalgia wonderfu...   \n2           camera inferior,money extra,camera cheap   \n3  eye red,light low room increase lense mm,slrs ...   \n4  memory internal camera service camera send sty...   \n5  elph regular,zooming powerful camera lightweig...   \n\n                                           FOP_sents  \\\n1  great quality though and a wonderful nostalgia...   \n2  pay the extra money and get this one rather th...   \n3  yes i sometimes get red eye in low light but a...   \n4  however on my next trip the camera s internal ...   \n5  if you are consider a regular canon elph go wi...   \n\n                              total_mention_features  \\\n1  item collector book price copy high buy price ...   \n2  cannon money camera buy decide camera picture ...   \n3  snapshot photographer year low lens red avoid ...   \n4  camera picture camera trip memory service comp...   \n5  canon zooming extra elph small print lightweig...   \n\n                                         bert_review  \\\n1  [CLS] this book is a collector item and i love...   \n2  [CLS] i would like to say the the cannon el ##...   \n3  [CLS] i have have and use 370 ##z for over a y...   \n4  [CLS] i use this camera once and it take relat...   \n5  [CLS] if you are consider a regular canon el #...   \n\n                                        bert_summary  bert_review_len  \\\n1             [CLS] wonderful book keeps ##ake [SEP]               64   \n2              [CLS] el ##ph 37 oz is the best [SEP]               69   \n3          [CLS] good all around ap ##s camera [SEP]              160   \n4  [CLS] terrible camera worse customer service [...              110   \n5                         [CLS] great purchase [SEP]              149   \n\n   bert_summary_len                                       FOP_keywords  \\\n1                 6             nostalgia wonderful quality great item   \n2                 9                  inferior camera extra cheap money   \n3                 8      room eye increase light lense mm red slrs low   \n4                 7  internal camera stylus olympus send service me...   \n5                 4  postcard professional camera elph small lightw...   \n\n   FOP_keywords_num  \n1                 5  \n2                 5  \n3                 9  \n4                 9  \n5                 9  "
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flit_key_df['FOP_keywords'] = flit_key_df['total_keyword']\n",
    "flit_key_df['FOP_keywords'] = flit_key_df['FOP_keywords'].apply(to_words)\n",
    "flit_key_df['FOP_keywords_num'] = flit_key_df['FOP_keywords'].apply(calc_keyword_num)\n",
    "flit_key_df = flit_key_df[(flit_key_df.FOP_keywords_num>=2) ] # 過濾single word summary\n",
    "flit_key_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheat Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 107132/107132 [03:08<00:00, 568.93it/s]\nTotal data : 33423\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_ID</th>\n      <th>review</th>\n      <th>summary</th>\n      <th>big_categories</th>\n      <th>small_categories</th>\n      <th>lemm_review</th>\n      <th>lemm_summary</th>\n      <th>lemm_review_len</th>\n      <th>lemm_summary_len</th>\n      <th>overall</th>\n      <th>...</th>\n      <th>total_keyword</th>\n      <th>FOP_sents</th>\n      <th>total_mention_features</th>\n      <th>bert_review</th>\n      <th>bert_summary</th>\n      <th>bert_review_len</th>\n      <th>bert_summary_len</th>\n      <th>FOP_keywords</th>\n      <th>FOP_keywords_num</th>\n      <th>Cheat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>967075200</td>\n      <td>i used this camera once and it took relatively...</td>\n      <td>terrible camera worse customer service</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>i use this camera once and it take relatively ...</td>\n      <td>&lt;s&gt; terrible camera worse customer service &lt;/s&gt;</td>\n      <td>99</td>\n      <td>7</td>\n      <td>1</td>\n      <td>...</td>\n      <td>memory internal camera service camera send sty...</td>\n      <td>however on my next trip the camera s internal ...</td>\n      <td>camera picture camera trip memory service comp...</td>\n      <td>[CLS] i use this camera once and it take relat...</td>\n      <td>[CLS] terrible camera worse customer service [...</td>\n      <td>110</td>\n      <td>7</td>\n      <td>internal camera stylus olympus send service me...</td>\n      <td>9</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>951868800</td>\n      <td>my review will focus on how well the 370z repl...</td>\n      <td>a good enough replacement for 35mm point and s...</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>my review will focus on how well the replace a...</td>\n      <td>&lt;s&gt; good enough replacement for 35 mm point an...</td>\n      <td>419</td>\n      <td>11</td>\n      <td>4</td>\n      <td>...</td>\n      <td>resolution indistinguishable definition precis...</td>\n      <td>resolution indistinguishable on 4x6 4x7 print ...</td>\n      <td>shoot point replace mm review focus replace si...</td>\n      <td>[CLS] my review will focus on how well the 370...</td>\n      <td>[CLS] good enough replacement for 35 mm point ...</td>\n      <td>525</td>\n      <td>11</td>\n      <td>continuous light special activity definition c...</td>\n      <td>29</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>974332800</td>\n      <td>if you want a point and shoot camera that you ...</td>\n      <td>best compact aps camera</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>if you want a point and shoot camera that you ...</td>\n      <td>&lt;s&gt; best compact aps camera &lt;/s&gt;</td>\n      <td>188</td>\n      <td>6</td>\n      <td>5</td>\n      <td>...</td>\n      <td>range zoon,range size,size zoon,set pretty bro...</td>\n      <td>i look at a pretty broad set of camera include...</td>\n      <td>shoot throw carry point bag camera shirt pocke...</td>\n      <td>[CLS] if you want a point and shoot camera tha...</td>\n      <td>[CLS] best compact ap ##s camera [SEP]</td>\n      <td>209</td>\n      <td>7</td>\n      <td>picture pretty size camera elph broad surprisi...</td>\n      <td>13</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>942883200</td>\n      <td>i have had my camera for two months and i love...</td>\n      <td>great first digital camera for non photographers</td>\n      <td>Electronics</td>\n      <td>Digital Cameras</td>\n      <td>i have have my camera for two month and i love...</td>\n      <td>&lt;s&gt; great first digital camera for non photogr...</td>\n      <td>105</td>\n      <td>9</td>\n      <td>4</td>\n      <td>...</td>\n      <td>quality good,quality very good,mb card</td>\n      <td>i wish that it would take the 16 mb and the 32...</td>\n      <td>month camera photographer professional picture...</td>\n      <td>[CLS] i have have my camera for two month and ...</td>\n      <td>[CLS] great first digital camera for non photo...</td>\n      <td>117</td>\n      <td>9</td>\n      <td>very quality mb card good</td>\n      <td>5</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>960163200</td>\n      <td>when i got this camera it is nice looking and ...</td>\n      <td>use 3 days to trouble shoot the connecting the...</td>\n      <td>Electronics</td>\n      <td>Digital Cameras</td>\n      <td>when i get this camera it is nice looking and ...</td>\n      <td>&lt;s&gt; use day to trouble shoot the connect the s...</td>\n      <td>190</td>\n      <td>12</td>\n      <td>3</td>\n      <td>...</td>\n      <td>buying worth picture nice photo upload cable s...</td>\n      <td>when i get this camera it is nice looking and ...</td>\n      <td>camera buying picture day photo computer uploa...</td>\n      <td>[CLS] when i get this camera it is nice lookin...</td>\n      <td>[CLS] use day to trouble shoot the connect the...</td>\n      <td>221</td>\n      <td>12</td>\n      <td>worth photo instal picture buying nice softwar...</td>\n      <td>11</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>",
      "text/plain": "    review_ID                                             review  \\\n4   967075200  i used this camera once and it took relatively...   \n8   951868800  my review will focus on how well the 370z repl...   \n10  974332800  if you want a point and shoot camera that you ...   \n35  942883200  i have had my camera for two months and i love...   \n44  960163200  when i got this camera it is nice looking and ...   \n\n                                              summary big_categories  \\\n4              terrible camera worse customer service    Electronics   \n8   a good enough replacement for 35mm point and s...    Electronics   \n10                            best compact aps camera    Electronics   \n35   great first digital camera for non photographers    Electronics   \n44  use 3 days to trouble shoot the connecting the...    Electronics   \n\n   small_categories                                        lemm_review  \\\n4       APS Cameras  i use this camera once and it take relatively ...   \n8       APS Cameras  my review will focus on how well the replace a...   \n10      APS Cameras  if you want a point and shoot camera that you ...   \n35  Digital Cameras  i have have my camera for two month and i love...   \n44  Digital Cameras  when i get this camera it is nice looking and ...   \n\n                                         lemm_summary  lemm_review_len  \\\n4     <s> terrible camera worse customer service </s>               99   \n8   <s> good enough replacement for 35 mm point an...              419   \n10                   <s> best compact aps camera </s>              188   \n35  <s> great first digital camera for non photogr...              105   \n44  <s> use day to trouble shoot the connect the s...              190   \n\n    lemm_summary_len  overall  ...    \\\n4                  7        1  ...     \n8                 11        4  ...     \n10                 6        5  ...     \n35                 9        4  ...     \n44                12        3  ...     \n\n                                        total_keyword  \\\n4   memory internal camera service camera send sty...   \n8   resolution indistinguishable definition precis...   \n10  range zoon,range size,size zoon,set pretty bro...   \n35             quality good,quality very good,mb card   \n44  buying worth picture nice photo upload cable s...   \n\n                                            FOP_sents  \\\n4   however on my next trip the camera s internal ...   \n8   resolution indistinguishable on 4x6 4x7 print ...   \n10  i look at a pretty broad set of camera include...   \n35  i wish that it would take the 16 mb and the 32...   \n44  when i get this camera it is nice looking and ...   \n\n                               total_mention_features  \\\n4   camera picture camera trip memory service comp...   \n8   shoot point replace mm review focus replace si...   \n10  shoot throw carry point bag camera shirt pocke...   \n35  month camera photographer professional picture...   \n44  camera buying picture day photo computer uploa...   \n\n                                          bert_review  \\\n4   [CLS] i use this camera once and it take relat...   \n8   [CLS] my review will focus on how well the 370...   \n10  [CLS] if you want a point and shoot camera tha...   \n35  [CLS] i have have my camera for two month and ...   \n44  [CLS] when i get this camera it is nice lookin...   \n\n                                         bert_summary bert_review_len  \\\n4   [CLS] terrible camera worse customer service [...             110   \n8   [CLS] good enough replacement for 35 mm point ...             525   \n10             [CLS] best compact ap ##s camera [SEP]             209   \n35  [CLS] great first digital camera for non photo...             117   \n44  [CLS] use day to trouble shoot the connect the...             221   \n\n    bert_summary_len                                       FOP_keywords  \\\n4                  7  internal camera stylus olympus send service me...   \n8                 11  continuous light special activity definition c...   \n10                 7  picture pretty size camera elph broad surprisi...   \n35                 9                          very quality mb card good   \n44                12  worth photo instal picture buying nice softwar...   \n\n   FOP_keywords_num  Cheat  \n4                 9   True  \n8                29   True  \n10               13   True  \n35                5   True  \n44               11   True  \n\n[5 rows x 21 columns]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flit_key_df['Cheat'] = False \n",
    "\n",
    "# flit_key_df.head()\n",
    "from tqdm import tqdm\n",
    "# 非符號alpha word重疊數\n",
    "with tqdm(total=len(flit_key_df)) as pbar:\n",
    "    for i ,row in flit_key_df.iterrows():\n",
    "        rev_tokens = set(row['lemm_review'].split(\" \"))\n",
    "        summ_tokens = set(row['lemm_summary'].split(\" \"))\n",
    "        key_words = rev_tokens & summ_tokens & (total_keywords| set(opinion_lexicon[\"total-words\"]))\n",
    "        if len(key_words) > 2: \n",
    "            flit_key_df.loc[i,'Cheat'] = True\n",
    "        pbar.update(1)\n",
    "    \n",
    "flit_key_df = flit_key_df[(flit_key_df.Cheat == True) ] # 過濾single word summary\n",
    "amount = len(flit_key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "flit_key_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRank_keywords 資料整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords as TextRank\n",
    "from summa.summarizer import summarize\n",
    "def textrank_keys(text):\n",
    "    keywords1 = list()\n",
    "    for words in TextRank.keywords(text).split('\\n'):\n",
    "        keywords1.extend(words.split(\" \"))\n",
    "    keywords1 = set(keywords1)    \n",
    "    \n",
    "    return \" \".join(list(keywords1))\n",
    "\n",
    "def textrank_summ_keys(text): \n",
    "    keywords2 = list()\n",
    "    for words in summarize(text, words=8).split('\\n'):\n",
    "        keywords2.extend(words.split(\" \"))\n",
    "    keywords2 = set(keywords2)\n",
    "    \n",
    "    return \" \".join(list(keywords2))\n",
    "\n",
    "# flit_key_df['TextRank_keywords'] = flit_key_df['lemm_review'].apply(textrank_to_words)\n",
    "# flit_key_df['TextRank_keywords_num'] = flit_key_df['TextRank_keywords'].apply(calc_num)\n",
    "flit_key_df['TextRank_keywords'] = flit_key_df['FOP_keywords'] \n",
    "flit_key_df['TextRank_keywords_num'] = flit_key_df['FOP_keywords_num'] \n",
    "flit_key_df.loc[:,'TextRank_keywords'] = ''\n",
    "flit_key_df.loc[:,'TextRank_keywords_num'] = 0\n",
    "# flit_key_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 33423/33423 [08:37<00:00, 64.53it/s]\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_ID</th>\n      <th>review</th>\n      <th>summary</th>\n      <th>big_categories</th>\n      <th>small_categories</th>\n      <th>lemm_review</th>\n      <th>lemm_summary</th>\n      <th>lemm_review_len</th>\n      <th>lemm_summary_len</th>\n      <th>overall</th>\n      <th>...</th>\n      <th>total_mention_features</th>\n      <th>bert_review</th>\n      <th>bert_summary</th>\n      <th>bert_review_len</th>\n      <th>bert_summary_len</th>\n      <th>FOP_keywords</th>\n      <th>FOP_keywords_num</th>\n      <th>Cheat</th>\n      <th>TextRank_keywords</th>\n      <th>TextRank_keywords_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>967075200</td>\n      <td>i used this camera once and it took relatively...</td>\n      <td>terrible camera worse customer service</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>i use this camera once and it take relatively ...</td>\n      <td>&lt;s&gt; terrible camera worse customer service &lt;/s&gt;</td>\n      <td>99</td>\n      <td>7</td>\n      <td>1</td>\n      <td>...</td>\n      <td>camera picture camera trip memory service comp...</td>\n      <td>[CLS] i use this camera once and it take relat...</td>\n      <td>[CLS] terrible camera worse customer service [...</td>\n      <td>110</td>\n      <td>7</td>\n      <td>internal camera stylus olympus send service me...</td>\n      <td>9</td>\n      <td>True</td>\n      <td>equipment good camera</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>951868800</td>\n      <td>my review will focus on how well the 370z repl...</td>\n      <td>a good enough replacement for 35mm point and s...</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>my review will focus on how well the replace a...</td>\n      <td>&lt;s&gt; good enough replacement for 35 mm point an...</td>\n      <td>419</td>\n      <td>11</td>\n      <td>4</td>\n      <td>...</td>\n      <td>shoot point replace mm review focus replace si...</td>\n      <td>[CLS] my review will focus on how well the 370...</td>\n      <td>[CLS] good enough replacement for 35 mm point ...</td>\n      <td>525</td>\n      <td>11</td>\n      <td>continuous light special activity definition c...</td>\n      <td>29</td>\n      <td>True</td>\n      <td>day excellent eye frame precise use mm wishlis...</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>974332800</td>\n      <td>if you want a point and shoot camera that you ...</td>\n      <td>best compact aps camera</td>\n      <td>Electronics</td>\n      <td>APS Cameras</td>\n      <td>if you want a point and shoot camera that you ...</td>\n      <td>&lt;s&gt; best compact aps camera &lt;/s&gt;</td>\n      <td>188</td>\n      <td>6</td>\n      <td>5</td>\n      <td>...</td>\n      <td>shoot throw carry point bag camera shirt pocke...</td>\n      <td>[CLS] if you want a point and shoot camera tha...</td>\n      <td>[CLS] best compact ap ##s camera [SEP]</td>\n      <td>209</td>\n      <td>7</td>\n      <td>picture pretty size camera elph broad surprisi...</td>\n      <td>13</td>\n      <td>True</td>\n      <td>shoot camera elph broad pocket shirt expensive...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>942883200</td>\n      <td>i have had my camera for two months and i love...</td>\n      <td>great first digital camera for non photographers</td>\n      <td>Electronics</td>\n      <td>Digital Cameras</td>\n      <td>i have have my camera for two month and i love...</td>\n      <td>&lt;s&gt; great first digital camera for non photogr...</td>\n      <td>105</td>\n      <td>9</td>\n      <td>4</td>\n      <td>...</td>\n      <td>month camera photographer professional picture...</td>\n      <td>[CLS] i have have my camera for two month and ...</td>\n      <td>[CLS] great first digital camera for non photo...</td>\n      <td>117</td>\n      <td>9</td>\n      <td>very quality mb card good</td>\n      <td>5</td>\n      <td>True</td>\n      <td>photographer professional quality</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>960163200</td>\n      <td>when i got this camera it is nice looking and ...</td>\n      <td>use 3 days to trouble shoot the connecting the...</td>\n      <td>Electronics</td>\n      <td>Digital Cameras</td>\n      <td>when i get this camera it is nice looking and ...</td>\n      <td>&lt;s&gt; use day to trouble shoot the connect the s...</td>\n      <td>190</td>\n      <td>12</td>\n      <td>3</td>\n      <td>...</td>\n      <td>camera buying picture day photo computer uploa...</td>\n      <td>[CLS] when i get this camera it is nice lookin...</td>\n      <td>[CLS] use day to trouble shoot the connect the...</td>\n      <td>221</td>\n      <td>12</td>\n      <td>worth photo instal picture buying nice softwar...</td>\n      <td>11</td>\n      <td>True</td>\n      <td>worth way buying picture nice looking</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 23 columns</p>\n</div>",
      "text/plain": "    review_ID                                             review  \\\n4   967075200  i used this camera once and it took relatively...   \n8   951868800  my review will focus on how well the 370z repl...   \n10  974332800  if you want a point and shoot camera that you ...   \n35  942883200  i have had my camera for two months and i love...   \n44  960163200  when i got this camera it is nice looking and ...   \n\n                                              summary big_categories  \\\n4              terrible camera worse customer service    Electronics   \n8   a good enough replacement for 35mm point and s...    Electronics   \n10                            best compact aps camera    Electronics   \n35   great first digital camera for non photographers    Electronics   \n44  use 3 days to trouble shoot the connecting the...    Electronics   \n\n   small_categories                                        lemm_review  \\\n4       APS Cameras  i use this camera once and it take relatively ...   \n8       APS Cameras  my review will focus on how well the replace a...   \n10      APS Cameras  if you want a point and shoot camera that you ...   \n35  Digital Cameras  i have have my camera for two month and i love...   \n44  Digital Cameras  when i get this camera it is nice looking and ...   \n\n                                         lemm_summary  lemm_review_len  \\\n4     <s> terrible camera worse customer service </s>               99   \n8   <s> good enough replacement for 35 mm point an...              419   \n10                   <s> best compact aps camera </s>              188   \n35  <s> great first digital camera for non photogr...              105   \n44  <s> use day to trouble shoot the connect the s...              190   \n\n    lemm_summary_len  overall          ...            \\\n4                  7        1          ...             \n8                 11        4          ...             \n10                 6        5          ...             \n35                 9        4          ...             \n44                12        3          ...             \n\n                               total_mention_features  \\\n4   camera picture camera trip memory service comp...   \n8   shoot point replace mm review focus replace si...   \n10  shoot throw carry point bag camera shirt pocke...   \n35  month camera photographer professional picture...   \n44  camera buying picture day photo computer uploa...   \n\n                                          bert_review  \\\n4   [CLS] i use this camera once and it take relat...   \n8   [CLS] my review will focus on how well the 370...   \n10  [CLS] if you want a point and shoot camera tha...   \n35  [CLS] i have have my camera for two month and ...   \n44  [CLS] when i get this camera it is nice lookin...   \n\n                                         bert_summary bert_review_len  \\\n4   [CLS] terrible camera worse customer service [...             110   \n8   [CLS] good enough replacement for 35 mm point ...             525   \n10             [CLS] best compact ap ##s camera [SEP]             209   \n35  [CLS] great first digital camera for non photo...             117   \n44  [CLS] use day to trouble shoot the connect the...             221   \n\n   bert_summary_len                                       FOP_keywords  \\\n4                 7  internal camera stylus olympus send service me...   \n8                11  continuous light special activity definition c...   \n10                7  picture pretty size camera elph broad surprisi...   \n35                9                          very quality mb card good   \n44               12  worth photo instal picture buying nice softwar...   \n\n    FOP_keywords_num  Cheat  \\\n4                  9   True   \n8                 29   True   \n10                13   True   \n35                 5   True   \n44                11   True   \n\n                                    TextRank_keywords  TextRank_keywords_num  \n4                               equipment good camera                      3  \n8   day excellent eye frame precise use mm wishlis...                     16  \n10  shoot camera elph broad pocket shirt expensive...                      9  \n35                  photographer professional quality                      3  \n44              worth way buying picture nice looking                      6  \n\n[5 rows x 23 columns]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "with tqdm(total=len(flit_key_df)) as pbar:\n",
    "    for i ,row in flit_key_df.iterrows():\n",
    "        TextRank_keywords = textrank_keys(row['lemm_review'])\n",
    "    #     TextRank_keywords = textrank_summ_keys(row['lemm_review'])  \n",
    "        num = calc_keyword_num(TextRank_keywords)\n",
    "        flit_key_df.loc[i,'TextRank_keywords'] = TextRank_keywords\n",
    "        flit_key_df.loc[i,'TextRank_keywords_num'] = num\n",
    "        pbar.update(1)\n",
    "        \n",
    "flit_key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "if not os.path.exists('XLSX/statistic'):\n",
    "    os.makedirs('XLSX/statistic')\n",
    "with open('XLSX/statistic/%s_%s_info.txt'%(category1,category2),'w') as f:\n",
    "    max_rev_len = flit_key_df['lemm_review_len'].max()\n",
    "    min_rev_len = flit_key_df['lemm_review_len'].min()\n",
    "    mean_rev_len = flit_key_df['lemm_review_len'].mean()\n",
    "    median_rev_len = flit_key_df['lemm_review_len'].median()\n",
    "\n",
    "    f.write('max_rev_len :%s \\n'%(max_rev_len))\n",
    "    f.write('min_rev_len :%s \\n'%(min_rev_len))\n",
    "    f.write('mean_rev_len :%s \\n'%(mean_rev_len))\n",
    "    f.write('median_rev_len :%s \\n'%(median_rev_len))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_summary_len = flit_key_df['lemm_summary_len'].max()\n",
    "    min_summary_len = flit_key_df['lemm_summary_len'].min()\n",
    "    mean_summary_len = flit_key_df['lemm_summary_len'].mean()\n",
    "    median_summary_len = flit_key_df['lemm_summary_len'].median()\n",
    "\n",
    "    f.write('max_summary_len :%s \\n'%(max_summary_len))\n",
    "    f.write('min_summary_len :%s \\n'%(min_summary_len))\n",
    "    f.write('mean_summary_len :%s \\n'%(mean_summary_len))\n",
    "    f.write('median_summary_len :%s \\n'%(median_summary_len))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_FOP_keywords_num = flit_key_df['FOP_keywords_num'].max()\n",
    "    min_FOP_keywords_num = flit_key_df['FOP_keywords_num'].min()\n",
    "    mean_FOP_keywords_num = flit_key_df['FOP_keywords_num'].mean()\n",
    "    median_FOP_keywords_num = flit_key_df['FOP_keywords_num'].median()\n",
    "\n",
    "    f.write('max_FOP_keywords_num :%s \\n'%(max_FOP_keywords_num))\n",
    "    f.write('min_FOP_keywords_num :%s \\n'%(min_FOP_keywords_num))\n",
    "    f.write('mean_FOP_keywords_num :%s \\n'%(mean_FOP_keywords_num))\n",
    "    f.write('median_FOP_keywords_num :%s \\n'%(median_FOP_keywords_num))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].max()\n",
    "    min_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].min()\n",
    "    mean_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].mean()\n",
    "    median_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].median()\n",
    "\n",
    "    f.write('max_TextRank_keywords_num :%s \\n'%(max_TextRank_keywords_num))\n",
    "    f.write('min_TextRank_keywords_num :%s \\n'%(min_TextRank_keywords_num))\n",
    "    f.write('mean_TextRank_keywords_num :%s \\n'%(mean_TextRank_keywords_num))\n",
    "    f.write('median_TextRank_keywords_num :%s \\n'%(median_TextRank_keywords_num))\n",
    "\n",
    "    f.write('\\n\\n\\n')    \n",
    "    max_bert_rev_len = flit_key_df['bert_review_len'].max()\n",
    "    min_bert_rev_len = flit_key_df['bert_review_len'].min()\n",
    "    mean_bert_rev_len = flit_key_df['bert_review_len'].mean()\n",
    "    median_bert_rev_len = flit_key_df['bert_review_len'].median()\n",
    "\n",
    "    f.write('max_bert_rev_len :%s \\n'%(max_bert_rev_len))\n",
    "    f.write('min_bert_rev_len :%s \\n'%(min_bert_rev_len))\n",
    "    f.write('mean_bert_rev_len :%s \\n'%(mean_bert_rev_len))\n",
    "    f.write('median_bert_rev_len :%s \\n'%(median_bert_rev_len))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_bert_summary_len = flit_key_df['bert_summary_len'].max()\n",
    "    min_bert_summary_len = flit_key_df['bert_summary_len'].min()\n",
    "    mean_bert_summary_len = flit_key_df['bert_summary_len'].mean()\n",
    "    median_bert_summary_len = flit_key_df['bert_summary_len'].median()\n",
    "\n",
    "    f.write('max_bert_summary_len :%s \\n'%(max_bert_summary_len))\n",
    "    f.write('min_bert_summary_len :%s \\n'%(min_bert_summary_len))\n",
    "    f.write('mean_bert_summary_len :%s \\n'%(mean_bert_summary_len))\n",
    "    f.write('median_bert_summary_len :%s \\n'%(median_bert_summary_len))\n",
    "\n",
    "    \n",
    "\n",
    "# plt.xlim(xmax = mean_rev_len)\n",
    "# plt.ylim(ymax = flit_key_df['lemm_review_len'].value_counts().max())\n",
    "\n",
    "flit_key_df['lemm_review_len'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/review_len_%s_%s.png'%(category1,category2))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# plt.xlim(xmax = max_summary_len)\n",
    "# plt.ylim(ymax = flit_key_df['lemm_summary_len'].value_counts().max())\n",
    "flit_key_df['lemm_summary_len'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/summary_len_%s_%s.png'%(category1,category2))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# plt.xlim(xmax = mean_keyword_num)\n",
    "flit_key_df['FOP_keywords_num'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/FOP_keywords_num_%s_%s.png'%(category1,category2))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "flit_key_df['TextRank_keywords_num'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/TextRank_keywords_num_%s_%s.png'%(category1,category2))\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 製作record bin檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 20053/20053 [00:06<00:00, 3190.61it/s]\n  4%|▍         | 282/6684 [00:00<00:02, 2818.73it/s] bin/category/train.bin finished... \n100%|██████████| 6684/6684 [00:02<00:00, 3231.01it/s]\n  3%|▎         | 233/6684 [00:00<00:02, 2321.02it/s] bin/category/test.bin finished... \n100%|██████████| 6684/6684 [00:02<00:00, 3230.21it/s] bin/category/valid.bin finished... \n\n"
    }
   ],
   "source": [
    "import shutil\n",
    "if not os.path.exists('bin'):\n",
    "    shutil.rmtree('/bin', ignore_errors=True)\n",
    "\n",
    "if not os.path.exists('bin/category/chunked'):\n",
    "    os.makedirs('bin/category/chunked')\n",
    "\n",
    "makevocab = True\n",
    "if makevocab:\n",
    "    vocab_counter = collections.Counter()\n",
    "    \n",
    "# train_file\n",
    "flit_key_train_df = flit_key_df.iloc[:int(amount*0.6)]\n",
    "\n",
    "# test_file\n",
    "flit_key_test_df = flit_key_df.iloc[int(amount*0.6)+1:int(amount*0.8)]\n",
    "\n",
    "# vald_file\n",
    "flit_key_valid_df = flit_key_df.iloc[int(amount*0.8)+1:]\n",
    "sentence_start = \"<s>\"\n",
    "sentence_end = \"</s>\"\n",
    "\n",
    "\n",
    "def xlsx2bin(set_name,df):\n",
    "    sents = []\n",
    "    with open(\"bin/category/%s.bin\"%(set_name), 'wb') as file:\n",
    "        i = 0\n",
    "        for idx in tqdm(range(len(df))):\n",
    "            series = df.iloc[idx]\n",
    "            data_dict = series.to_dict()\n",
    "#             review_ID , big_categories , small_categories , \\\n",
    "#             review , lemm_review , summary , lemm_summary , FOP_keywords ,TextRank_keywords = \\\n",
    "#             data_dict['review_ID'],data_dict['big_categories'],data_dict['small_categories'],data_dict['review'],data_dict['lemm_review'], \\\n",
    "#             data_dict['summary'],data_dict['lemm_summary'],data_dict['FOP_keywords'] ,data_dict['TextRank_keywords']\n",
    "\n",
    "\n",
    "            \n",
    "            review_ID , big_categories , small_categories , \\\n",
    "            orign_review , lemm_review , orign_summary , lemm_summary , \\\n",
    "            bert_review , bert_summary ,FOP_keywords ,TextRank_keywords = \\\n",
    "            data_dict['review_ID'],data_dict['big_categories'],data_dict['small_categories'], \\\n",
    "            data_dict['review'],data_dict['lemm_review'], data_dict['summary'],data_dict['lemm_summary'], \\\n",
    "            data_dict['bert_review'], data_dict['bert_summary'] , data_dict['FOP_keywords'] ,data_dict['TextRank_keywords']\n",
    "            \n",
    "#             print(FOP_keywords)\n",
    "\n",
    "            # save Embedding/word2Vec calculate sents\n",
    "#             for sent in nltk.sent_tokenize(lemm_review):\n",
    "#                 sent = sent.replace(\".\" ,\"\")\n",
    "#                 sents.append(str(sent).split()) # 切分词汇 \n",
    "\n",
    "#             for sent in nltk.sent_tokenize(lemm_summary):\n",
    "#                 sent = sent.replace(sentence_start ,\"\").replace(sentence_end ,\"\")\n",
    "#                 sents.append(str(sent).split()) # 切分词汇 \n",
    "\n",
    "            lemm_review = lemm_review.replace(\"\\n\",\"\")\n",
    "            lemm_summary = lemm_summary.replace(\"\\n\",\"\").replace(\".\",\" \")\n",
    "            # lemm_summary = sentence_start + ' '+ lemm_summary + ' ' + sentence_end\n",
    "#             print(lemm_summary)\n",
    "            # Write to tf.Example\n",
    "            tf_example = example_pb2.Example()\n",
    "            try:\n",
    "                tf_example.features.feature['orign_review'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(orign_review, encoding='utf-8')])\n",
    "\n",
    "                tf_example.features.feature['orign_summary'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(orign_summary, encoding='utf-8')])\n",
    "                \n",
    "                tf_example.features.feature['review'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(lemm_review, encoding='utf-8')])\n",
    "\n",
    "                tf_example.features.feature['summary'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(lemm_summary, encoding='utf-8')]) \n",
    "                    \n",
    "                tf_example.features.feature['bert_review'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(bert_review, encoding='utf-8')])\n",
    "\n",
    "                tf_example.features.feature['bert_summary'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(bert_summary, encoding='utf-8')])\n",
    "        \n",
    "                tf_example.features.feature['FOP_keywords'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(FOP_keywords, encoding='utf-8')]) \n",
    "            \n",
    "                tf_example.features.feature['TextRank_keywords'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(TextRank_keywords, encoding='utf-8')]) \n",
    "\n",
    "                tf_example_str = tf_example.SerializeToString()\n",
    "                str_len = len(tf_example_str)  \n",
    "                file.write(struct.pack('q', str_len))\n",
    "                file.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "    print(\" %s finished... \"%(file.name))\n",
    "    return sents\n",
    "    \n",
    "    \n",
    "sents1 = xlsx2bin('train',flit_key_train_df)\n",
    "sents2 = xlsx2bin('test',flit_key_test_df)\n",
    "sents3 = xlsx2bin('valid',flit_key_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bin/category/bin-info.txt\",'w',encoding='utf-8') as f :\n",
    "    f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割record bin檔(1000為單位)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Splitting train data into chunks...\nSplitting valid data into chunks...\nSplitting test data into chunks...\nSaved chunked data in bin/category/chunked\n"
    }
   ],
   "source": [
    "def chunk_file(set_name, chunks_dir):\n",
    "    in_file = 'bin/category/%s.bin' % set_name\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "#         chunk_fname = os.path.join('bin', '/%s/%s_%03d.bin' % (chunks_dir,set_name, chunk))  # new chunk\n",
    "        chunk_fname = '%s/%s/%s_%03d.bin' % (chunks_dir,set_name,set_name, chunk)\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(CHUNK_SIZE):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1\n",
    "\n",
    "\n",
    "def chunk_all(chunks_dir = 'bin/category/chunked'):\n",
    "    # Make a dir to hold the chunks\n",
    "    \n",
    "    # Chunk the data\n",
    "    for set_name in ['train', 'valid', 'test']:\n",
    "        if not os.path.isdir(os.path.join(chunks_dir,set_name)):\n",
    "            os.mkdir(os.path.join(chunks_dir,set_name))\n",
    "        print(\"Splitting %s data into chunks...\" % set_name)\n",
    "        chunk_file(set_name, chunks_dir)\n",
    "    print(\"Saved chunked data in %s\" % chunks_dir)\n",
    "    \n",
    "chunk_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_valid():\n",
    "    #Performing rouge evaluation on 1.9 lakh sentences takes lot of time. So, create mini validation set & test set by borrowing 15k samples each from these 1.9 lakh sentences\n",
    "    bin_valid_chuncks = os.listdir('bin/category/chunked/valid/')\n",
    "    bin_valid_chuncks.sort()\n",
    "    if not os.path.exists('bin/category/chunked/main_valid'):\n",
    "        os.makedirs('bin/category/chunked/main_valid')\n",
    "        \n",
    "    samples = random.sample(set(bin_valid_chuncks[:-1]), 2)      #Exclude last bin file; contains only 9k sentences\n",
    "    valid_chunk, test_chunk = samples[0], samples[1]\n",
    "    shutil.copyfile(os.path.join('bin/category/chunked/valid', valid_chunk), os.path.join(\"bin/category/chunked/main_valid\", \"valid_00.bin\"))\n",
    "    shutil.copyfile(os.path.join('bin/category/chunked/valid', test_chunk), os.path.join(\"bin/category/chunked/main_valid\", \"test_00.bin\"))\n",
    "main_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding/word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 154488/154488 [00:58<00:00, 2622.56it/s]word2Vec training sentence finished...\n\n"
    }
   ],
   "source": [
    "sentences = [] # total sentence\n",
    "for idx in tqdm(range(len(orign_key_df))):\n",
    "    series = orign_key_df.iloc[idx]\n",
    "    data_dict = series.to_dict()\n",
    "    lemm_review_sents , lemm_summary  = data_dict['lemm_review'],data_dict['lemm_summary'] \n",
    "    lemm_review_sents = eval(lemm_review_sents)\n",
    "    for sent in lemm_review_sents:\n",
    "        sent_tokens = sent.split(\" \")\n",
    "        tokens = [str(token) for token in sent.split() if (\" \" not in str(token))and (str(token) == '.' or str(token).isalpha())]\n",
    "#         tokens = [str(token) for token in sent.split() if (\" \" not in str(token))]\n",
    "        sentences.append(tokens)   \n",
    "    \n",
    "        dot_tokens = [token for token in tokens if (token[0] == \".\" or token[-1] == \".\") and (len(token)>1)]\n",
    "        if len(dot_tokens) > 0 :print(dot_tokens)\n",
    "        \n",
    "    sentences.append([t for t in lemm_summary.split(\" \") if t not in [\"<s>\" , \"</s>\"]])\n",
    "    \n",
    "    dot_tokens = [token for token in lemm_summary.split(\" \") if (token[0] == \".\" or token[-1] == \".\") and (len(token)>1)]\n",
    "    if len(dot_tokens) > 0 :print(dot_tokens)\n",
    "    \n",
    "print('word2Vec training sentence finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 word2vec\n",
    "from gensim.models import word2vec\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsnooper\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "vocab_count = 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": ": EPOCH 93 - PROGRESS: at 50.54% examples, 1036549 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:27,772 : INFO : EPOCH 93 - PROGRESS: at 55.27% examples, 1033447 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:28,774 : INFO : EPOCH 93 - PROGRESS: at 60.02% examples, 1029403 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:29,785 : INFO : EPOCH 93 - PROGRESS: at 64.90% examples, 1028186 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:30,787 : INFO : EPOCH 93 - PROGRESS: at 70.13% examples, 1029869 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:31,791 : INFO : EPOCH 93 - PROGRESS: at 75.32% examples, 1032123 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:32,796 : INFO : EPOCH 93 - PROGRESS: at 80.50% examples, 1031826 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:33,797 : INFO : EPOCH 93 - PROGRESS: at 85.75% examples, 1032222 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:34,798 : INFO : EPOCH 93 - PROGRESS: at 90.83% examples, 1029597 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:35,800 : INFO : EPOCH 93 - PROGRESS: at 96.20% examples, 1030632 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:36,500 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-01-27 05:44:36,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-01-27 05:44:36,512 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-01-27 05:44:36,513 : INFO : EPOCH - 93 : training on 29733777 raw words (20389937 effective words) took 19.8s, 1031030 effective words/s\n2020-01-27 05:44:37,523 : INFO : EPOCH 94 - PROGRESS: at 4.90% examples, 966512 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:38,525 : INFO : EPOCH 94 - PROGRESS: at 9.68% examples, 963474 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:44:39,528 : INFO : EPOCH 94 - PROGRESS: at 14.96% examples, 994570 words/s, in_qsize 4, out_qsize 1\n2020-01-27 05:44:40,533 : INFO : EPOCH 94 - PROGRESS: at 20.25% examples, 1008671 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:41,541 : INFO : EPOCH 94 - PROGRESS: at 25.42% examples, 1020310 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:42,544 : INFO : EPOCH 94 - PROGRESS: at 30.46% examples, 1024350 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:43,549 : INFO : EPOCH 94 - PROGRESS: at 35.45% examples, 1026224 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:44,552 : INFO : EPOCH 94 - PROGRESS: at 40.03% examples, 1017503 words/s, in_qsize 6, out_qsize 1\n2020-01-27 05:44:45,560 : INFO : EPOCH 94 - PROGRESS: at 44.85% examples, 1016279 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:44:46,563 : INFO : EPOCH 94 - PROGRESS: at 49.84% examples, 1020426 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:44:47,565 : INFO : EPOCH 94 - PROGRESS: at 54.63% examples, 1019928 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:48,576 : INFO : EPOCH 94 - PROGRESS: at 59.55% examples, 1019711 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:44:49,579 : INFO : EPOCH 94 - PROGRESS: at 64.34% examples, 1018263 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:50,582 : INFO : EPOCH 94 - PROGRESS: at 69.54% examples, 1020656 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:51,590 : INFO : EPOCH 94 - PROGRESS: at 74.63% examples, 1021767 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:52,605 : INFO : EPOCH 94 - PROGRESS: at 79.83% examples, 1021465 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:53,608 : INFO : EPOCH 94 - PROGRESS: at 85.14% examples, 1023203 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:54,613 : INFO : EPOCH 94 - PROGRESS: at 90.60% examples, 1024966 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:55,615 : INFO : EPOCH 94 - PROGRESS: at 95.79% examples, 1024448 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:56,385 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-01-27 05:44:56,393 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-01-27 05:44:56,396 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-01-27 05:44:56,396 : INFO : EPOCH - 94 : training on 29733777 raw words (20386184 effective words) took 19.9s, 1025638 effective words/s\n2020-01-27 05:44:57,404 : INFO : EPOCH 95 - PROGRESS: at 5.19% examples, 1023067 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:58,408 : INFO : EPOCH 95 - PROGRESS: at 10.15% examples, 1010693 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:44:59,415 : INFO : EPOCH 95 - PROGRESS: at 15.26% examples, 1013517 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:00,419 : INFO : EPOCH 95 - PROGRESS: at 20.36% examples, 1012938 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:01,420 : INFO : EPOCH 95 - PROGRESS: at 25.39% examples, 1019936 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:45:02,420 : INFO : EPOCH 95 - PROGRESS: at 30.56% examples, 1029087 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:03,424 : INFO : EPOCH 95 - PROGRESS: at 35.68% examples, 1034216 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:04,427 : INFO : EPOCH 95 - PROGRESS: at 40.47% examples, 1030529 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:05,430 : INFO : EPOCH 95 - PROGRESS: at 45.25% examples, 1027043 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:06,430 : INFO : EPOCH 95 - PROGRESS: at 49.98% examples, 1025065 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:07,431 : INFO : EPOCH 95 - PROGRESS: at 54.91% examples, 1027441 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:08,434 : INFO : EPOCH 95 - PROGRESS: at 60.05% examples, 1030614 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:09,435 : INFO : EPOCH 95 - PROGRESS: at 64.96% examples, 1030577 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:10,439 : INFO : EPOCH 95 - PROGRESS: at 70.13% examples, 1031045 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:11,441 : INFO : EPOCH 95 - PROGRESS: at 74.94% examples, 1028170 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:12,449 : INFO : EPOCH 95 - PROGRESS: at 80.16% examples, 1028449 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:45:13,451 : INFO : EPOCH 95 - PROGRESS: at 85.49% examples, 1029759 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:45:14,456 : INFO : EPOCH 95 - PROGRESS: at 90.66% examples, 1028110 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:15,460 : INFO : EPOCH 95 - PROGRESS: at 95.89% examples, 1027694 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:16,231 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-01-27 05:45:16,237 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-01-27 05:45:16,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-01-27 05:45:16,241 : INFO : EPOCH - 95 : training on 29733777 raw words (20388873 effective words) took 19.8s, 1027767 effective words/s\n2020-01-27 05:45:17,262 : INFO : EPOCH 96 - PROGRESS: at 5.19% examples, 1014512 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:18,263 : INFO : EPOCH 96 - PROGRESS: at 10.52% examples, 1045764 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:19,268 : INFO : EPOCH 96 - PROGRESS: at 15.96% examples, 1057705 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:45:20,271 : INFO : EPOCH 96 - PROGRESS: at 21.00% examples, 1044639 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:21,277 : INFO : EPOCH 96 - PROGRESS: at 26.03% examples, 1044384 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:22,280 : INFO : EPOCH 96 - PROGRESS: at 31.14% examples, 1047876 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:23,282 : INFO : EPOCH 96 - PROGRESS: at 36.28% examples, 1050718 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:24,287 : INFO : EPOCH 96 - PROGRESS: at 41.40% examples, 1053216 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:25,288 : INFO : EPOCH 96 - PROGRESS: at 46.33% examples, 1052681 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:26,297 : INFO : EPOCH 96 - PROGRESS: at 51.48% examples, 1054640 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:27,308 : INFO : EPOCH 96 - PROGRESS: at 56.45% examples, 1053049 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:45:28,313 : INFO : EPOCH 96 - PROGRESS: at 61.53% examples, 1054191 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:29,331 : INFO : EPOCH 96 - PROGRESS: at 66.49% examples, 1050282 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:45:30,338 : INFO : EPOCH 96 - PROGRESS: at 71.60% examples, 1048744 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:31,339 : INFO : EPOCH 96 - PROGRESS: at 76.72% examples, 1048344 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:32,340 : INFO : EPOCH 96 - PROGRESS: at 81.84% examples, 1046112 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:33,342 : INFO : EPOCH 96 - PROGRESS: at 87.15% examples, 1046001 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:34,352 : INFO : EPOCH 96 - PROGRESS: at 92.51% examples, 1045468 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:35,366 : INFO : EPOCH 96 - PROGRESS: at 97.60% examples, 1042133 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:35,799 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-01-27 05:45:35,799 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-01-27 05:45:35,810 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-01-27 05:45:35,811 : INFO : EPOCH - 96 : training on 29733777 raw words (20387062 effective words) took 19.6s, 1042387 effective words/s\n2020-01-27 05:45:36,821 : INFO : EPOCH 97 - PROGRESS: at 5.26% examples, 1031758 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:37,826 : INFO : EPOCH 97 - PROGRESS: at 10.32% examples, 1025065 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:38,834 : INFO : EPOCH 97 - PROGRESS: at 15.71% examples, 1040523 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:45:39,841 : INFO : EPOCH 97 - PROGRESS: at 21.10% examples, 1047464 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:40,842 : INFO : EPOCH 97 - PROGRESS: at 25.89% examples, 1037928 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:41,847 : INFO : EPOCH 97 - PROGRESS: at 30.98% examples, 1040932 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:42,850 : INFO : EPOCH 97 - PROGRESS: at 36.11% examples, 1044616 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:43,856 : INFO : EPOCH 97 - PROGRESS: at 40.79% examples, 1036732 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:44,862 : INFO : EPOCH 97 - PROGRESS: at 45.67% examples, 1034267 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:45,865 : INFO : EPOCH 97 - PROGRESS: at 50.54% examples, 1033972 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:45:46,866 : INFO : EPOCH 97 - PROGRESS: at 55.24% examples, 1031066 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:47,867 : INFO : EPOCH 97 - PROGRESS: at 60.32% examples, 1033080 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:48,871 : INFO : EPOCH 97 - PROGRESS: at 65.33% examples, 1034281 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:49,877 : INFO : EPOCH 97 - PROGRESS: at 70.50% examples, 1034305 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:50,877 : INFO : EPOCH 97 - PROGRESS: at 75.42% examples, 1032700 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:51,881 : INFO : EPOCH 97 - PROGRESS: at 80.68% examples, 1033394 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:52,882 : INFO : EPOCH 97 - PROGRESS: at 85.89% examples, 1033271 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:53,889 : INFO : EPOCH 97 - PROGRESS: at 91.29% examples, 1033543 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:45:54,891 : INFO : EPOCH 97 - PROGRESS: at 96.67% examples, 1034743 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:55,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-01-27 05:45:55,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-01-27 05:45:55,510 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-01-27 05:45:55,511 : INFO : EPOCH - 97 : training on 29733777 raw words (20384241 effective words) took 19.7s, 1035010 effective words/s\n2020-01-27 05:45:56,517 : INFO : EPOCH 98 - PROGRESS: at 5.01% examples, 988212 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:57,520 : INFO : EPOCH 98 - PROGRESS: at 10.18% examples, 1014054 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:45:58,522 : INFO : EPOCH 98 - PROGRESS: at 15.29% examples, 1017308 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:45:59,525 : INFO : EPOCH 98 - PROGRESS: at 20.57% examples, 1024377 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:00,533 : INFO : EPOCH 98 - PROGRESS: at 25.52% examples, 1025130 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:01,536 : INFO : EPOCH 98 - PROGRESS: at 30.40% examples, 1022788 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:02,539 : INFO : EPOCH 98 - PROGRESS: at 35.35% examples, 1024355 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:03,543 : INFO : EPOCH 98 - PROGRESS: at 40.51% examples, 1031026 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:04,547 : INFO : EPOCH 98 - PROGRESS: at 45.60% examples, 1034775 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:05,552 : INFO : EPOCH 98 - PROGRESS: at 50.74% examples, 1039733 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:06,552 : INFO : EPOCH 98 - PROGRESS: at 55.57% examples, 1039003 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:07,557 : INFO : EPOCH 98 - PROGRESS: at 60.63% examples, 1039993 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:08,571 : INFO : EPOCH 98 - PROGRESS: at 65.77% examples, 1041272 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:09,571 : INFO : EPOCH 98 - PROGRESS: at 70.95% examples, 1041261 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:10,574 : INFO : EPOCH 98 - PROGRESS: at 76.08% examples, 1041763 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:11,579 : INFO : EPOCH 98 - PROGRESS: at 80.60% examples, 1032826 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:12,581 : INFO : EPOCH 98 - PROGRESS: at 85.99% examples, 1034699 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:13,588 : INFO : EPOCH 98 - PROGRESS: at 91.54% examples, 1036483 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:14,589 : INFO : EPOCH 98 - PROGRESS: at 96.78% examples, 1036080 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:15,152 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-01-27 05:46:15,158 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-01-27 05:46:15,159 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-01-27 05:46:15,160 : INFO : EPOCH - 98 : training on 29733777 raw words (20388132 effective words) took 19.6s, 1037836 effective words/s\n2020-01-27 05:46:16,173 : INFO : EPOCH 99 - PROGRESS: at 5.15% examples, 1008131 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:17,182 : INFO : EPOCH 99 - PROGRESS: at 10.35% examples, 1024452 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:18,188 : INFO : EPOCH 99 - PROGRESS: at 15.64% examples, 1034393 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:19,192 : INFO : EPOCH 99 - PROGRESS: at 20.83% examples, 1033540 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:20,196 : INFO : EPOCH 99 - PROGRESS: at 25.89% examples, 1037121 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:21,208 : INFO : EPOCH 99 - PROGRESS: at 30.84% examples, 1034475 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:22,216 : INFO : EPOCH 99 - PROGRESS: at 35.95% examples, 1037267 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:23,225 : INFO : EPOCH 99 - PROGRESS: at 40.86% examples, 1035905 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:24,227 : INFO : EPOCH 99 - PROGRESS: at 45.89% examples, 1038700 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:25,228 : INFO : EPOCH 99 - PROGRESS: at 50.97% examples, 1041389 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:26,233 : INFO : EPOCH 99 - PROGRESS: at 55.84% examples, 1040714 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:27,235 : INFO : EPOCH 99 - PROGRESS: at 60.88% examples, 1041859 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:28,249 : INFO : EPOCH 99 - PROGRESS: at 66.02% examples, 1042523 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:29,251 : INFO : EPOCH 99 - PROGRESS: at 70.81% examples, 1036927 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:30,253 : INFO : EPOCH 99 - PROGRESS: at 75.77% examples, 1035499 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:31,255 : INFO : EPOCH 99 - PROGRESS: at 80.64% examples, 1031455 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:32,268 : INFO : EPOCH 99 - PROGRESS: at 85.78% examples, 1029911 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:33,270 : INFO : EPOCH 99 - PROGRESS: at 90.97% examples, 1028464 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:34,276 : INFO : EPOCH 99 - PROGRESS: at 96.27% examples, 1028601 words/s, in_qsize 4, out_qsize 1\n2020-01-27 05:46:35,028 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-01-27 05:46:35,032 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-01-27 05:46:35,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-01-27 05:46:35,037 : INFO : EPOCH - 99 : training on 29733777 raw words (20387416 effective words) took 19.9s, 1025885 effective words/s\n2020-01-27 05:46:36,054 : INFO : EPOCH 100 - PROGRESS: at 5.11% examples, 1001342 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:37,063 : INFO : EPOCH 100 - PROGRESS: at 10.28% examples, 1017647 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:38,065 : INFO : EPOCH 100 - PROGRESS: at 15.49% examples, 1026378 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:39,071 : INFO : EPOCH 100 - PROGRESS: at 20.43% examples, 1013281 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:40,072 : INFO : EPOCH 100 - PROGRESS: at 25.49% examples, 1021464 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:41,075 : INFO : EPOCH 100 - PROGRESS: at 30.69% examples, 1031115 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:42,078 : INFO : EPOCH 100 - PROGRESS: at 35.68% examples, 1032142 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:43,082 : INFO : EPOCH 100 - PROGRESS: at 40.60% examples, 1031944 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:44,084 : INFO : EPOCH 100 - PROGRESS: at 45.63% examples, 1034258 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:45,093 : INFO : EPOCH 100 - PROGRESS: at 50.60% examples, 1035369 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:46,109 : INFO : EPOCH 100 - PROGRESS: at 55.47% examples, 1034114 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:47,117 : INFO : EPOCH 100 - PROGRESS: at 60.41% examples, 1033034 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:48,118 : INFO : EPOCH 100 - PROGRESS: at 65.26% examples, 1031802 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:49,121 : INFO : EPOCH 100 - PROGRESS: at 70.37% examples, 1031201 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:50,126 : INFO : EPOCH 100 - PROGRESS: at 75.49% examples, 1032245 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:51,139 : INFO : EPOCH 100 - PROGRESS: at 80.86% examples, 1033626 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:52,143 : INFO : EPOCH 100 - PROGRESS: at 86.07% examples, 1033267 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:53,144 : INFO : EPOCH 100 - PROGRESS: at 91.26% examples, 1031708 words/s, in_qsize 6, out_qsize 0\n2020-01-27 05:46:54,144 : INFO : EPOCH 100 - PROGRESS: at 96.70% examples, 1033774 words/s, in_qsize 5, out_qsize 0\n2020-01-27 05:46:54,768 : INFO : worker thread finished; awaiting finish of 2 more threads\n2020-01-27 05:46:54,774 : INFO : worker thread finished; awaiting finish of 1 more threads\n2020-01-27 05:46:54,778 : INFO : worker thread finished; awaiting finish of 0 more threads\n2020-01-27 05:46:54,778 : INFO : EPOCH - 100 : training on 29733777 raw words (20384479 effective words) took 19.7s, 1032959 effective words/s\n2020-01-27 05:46:54,779 : INFO : training on a 2973377700 raw words (2038687663 effective words) took 1962.4s, 1038864 effective words/s\n2020-01-27 05:46:54,779 : INFO : storing 39048x300 projection weights into Embedding/category/word2Vec/word2Vec.300d.txt\n"
    }
   ],
   "source": [
    "# write vocab to file\n",
    "if not os.path.exists('Embedding/category/word2Vec'):\n",
    "    os.makedirs('Embedding/category/word2Vec')\n",
    "    \n",
    "if not os.path.exists(\"Embedding/category/word2Vec/word2Vec.300d.txt\"):\n",
    "\n",
    "    w2vec = word2vec.Word2Vec(sentences, size=300, min_count=2,max_vocab_size=None,iter=100,\n",
    "                              sorted_vocab=1,max_final_vocab=vocab_count)\n",
    "\n",
    "    \n",
    "\n",
    "    w2vec.wv.save_word2vec_format('Embedding/category/word2Vec/word2Vec.300d.txt', binary=False)\n",
    "\n",
    "    #保存模型，供日後使用\n",
    "    # w2vec.save(\"Embedding/word2Vec/word2vec.model\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = sents1 + sents2 + sents3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2020-01-27 05:47:00,644 : INFO : loading projection weights from Embedding/category/word2Vec/word2Vec.300d.txt\n2020-01-27 05:47:08,619 : INFO : loaded (39048, 300) matrix from Embedding/category/word2Vec/word2Vec.300d.txt\n2020-01-27 05:47:08,621 : INFO : precomputing L2-norms of word weight vectors\n"
    },
    {
     "data": {
      "text/plain": "[('vlc', 0.44760793447494507),\n ('quicktime', 0.4168507158756256),\n ('tv', 0.41560834646224976),\n ('editor', 0.4100143611431122),\n ('microsoft', 0.4048650860786438),\n ('avi', 0.39370083808898926),\n ('realplayer', 0.3936995267868042),\n ('application', 0.39163148403167725),\n ('wma', 0.39099571108818054),\n ('dvd', 0.3842791020870209)]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型讀取方式\n",
    "# model = word2vec.Word2Vec.load(\"Embedding/word2Vec/word2vec.model\")\n",
    "\n",
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'Embedding/category/word2Vec/word2Vec.300d.txt', binary=False, encoding='utf-8')\n",
    "\n",
    "wvmodel.most_similar(u\"player\", topn=10)\n",
    "# wvmodel.most_similar(['dvd','player','changer','machine','video'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  after removing the cwd from sys.path.\n/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  \n/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  # Remove the CWD from sys.path while we load stuff.\nWriting vocab file...\nFinished writing vocab file\n"
    }
   ],
   "source": [
    "vocab_file = \"Embedding/category/word2Vec/word.vocab\"\n",
    "\n",
    "if not os.path.exists(vocab_file):\n",
    "    vocab_count = len(wvmodel.wv.index2entity)    \n",
    "\n",
    "    print(\"Writing vocab file...\")\n",
    "    with open(vocab_file, 'w',encoding='utf-8') as writer:\n",
    "        for word in wvmodel.wv.index2entity[:vocab_count]:\n",
    "            # print(word, w2vec.wv.vocab[word].count)\n",
    "            writer.write(word + ' ' + str(wvmodel.wv.vocab[word].count) + '\\n') # Output vocab count\n",
    "    print(\"Finished writing vocab file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "take\n/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  \"\"\"Entry point for launching an IPython kernel.\n/home/eagleuser/.conda/envs/Leyan/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  \n"
    }
   ],
   "source": [
    "word = wvmodel.wv.index2entity[25]\n",
    "vector = wvmodel.wv.vectors[25]\n",
    "print(word)\n",
    "# print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(39049, 300)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from data_util.data import Vocab\n",
    "vocab_size = len(wvmodel.vocab) + 1\n",
    "\n",
    "\n",
    "vocab = Vocab('Embedding/category/word2Vec/word.vocab', vocab_size)\n",
    "\n",
    "embed_size = 300\n",
    "weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "for i in range(len(vocab._id_to_word.keys())):\n",
    "    try:\n",
    "        vocab_word = vocab._id_to_word[i+4]\n",
    "        w2vec_word = w2vec.wv.index2entity[i]\n",
    "    except Exception as e :\n",
    "        continue\n",
    "    if i + 4 > vocab_size: break\n",
    "#     print(vocab_word,w2vec_word)\n",
    "    weight[i+4, :] = torch.from_numpy(w2vec.wv.vectors[i])\n",
    "        \n",
    "embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "embedding.weight.requires_grad = True\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.word2id('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding/glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Glove\n",
    "from glove import Corpus\n",
    "\n",
    "vocab_count = 50000\n",
    "# write vocab to file\n",
    "if not os.path.exists('Embedding/category/glove'):\n",
    "    os.makedirs('Embedding/category/glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Dict size: 70986\nCollocations: 9370642\nPerforming 100 training epochs with 10 threads\nEpoch 0\nEpoch 1\nEpoch 2\nEpoch 3\nEpoch 4\nEpoch 5\nEpoch 6\nEpoch 7\nEpoch 8\nEpoch 9\nEpoch 10\nEpoch 11\nEpoch 12\nEpoch 13\nEpoch 14\nEpoch 15\nEpoch 16\nEpoch 17\nEpoch 18\nEpoch 19\nEpoch 20\nEpoch 21\nEpoch 22\nEpoch 23\nEpoch 24\nEpoch 25\nEpoch 26\nEpoch 27\nEpoch 28\nEpoch 29\nEpoch 30\nEpoch 31\nEpoch 32\nEpoch 33\nEpoch 34\nEpoch 35\nEpoch 36\nEpoch 37\nEpoch 38\nEpoch 39\nEpoch 40\nEpoch 41\nEpoch 42\nEpoch 43\nEpoch 44\nEpoch 45\nEpoch 46\nEpoch 47\nEpoch 48\nEpoch 49\nEpoch 50\nEpoch 51\nEpoch 52\nEpoch 53\nEpoch 54\nEpoch 55\nEpoch 56\nEpoch 57\nEpoch 58\nEpoch 59\nEpoch 60\nEpoch 61\nEpoch 62\nEpoch 63\nEpoch 64\nEpoch 65\nEpoch 66\nEpoch 67\nEpoch 68\nEpoch 69\nEpoch 70\nEpoch 71\nEpoch 72\nEpoch 73\nEpoch 74\nEpoch 75\nEpoch 76\nEpoch 77\nEpoch 78\nEpoch 79\nEpoch 80\nEpoch 81\nEpoch 82\nEpoch 83\nEpoch 84\nEpoch 85\nEpoch 86\nEpoch 87\nEpoch 88\nEpoch 89\nEpoch 90\nEpoch 91\nEpoch 92\nEpoch 93\nEpoch 94\nEpoch 95\nEpoch 96\nEpoch 97\nEpoch 98\nEpoch 99\n"
    }
   ],
   "source": [
    "if not os.path.exists(\"Embedding/category/glove/glove.model\"):\n",
    "\n",
    "    corpus_model = Corpus()\n",
    "    corpus_model.fit(sentences, window=10)\n",
    "    #corpus_model.save('corpus.model')\n",
    "    print('Dict size: %s' % len(corpus_model.dictionary))\n",
    "    print('Collocations: %s' % corpus_model.matrix.nnz)\n",
    "    \n",
    "    glove = Glove(no_components=300, learning_rate=0.05)\n",
    "    glove.fit(corpus_model.matrix, epochs=100,\n",
    "              no_threads=10, verbose=True)\n",
    "    glove.add_dictionary(corpus_model.dictionary)\n",
    "    \n",
    "    glove.save('Embedding/category/glove/glove.model') # 存模型\n",
    "    corpus_model.save('Embedding/category/glove/corpus.model') # 存字典\n",
    "\n",
    "\n",
    "glove = Glove.load('Embedding/category/glove/glove.model')\n",
    "corpus_model = Corpus.load('Embedding/category/glove/corpus.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Writing vocab file...\nFinished writing vocab file 39045\n"
    }
   ],
   "source": [
    "vocab_file = \"Embedding/category/glove/word.vocab\"\n",
    "\n",
    "if not os.path.exists(vocab_file):\n",
    "#     vocab_count = len(glove.dictionary)    \n",
    "    vocab_count = 0\n",
    "    print(\"Writing vocab file...\")\n",
    "    with open(vocab_file, 'w',encoding='utf-8') as writer:\n",
    "        for word,idx in glove.dictionary.items():\n",
    "            if word in vocab._word_to_id.keys():\n",
    "                vocab_count += 1\n",
    "                writer.write(word + ' ' + str(idx) + '\\n') # Output vocab count\n",
    "    print(\"Finished writing vocab file %s\" %(vocab_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(300,)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.word_vectors[glove.dictionary['.']].shape\n",
    "# vocab._word_to_id.keys()\n",
    "# len(glove.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "39045\n"
    },
    {
     "data": {
      "text/plain": "Embedding(39045, 300)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(open('Embedding/category/glove/word.vocab').readlines())\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = Vocab('Embedding/category/glove/word.vocab', vocab_size)\n",
    "embed_size = 300\n",
    "weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "for word,idx in glove.dictionary.items():\n",
    "    if word in vocab._word_to_id.keys():\n",
    "        wid = vocab.word2id(word) \n",
    "        vector = np.asarray(glove.word_vectors[glove.dictionary[word]], \"float32\")\n",
    "        weight[wid, :] = torch.from_numpy(vector)\n",
    "\n",
    "embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "embedding.weight.requires_grad = True\n",
    "embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding/Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2020-01-27 06:03:20,921 : INFO : loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/eagleuser/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n2020-01-27 06:03:20,927 : INFO : extracting archive file /home/eagleuser/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpvk3vkdaw\n2020-01-27 06:03:23,655 : INFO : Model config {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 30522\n}\n\n"
    },
    {
     "data": {
      "text/plain": "Embedding(30522, 768, padding_idx=0)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "# BERT\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model.eval()\n",
    "model.embeddings.word_embeddings\n",
    "\n",
    "\n",
    "# vocab = Vocab('Embedding/word2Vec/word2Vec.vocab', vocab_size)\n",
    "\n",
    "# embed_size = 300\n",
    "# weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "\n",
    "# embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# # requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "# embedding.weight.requires_grad = True\n",
    "# embedding        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}