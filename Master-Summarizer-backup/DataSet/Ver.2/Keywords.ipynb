{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make data dict from category1 : Electronics category2 : Cameras\n",
      "Connect to MongoDB\n",
      "conn_mongo -- uri: mongodb://root:1234@localhost:27017/admin?authMechanism=SCRAM-SHA-1\n",
      "Auth :  True\n",
      "Connect to db : Amazon \n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# NEW_PROD_DICT \n",
    "from data_util.product import *\n",
    "from data_util.MongoDB import *\n",
    "from data_util.stopwords import *\n",
    "from data_util.preprocess import *\n",
    "#%%\n",
    "import spacy\n",
    "# gpu = spacy.prefer_gpu()\n",
    "# print('GPU:', gpu)\n",
    "# pip install -U spacy[cuda100]\n",
    "# python -m spacy validate\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "import re\n",
    "# from stopwords import *\n",
    "import nltk\n",
    "# from preprocess import *\n",
    "# from feature import *\n",
    "\n",
    "from textblob import TextBlob\n",
    "import collections\n",
    "\n",
    "from spacy.symbols import cop, acomp, amod, conj, neg, nn, nsubj, dobj\n",
    "from spacy.symbols import VERB, NOUN, PROPN, ADJ, ADV, AUX, PART\n",
    "\n",
    "pattern_counter = collections.Counter()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "# from MongoDB import MongoDB\n",
    "import os\n",
    "\n",
    "import dateutil.parser as parser\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Spec From Mongo\n",
    "# from product import *\n",
    "# mongoObj = DVD_Player()\n",
    "mongoObj = Cameras()\n",
    "# mongoObj = Cell_Phones()\n",
    "# mongoObj = GPS()\n",
    "# mongoObj = Keyboards()\n",
    "#--------------------------------#\n",
    "# mongoObj = Home_Kitchen()\n",
    "# mongoObj = Cloth_Shoes_Jewelry()\n",
    "# mongoObj = Grocery_Gourmet_Food()\n",
    "# mongoObj = Automotive()\n",
    "# mongoObj = Toys_Games()\n",
    "#--------------------------------#\n",
    "'''\n",
    "cd D:\\WorkSpace\\JupyterWorkSpace\\Text-Summarizer-BERT2\\\n",
    "D:\n",
    "activate tensorflow\n",
    "python makeDataDict.py\n",
    "'''\n",
    "main_cat, category1, category2, cond_date = mongoObj.getAttr()\n",
    "print(\"make data dict from category1 : %s category2 : %s\" % (category1, category2))\n",
    "\n",
    "# Connect MongoDB\n",
    "print(\"Connect to MongoDB\")\n",
    "mongo = MongoDB()\n",
    "mongo.conn_db(db_name='Amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer \n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "import re\n",
    "from data_util.stopwords import *\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def remove_word5(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    for k, v in contractions.items():\n",
    "        if k in text:\n",
    "            text = text.replace(k, v)\n",
    "\n",
    "    for k in html_escape_table:\n",
    "        if k in text:\n",
    "            text = text.replace(k, \"\")\n",
    "\n",
    "    text = text.replace(\"-\", '')   \n",
    "    text = text.replace(\"\\\"\", '')\n",
    "    text = text.replace(\"\\n\", '')     \n",
    "\n",
    "    remove_chars = '[\"#$%&\\'\\\"\\()*+:<=>?@★【】《》“”‘’[\\\\]^_`{|}~]+'\n",
    "    text = re.sub(remove_chars, \"\", text)  # remove number and segment\n",
    "\n",
    "    text = text.replace(\"\\\\\", '')\n",
    "    text = text.replace(\"/\", '')\n",
    "    return text\n",
    "\n",
    "def process(text):\n",
    "    if type(text) == list: \n",
    "        if len(text) == 1 : text = text[0]\n",
    "        else: text = \"\\n\".join(text)\n",
    "    text = remove_tags(text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = remove_word5(text)\n",
    "    text = [line.strip() for line in text.split(\"\\n\") if line != '']\n",
    "    text = \"\\n\".join(text)\n",
    "    \n",
    "    text = \" \".join(bert_tokenizer.tokenize(text))\n",
    "    text = text.replace(\" ##\",\"\")\n",
    "    text = text.split(\" . \")\n",
    "    text = [line.replace(\" .\",\"\") for line in text]\n",
    "    text = [line for line in text if len(line) > 0]\n",
    "\n",
    "    return text\n",
    "\n",
    "descr = \"\"\"\n",
    "\"Amerock BP5322BJ Allison Value Hardware 1-1/2\\\" Round Knob, BlackAmerock is introducing the launch of 36 new skus, 12 size extensions and applying its best selling finishes to the Allison Value Hardware collection. The new product assortment will add additional depth and breadth to this collection.Amerock BP5322BJ Allison Value Hardware 1-1/2\\\" Round Knob, Black Features: From rustic to modern-day casual to sophisticated beauty, the Allison Value Hardware collection offers a variety of designs, making on-trend, quality hardware affordable 1 x #8 32 x 1''Amerock BP5322BJ Allison Value Hardware 1-1/2\\\" Round Knob, Black Specifications: Collection: Allison Value Hardware Diameter (Inches): 1.5 Finish: Black Projection (Inches): 1.125 Type: Knob Model Variations: BP5322-BJ, BP5322, BP5322BJ Previously known as: BP5322-BJ\", \n",
    "        \"The Amerock Ceramic Knob comes in black and is 1-1/2 inches in diameter. This collection is a favorite accent for wood cabinetry and a popular match for porcelain fixtures. Choose from a wide variety of styles and finishes to create the look you want. These high quality ceramics are skillfully crafted to ensure durability and lasting beauty. For more than 70 years, Amerock has manufactured quality cabinet hardware and provided dependable service nationwide.\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "斷詞辭典 已取得\n",
      "negative-words.txt\n",
      "positive-words.txt\n",
      "total-words 已取得\n"
     ]
    }
   ],
   "source": [
    "from data_util.stopwords import *\n",
    "from data_util.preprocess import *\n",
    "from data_util.eda import *\n",
    "\n",
    "def lemm_keyword(text):\n",
    "    text_keywords = []\n",
    "    lemm_sents = []\n",
    "    for line in descr:\n",
    "        lemm_text = lemm_sent_process4(line, remove_stopwords=False, summary=False, mode=\"spacy\",withdot=False)\n",
    "        lemm_text = lemm_text.replace(\"\\n\",'')\n",
    "        text_keywords2 = PF_rule_POS(lemm_text).run()\n",
    "        lemm_sents.append(lemm_text)\n",
    "        text_keywords.extend(text_keywords2)\n",
    "\n",
    "    return lemm_sents, text_keywords\n",
    "\n",
    "descr = process(descr)\n",
    "# lemm_keyword(descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make product spec feature from 30251 products...\n"
     ]
    }
   ],
   "source": [
    "def loadProdSpecData():\n",
    "    global big_categories, small_categories, main_cat, cond_date\n",
    "    db_col = 'new_Product2'\n",
    "    PROD_DICT = {}\n",
    "    SPEC_LIST, ASIN_LIST, TITLE_LIST = [], [], []\n",
    "    prod_cursor = mongo.searchInDB(mongoObj.getProductKey(), db_col=db_col)\n",
    "    docCount = prod_cursor.count()\n",
    "    print(\"make product spec feature from %s products...\" % (docCount))\n",
    "    return prod_cursor , docCount    \n",
    "prod_cursor , docCount = loadProdSpecData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30251/30251 [01:49<00:00, 275.97it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameras/prod.xlsx Write finished\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "asin_list, title_list, description_list, big_categories_list, small_categories_list, salesRank_list, feature_list = \\\n",
    "[] , [] , [] , [] , [] , [] , []\n",
    "\n",
    "with tqdm(total=docCount) as pbar:\n",
    "    for i1, prod in enumerate(prod_cursor):\n",
    "        asin, title, description, big_categories, small_categories, salesRank, feature = prod[\"asin\"], prod[\"title\"], \\\n",
    "        prod[\"description\"], prod[\"category1\"], prod[\"category2\"], prod[\"salesRank\"], prod[\"feature\"]\n",
    "        asin_list.append(asin)\n",
    "        title_list.append(title)\n",
    "        description_list.append(description)\n",
    "        big_categories_list.append(big_categories)\n",
    "        small_categories_list.append(small_categories)\n",
    "        salesRank_list.append(salesRank)\n",
    "        feature_list.append(feature)       \n",
    "        pbar.update(1)\n",
    "        \n",
    "    df = pd.DataFrame({\"asin\":asin_list, \"title\": title_list, \"description\": description_list, \"big_categories\": big_categories_list,\n",
    "                        \"small_categories\": small_categories_list , \"salesRank\": salesRank_list,\"feature\": feature_list})\n",
    "\n",
    "if not os.path.exists(category2):\n",
    "    os.makedirs(category2)\n",
    "        \n",
    "csv_path = '%s/prod.xlsx'%(category2)    \n",
    "df.head()\n",
    "df.to_excel(csv_path, encoding='utf8')\n",
    "print(csv_path + \" Write finished\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Electronics PROD_DICT :  31%|███       | 9425/30251 [57:25<2:13:52,  2.59it/s]"
     ]
    }
   ],
   "source": [
    "feature_counter = Counter()\n",
    "with tqdm(total=docCount) as pbar:\n",
    "    for i1 in range(docCount):\n",
    "        prod = df.iloc[i]\n",
    "        pbar.update(1)\n",
    "        asin, title, description, big_categories, small_categories, salesRank, feature = prod[\"asin\"], prod[\"title\"], \\\n",
    "        prod[\"description\"], prod[\"big_categories\"], prod[\"small_categories\"], prod[\"salesRank\"], prod[\"feature\"]\n",
    "\n",
    "        try:            \n",
    "            description = process(description)\n",
    "            _, keywords1 = lemm_keyword(description)\n",
    "            feature_counter.update(keywords1)\n",
    "        except Exception as e :\n",
    "            print(e)\n",
    "            \n",
    "        try:                  \n",
    "            feature = process(feature)\n",
    "            _, keywords2 = lemm_keyword(feature)\n",
    "            feature_counter.update(keywords2)\n",
    "        except Exception as e :\n",
    "            print(e)        \n",
    "        pbar.set_description(\"%s keyword \" % (big_categories))\n",
    "#         if i1 == 100: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = OrderedDict(sorted(feature_counter.items(), key=lambda pair: pair[1], reverse=True))\n",
    "important_features = [(word, important_features[word]) for word in important_features if important_features[word] > 0]\n",
    "print(\"Count : %s\" % (len(important_features)))\n",
    "\n",
    "fn3 = '%s/prod_keywords.txt'%(category2)\n",
    "with open(fn3, 'w', encoding=\"utf-8\") as f:\n",
    "    total_keywords = set()\n",
    "    for word, v in important_features:\n",
    "        f.write(\"%s:%s \\n\" % (word, v))\n",
    "        total_keywords.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script Keywords.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
