{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# from product import *\n",
    "# from data_util.product import *\n",
    "from data_util.mainCat import *\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "import nltk\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "# from product import *\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "CHUNK_SIZE = 1000  # num examples per chunk, for the chunked data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key word Attention DataSet 讀取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLSX/main_cat/Camera & Photo_key.xlsx Read finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_ID</th>\n",
       "      <th>review</th>\n",
       "      <th>summary</th>\n",
       "      <th>big_categories</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>small_categories</th>\n",
       "      <th>lemm_review</th>\n",
       "      <th>lemm_summary</th>\n",
       "      <th>lemm_review_len</th>\n",
       "      <th>lemm_summary_len</th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>total_keyword</th>\n",
       "      <th>FOP_sents</th>\n",
       "      <th>total_mention_features</th>\n",
       "      <th>bert_review</th>\n",
       "      <th>bert_summary</th>\n",
       "      <th>bert_review_len</th>\n",
       "      <th>bert_summary_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1453161600</td>\n",
       "      <td>this antenna set contains a 3 petal cloverleaf...</td>\n",
       "      <td>antenna specifications</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>this antenna set contain a 3 petal cloverleaf ...</td>\n",
       "      <td>&lt;s&gt; antenna specification &lt;/s&gt; \\n</td>\n",
       "      <td>91</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>antenna cloverleaf,antenna petal sma reverse,p...</td>\n",
       "      <td>this antenna set contain a 3 petal cloverleaf ...</td>\n",
       "      <td>antenna set petal reverse connector polarity r...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1476921600</td>\n",
       "      <td>i used the item maybe 3 times and when i was r...</td>\n",
       "      <td>bad investment i would not recommend</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>i use the item maybe 3 time and when i was rid...</td>\n",
       "      <td>&lt;s&gt; bad investment i would not recommend &lt;/s&gt; \\n</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ride item time camera policy spend day pass re...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1474416000</td>\n",
       "      <td>there is no real reviews so i think i should c...</td>\n",
       "      <td>bummer.</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>there is no real review so i think i should ch...</td>\n",
       "      <td>&lt;s&gt; bummer &lt;/s&gt; \\n</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>review real sidewalk even tiny,sidewalk tiny</td>\n",
       "      <td>there is no real review so i think i should ch...</td>\n",
       "      <td>chime attachment purchase real osmo review ass...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1486512000</td>\n",
       "      <td>despite the shock the iphone 7 stabilizer usin...</td>\n",
       "      <td>despite the shock absorber it did not do much ...</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>despite the shock the iphone 7 stabilizer use ...</td>\n",
       "      <td>&lt;s&gt; despite the shock absorber it did not do ...</td>\n",
       "      <td>50</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>create version stabilizer osmo iphone close os...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480204800</td>\n",
       "      <td>lots of vibration when the unit is facing forw...</td>\n",
       "      <td>lots of vibration.</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>lot of vibration when the unit is face forward...</td>\n",
       "      <td>&lt;s&gt; lot of vibration &lt;/s&gt; \\n</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>footage usable adapter axis</td>\n",
       "      <td>\\nthe only usable footage was when you pan to ...</td>\n",
       "      <td>forward lot unit face side footage shutter spe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_ID                                             review  \\\n",
       "0  1453161600  this antenna set contains a 3 petal cloverleaf...   \n",
       "1  1476921600  i used the item maybe 3 times and when i was r...   \n",
       "2  1474416000  there is no real reviews so i think i should c...   \n",
       "3  1486512000  despite the shock the iphone 7 stabilizer usin...   \n",
       "4  1480204800  lots of vibration when the unit is facing forw...   \n",
       "\n",
       "                                             summary big_categories  \\\n",
       "0                             antenna specifications     Appliances   \n",
       "1               bad investment i would not recommend     Appliances   \n",
       "2                                            bummer.     Appliances   \n",
       "3  despite the shock absorber it did not do much ...     Appliances   \n",
       "4                                 lots of vibration.     Appliances   \n",
       "\n",
       "         main_cat     small_categories  \\\n",
       "0  Camera & Photo  Parts & Accessories   \n",
       "1  Camera & Photo  Parts & Accessories   \n",
       "2  Camera & Photo  Parts & Accessories   \n",
       "3  Camera & Photo  Parts & Accessories   \n",
       "4  Camera & Photo  Parts & Accessories   \n",
       "\n",
       "                                         lemm_review  \\\n",
       "0  this antenna set contain a 3 petal cloverleaf ...   \n",
       "1  i use the item maybe 3 time and when i was rid...   \n",
       "2  there is no real review so i think i should ch...   \n",
       "3  despite the shock the iphone 7 stabilizer use ...   \n",
       "4  lot of vibration when the unit is face forward...   \n",
       "\n",
       "                                        lemm_summary  lemm_review_len  \\\n",
       "0                  <s> antenna specification </s> \\n               91   \n",
       "1   <s> bad investment i would not recommend </s> \\n               47   \n",
       "2                                 <s> bummer </s> \\n              242   \n",
       "3   <s> despite the shock absorber it did not do ...               50   \n",
       "4                       <s> lot of vibration </s> \\n               59   \n",
       "\n",
       "   lemm_summary_len  overall  vote  \\\n",
       "0                 6        5     2   \n",
       "1                10        1     5   \n",
       "2                 5        2    13   \n",
       "3                22        2     3   \n",
       "4                 7        1     7   \n",
       "\n",
       "                                       total_keyword  \\\n",
       "0  antenna cloverleaf,antenna petal sma reverse,p...   \n",
       "1                                                NaN   \n",
       "2       review real sidewalk even tiny,sidewalk tiny   \n",
       "3                                                NaN   \n",
       "4                        footage usable adapter axis   \n",
       "\n",
       "                                           FOP_sents  \\\n",
       "0  this antenna set contain a 3 petal cloverleaf ...   \n",
       "1                                                NaN   \n",
       "2  there is no real review so i think i should ch...   \n",
       "3                                                NaN   \n",
       "4  \\nthe only usable footage was when you pan to ...   \n",
       "\n",
       "                              total_mention_features bert_review bert_summary  \\\n",
       "0  antenna set petal reverse connector polarity r...                            \n",
       "1  ride item time camera policy spend day pass re...                            \n",
       "2  chime attachment purchase real osmo review ass...                            \n",
       "3  create version stabilizer osmo iphone close os...                            \n",
       "4  forward lot unit face side footage shutter spe...                            \n",
       "\n",
       "   bert_review_len  bert_summary_len  \n",
       "0                0                 0  \n",
       "1                0                 0  \n",
       "2                0                 0  \n",
       "3                0                 0  \n",
       "4                0                 0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main_cat = All_Electronics().getAttr()\n",
    "# main_cat = Pet_Supplies().getAttr()\n",
    "# main_cat = Sports_Outdoors().getAttr()\n",
    "# main_cat = Health_personal_Care().getAttr()\n",
    "# -------------------------------------#\n",
    "# main_cat = CellPhones_Accessories().getAttr()\n",
    "main_cat = Camera_Photo().getAttr()\n",
    "# main_cat = GPS_Navigation().getAttr()\n",
    "# main_cat = Music_Instrum().getAttr()\n",
    "# main_cat = Software().getAttr()\n",
    "# main_cat = Computers().getAttr()\n",
    "# main_cat = Video_Games().getAttr()\n",
    "# -------------------------------------#\n",
    "\n",
    "xlsx_path = \"XLSX/main_cat/%s_key.xlsx\"%(main_cat)\n",
    "# df.to_csv(csv_path) #默认dt是DataFrame的一个实例，参数解释如下\n",
    "# key_train_df.to_excel(csv_path, encoding='utf8')\n",
    "orign_key_df = pd.read_excel(xlsx_path)\n",
    "print(xlsx_path + \" Read finished\")\n",
    "len(orign_key_df)\n",
    "\n",
    "orign_key_df['bert_review'] = '' \n",
    "orign_key_df['bert_summary'] = ''\n",
    "orign_key_df['bert_review_len'] = 0\n",
    "orign_key_df['bert_summary_len'] = 0\n",
    "\n",
    "orign_key_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key word load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load FOP-View/Camera & Photo_keywords2.txt keywords...\n"
     ]
    }
   ],
   "source": [
    "fn = 'FOP-View/%s_keywords2.txt' % (main_cat)\n",
    "print('load %s keywords...' % (fn))\n",
    "total_keywords = set()\n",
    "with open(fn, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        k, v = line.split(\":\")\n",
    "        total_keywords.add(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative-words.txt\n",
      "positive-words.txt\n",
      "total-words 已取得\n"
     ]
    }
   ],
   "source": [
    "opinion_lexicon = {}\n",
    "for filename in os.listdir('opinion-lexicon-English/'):      \n",
    "    if \"txt\" not in filename: continue\n",
    "    print(filename)\n",
    "    with open('opinion-lexicon-English/'+filename,'r') as f_input:\n",
    "        lexion = []\n",
    "        for line in f_input:\n",
    "            if line.startswith(\";\"):\n",
    "                continue\n",
    "            word = line.replace(\"\\n\",\"\")\n",
    "            if word != \"\" : lexion.append(word)\n",
    "        pos = filename.replace(\".txt\",\"\")\n",
    "        opinion_lexicon[pos] = lexion\n",
    "\n",
    "opinion_lexicon[\"total-words\"] = opinion_lexicon[\"negative-words\"] + opinion_lexicon[\"positive-words\"]\n",
    "print(\"total-words 已取得\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 裡頭的字典資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小： 30522\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "vocab = tokenizer.vocab # word_to_id\n",
    "print(\"字典大小：\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert-Summary 資料清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def isnumber(aString):\n",
    "    try:\n",
    "        float(aString)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def create_custom_tokenizer(nlp):\n",
    "    prefix_re = re.compile(r'[0-9]\\.')\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search)\n",
    "\n",
    "nlp.tokenizer = create_custom_tokenizer(nlp)\n",
    "\n",
    "alphbet_stopword = ['b','c','d','e','f','g','h','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_ID</th>\n",
       "      <th>review</th>\n",
       "      <th>summary</th>\n",
       "      <th>big_categories</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>small_categories</th>\n",
       "      <th>lemm_review</th>\n",
       "      <th>lemm_summary</th>\n",
       "      <th>lemm_review_len</th>\n",
       "      <th>lemm_summary_len</th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>total_keyword</th>\n",
       "      <th>FOP_sents</th>\n",
       "      <th>total_mention_features</th>\n",
       "      <th>bert_review</th>\n",
       "      <th>bert_summary</th>\n",
       "      <th>bert_review_len</th>\n",
       "      <th>bert_summary_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1453161600</td>\n",
       "      <td>this antenna set contains a 3 petal cloverleaf...</td>\n",
       "      <td>antenna specifications</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>this antenna set contain a 3 petal cloverleaf ...</td>\n",
       "      <td>&lt;s&gt; antenna specification &lt;/s&gt; \\n</td>\n",
       "      <td>91</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>antenna cloverleaf,antenna petal sma reverse,p...</td>\n",
       "      <td>this antenna set contain a 3 petal cloverleaf ...</td>\n",
       "      <td>antenna set petal reverse connector polarity r...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1476921600</td>\n",
       "      <td>i used the item maybe 3 times and when i was r...</td>\n",
       "      <td>bad investment i would not recommend</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>i use the item maybe 3 time and when i was rid...</td>\n",
       "      <td>&lt;s&gt; bad investment i would not recommend &lt;/s&gt; \\n</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ride item time camera policy spend day pass re...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1474416000</td>\n",
       "      <td>there is no real reviews so i think i should c...</td>\n",
       "      <td>bummer.</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>there is no real review so i think i should ch...</td>\n",
       "      <td>&lt;s&gt; bummer &lt;/s&gt; \\n</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>review real sidewalk even tiny,sidewalk tiny</td>\n",
       "      <td>there is no real review so i think i should ch...</td>\n",
       "      <td>chime attachment purchase real osmo review ass...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1486512000</td>\n",
       "      <td>despite the shock the iphone 7 stabilizer usin...</td>\n",
       "      <td>despite the shock absorber it did not do much ...</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>despite the shock the iphone 7 stabilizer use ...</td>\n",
       "      <td>&lt;s&gt; despite the shock absorber it did not do ...</td>\n",
       "      <td>50</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>create version stabilizer osmo iphone close os...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480204800</td>\n",
       "      <td>lots of vibration when the unit is facing forw...</td>\n",
       "      <td>lots of vibration.</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Camera &amp; Photo</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>lot of vibration when the unit is face forward...</td>\n",
       "      <td>&lt;s&gt; lot of vibration &lt;/s&gt; \\n</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>footage usable adapter axis</td>\n",
       "      <td>\\nthe only usable footage was when you pan to ...</td>\n",
       "      <td>forward lot unit face side footage shutter spe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_ID                                             review  \\\n",
       "0  1453161600  this antenna set contains a 3 petal cloverleaf...   \n",
       "1  1476921600  i used the item maybe 3 times and when i was r...   \n",
       "2  1474416000  there is no real reviews so i think i should c...   \n",
       "3  1486512000  despite the shock the iphone 7 stabilizer usin...   \n",
       "4  1480204800  lots of vibration when the unit is facing forw...   \n",
       "\n",
       "                                             summary big_categories  \\\n",
       "0                             antenna specifications     Appliances   \n",
       "1               bad investment i would not recommend     Appliances   \n",
       "2                                            bummer.     Appliances   \n",
       "3  despite the shock absorber it did not do much ...     Appliances   \n",
       "4                                 lots of vibration.     Appliances   \n",
       "\n",
       "         main_cat     small_categories  \\\n",
       "0  Camera & Photo  Parts & Accessories   \n",
       "1  Camera & Photo  Parts & Accessories   \n",
       "2  Camera & Photo  Parts & Accessories   \n",
       "3  Camera & Photo  Parts & Accessories   \n",
       "4  Camera & Photo  Parts & Accessories   \n",
       "\n",
       "                                         lemm_review  \\\n",
       "0  this antenna set contain a 3 petal cloverleaf ...   \n",
       "1  i use the item maybe 3 time and when i was rid...   \n",
       "2  there is no real review so i think i should ch...   \n",
       "3  despite the shock the iphone 7 stabilizer use ...   \n",
       "4  lot of vibration when the unit is face forward...   \n",
       "\n",
       "                                        lemm_summary  lemm_review_len  \\\n",
       "0                  <s> antenna specification </s> \\n               91   \n",
       "1   <s> bad investment i would not recommend </s> \\n               47   \n",
       "2                                 <s> bummer </s> \\n              242   \n",
       "3   <s> despite the shock absorber it did not do ...               50   \n",
       "4                       <s> lot of vibration </s> \\n               59   \n",
       "\n",
       "   lemm_summary_len  overall  vote  \\\n",
       "0                 6        5     2   \n",
       "1                10        1     5   \n",
       "2                 5        2    13   \n",
       "3                22        2     3   \n",
       "4                 7        1     7   \n",
       "\n",
       "                                       total_keyword  \\\n",
       "0  antenna cloverleaf,antenna petal sma reverse,p...   \n",
       "1                                                NaN   \n",
       "2       review real sidewalk even tiny,sidewalk tiny   \n",
       "3                                                NaN   \n",
       "4                        footage usable adapter axis   \n",
       "\n",
       "                                           FOP_sents  \\\n",
       "0  this antenna set contain a 3 petal cloverleaf ...   \n",
       "1                                                NaN   \n",
       "2  there is no real review so i think i should ch...   \n",
       "3                                                NaN   \n",
       "4  \\nthe only usable footage was when you pan to ...   \n",
       "\n",
       "                              total_mention_features bert_review bert_summary  \\\n",
       "0  antenna set petal reverse connector polarity r...                            \n",
       "1  ride item time camera policy spend day pass re...                            \n",
       "2  chime attachment purchase real osmo review ass...                            \n",
       "3  create version stabilizer osmo iphone close os...                            \n",
       "4  forward lot unit face side footage shutter spe...                            \n",
       "\n",
       "   bert_review_len  bert_summary_len  \n",
       "0                0                 0  \n",
       "1                0                 0  \n",
       "2                0                 0  \n",
       "3                0                 0  \n",
       "4                0                 0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compose_summary(x):\n",
    "    x = x.replace(\"\\n\",\" \")\n",
    "    #x = x.replace(\"\\n\",\"\").replace(\"</s>\",\"\").replace(\"<s>\",\"\")\n",
    "    #x = \"<s>\" + x + \"</s>\"  \n",
    "    tokens = [str(token) for token in x.split(\" \") if (\" \" not in str(token)) and \\\n",
    "                   #(str(token).isalpha()) and \\\n",
    "                  (len(str(token)) > 1)   ]\n",
    "    # print(tokens)\n",
    "    #return \" \".join(tokens),tokens\n",
    "    \n",
    "    newtokens = []\n",
    "    for token in tokens:\n",
    "        if (isnumber(token) or len(token) == 1 or token == \".\") and (token not in alphbet_stopword):\n",
    "            newtokens.append(token)\n",
    "        else:\n",
    "            token = token.replace(\".\",\" . \")\n",
    "            sub_tokens = token.split(\" \")\n",
    "            sub_tokens = [t for t in sub_tokens if t != \"\" and t not in alphbet_stopword]\n",
    "            newtokens.extend(sub_tokens)\n",
    "    \n",
    "#     dot_tokens = [token for token in newtokens if (token[0] == \".\" or token[-1] == \".\") and (len(token)>1)]\n",
    "#     if len(dot_tokens) > 0 :print(dot_tokens)\n",
    "        \n",
    "    return \" \".join(newtokens).replace(' . . ',' . '),newtokens\n",
    "    \n",
    "\n",
    "def bert_compose_summary(newtokens):\n",
    "#     x = x.replace(\"\\n\",\"\").replace(\"</s>\",\"\").replace(\"<s>\",\"\")\n",
    "#     x = \"<s>\" + x + \"</s>\"  \n",
    "#     tokens = [str(token) for token in nlp(x) if (\" \" not in str(token)) and \\\n",
    "#                   (str(token).isalpha()) and \\\n",
    "#                   (len(str(token)) > 1)   ]\n",
    "    \n",
    "#     newtokens = []\n",
    "#     for token in tokens:\n",
    "#         if (isnumber(token) or len(token) == 1 or token == \".\") and (token not in alphbet_stopword):\n",
    "#             newtokens.append(token)\n",
    "#         else:\n",
    "#             token = token.replace(\".\",\" . \")\n",
    "#             sub_tokens = token.split(\" \")\n",
    "#             sub_tokens = [t for t in sub_tokens if t != \"\" and t not in alphbet_stopword]\n",
    "#             newtokens.extend(sub_tokens)\n",
    "\n",
    "#     dot_tokens = [t for t in newtokens if (\".\" in t) and (len(t) > 1) ]\n",
    "#     if len(dot_tokens) > 0 :print(dot_tokens)\n",
    "    newtokens = [t for t in newtokens if t not in [\"<s>\",\"</s>\"]]\n",
    "    newtokens = ['[CLS]'] + tokenizer.tokenize(\" \".join(newtokens)) + ['[SEP]']\n",
    "    return \" \".join(newtokens)\n",
    "\n",
    "'''\n",
    "def calc_summary_len(x):\n",
    "#     tokens = [token for token in nlp(x)]\n",
    "#     print(tokens)\n",
    "#     print([len(t) for t in tokens])\n",
    "#     return len(tokens)\n",
    "    return len(x.split(\" \"))\n",
    "\n",
    "nlp.tokenizer = create_custom_tokenizer(nlp)\n",
    "\n",
    "orign_key_df['lemm_summary'] = orign_key_df['lemm_summary'].apply(compose_summary)\n",
    "orign_key_df['lemm_summary_len'] = orign_key_df['lemm_summary'].apply(calc_summary_len)\n",
    "\n",
    "\n",
    "amount = len(orign_key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "\n",
    "orign_key_df.head()\n",
    "'''\n",
    "orign_key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "row 158542 :  42%|████▏     | 158543/379628 [2:25:52<3:35:41, 17.08it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 非符號alpha word重疊數\n",
    "with tqdm(total=len(orign_key_df)) as pbar:\n",
    "    for i ,row in orign_key_df.iterrows():        \n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\"row %s \" % (i))\n",
    "\n",
    "        lemm_summary,newtokens = compose_summary(row['lemm_summary'])\n",
    "        # bert_summary = bert_compose_summary(newtokens)\n",
    "        \n",
    "        orign_key_df.loc[i,'lemm_summary'] = lemm_summary\n",
    "        # orign_key_df.loc[i,'bert_summary'] = bert_summary\n",
    "        \n",
    "        orign_key_df.loc[i,'lemm_summary_len'] = len(lemm_summary.split(\" \"))       \n",
    "        # orign_key_df.loc[i,'bert_summary_len'] = len(bert_summary.split(\" \"))\n",
    "\n",
    "        \n",
    "        \n",
    "amount = len(orign_key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "orign_key_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert-review 多句合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def compose_review(x):\n",
    "#    x = eval(x)\n",
    "#    x = \"\\n\".join(x)\n",
    "    x = x.replace(\".\\n\",\" . \").replace(\"\\n.\",\" . \").replace(\"\\n\",\" \")\n",
    "#     x = x.replace(\"\\n\",\" \")    \n",
    "    tokens = [str(token) for token in x.split(\" \") if (\" \" not in str(token))and (str(token) == '.' or str(token).isalpha())]\n",
    "#    tokens = [str(token) for token in x.split(\" \") if (\" \" not in str(token)) ]\n",
    "\n",
    "    #return \" \".join(tokens)\n",
    "    \n",
    "    newtokens = []\n",
    "    for token in tokens:\n",
    "#         if (len(token) == 1 or token == \".\"):\n",
    "        if (isnumber(token) or len(token) == 1 or token == \".\")and (token not in alphbet_stopword):\n",
    "            newtokens.append(token)\n",
    "#             if (token not in ['a','i']) and (token != \".\"): print(token)\n",
    "        else:\n",
    "#             token = token.replace(\".\",\" . \")\n",
    "            token = token.replace(\".\",\" . \")\n",
    "            sub_tokens = token.split(\" \")\n",
    "            sub_tokens = [t for t in sub_tokens if t != \"\" and t not in alphbet_stopword]\n",
    "            if len(sub_tokens) == 0: continue\n",
    "            newtokens.extend(sub_tokens)\n",
    "\n",
    "    dot_tokens = [token for token in newtokens if (token[0] == \".\" or token[-1] == \".\") and (len(token)>1)]\n",
    "    if len(dot_tokens) > 0 :print(dot_tokens)\n",
    "    \n",
    "    return \" \".join(newtokens).replace(' . . ',' . ')\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def calc_review_len(x):\n",
    "#     tokens = [token for token in nlp(x)]\n",
    "#     print(tokens)\n",
    "#     print([len(t) for t in tokens])\n",
    "#     return len(tokens)\n",
    "    return len(x.split(\" \"))\n",
    "key_df = deepcopy(orign_key_df)\n",
    "key_df['lemm_review'] = key_df['lemm_review'].apply(compose_review)\n",
    "key_df['lemm_review_len'] = key_df['lemm_review'].apply(calc_review_len)\n",
    "key_df.head()\n",
    "\n",
    "'''\n",
    "def bert_compose_review(x):\n",
    "    x = eval(x)\n",
    "    review_sents = deepcopy(x)\n",
    "    total_tokens = []\n",
    "    for sent in review_sents:\n",
    "        sent = sent.replace('\\n','[SEP]')\n",
    "        tokens = [str(token) for token in sent.split(\" \") if (\" \" not in str(token)) ]\n",
    "        newtokens = []\n",
    "        for token in tokens:\n",
    "            if (isnumber(token) or len(token) == 1 or token == \".\")and (token not in alphbet_stopword):\n",
    "                newtokens.append(token)\n",
    "            else:\n",
    "                token = token.replace(\".\",\" . \")\n",
    "                sub_tokens = token.split(\" \")\n",
    "                sub_tokens = [t for t in sub_tokens if t != \"\" and t not in alphbet_stopword]\n",
    "                newtokens.extend(sub_tokens)\n",
    "        newtokens = newtokens + ['[SEP]']        \n",
    "        total_tokens.extend(newtokens)\n",
    "    total_tokens = ['[CLS]'] + tokenizer.tokenize(\" \".join(total_tokens))    \n",
    "    return \" \".join(total_tokens)\n",
    "\n",
    "'''\n",
    "# def compose_review(bert_review):\n",
    "#     return bert_review.replace('[CLS] ','').replace('[SEP] ','')\n",
    "'''\n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orign_key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_df = deepcopy(orign_key_df)\n",
    "\n",
    "from tqdm import tqdm\n",
    "# 非符號alpha word重疊數\n",
    "with tqdm(total=len(key_df)) as pbar:\n",
    "    for i ,row in key_df.iterrows():  \n",
    "        try:\n",
    "            lemm_review = compose_review(row['lemm_review'])\n",
    "            # bert_review = bert_compose_review(row['lemm_review'])\n",
    "\n",
    "            # key_df.loc[i,'bert_review'] = bert_review\n",
    "            key_df.loc[i,'lemm_review'] = lemm_review\n",
    "            \n",
    "            # key_df.loc[i,'bert_review_len'] = len(bert_review.split(\" \"))\n",
    "            key_df.loc[i,'lemm_review_len'] = len(lemm_review.split(\" \"))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            # key_df.loc[i,'bert_review_len'] = 0\n",
    "            # key_df.loc[i,'lemm_review_len'] = 0     \n",
    "        \n",
    "        pbar.set_description(\"row %s \" % (i))\n",
    "        pbar.update(1)\n",
    "\n",
    "key_df = key_df[(key_df.lemm_review_len > 0) ] # 過濾 lemm_review_len = 0\n",
    "\n",
    "amount = len(key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "\n",
    "key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_df.loc[66152]['lemm_review']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 過濾不合適的訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(text):\n",
    "    keywords = set()\n",
    "    for words in text.split(\",\"):\n",
    "        for word in words.split(\" \"):\n",
    "            keywords.add(word)\n",
    "    keywords = \" \".join(keywords)\n",
    "    return keywords\n",
    "\n",
    "def calc_keyword_num(x):\n",
    "    return len(x.split(\" \"))\n",
    "\n",
    "# and(key_df.lemm_review_len>20)\n",
    "flit_key_df = key_df[(key_df.lemm_summary_len>=4) ] # 過濾single word summary\n",
    "flit_key_df = flit_key_df[(flit_key_df.lemm_review_len <= 1000) ] # 過濾single word summary\n",
    "flit_key_df = flit_key_df[(flit_key_df.lemm_review_len >= 50) ] # 過濾single word summary\n",
    "\n",
    "flit_key_df = flit_key_df.dropna(\n",
    "    axis=0,     # 0: 对行进行操作; 1: 对列进行操作\n",
    "    how='any'   # 'any': 只要存在 NaN 就 drop 掉; 'all': 必须全部是 NaN 才 drop \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOP_keywords 資料整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flit_key_df['FOP_keywords'] = flit_key_df['total_keyword']\n",
    "flit_key_df['FOP_keywords'] = flit_key_df['FOP_keywords'].apply(to_words)\n",
    "flit_key_df['FOP_keywords_num'] = flit_key_df['FOP_keywords'].apply(calc_keyword_num)\n",
    "flit_key_df = flit_key_df[(flit_key_df.FOP_keywords_num>=2) ] # 過濾single word summary\n",
    "flit_key_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flit_key_df['Cheat'] = False \n",
    "\n",
    "# flit_key_df.head()\n",
    "from tqdm import tqdm\n",
    "# 非符號alpha word重疊數\n",
    "with tqdm(total=len(flit_key_df)) as pbar:\n",
    "    for i ,row in flit_key_df.iterrows():\n",
    "        rev_tokens = set(row['lemm_review'].split(\" \"))\n",
    "        summ_tokens = set(row['lemm_summary'].split(\" \"))\n",
    "        key_words = rev_tokens & summ_tokens & (total_keywords| set(opinion_lexicon[\"total-words\"]))\n",
    "        if len(key_words) > 2: \n",
    "            flit_key_df.loc[i,'Cheat'] = True\n",
    "        pbar.update(1)\n",
    "    \n",
    "flit_key_df = flit_key_df[(flit_key_df.Cheat == True) ] # 過濾single word summary\n",
    "amount = len(flit_key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "flit_key_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank_keywords 資料整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords as TextRank\n",
    "from summa.summarizer import summarize\n",
    "def textrank_keys(text):\n",
    "    keywords1 = list()\n",
    "    for words in TextRank.keywords(text).split('\\n'):\n",
    "        keywords1.extend(words.split(\" \"))\n",
    "    keywords1 = set(keywords1)    \n",
    "    \n",
    "    return \" \".join(list(keywords1))\n",
    "\n",
    "def textrank_summ_keys(text): \n",
    "    keywords2 = list()\n",
    "    for words in summarize(text, words=8).split('\\n'):\n",
    "        keywords2.extend(words.split(\" \"))\n",
    "    keywords2 = set(keywords2)\n",
    "    \n",
    "    return \" \".join(list(keywords2))\n",
    "\n",
    "flit_key_df.loc[:,'TextRank_keywords'] = ''\n",
    "flit_key_df.loc[:,'TextRank_summary'] = ''\n",
    "# flit_key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "with tqdm(total=len(flit_key_df)) as pbar:\n",
    "    for i ,row in flit_key_df.iterrows():\n",
    "        TextRank_keywords = textrank_keys(row['lemm_review'])\n",
    "        TextRank_summary = textrank_summ_keys(row['lemm_review'])  \n",
    "#         num = calc_keyword_num(TextRank_keywords)\n",
    "        flit_key_df.loc[i,'TextRank_keywords'] = TextRank_keywords\n",
    "        flit_key_df.loc[i,'TextRank_summary'] = TextRank_summary\n",
    "#         flit_key_df.loc[i,'TextRank_keywords_num'] = num\n",
    "        pbar.update(1)\n",
    "        \n",
    "flit_key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "if not os.path.exists('XLSX/statistic'):\n",
    "    os.makedirs('XLSX/statistic')\n",
    "with open('XLSX/statistic/%s_info.txt'%(main_cat),'w') as f:\n",
    "    max_rev_len = flit_key_df['lemm_review_len'].max()\n",
    "    min_rev_len = flit_key_df['lemm_review_len'].min()\n",
    "    mean_rev_len = flit_key_df['lemm_review_len'].mean()\n",
    "    median_rev_len = flit_key_df['lemm_review_len'].median()\n",
    "\n",
    "    f.write('max_rev_len :%s \\n'%(max_rev_len))\n",
    "    f.write('min_rev_len :%s \\n'%(min_rev_len))\n",
    "    f.write('mean_rev_len :%s \\n'%(mean_rev_len))\n",
    "    f.write('median_rev_len :%s \\n'%(median_rev_len))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_summary_len = flit_key_df['lemm_summary_len'].max()\n",
    "    min_summary_len = flit_key_df['lemm_summary_len'].min()\n",
    "    mean_summary_len = flit_key_df['lemm_summary_len'].mean()\n",
    "    median_summary_len = flit_key_df['lemm_summary_len'].median()\n",
    "\n",
    "    f.write('max_summary_len :%s \\n'%(max_summary_len))\n",
    "    f.write('min_summary_len :%s \\n'%(min_summary_len))\n",
    "    f.write('mean_summary_len :%s \\n'%(mean_summary_len))\n",
    "    f.write('median_summary_len :%s \\n'%(median_summary_len))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_FOP_keywords_num = flit_key_df['FOP_keywords_num'].max()\n",
    "    min_FOP_keywords_num = flit_key_df['FOP_keywords_num'].min()\n",
    "    mean_FOP_keywords_num = flit_key_df['FOP_keywords_num'].mean()\n",
    "    median_FOP_keywords_num = flit_key_df['FOP_keywords_num'].median()\n",
    "\n",
    "    f.write('max_FOP_keywords_num :%s \\n'%(max_FOP_keywords_num))\n",
    "    f.write('min_FOP_keywords_num :%s \\n'%(min_FOP_keywords_num))\n",
    "    f.write('mean_FOP_keywords_num :%s \\n'%(mean_FOP_keywords_num))\n",
    "    f.write('median_FOP_keywords_num :%s \\n'%(median_FOP_keywords_num))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "#     max_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].max()\n",
    "#     min_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].min()\n",
    "#     mean_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].mean()\n",
    "#     median_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].median()\n",
    "\n",
    "#     f.write('max_TextRank_keywords_num :%s \\n'%(max_TextRank_keywords_num))\n",
    "#     f.write('min_TextRank_keywords_num :%s \\n'%(min_TextRank_keywords_num))\n",
    "#     f.write('mean_TextRank_keywords_num :%s \\n'%(mean_TextRank_keywords_num))\n",
    "#     f.write('median_TextRank_keywords_num :%s \\n'%(median_TextRank_keywords_num))\n",
    "\n",
    "    '''\n",
    "    f.write('\\n\\n\\n')    \n",
    "    max_bert_rev_len = flit_key_df['bert_review_len'].max()\n",
    "    min_bert_rev_len = flit_key_df['bert_review_len'].min()\n",
    "    mean_bert_rev_len = flit_key_df['bert_review_len'].mean()\n",
    "    median_bert_rev_len = flit_key_df['bert_review_len'].median()\n",
    "\n",
    "    f.write('max_bert_rev_len :%s \\n'%(max_bert_rev_len))\n",
    "    f.write('min_bert_rev_len :%s \\n'%(min_bert_rev_len))\n",
    "    f.write('mean_bert_rev_len :%s \\n'%(mean_bert_rev_len))\n",
    "    f.write('median_bert_rev_len :%s \\n'%(median_bert_rev_len))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_bert_summary_len = flit_key_df['bert_summary_len'].max()\n",
    "    min_bert_summary_len = flit_key_df['bert_summary_len'].min()\n",
    "    mean_bert_summary_len = flit_key_df['bert_summary_len'].mean()\n",
    "    median_bert_summary_len = flit_key_df['bert_summary_len'].median()\n",
    "\n",
    "    f.write('max_bert_summary_len :%s \\n'%(max_bert_summary_len))\n",
    "    f.write('min_bert_summary_len :%s \\n'%(min_bert_summary_len))\n",
    "    f.write('mean_bert_summary_len :%s \\n'%(mean_bert_summary_len))\n",
    "    f.write('median_bert_summary_len :%s \\n'%(median_bert_summary_len))\n",
    "    '''\n",
    "\n",
    "    \n",
    "\n",
    "# plt.xlim(xmax = mean_rev_len)\n",
    "# plt.ylim(ymax = flit_key_df['lemm_review_len'].value_counts().max())\n",
    "\n",
    "flit_key_df['lemm_review_len'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/review_len_%s.png'%(main_cat))\n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "# plt.xlim(xmax = max_summary_len)\n",
    "# plt.ylim(ymax = flit_key_df['lemm_summary_len'].value_counts().max())\n",
    "flit_key_df['lemm_summary_len'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/summary_len_%s.png'%(main_cat))\n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "# plt.xlim(xmax = mean_keyword_num)\n",
    "flit_key_df['FOP_keywords_num'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/FOP_keywords_num_%s.png'%(main_cat))\n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "# flit_key_df['TextRank_keywords_num'].value_counts().hist()\n",
    "# plt.savefig('XLSX/statistic/TextRank_keywords_num_%s.png'%(main_cat))\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 製作record bin檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if os.path.exists('bin'):\n",
    "    shutil.rmtree('/bin', ignore_errors=True)\n",
    "\n",
    "if not os.path.exists('bin/main_cat/chunked'):\n",
    "    os.makedirs('bin/main_cat/chunked')\n",
    "\n",
    "makevocab = True\n",
    "if makevocab:\n",
    "    vocab_counter = collections.Counter()\n",
    "    \n",
    "# train_file\n",
    "flit_key_train_df = flit_key_df.iloc[:int(amount*0.8)]\n",
    "\n",
    "# test_file\n",
    "flit_key_test_df = flit_key_df.iloc[int(amount*0.8)+1:int(amount*0.9)]\n",
    "\n",
    "# vald_file\n",
    "flit_key_valid_df = flit_key_df.iloc[int(amount*0.9)+1:]\n",
    "sentence_start = \"<s>\"\n",
    "sentence_end = \"</s>\"\n",
    "\n",
    "\n",
    "def xlsx2bin(set_name,df):\n",
    "    sents = []\n",
    "    with open(\"bin/main_cat/%s.bin\"%(set_name), 'wb') as file:\n",
    "        i = 0\n",
    "        for idx in tqdm(range(len(df))):\n",
    "            series = df.iloc[idx]\n",
    "            data_dict = series.to_dict()\n",
    "            review_ID , big_categories , small_categories , \\\n",
    "            orign_review , lemm_review , orign_summary , lemm_summary , \\\n",
    "            FOP_keywords ,TextRank_keywords , TextRank_summary = \\\n",
    "            data_dict['review_ID'],data_dict['big_categories'],data_dict['small_categories'], \\\n",
    "            data_dict['review'],data_dict['lemm_review'], data_dict['summary'],data_dict['lemm_summary'], \\\n",
    "            data_dict['FOP_keywords'] , data_dict['TextRank_keywords'] , data_dict['TextRank_summary']\n",
    "\n",
    "\n",
    "            '''\n",
    "            review_ID , big_categories , small_categories , \\\n",
    "            orign_review , lemm_review , orign_summary , lemm_summary , \\\n",
    "            bert_review , bert_summary ,FOP_keywords ,TextRank_keywords = \\\n",
    "            data_dict['review_ID'],data_dict['big_categories'],data_dict['small_categories'], \\\n",
    "            data_dict['review'],data_dict['lemm_review'], data_dict['summary'],data_dict['lemm_summary'], \\\n",
    "            data_dict['bert_review'], data_dict['bert_summary'] , data_dict['FOP_keywords'] ,data_dict['TextRank_keywords']\n",
    "            '''\n",
    "            \n",
    "#             print(FOP_keywords)\n",
    "\n",
    "            # save Embedding/word2Vec calculate sents\n",
    "#             for sent in nltk.sent_tokenize(lemm_review):\n",
    "#                 sent = sent.replace(\".\" ,\"\")\n",
    "#                 sents.append(str(sent).split()) # 切分词汇 \n",
    "\n",
    "#             for sent in nltk.sent_tokenize(lemm_summary):\n",
    "#                 sent = sent.replace(sentence_start ,\"\").replace(sentence_end ,\"\")\n",
    "#                 sents.append(str(sent).split()) # 切分词汇 \n",
    "\n",
    "            lemm_review = lemm_review.replace(\"\\n\",\"\")\n",
    "            lemm_summary = lemm_summary.replace(\"\\n\",\"\").replace(\".\",\" \")\n",
    "            # lemm_summary = sentence_start + ' '+ lemm_summary + ' ' + sentence_end\n",
    "#             print(lemm_summary)\n",
    "            # Write to tf.Example\n",
    "            tf_example = example_pb2.Example()\n",
    "            try:\n",
    "                tf_example.features.feature['orign_review'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(orign_review, encoding='utf-8')])\n",
    "\n",
    "                tf_example.features.feature['orign_summary'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(orign_summary, encoding='utf-8')])\n",
    "                \n",
    "                tf_example.features.feature['review'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(lemm_review, encoding='utf-8')])\n",
    "\n",
    "                tf_example.features.feature['summary'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(lemm_summary, encoding='utf-8')]) \n",
    "                '''    \n",
    "                tf_example.features.feature['bert_review'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(bert_review, encoding='utf-8')])\n",
    "\n",
    "                tf_example.features.feature['bert_summary'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(bert_summary, encoding='utf-8')])\n",
    "                '''\n",
    "                tf_example.features.feature['FOP_keywords'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(FOP_keywords, encoding='utf-8')]) \n",
    "            \n",
    "                tf_example.features.feature['TextRank_keywords'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(TextRank_keywords, encoding='utf-8')]) \n",
    "                \n",
    "                tf_example.features.feature['TextRank_summary'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(TextRank_summary, encoding='utf-8')])\n",
    "\n",
    "                tf_example_str = tf_example.SerializeToString()\n",
    "                str_len = len(tf_example_str)  \n",
    "                file.write(struct.pack('q', str_len))\n",
    "                file.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "    print(\" %s finished... \"%(file.name))\n",
    "    return sents\n",
    "    \n",
    "    \n",
    "sents1 = xlsx2bin('train',flit_key_train_df)\n",
    "sents2 = xlsx2bin('test',flit_key_test_df)\n",
    "sents3 = xlsx2bin('valid',flit_key_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bin/main_cat/bin-info.txt\",'w',encoding='utf-8') as f :\n",
    "    f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割record bin檔(1000為單位)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(set_name, chunks_dir):\n",
    "    in_file = 'bin/main_cat/%s.bin' % set_name\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "#         chunk_fname = os.path.join('bin', '/%s/%s_%03d.bin' % (chunks_dir,set_name, chunk))  # new chunk\n",
    "        chunk_fname = '%s/%s/%s_%03d.bin' % (chunks_dir,set_name,set_name, chunk)\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(CHUNK_SIZE):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1\n",
    "\n",
    "\n",
    "def chunk_all(chunks_dir = 'bin/main_cat/chunked'):\n",
    "    # Make a dir to hold the chunks\n",
    "    \n",
    "    # Chunk the data\n",
    "    for set_name in ['train', 'valid', 'test']:\n",
    "        if not os.path.isdir(os.path.join(chunks_dir,set_name)):\n",
    "            os.mkdir(os.path.join(chunks_dir,set_name))\n",
    "        print(\"Splitting %s data into chunks...\" % set_name)\n",
    "        chunk_file(set_name, chunks_dir)\n",
    "    print(\"Saved chunked data in %s\" % chunks_dir)\n",
    "    \n",
    "chunk_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_valid():\n",
    "    #Performing rouge evaluation on 1.9 lakh sentences takes lot of time. So, create mini validation set & test set by borrowing 15k samples each from these 1.9 lakh sentences\n",
    "    bin_valid_chuncks = os.listdir('bin/main_cat/chunked/valid/')\n",
    "    bin_valid_chuncks.sort()\n",
    "    if not os.path.exists('bin/main_cat/chunked/main_valid'):\n",
    "        os.makedirs('bin/main_cat/chunked/main_valid')\n",
    "        \n",
    "    samples = random.sample(set(bin_valid_chuncks[:-1]), 2)      #Exclude last bin file; contains only 9k sentences\n",
    "    valid_chunk, test_chunk = samples[0], samples[1]\n",
    "    shutil.copyfile(os.path.join('bin/main_cat/chunked/valid', valid_chunk), os.path.join(\"bin/main_cat/chunked/main_valid\", \"valid_00.bin\"))\n",
    "    shutil.copyfile(os.path.join('bin/main_cat/chunked/valid', test_chunk), os.path.join(\"bin/main_cat/chunked/main_valid\", \"test_00.bin\"))\n",
    "main_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [] # total sentence\n",
    "for idx in tqdm(range(len(orign_key_df))):\n",
    "    series = orign_key_df.iloc[idx]\n",
    "    data_dict = series.to_dict()\n",
    "    lemm_review_sents , lemm_summary  = data_dict['lemm_review'],data_dict['lemm_summary'] \n",
    "    try:\n",
    "        #lemm_review_sents = eval(lemm_review_sents)\n",
    "        lemm_review_sents = lemm_review_sents.split(\"\\n\")\n",
    "        for sent in lemm_review_sents:\n",
    "            sent_tokens = sent.split(\" \")\n",
    "            tokens = [str(token) for token in sent.split() if (\" \" not in str(token))and (str(token) == '.' or str(token).isalpha())]\n",
    "    #         tokens = [str(token) for token in sent.split() if (\" \" not in str(token))]\n",
    "            sentences.append(tokens)   \n",
    "        \n",
    "            dot_tokens = [token for token in tokens if (token[0] == \".\" or token[-1] == \".\") and (len(token)>1)]\n",
    "            if len(dot_tokens) > 0 :print(dot_tokens)\n",
    "            \n",
    "        sentences.append([t for t in lemm_summary.split(\" \") if t not in [\"<s>\" , \"</s>\"]])\n",
    "        \n",
    "        dot_tokens = [token for token in lemm_summary.split(\" \") if (token[0] == \".\" or token[-1] == \".\") and (len(token)>1)]\n",
    "        if len(dot_tokens) > 0 :print(dot_tokens)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "print('word2Vec training sentence finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 word2vec\n",
    "from gensim.models import word2vec\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsnooper\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "vocab_count = 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write vocab to file\n",
    "if not os.path.exists('Embedding/main_cat/word2Vec'):\n",
    "    os.makedirs('Embedding/main_cat/word2Vec')\n",
    "    \n",
    "if not os.path.exists(\"Embedding/main_cat/word2Vec/word2Vec.300d.txt\"):\n",
    "\n",
    "    w2vec = word2vec.Word2Vec(sentences, size=300, min_count=2,max_vocab_size=None,iter=100,\n",
    "                              sorted_vocab=1,max_final_vocab=vocab_count)\n",
    "\n",
    "    \n",
    "\n",
    "    w2vec.wv.save_word2vec_format('Embedding/main_cat/word2Vec/word2Vec.300d.txt', binary=False)\n",
    "\n",
    "    #保存模型，供日後使用\n",
    "    # w2vec.save(\"Embedding/word2Vec/word2vec.model\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = sents1 + sents2 + sents3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型讀取方式\n",
    "# model = word2vec.Word2Vec.load(\"Embedding/word2Vec/word2vec.model\")\n",
    "\n",
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'Embedding/main_cat/word2Vec/word2Vec.300d.txt', binary=False, encoding='utf-8')\n",
    "\n",
    "wvmodel.most_similar(u\"player\", topn=10)\n",
    "# wvmodel.most_similar(['dvd','player','changer','machine','video'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = \"Embedding/main_cat/word2Vec/word.vocab\"\n",
    "\n",
    "if not os.path.exists(vocab_file):\n",
    "    vocab_count = len(wvmodel.wv.index2entity)    \n",
    "\n",
    "    print(\"Writing vocab file...\")\n",
    "    with open(vocab_file, 'w',encoding='utf-8') as writer:\n",
    "        for word in wvmodel.wv.index2entity[:vocab_count]:\n",
    "            # print(word, w2vec.wv.vocab[word].count)\n",
    "            writer.write(word + ' ' + str(wvmodel.wv.vocab[word].count) + '\\n') # Output vocab count\n",
    "    print(\"Finished writing vocab file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = wvmodel.wv.index2entity[25]\n",
    "vector = wvmodel.wv.vectors[25]\n",
    "print(word)\n",
    "# print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data_util.data import Vocab\n",
    "vocab_size = len(wvmodel.vocab) + 1\n",
    "\n",
    "\n",
    "vocab = Vocab('Embedding/main_cat/word2Vec/word.vocab', vocab_size)\n",
    "\n",
    "embed_size = 300\n",
    "weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "for i in range(len(vocab._id_to_word.keys())):\n",
    "    try:\n",
    "        vocab_word = vocab._id_to_word[i+4]\n",
    "        w2vec_word = w2vec.wv.index2entity[i]\n",
    "    except Exception as e :\n",
    "        continue\n",
    "    if i + 4 > vocab_size: break\n",
    "#     print(vocab_word,w2vec_word)\n",
    "    weight[i+4, :] = torch.from_numpy(w2vec.wv.vectors[i])\n",
    "        \n",
    "embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "embedding.weight.requires_grad = True\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.word2id('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Glove\n",
    "from glove import Corpus\n",
    "\n",
    "vocab_count = 50000\n",
    "# write vocab to file\n",
    "if not os.path.exists('Embedding/main_cat/glove'):\n",
    "    os.makedirs('Embedding/main_cat/glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"Embedding/main_cat/glove/glove.model\"):\n",
    "\n",
    "    corpus_model = Corpus()\n",
    "    corpus_model.fit(sentences, window=10)\n",
    "    #corpus_model.save('corpus.model')\n",
    "    print('Dict size: %s' % len(corpus_model.dictionary))\n",
    "    print('Collocations: %s' % corpus_model.matrix.nnz)\n",
    "    \n",
    "    glove = Glove(no_components=300, learning_rate=0.05)\n",
    "    glove.fit(corpus_model.matrix, epochs=100,\n",
    "              no_threads=10, verbose=True)\n",
    "    glove.add_dictionary(corpus_model.dictionary)\n",
    "    \n",
    "    glove.save('Embedding/main_cat/glove/glove.model') # 存模型\n",
    "    corpus_model.save('Embedding/main_cat/glove/corpus.model') # 存字典\n",
    "\n",
    "\n",
    "glove = Glove.load('Embedding/main_cat/glove/glove.model')\n",
    "corpus_model = Corpus.load('Embedding/main_cat/glove/corpus.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = \"Embedding/main_cat/glove/word.vocab\"\n",
    "\n",
    "if not os.path.exists(vocab_file):\n",
    "#     vocab_count = len(glove.dictionary)    \n",
    "    vocab_count = 0\n",
    "    print(\"Writing vocab file...\")\n",
    "    with open(vocab_file, 'w',encoding='utf-8') as writer:\n",
    "        for word,idx in glove.dictionary.items():\n",
    "            if word in vocab._word_to_id.keys():\n",
    "                vocab_count += 1\n",
    "                writer.write(word + ' ' + str(idx) + '\\n') # Output vocab count\n",
    "    print(\"Finished writing vocab file %s\" %(vocab_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.word_vectors[glove.dictionary['.']].shape\n",
    "# vocab._word_to_id.keys()\n",
    "# len(glove.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(open('Embedding/main_cat/glove/word.vocab').readlines())\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = Vocab('Embedding/main_cat/glove/word.vocab', vocab_size)\n",
    "embed_size = 300\n",
    "weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "for word,idx in glove.dictionary.items():\n",
    "    if word in vocab._word_to_id.keys():\n",
    "        wid = vocab.word2id(word) \n",
    "        vector = np.asarray(glove.word_vectors[glove.dictionary[word]], \"float32\")\n",
    "        weight[wid, :] = torch.from_numpy(vector)\n",
    "\n",
    "embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "embedding.weight.requires_grad = True\n",
    "embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "# BERT\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model.eval()\n",
    "model.embeddings.word_embeddings\n",
    "\n",
    "\n",
    "# vocab = Vocab('Embedding/word2Vec/word2Vec.vocab', vocab_size)\n",
    "\n",
    "# embed_size = 300\n",
    "# weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "\n",
    "# embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# # requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "# embedding.weight.requires_grad = True\n",
    "# embedding        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script makeRecord-Cat.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
