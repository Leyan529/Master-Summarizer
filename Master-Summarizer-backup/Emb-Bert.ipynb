{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 01:43:50.612472 140157232645952 file_utils.py:35] PyTorch version 1.3.1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  30526 bert tokens now\n",
      "We have added 3 XL tokens\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from model import Model\n",
    "\n",
    "\n",
    "from data_util import config\n",
    "from data_util import bert_data as data\n",
    "from data_util.bert_batcher import Batcher\n",
    "from data_util.bert_data import Vocab\n",
    "\n",
    "# from data_util import data\n",
    "# from data_util.batcher import Batcher\n",
    "# from data_util.data import Vocab\n",
    "\n",
    "\n",
    "from train_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse\n",
    "import torchsnooper\n",
    "import logging\n",
    "transformers_logger = logging.getLogger(\"transformers.tokenization_utils\")\n",
    "transformers_logger.setLevel(logging.ERROR)\n",
    "transformers_logger.disabled = True\n",
    "\n",
    "# -------- Test Packages -------\n",
    "from bert_beam_search import *\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# from pytorch_pretrained_bert import BertModel\n",
    "from transformers import BertModel, BertTokenizer \n",
    "from transformers import TransfoXLTokenizer, TransfoXLModel, TransfoXLConfig\n",
    "\n",
    "config.batch_size = 2\n",
    "config.emb_dim = 768\n",
    "config.max_enc_steps = 512\n",
    "config.lr = 0.0001 # 0.001\n",
    "\n",
    "# config.keywords = \"TextRank_keywords\"\n",
    "# config.max_key_num = 8\n",
    "# help(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## batch_size : 2\n",
      "## beam_size : 16\n",
      "## ber_layer : 11\n",
      "## data_type : Cameras_new5\n",
      "## emb_dim : 768\n",
      "## eps : 1e-12\n",
      "## gound_truth_prob : 0.1\n",
      "## hidden_dim : 512\n",
      "## intra_decoder : True\n",
      "## intra_encoder : True\n",
      "## key_attention : False\n",
      "## keywords : POS_FOP_keywords\n",
      "## loggerName : Text-Summary\n",
      "## lr : 0.0001\n",
      "## max_dec_steps : 50\n",
      "## max_enc_steps : 512\n",
      "## max_epochs : 100\n",
      "## max_iterations : 500000\n",
      "## max_key_num : 8\n",
      "## min_dec_steps : 4\n",
      "## rand_unif_init_mag : 0.02\n",
      "## trunc_norm_init_std : 0.0001\n",
      "## vocab_size : 100000\n",
      "## word_emb_type : word2Vec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_str = ''\n",
    "for a in dir(config):\n",
    "    if type(getattr(config, a)) in [str,int,float,bool] \\\n",
    "    and 'path' not in str(a) \\\n",
    "    and '__' not in str(a) \\\n",
    "    and 'info' not in str(a):\n",
    "\n",
    "        info_str += '## %s : %s\\n'%(a,getattr(config, a))\n",
    "\n",
    "# [print(a,getattr(config, a)) for a in dir(config)\n",
    "# if type(getattr(config, a)) in [str,int,float]\n",
    "#  and 'path' not in str(a)\n",
    "#  and '__' not in str(a)\n",
    "#  and 'info' not in str(a)\n",
    "# ]\n",
    "print(info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def getLogger(loggerName, loggerPath):\n",
    "    # 設置logger\n",
    "    logger = logging.getLogger(loggerName)  # 不加名稱設置root logger\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s: - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    logging.Filter(loggerName)\n",
    "\n",
    "    # 使用FileHandler輸出到文件\n",
    "    directory = os.path.dirname(loggerPath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fh = logging.FileHandler(loggerPath)\n",
    "\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # 使用StreamHandler輸出到屏幕\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    ch.setFormatter(formatter)\n",
    "    # 添加兩個Handler\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    # Handler只啟動一次\n",
    "    # 設置logger\n",
    "    logger.info(u'logger已啟動')\n",
    "    return logger\n",
    "\n",
    "def removeLogger(logger):\n",
    "    logger.info(u'logger已關閉')\n",
    "    handlers = logger.handlers[:]\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_summary_sents :  <s> other than that the camera take amazing underwater shot </s>\n",
      "key_words :  ['battery', 'auxiliary', 'battery', 'backup', 'underwater', 'amazing']\n",
      "original_summary_sents :  <s> this camera is the best affordable action camera there is on the market </s>\n",
      "key_words :  ['action', 'affordable', 'screen', 'lcd', 'clip', 'small']\n"
     ]
    }
   ],
   "source": [
    "def test_batch():\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    # with torchsnooper.snoop():\n",
    "    while batch is not None:\n",
    "        example_list = batch.example_list\n",
    "        for ex in example_list:\n",
    "            r = str(ex.original_review)\n",
    "            s = str(ex.original_summary)\n",
    "            k = str(ex.key_words)\n",
    "            sent = ex.original_summary_sents\n",
    "#             print(\"original_review_sents:\", r)\n",
    "            print(\"original_summary_sents : \", s)\n",
    "            print(\"key_words : \", k)\n",
    "#             print('------------------------------------------------------------\\n')\n",
    "        batch = batcher.next_batch()        \n",
    "        break\n",
    "test_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bin Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 16497\n",
      "\n",
      "test : 2061\n",
      "\n",
      "valid : 2062\n",
      "\n"
     ]
    }
   ],
   "source": [
    " with open(config.bin_info,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    [print(line) for line in lines]\n",
    "    train_num = int(lines[0].split(\":\")[1])\n",
    "    test_num = int(lines[1].split(\":\")[1])\n",
    "    val_num = int(lines[2].split(\":\")[1])\n",
    "    # f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    # f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    # f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View model summary\n",
    "#### 只有torchsummaryX成功\n",
    "#### 日後將以此模擬呈現結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary # 不支援RNN\n",
    "# from model import Encoder,Model\n",
    "# # https://www.cnblogs.com/lindaxin/p/8052043.html\n",
    "# device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# encoder = Encoder().to(device)    \n",
    "\n",
    "# vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "# batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "#                        batch_size=config.batch_size, single_pass=False)\n",
    "# batch = batcher.next_batch()\n",
    "# enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# enc_batch = Model().embeds(enc_batch) # Get embeddings for encoder input\n",
    "\n",
    "# # summary(encoder, enc_batch, enc_lens, show_hierarchical=True) \n",
    "# # summary(encoder, [enc_batch, enc_lens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modelsummary import summary # 未知問題\n",
    "# from model import Encoder,Model\n",
    "# # https://www.cnblogs.com/lindaxin/p/8052043.html\n",
    "# device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# encoder = Encoder().to(device)    \n",
    "\n",
    "# vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "# batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "#                        batch_size=config.batch_size, single_pass=False)\n",
    "# batch = batcher.next_batch()\n",
    "# enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# enc_batch = Model().embeds(enc_batch) # Get embeddings for encoder input\n",
    "\n",
    "# # summary(encoder, enc_batch, enc_lens, show_hierarchical=True) \n",
    "# # summary(encoder, enc_batch, enc_lens, show_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 01:44:00.505785 140157232645952 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/eagleuser/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0317 01:44:00.602400 140157232645952 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0317 01:44:02.371857 140157232645952 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/eagleuser/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 119, 768])\n",
      "========================================================\n",
      "           Kernel Shape Output Shape   Params  Mult-Adds\n",
      "Layer                                                   \n",
      "0_lstm                -  [163, 1024]  5251072    5242880\n",
      "1_reduce_h  [1024, 512]     [2, 512]   524800     524288\n",
      "2_reduce_c  [1024, 512]     [2, 512]   524800     524288\n",
      "--------------------------------------------------------\n",
      "                       Totals\n",
      "Total params          6300672\n",
      "Trainable params      6300672\n",
      "Non-trainable params        0\n",
      "Mult-Adds             6291456\n",
      "========================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[163, 1024]</td>\n",
       "      <td>5251072</td>\n",
       "      <td>5242880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_reduce_h</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[2, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_reduce_c</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[2, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kernel Shape Output Shape   Params  Mult-Adds\n",
       "Layer                                                   \n",
       "0_lstm                -  [163, 1024]  5251072    5242880\n",
       "1_reduce_h  [1024, 512]     [2, 512]   524800     524288\n",
       "2_reduce_c  [1024, 512]     [2, 512]   524800     524288"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Encoder,Model\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "encoder = Encoder().to(device)    \n",
    "\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "\n",
    "enc_batch = enc_batch.type(T.LongTensor).cuda() #  `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "     \n",
    "bert_model = get_cuda(BertModel.from_pretrained('bert-base-uncased'))\n",
    "# xl_config = TransfoXLConfig()\n",
    "# xl_config.d_embed = 1024\n",
    "# bert_model = TransfoXLModel(xl_config) # 更改參數以傳入TransfoXLModel\n",
    "\n",
    "# bert_model = get_cuda(TransfoXLModel.from_pretrained('transfo-xl-wt103'))\n",
    "all_hidden_states, _ = bert_model(enc_batch)[-2:]\n",
    "print(all_hidden_states.shape)\n",
    "\n",
    "# enc_batch = self.bert_model(enc_batch)[0][0] ; print(enc_batch.shape)\n",
    "        \n",
    "summary(encoder, all_hidden_states, enc_lens) # encoder summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "def write_enc_graph():\n",
    "    encoder_writer = SummaryWriter('runs/Pointer-Generator/word2Vec/Encoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    encoder = Encoder().to(device) \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "#     enc_batch = Variable(torch.rand(enc_batch.shape)).to(device) \n",
    "    enc_lens = torch.from_numpy(enc_lens).to(device) \n",
    "\n",
    "    encoder_writer.add_graph(encoder, (enc_batch, enc_lens), verbose=True)\n",
    "    encoder_writer.close()\n",
    "\n",
    "def write_dec_graph():\n",
    "    decoder_writer = SummaryWriter('runs/Pointer-Generator/word2Vec/Decoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    # decoder = Decoder().to(device)    \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    model = Model(False,'word2Vec',vocab)\n",
    "    \n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "    # train_batch_MLE\n",
    "    dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "    step_losses = []\n",
    "    s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "    # x_t 為decoder每一個time step 的batch input\n",
    "    x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "    prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    sum_temporal_srcs = None     \n",
    "\n",
    "\n",
    "    for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "        use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "        # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "        # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "        if t == 0 :temp_batch = dec_batch[:, t]\n",
    "        x_t = use_gound_truth * temp_batch + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "        x_t = model.embeds(x_t)\n",
    "    #     final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "        final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "        x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "        extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "        enc_key_batch, enc_key_lens)        \n",
    "\n",
    "\n",
    "        #         decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "#         x_t = Variable(torch.rand(x_t.shape)).to(device) \n",
    "        #             s_t = Variable(torch.rand(s_t.shape)).to(device)\n",
    "#         enc_out = Variable(torch.rand(enc_out.shape)).to(device)\n",
    "#         enc_padding_mask = Variable(torch.rand(enc_padding_mask.shape)).to(device,dtype=torch.long)\n",
    "#         context = Variable(torch.rand(context.shape)).to(device)\n",
    "#         extra_zeros = Variable(torch.rand(extra_zeros.shape)).to(device)\n",
    "#         enc_batch_extend_vocab = Variable(torch.rand(enc_batch_extend_vocab.shape)).to(device)\n",
    "        #             sum_temporal_srcs = Variable(torch.rand(sum_temporal_srcs.shape)).to(device)\n",
    "        #             prev_s = Variable(torch.rand(prev_s.shape)).to(device)\n",
    "#         enc_key_batch = Variable(torch.rand(enc_key_batch.shape)).to(device)\n",
    "        enc_key_lens = torch.from_numpy(enc_key_lens).to(device) \n",
    "        \n",
    "        decoder_writer.add_graph(model.decoder, \n",
    "                         (x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens), verbose=True)\n",
    "        decoder_writer.close()\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 01:44:23 - Text-Summary - INFO: - logger已啟動\n",
      "I0317 01:44:23.236703 140157232645952 <ipython-input-3-00f9962e7fdd>:30] logger已啟動\n",
      "2020-03-17 01:44:23 - Text-Summary - INFO: - ------Training Setting--------\n",
      "I0317 01:44:23.242853 140157232645952 bert_emb_run.py:59] ------Training Setting--------\n",
      "2020-03-17 01:44:23 - Text-Summary - INFO: - Traing Type :Cameras_new5\n",
      "I0317 01:44:23.246093 140157232645952 bert_emb_run.py:61] Traing Type :Cameras_new5\n",
      "2020-03-17 01:44:23 - Text-Summary - INFO: - Training mle: True, mle weight: 1.00\n",
      "I0317 01:44:23.247637 140157232645952 bert_emb_run.py:63] Training mle: True, mle weight: 1.00\n",
      "2020-03-17 01:44:23 - Text-Summary - INFO: - use pre_train_bert vocab_size 100000 \n",
      "\n",
      "I0317 01:44:23.249203 140157232645952 bert_emb_run.py:70] use pre_train_bert vocab_size 100000 \n",
      "\n",
      "2020-03-17 01:44:23 - Text-Summary - INFO: - intra_encoder: True intra_decoder: True \n",
      "\n",
      "I0317 01:44:23.251284 140157232645952 bert_emb_run.py:75] intra_encoder: True intra_decoder: True \n",
      "\n",
      "I0317 01:44:30.214412 140157232645952 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/eagleuser/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0317 01:44:30.284519 140157232645952 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0317 01:44:31.977417 140157232645952 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/eagleuser/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0317 01:44:56.116724 140157232645952 modeling.py:580] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/eagleuser/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "I0317 01:44:56.420150 140157232645952 modeling.py:588] extracting archive file /home/eagleuser/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp_cm_n4e1\n",
      "I0317 01:45:10.476603 140157232645952 modeling.py:598] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0317 01:45:12.648379 140157232645952 tokenization.py:190] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/eagleuser/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2020-03-17 01:45:14 - Text-Summary - INFO: - Model(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(768, 512, batch_first=True, bidirectional=True)\n",
      "    (reduce_h): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (reduce_c): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (enc_attention): encoder_attention(\n",
      "      (W_h): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (W_s): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (W_t): Linear(in_features=768, out_features=1024, bias=True)\n",
      "      (v): Linear(in_features=1024, out_features=1, bias=False)\n",
      "    )\n",
      "    (dec_attention): decoder_attention(\n",
      "      (W_prev): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (W_s): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (W_t): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (x_context): Linear(in_features=1792, out_features=768, bias=True)\n",
      "    (x_key_context): Linear(in_features=2560, out_features=768, bias=True)\n",
      "    (lstm): LSTMCell(768, 512)\n",
      "    (p_gen_linear): Linear(in_features=3328, out_features=1, bias=True)\n",
      "    (V): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (V1): Linear(in_features=512, out_features=100000, bias=True)\n",
      "  )\n",
      "  (embeds): Embedding(100000, 768)\n",
      ")\n",
      "I0317 01:45:14.081943 140157232645952 bert_emb_run.py:132] Model(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(768, 512, batch_first=True, bidirectional=True)\n",
      "    (reduce_h): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (reduce_c): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (enc_attention): encoder_attention(\n",
      "      (W_h): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (W_s): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (W_t): Linear(in_features=768, out_features=1024, bias=True)\n",
      "      (v): Linear(in_features=1024, out_features=1, bias=False)\n",
      "    )\n",
      "    (dec_attention): decoder_attention(\n",
      "      (W_prev): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (W_s): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (W_t): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (x_context): Linear(in_features=1792, out_features=768, bias=True)\n",
      "    (x_key_context): Linear(in_features=2560, out_features=768, bias=True)\n",
      "    (lstm): LSTMCell(768, 512)\n",
      "    (p_gen_linear): Linear(in_features=3328, out_features=1, bias=True)\n",
      "    (V): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (V1): Linear(in_features=512, out_features=100000, bias=True)\n",
      "  )\n",
      "  (embeds): Embedding(100000, 768)\n",
      ")\n",
      "2020-03-17 01:45:14 - Text-Summary - INFO: - ------Training START--------\n",
      "I0317 01:45:14.089121 140157232645952 bert_emb_run.py:455] ------Training START--------\n",
      "2020-03-17 01:45:17 - Text-Summary - INFO: - iter: 0 train_rouge_l_f: 0.000 test_rouge_l_f: 0.000 \n",
      "\n",
      "I0317 01:45:17.234213 140157232645952 bert_emb_run.py:410] iter: 0 train_rouge_l_f: 0.000 test_rouge_l_f: 0.000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/u012869752/article/details/72513141\n",
    "# 由于在jupyter notebook中，args不为空\n",
    "from glob import glob\n",
    "from bert_emb_run import *\n",
    "\n",
    "# nvidia-smi -pm 1\n",
    "if __name__ == \"__main__\":   \n",
    "    try:\n",
    "        # --------------------------Training ----------------------------------\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--train_mle', type=bool, default=True)\n",
    "        parser.add_argument('--train_rl', type=bool, default=False)\n",
    "        parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "    #         parser.add_argument('--load_model', type=str, default='/0015000_3.29_0.00.tar')\n",
    "        parser.add_argument('--load_model', type=str, default=None)\n",
    "        parser.add_argument('--new_lr', type=float, default=None)\n",
    "        parser.add_argument('--multi_device', type=bool, default=True)\n",
    "        parser.add_argument('--view', type=bool, default=True)\n",
    "        parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "        parser.add_argument('--word_emb_type', type=str, default='bert')\n",
    "        parser.add_argument('--train_action', type=bool, default=True)\n",
    "        opt = parser.parse_args(args=[])\n",
    "\n",
    "        today = dt.now()\n",
    "    #         loggerPath = \"LOG//%s-(%s_%s_%s)-(%s:%s:%s)\"%(opt.word_emb_type,\n",
    "    #                   today.year,today.month,today.day,\n",
    "    #                   today.hour,today.minute,today.second)\n",
    "    #         logger = getLogger(config.loggerName,loggerPath)   \n",
    "\n",
    "        loggerPath = \"LOG//%s\"%(opt.word_emb_type)\n",
    "        logger = getLogger(config.loggerName,loggerPath) \n",
    "\n",
    "        if opt.load_model == None:\n",
    "            shutil.rmtree('runs/Pointer-Generator/bert', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/bert/exp-4', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/bert/Eecoder', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/bert/Decoder', ignore_errors=True) # clear previous \n",
    "\n",
    "        writer = SummaryWriter('runs/Pointer-Generator/bert/exp-4')\n",
    "        writer.add_text('Train_Para/',info_str,0)\n",
    "    #         write_enc_graph()\n",
    "    #         write_dec_graph()\n",
    "\n",
    "        if opt.train_action: train_action(opt, logger, writer, train_num)\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback = sys.exc_info()[2]\n",
    "        print(sys.exc_info())\n",
    "        print(traceback.tb_lineno)\n",
    "        print(e)\n",
    "    finally:\n",
    "        removeLogger(logger)\n",
    "        # export scalar data to JSON for external processing\n",
    "        # tensorboard --logdir /home/eagleuser/Users/leyan/Text-Summarizer-FOP/TensorBoard\n",
    "#         tensorboard --logdir ./runs\n",
    "#         if not os.path.exists('TensorBoard'): os.makedirs('TensorBoard')\n",
    "#         writer.export_scalars_to_json(\"TensorBoard/test.json\")\n",
    "        writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
