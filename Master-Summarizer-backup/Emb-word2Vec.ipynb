{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from model import Model\n",
    "\n",
    "\n",
    "from data_util import config, data\n",
    "from data_util.batcher import Batcher\n",
    "from data_util.data import Vocab\n",
    "from write_result import *\n",
    "\n",
    "from train_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse\n",
    "import torchsnooper\n",
    "import logging\n",
    "\n",
    "# -------- Test Packages -------\n",
    "from beam_search import *\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# fix\n",
    "config.max_enc_steps = 1000\n",
    "config.max_dec_steps = 50\n",
    "config.min_dec_steps = 8\n",
    "config.batch_size = 8\n",
    "config.gound_truth_prob = 0.1\n",
    "\n",
    "\n",
    "# _4\n",
    "# config.lr = 0.0001\n",
    "# config.eps = 1e-12\n",
    "\n",
    "# _5\n",
    "# config.lr = 0.00005\n",
    "# config.eps = 1e-17\n",
    "\n",
    "# _6\n",
    "# config.lr = 0.0001\n",
    "# config.eps = 1e-12\n",
    "# config.min_dec_steps = 4\n",
    "\n",
    "# _6\n",
    "# config.lr = 0.0001\n",
    "# config.eps = 1e-12\n",
    "# config.min_dec_steps = 4\n",
    "# config.vocab_size = 50000\n",
    "# best\n",
    "\n",
    "# _6\n",
    "config.lr = 0.0001\n",
    "config.eps = 1e-12\n",
    "config.min_dec_steps = 4\n",
    "config.vocab_size = 50000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## batch_size : 8\n",
      "## beam_size : 16\n",
      "## ber_layer : 11\n",
      "## data_type : Cameras_new8\n",
      "## emb_dim : 300\n",
      "## emb_grad : False\n",
      "## eps : 1e-12\n",
      "## gound_truth_prob : 0.1\n",
      "## hidden_dim : 512\n",
      "## intra_decoder : True\n",
      "## intra_encoder : True\n",
      "## key_attention : False\n",
      "## keywords : POS_FOP_keywords\n",
      "## loggerName : Text-Summary\n",
      "## lr : 0.0001\n",
      "## max_dec_steps : 50\n",
      "## max_enc_steps : 1000\n",
      "## max_epochs : 100\n",
      "## max_iterations : 500000\n",
      "## max_key_num : 8\n",
      "## min_dec_steps : 4\n",
      "## rand_unif_init_mag : 0.02\n",
      "## trunc_norm_init_std : 0.0001\n",
      "## vocab_size : 50000\n",
      "## word_emb_type : word2Vec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_str = ''\n",
    "for a in dir(config):\n",
    "    if type(getattr(config, a)) in [str,int,float,bool] \\\n",
    "    and 'path' not in str(a) \\\n",
    "    and '__' not in str(a) \\\n",
    "    and 'info' not in str(a):\n",
    "\n",
    "        info_str += '## %s : %s\\n'%(a,getattr(config, a))\n",
    "\n",
    "# [print(a,getattr(config, a)) for a in dir(config)\n",
    "# if type(getattr(config, a)) in [str,int,float]\n",
    "#  and 'path' not in str(a)\n",
    "#  and '__' not in str(a)\n",
    "#  and 'info' not in str(a)\n",
    "# ]\n",
    "print(info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def getLogger(loggerName, loggerPath):\n",
    "    # 設置logger\n",
    "    logger = logging.getLogger(loggerName)  # 不加名稱設置root logger\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s: - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    logging.Filter(loggerName)\n",
    "\n",
    "    # 使用FileHandler輸出到文件\n",
    "    directory = os.path.dirname(loggerPath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fh = logging.FileHandler(loggerPath)\n",
    "\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # 使用StreamHandler輸出到屏幕\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    ch.setFormatter(formatter)\n",
    "    # 添加兩個Handler\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    # Handler只啟動一次\n",
    "    # 設置logger\n",
    "    logger.info(u'logger已啟動')\n",
    "    return logger\n",
    "\n",
    "def removeLogger(logger):\n",
    "    logger.info(u'logger已關閉')\n",
    "    handlers = logger.handlers[:]\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_summary_sents :  <s> great camera read review if your try to decide on this or some other similarly price camera </s>\n",
      "key_words :  ['camera', 'more', 'electronic']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> upgrade from my canon rebel xti and am so glad did </s>\n",
      "key_words :  ['mode', 'not', 'excellent']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> good little camera for the price </s>\n",
      "key_words :  ['image', 'most', 'small']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> nice canon good feature </s>\n",
      "key_words :  ['feature', 'external']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> zoom is amazing low light night day sunshine or not its great </s>\n",
      "key_words :  ['camera', 'direct']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> spend the extra and get this camera </s>\n",
      "key_words :  ['money', 'unbelievable']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> this is an ideal camera and lens for non professional </s>\n",
      "key_words :  ['camera', 'extra']\n",
      "------------------------------------------------------------\n",
      "\n",
      "original_summary_sents :  <s> xsi for about year and this is just wonderful it bring new depth to old lense and </s>\n",
      "key_words :  ['display', 'odd']\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_batch():\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    # with torchsnooper.snoop():\n",
    "    while batch is not None:\n",
    "        example_list = batch.example_list\n",
    "        for ex in example_list:\n",
    "            r = str(ex.original_review)\n",
    "            s = str(ex.original_summary)\n",
    "            k = str(ex.key_words)\n",
    "            sent = ex.original_summary_sents\n",
    "#             print(\"original_review_sents:\", r)\n",
    "            print(\"original_summary_sents : \", s)\n",
    "            print(\"key_words : \", k)\n",
    "            print('------------------------------------------------------------\\n')\n",
    "        batch = batcher.next_batch()        \n",
    "        break\n",
    "test_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bin Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 33574\n",
      "\n",
      "test : 4196\n",
      "\n",
      "valid : 4196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(config.bin_info,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    [print(line) for line in lines]\n",
    "    train_num = int(lines[0].split(\":\")[1])\n",
    "    test_num = int(lines[1].split(\":\")[1])\n",
    "    val_num = int(lines[2].split(\":\")[1])\n",
    "    # f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    # f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    # f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "           Kernel Shape  Output Shape   Params  Mult-Adds\n",
      "Layer                                                    \n",
      "0_lstm                -  [1631, 1024]  3334144    3325952\n",
      "1_reduce_h  [1024, 512]      [8, 512]   524800     524288\n",
      "2_reduce_c  [1024, 512]      [8, 512]   524800     524288\n",
      "---------------------------------------------------------\n",
      "                       Totals\n",
      "Total params          4383744\n",
      "Trainable params      4383744\n",
      "Non-trainable params        0\n",
      "Mult-Adds             4374528\n",
      "=========================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[1631, 1024]</td>\n",
       "      <td>3334144</td>\n",
       "      <td>3325952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_reduce_h</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_reduce_c</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kernel Shape  Output Shape   Params  Mult-Adds\n",
       "Layer                                                    \n",
       "0_lstm                -  [1631, 1024]  3334144    3325952\n",
       "1_reduce_h  [1024, 512]      [8, 512]   524800     524288\n",
       "2_reduce_c  [1024, 512]      [8, 512]   524800     524288"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Encoder,Model\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "encoder = Encoder().to(device)    \n",
    "\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "summary(encoder, enc_batch, enc_lens) # encoder summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                               Kernel Shape    Output Shape    Params  \\\n",
      "Layer                                                                   \n",
      "0_x_context                     [1324, 300]        [8, 300]    397500   \n",
      "1_lstm                                    -        [8, 512]   1667072   \n",
      "2_enc_attention.Linear_W_h     [1024, 1024]  [8, 330, 1024]   1048576   \n",
      "3_enc_attention.Linear_W_s     [1024, 1024]       [8, 1024]   1049600   \n",
      "4_enc_attention.Linear_v          [1024, 1]     [8, 330, 1]      1024   \n",
      "5_dec_attention.Linear_W_prev    [512, 512]     [8, 1, 512]    262144   \n",
      "6_dec_attention.Linear_W_s       [512, 512]        [8, 512]    262656   \n",
      "7_dec_attention.Linear_v           [512, 1]       [8, 1, 1]       512   \n",
      "8_p_gen_linear                    [2860, 1]          [8, 1]      2861   \n",
      "9_V                             [2048, 512]        [8, 512]   1049088   \n",
      "10_V1                          [512, 50000]      [8, 50000]  25650000   \n",
      "\n",
      "                               Mult-Adds  \n",
      "Layer                                     \n",
      "0_x_context                       397200  \n",
      "1_lstm                           1662976  \n",
      "2_enc_attention.Linear_W_h       1048576  \n",
      "3_enc_attention.Linear_W_s       1048576  \n",
      "4_enc_attention.Linear_v            1024  \n",
      "5_dec_attention.Linear_W_prev     262144  \n",
      "6_dec_attention.Linear_W_s        262144  \n",
      "7_dec_attention.Linear_v             512  \n",
      "8_p_gen_linear                      2860  \n",
      "9_V                              1048576  \n",
      "10_V1                           25600000  \n",
      "--------------------------------------------------------------------------------\n",
      "                        Totals\n",
      "Total params          31391033\n",
      "Trainable params      31391033\n",
      "Non-trainable params         0\n",
      "Mult-Adds             31334588\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_x_context</th>\n",
       "      <td>[1324, 300]</td>\n",
       "      <td>[8, 300]</td>\n",
       "      <td>397500</td>\n",
       "      <td>397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>1667072</td>\n",
       "      <td>1662976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_enc_attention.Linear_W_h</th>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>[8, 330, 1024]</td>\n",
       "      <td>1048576</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_enc_attention.Linear_W_s</th>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>[8, 1024]</td>\n",
       "      <td>1049600</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_enc_attention.Linear_v</th>\n",
       "      <td>[1024, 1]</td>\n",
       "      <td>[8, 330, 1]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_dec_attention.Linear_W_prev</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>[8, 1, 512]</td>\n",
       "      <td>262144</td>\n",
       "      <td>262144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_dec_attention.Linear_W_s</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>262656</td>\n",
       "      <td>262144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_dec_attention.Linear_v</th>\n",
       "      <td>[512, 1]</td>\n",
       "      <td>[8, 1, 1]</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_p_gen_linear</th>\n",
       "      <td>[2860, 1]</td>\n",
       "      <td>[8, 1]</td>\n",
       "      <td>2861</td>\n",
       "      <td>2860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9_V</th>\n",
       "      <td>[2048, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>1049088</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_V1</th>\n",
       "      <td>[512, 50000]</td>\n",
       "      <td>[8, 50000]</td>\n",
       "      <td>25650000</td>\n",
       "      <td>25600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Kernel Shape    Output Shape    Params  \\\n",
       "Layer                                                                   \n",
       "0_x_context                     [1324, 300]        [8, 300]    397500   \n",
       "1_lstm                                    -        [8, 512]   1667072   \n",
       "2_enc_attention.Linear_W_h     [1024, 1024]  [8, 330, 1024]   1048576   \n",
       "3_enc_attention.Linear_W_s     [1024, 1024]       [8, 1024]   1049600   \n",
       "4_enc_attention.Linear_v          [1024, 1]     [8, 330, 1]      1024   \n",
       "5_dec_attention.Linear_W_prev    [512, 512]     [8, 1, 512]    262144   \n",
       "6_dec_attention.Linear_W_s       [512, 512]        [8, 512]    262656   \n",
       "7_dec_attention.Linear_v           [512, 1]       [8, 1, 1]       512   \n",
       "8_p_gen_linear                    [2860, 1]          [8, 1]      2861   \n",
       "9_V                             [2048, 512]        [8, 512]   1049088   \n",
       "10_V1                          [512, 50000]      [8, 50000]  25650000   \n",
       "\n",
       "                               Mult-Adds  \n",
       "Layer                                     \n",
       "0_x_context                       397200  \n",
       "1_lstm                           1662976  \n",
       "2_enc_attention.Linear_W_h       1048576  \n",
       "3_enc_attention.Linear_W_s       1048576  \n",
       "4_enc_attention.Linear_v            1024  \n",
       "5_dec_attention.Linear_W_prev     262144  \n",
       "6_dec_attention.Linear_W_s        262144  \n",
       "7_dec_attention.Linear_v             512  \n",
       "8_p_gen_linear                      2860  \n",
       "9_V                              1048576  \n",
       "10_V1                           25600000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Decoder,Model\n",
    "from train_util import *\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# decoder = Decoder().to(device)    \n",
    "\n",
    "model = Model(False,'word2Vec',vocab)\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "# train_batch_MLE\n",
    "dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "step_losses = []\n",
    "s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "# x_t 為decoder每一個time step 的batch input\n",
    "x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "sum_temporal_srcs = None     \n",
    "             \n",
    "    \n",
    "for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "    use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "    # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "    # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "    x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "    x_t = model.embeds(x_t)\n",
    "    enc_key_batch = model.embeds(enc_key_batch)\n",
    "#     final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "    final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "    x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "    extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "    enc_key_batch, enc_key_lens)\n",
    "    \n",
    "    decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "    break\n",
    "decoder_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "def write_enc_graph():\n",
    "    encoder_writer = SummaryWriter('runs/Pointer-Generator/word2Vec/Encoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    encoder = Encoder().to(device) \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "#     enc_batch = Variable(torch.rand(enc_batch.shape)).to(device) \n",
    "    enc_lens = torch.from_numpy(enc_lens).to(device) \n",
    "\n",
    "    encoder_writer.add_graph(encoder, (enc_batch, enc_lens), verbose=True)\n",
    "    encoder_writer.close()\n",
    "\n",
    "def write_dec_graph():\n",
    "    decoder_writer = SummaryWriter('runs/Pointer-Generator/word2Vec/Decoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    # decoder = Decoder().to(device)    \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    model = Model(False,'word2Vec',vocab)\n",
    "    \n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "    # train_batch_MLE\n",
    "    dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "    step_losses = []\n",
    "    s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "    # x_t 為decoder每一個time step 的batch input\n",
    "    x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "    prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    sum_temporal_srcs = None     \n",
    "\n",
    "\n",
    "    for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "        use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "        # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "        # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "        if t == 0 :temp_batch = dec_batch[:, t]\n",
    "        x_t = use_gound_truth * temp_batch + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "        x_t = model.embeds(x_t)\n",
    "    #     final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "        final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "        x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "        extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "        enc_key_batch, enc_key_lens)        \n",
    "\n",
    "\n",
    "        #         decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "#         x_t = Variable(torch.rand(x_t.shape)).to(device) \n",
    "        #             s_t = Variable(torch.rand(s_t.shape)).to(device)\n",
    "#         enc_out = Variable(torch.rand(enc_out.shape)).to(device)\n",
    "#         enc_padding_mask = Variable(torch.rand(enc_padding_mask.shape)).to(device,dtype=torch.long)\n",
    "#         context = Variable(torch.rand(context.shape)).to(device)\n",
    "#         extra_zeros = Variable(torch.rand(extra_zeros.shape)).to(device)\n",
    "#         enc_batch_extend_vocab = Variable(torch.rand(enc_batch_extend_vocab.shape)).to(device)\n",
    "        #             sum_temporal_srcs = Variable(torch.rand(sum_temporal_srcs.shape)).to(device)\n",
    "        #             prev_s = Variable(torch.rand(prev_s.shape)).to(device)\n",
    "#         enc_key_batch = Variable(torch.rand(enc_key_batch.shape)).to(device)\n",
    "        enc_key_lens = torch.from_numpy(enc_key_lens).to(device) \n",
    "        \n",
    "        decoder_writer.add_graph(model.decoder, \n",
    "                         (x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens), verbose=True)\n",
    "        decoder_writer.close()\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-19 11:05:19 - Text-Summary - INFO: - logger已啟動\n",
      "2020-03-19 11:05:19 - Text-Summary - INFO: - ------Training Setting--------\n",
      "2020-03-19 11:05:19 - Text-Summary - INFO: - Traing Type :Cameras_new8\n",
      "2020-03-19 11:05:19 - Text-Summary - INFO: - Training mle: True, mle weight: 1.00\n",
      "2020-03-19 11:05:19 - Text-Summary - INFO: - use pre_train_word2Vec vocab_size 50000 \n",
      "\n",
      "2020-03-19 11:05:19 - Text-Summary - INFO: - intra_encoder: True intra_decoder: True \n",
      "\n",
      "2020-03-19 11:05:36 - Text-Summary - INFO: - Model(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
      "    (reduce_h): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (reduce_c): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (enc_attention): encoder_attention(\n",
      "      (W_h): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (W_s): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (W_t): Linear(in_features=300, out_features=1024, bias=True)\n",
      "      (v): Linear(in_features=1024, out_features=1, bias=False)\n",
      "    )\n",
      "    (dec_attention): decoder_attention(\n",
      "      (W_prev): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (W_s): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (W_t): Linear(in_features=300, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (x_context): Linear(in_features=1324, out_features=300, bias=True)\n",
      "    (x_key_context): Linear(in_features=1624, out_features=300, bias=True)\n",
      "    (lstm): LSTMCell(300, 512)\n",
      "    (p_gen_linear): Linear(in_features=2860, out_features=1, bias=True)\n",
      "    (V): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (V1): Linear(in_features=512, out_features=50000, bias=True)\n",
      "  )\n",
      "  (embeds): Embedding(50000, 300)\n",
      ")\n",
      "2020-03-19 11:05:36 - Text-Summary - INFO: - ------Training START--------\n",
      "2020-03-19 11:05:42 - Text-Summary - INFO: - iter: 0 train_rouge_l_f: 0.014 test_rouge_l_f: 0.029 \n",
      "\n",
      "2020-03-19 11:11:22 - Text-Summary - INFO: - iter: 1000 train_rouge_l_f: 0.086 test_rouge_l_f: 0.310 \n",
      "\n",
      "2020-03-19 11:17:10 - Text-Summary - INFO: - iter: 2000 train_rouge_l_f: 0.103 test_rouge_l_f: 0.205 \n",
      "\n",
      "2020-03-19 11:22:57 - Text-Summary - INFO: - iter: 3000 train_rouge_l_f: 0.321 test_rouge_l_f: 0.400 \n",
      "\n",
      "2020-03-19 11:28:42 - Text-Summary - INFO: - iter: 4000 train_rouge_l_f: 0.270 test_rouge_l_f: 0.293 \n",
      "\n",
      "2020-03-19 11:34:37 - Text-Summary - INFO: - iter: 5000 train_rouge_l_f: 0.357 test_rouge_l_f: 0.435 \n",
      "\n",
      "2020-03-19 11:40:05 - Text-Summary - INFO: - iter: 6000 train_rouge_l_f: 0.389 test_rouge_l_f: 0.416 \n",
      "\n",
      "2020-03-19 11:45:57 - Text-Summary - INFO: - iter: 7000 train_rouge_l_f: 0.350 test_rouge_l_f: 0.333 \n",
      "\n",
      "2020-03-19 11:51:44 - Text-Summary - INFO: - iter: 8000 train_rouge_l_f: 0.118 test_rouge_l_f: 0.377 \n",
      "\n",
      "2020-03-19 11:57:33 - Text-Summary - INFO: - iter: 9000 train_rouge_l_f: 0.164 test_rouge_l_f: 0.610 \n",
      "\n",
      "2020-03-19 12:03:25 - Text-Summary - INFO: - iter: 10000 train_rouge_l_f: 0.328 test_rouge_l_f: 0.361 \n",
      "\n",
      "2020-03-19 12:09:07 - Text-Summary - INFO: - iter: 11000 train_rouge_l_f: 0.332 test_rouge_l_f: 0.323 \n",
      "\n",
      "2020-03-19 12:14:50 - Text-Summary - INFO: - iter: 12000 train_rouge_l_f: 0.375 test_rouge_l_f: 0.537 \n",
      "\n",
      "2020-03-19 12:20:39 - Text-Summary - INFO: - iter: 13000 train_rouge_l_f: 0.221 test_rouge_l_f: 0.627 \n",
      "\n",
      "2020-03-19 12:26:33 - Text-Summary - INFO: - iter: 14000 train_rouge_l_f: 0.557 test_rouge_l_f: 0.570 \n",
      "\n",
      "2020-03-19 12:32:21 - Text-Summary - INFO: - iter: 15000 train_rouge_l_f: 0.286 test_rouge_l_f: 0.396 \n",
      "\n",
      "2020-03-19 12:38:20 - Text-Summary - INFO: - iter: 16000 train_rouge_l_f: 0.665 test_rouge_l_f: 0.590 \n",
      "\n",
      "2020-03-19 12:44:03 - Text-Summary - INFO: - iter: 17000 train_rouge_l_f: 0.440 test_rouge_l_f: 0.428 \n",
      "\n",
      "2020-03-19 12:49:59 - Text-Summary - INFO: - iter: 18000 train_rouge_l_f: 0.366 test_rouge_l_f: 0.571 \n",
      "\n",
      "2020-03-19 12:55:44 - Text-Summary - INFO: - iter: 19000 train_rouge_l_f: 0.431 test_rouge_l_f: 0.571 \n",
      "\n",
      "2020-03-19 13:01:37 - Text-Summary - INFO: - iter: 20000 train_rouge_l_f: 0.245 test_rouge_l_f: 0.564 \n",
      "\n",
      "2020-03-19 13:07:23 - Text-Summary - INFO: - iter: 21000 train_rouge_l_f: 0.479 test_rouge_l_f: 0.471 \n",
      "\n",
      "2020-03-19 13:13:16 - Text-Summary - INFO: - iter: 22000 train_rouge_l_f: 0.180 test_rouge_l_f: 0.278 \n",
      "\n",
      "2020-03-19 13:19:05 - Text-Summary - INFO: - iter: 23000 train_rouge_l_f: 0.243 test_rouge_l_f: 0.562 \n",
      "\n",
      "2020-03-19 13:24:57 - Text-Summary - INFO: - iter: 24000 train_rouge_l_f: 0.183 test_rouge_l_f: 0.571 \n",
      "\n",
      "2020-03-19 13:30:43 - Text-Summary - INFO: - iter: 25000 train_rouge_l_f: 0.411 test_rouge_l_f: 0.471 \n",
      "\n",
      "2020-03-19 13:36:32 - Text-Summary - INFO: - iter: 26000 train_rouge_l_f: 0.549 test_rouge_l_f: 0.577 \n",
      "\n",
      "2020-03-19 13:42:21 - Text-Summary - INFO: - iter: 27000 train_rouge_l_f: 0.725 test_rouge_l_f: 0.485 \n",
      "\n",
      "2020-03-19 13:48:05 - Text-Summary - INFO: - iter: 28000 train_rouge_l_f: 0.333 test_rouge_l_f: 0.527 \n",
      "\n",
      "2020-03-19 13:53:53 - Text-Summary - INFO: - iter: 29000 train_rouge_l_f: 0.181 test_rouge_l_f: 0.625 \n",
      "\n",
      "2020-03-19 13:59:49 - Text-Summary - INFO: - iter: 30000 train_rouge_l_f: 0.211 test_rouge_l_f: 0.401 \n",
      "\n",
      "2020-03-19 14:05:43 - Text-Summary - INFO: - iter: 31000 train_rouge_l_f: 0.272 test_rouge_l_f: 0.571 \n",
      "\n",
      "2020-03-19 14:11:31 - Text-Summary - INFO: - iter: 32000 train_rouge_l_f: 0.670 test_rouge_l_f: 0.562 \n",
      "\n",
      "2020-03-19 14:17:30 - Text-Summary - INFO: - iter: 33000 train_rouge_l_f: 0.308 test_rouge_l_f: 0.618 \n",
      "\n",
      "2020-03-19 14:23:12 - Text-Summary - INFO: - iter: 34000 train_rouge_l_f: 0.501 test_rouge_l_f: 0.562 \n",
      "\n",
      "2020-03-19 14:28:55 - Text-Summary - INFO: - iter: 35000 train_rouge_l_f: 0.352 test_rouge_l_f: 0.485 \n",
      "\n",
      "2020-03-19 14:34:32 - Text-Summary - INFO: - iter: 36000 train_rouge_l_f: 0.344 test_rouge_l_f: 0.457 \n",
      "\n",
      "2020-03-19 14:40:31 - Text-Summary - INFO: - iter: 37000 train_rouge_l_f: 0.491 test_rouge_l_f: 0.597 \n",
      "\n",
      "2020-03-19 14:46:30 - Text-Summary - INFO: - iter: 38000 train_rouge_l_f: 0.618 test_rouge_l_f: 0.625 \n",
      "\n",
      "2020-03-19 14:52:23 - Text-Summary - INFO: - iter: 39000 train_rouge_l_f: 0.442 test_rouge_l_f: 0.618 \n",
      "\n",
      "2020-03-19 14:58:11 - Text-Summary - INFO: - iter: 40000 train_rouge_l_f: 0.433 test_rouge_l_f: 0.639 \n",
      "\n",
      "2020-03-19 15:03:59 - Text-Summary - INFO: - iter: 41000 train_rouge_l_f: 0.565 test_rouge_l_f: 0.454 \n",
      "\n",
      "2020-03-19 15:09:44 - Text-Summary - INFO: - iter: 42000 train_rouge_l_f: 0.582 test_rouge_l_f: 0.528 \n",
      "\n",
      "2020-03-19 15:15:35 - Text-Summary - INFO: - iter: 43000 train_rouge_l_f: 0.431 test_rouge_l_f: 0.606 \n",
      "\n",
      "2020-03-19 15:21:25 - Text-Summary - INFO: - iter: 44000 train_rouge_l_f: 0.598 test_rouge_l_f: 0.595 \n",
      "\n",
      "2020-03-19 15:27:17 - Text-Summary - INFO: - iter: 45000 train_rouge_l_f: 0.474 test_rouge_l_f: 0.603 \n",
      "\n",
      "2020-03-19 15:32:56 - Text-Summary - INFO: - iter: 46000 train_rouge_l_f: 0.683 test_rouge_l_f: 0.528 \n",
      "\n",
      "2020-03-19 15:38:53 - Text-Summary - INFO: - iter: 47000 train_rouge_l_f: 0.453 test_rouge_l_f: 0.605 \n",
      "\n",
      "2020-03-19 15:44:25 - Text-Summary - INFO: - iter: 48000 train_rouge_l_f: 0.584 test_rouge_l_f: 0.588 \n",
      "\n",
      "2020-03-19 15:50:09 - Text-Summary - INFO: - iter: 49000 train_rouge_l_f: 0.286 test_rouge_l_f: 0.566 \n",
      "\n",
      "2020-03-19 15:55:56 - Text-Summary - INFO: - iter: 50000 train_rouge_l_f: 0.607 test_rouge_l_f: 0.403 \n",
      "\n",
      "2020-03-19 16:01:32 - Text-Summary - INFO: - iter: 51000 train_rouge_l_f: 0.746 test_rouge_l_f: 0.492 \n",
      "\n",
      "2020-03-19 16:07:19 - Text-Summary - INFO: - iter: 52000 train_rouge_l_f: 0.864 test_rouge_l_f: 0.467 \n",
      "\n",
      "2020-03-19 16:13:12 - Text-Summary - INFO: - iter: 53000 train_rouge_l_f: 0.576 test_rouge_l_f: 0.602 \n",
      "\n",
      "2020-03-19 16:18:56 - Text-Summary - INFO: - iter: 54000 train_rouge_l_f: 0.462 test_rouge_l_f: 0.511 \n",
      "\n",
      "2020-03-19 16:24:46 - Text-Summary - INFO: - iter: 55000 train_rouge_l_f: 0.609 test_rouge_l_f: 0.500 \n",
      "\n",
      "2020-03-19 16:30:38 - Text-Summary - INFO: - iter: 56000 train_rouge_l_f: 0.681 test_rouge_l_f: 0.582 \n",
      "\n",
      "2020-03-19 16:36:33 - Text-Summary - INFO: - iter: 57000 train_rouge_l_f: 0.558 test_rouge_l_f: 0.497 \n",
      "\n",
      "2020-03-19 16:42:29 - Text-Summary - INFO: - iter: 58000 train_rouge_l_f: 0.671 test_rouge_l_f: 0.579 \n",
      "\n",
      "2020-03-19 16:48:24 - Text-Summary - INFO: - iter: 59000 train_rouge_l_f: 0.755 test_rouge_l_f: 0.610 \n",
      "\n",
      "2020-03-19 16:54:21 - Text-Summary - INFO: - iter: 60000 train_rouge_l_f: 0.746 test_rouge_l_f: 0.552 \n",
      "\n",
      "2020-03-19 17:00:14 - Text-Summary - INFO: - iter: 61000 train_rouge_l_f: 0.526 test_rouge_l_f: 0.521 \n",
      "\n",
      "2020-03-19 17:05:59 - Text-Summary - INFO: - iter: 62000 train_rouge_l_f: 0.476 test_rouge_l_f: 0.514 \n",
      "\n",
      "2020-03-19 17:11:56 - Text-Summary - INFO: - iter: 63000 train_rouge_l_f: 0.724 test_rouge_l_f: 0.549 \n",
      "\n",
      "2020-03-19 17:13:03 - Text-Summary - INFO: - logger已關閉\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f18977931c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#         write_enc_graph()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#         write_dec_graph()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_action\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Users/leyan/Text-Summarizer-FOP/run.py\u001b[0m in \u001b[0;36mtrain_action\u001b[0;34m(opt, logger, writer, train_num)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtrain_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtrain_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Users/leyan/Text-Summarizer-FOP/run.py\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mtrain_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mtrain_mle_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_mle_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 self.writer.add_scalars('Compare/mle_loss' ,  \n",
      "\u001b[0;32m~/Users/leyan/Text-Summarizer-FOP/run.py\u001b[0m in \u001b[0;36mtrain_one_batch\u001b[0;34m(self, batch, test_batch, iter)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# perform MLE training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros,\n\u001b[0;32m--> 342\u001b[0;31m                                             enc_batch_extend_vocab, enc_key_batch, enc_key_lens, batch)\n\u001b[0m\u001b[1;32m    343\u001b[0m             mle_loss_2 = self.train_batch_MLE(enc_out2, enc_hidden2, enc_padding_mask2, context2, extra_zeros2,\n\u001b[1;32m    344\u001b[0m                                             enc_batch_extend_vocab2, enc_key_batch2, enc_key_lens2, test_batch)\n",
      "\u001b[0;32m~/Users/leyan/Text-Summarizer-FOP/run.py\u001b[0m in \u001b[0;36mtrain_batch_MLE\u001b[0;34m(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens, batch)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m#                 print(config.vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m#                 print('x_t',x_t.shape);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mis_oov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Mask indicating whether sampled word is OOV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mis_oov\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_oov\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_id\u001b[0m  \u001b[0;31m# Replace OOVs with [UNK] token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;31m#                 print('x_t_22',x_t.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/u012869752/article/details/72513141\n",
    "# 由于在jupyter notebook中，args不为空\n",
    "from glob import glob\n",
    "from run import *\n",
    "# nvidia-smi -pm 1\n",
    "if __name__ == \"__main__\":   \n",
    "    try:\n",
    "        # --------------------------Training ----------------------------------\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--train_mle', type=bool, default=True)\n",
    "        parser.add_argument('--train_rl', type=bool, default=False)\n",
    "        parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "#         parser.add_argument('--load_model', type=str, default='/0093000_1.71_0.00.tar')\n",
    "        parser.add_argument('--load_model', type=str, default=None)\n",
    "        parser.add_argument('--new_lr', type=float, default=None)\n",
    "        parser.add_argument('--multi_device', type=bool, default=True)\n",
    "        parser.add_argument('--view', type=bool, default=True)\n",
    "        parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "        parser.add_argument('--word_emb_type', type=str, default='word2Vec')\n",
    "        parser.add_argument('--train_action', type=bool, default=True)\n",
    "        opt = parser.parse_args(args=[])\n",
    "        \n",
    "        today = dt.now()\n",
    "        loggerPath = \"LOG/%s-(%s_%s_%s)-(%s:%s:%s)\"%(opt.word_emb_type,\n",
    "                  today.year,today.month,today.day,\n",
    "                  today.hour,today.minute,today.second)\n",
    "\n",
    "        logger = getLogger(config.loggerName,loggerPath)   \n",
    "        \n",
    "        if opt.load_model == None:\n",
    "            shutil.rmtree('runs/Pointer-Generator/word2Vec', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/word2Vec/exp-4', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/word2Vec/Eecoder', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/word2Vec/Decoder', ignore_errors=True) # clear previous \n",
    "\n",
    "        writer = SummaryWriter('runs/Pointer-Generator/word2Vec/exp-4')\n",
    "#         writer = SummaryWriter('runs/Pointer-Generator/word2Vec')\n",
    "        writer.add_text('Train_Para/',info_str,0)\n",
    "#         write_enc_graph()\n",
    "#         write_dec_graph()\n",
    "        if opt.train_action: train_action(opt, logger, writer, train_num)\n",
    "\n",
    "    except KeyError as e:\n",
    "        traceback = sys.exc_info()[2]\n",
    "        print(sys.exc_info())\n",
    "        print(traceback.tb_lineno)\n",
    "        print(e)\n",
    "    finally:\n",
    "        removeLogger(logger)\n",
    "        \n",
    "        # export scalar data to JSON for external processing\n",
    "        # tensorboard --logdir /home/eagleuser/Users/leyan/Text-Summarizer-FOP/TensorBoard\n",
    "#         tensorboard --logdir ./runs\n",
    "#         if not os.path.exists('TensorBoard'): os.makedirs('TensorBoard')\n",
    "#         writer.export_scalars_to_json(\"TensorBoard/test.json\")\n",
    "        writer.close()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
