{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from model import Model\n",
    "\n",
    "\n",
    "from data_util import config, data\n",
    "from data_util.batcher import Batcher\n",
    "from data_util.data import Vocab\n",
    "\n",
    "\n",
    "from train_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse\n",
    "import torchsnooper\n",
    "import logging\n",
    "\n",
    "# -------- Test Packages -------\n",
    "from beam_search import *\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "config.lr = 0.0001\n",
    "config.batch_size = 2\n",
    "config.gound_truth_prob = 0.1\n",
    "\n",
    "\n",
    "config.intra_encoder = False\n",
    "config.intra_decoder = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def getLogger(loggerName, loggerPath):\n",
    "    # 設置logger\n",
    "    logger = logging.getLogger(loggerName)  # 不加名稱設置root logger\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s: - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    logging.Filter(loggerName)\n",
    "\n",
    "    # 使用FileHandler輸出到文件\n",
    "    directory = os.path.dirname(loggerPath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fh = logging.FileHandler(loggerPath)\n",
    "\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # 使用StreamHandler輸出到屏幕\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    ch.setFormatter(formatter)\n",
    "    # 添加兩個Handler\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    # Handler只啟動一次\n",
    "    # 設置logger\n",
    "    logger.info(u'logger已啟動')\n",
    "    return logger\n",
    "\n",
    "def removeLogger(logger):\n",
    "    logger.info(u'logger已關閉')\n",
    "    handlers = logger.handlers[:]\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch():\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    # with torchsnooper.snoop():\n",
    "    while batch is not None:\n",
    "        example_list = batch.example_list\n",
    "        for ex in example_list:\n",
    "            r = str(ex.original_review)\n",
    "            s = str(ex.original_summary)\n",
    "            k = str(ex.key_words)\n",
    "            sent = ex.original_summary_sents\n",
    "#             print(\"original_review_sents:\", r)\n",
    "            print(\"original_summary_sents : \", s)\n",
    "            print(\"key_words : \", k)\n",
    "            print('------------------------------------------------------------\\n')\n",
    "        batch = batcher.next_batch()        \n",
    "# test_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bin Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 29540\n",
      "\n",
      "test : 5847\n",
      "\n",
      "valid : 4243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(config.bin_info,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    [print(line) for line in lines]\n",
    "    train_num = int(lines[0].split(\":\")[1])\n",
    "    test_num = int(lines[1].split(\":\")[1])\n",
    "    val_num = int(lines[2].split(\":\")[1])\n",
    "    # f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    # f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    # f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Encoder,Model\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "encoder = Encoder().to(device)    \n",
    "\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "# summary(encoder, enc_batch, enc_lens) # encoder summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_out torch.Size([2, 200, 1024])\n",
      "enc_hidden torch.Size([2, 512])\n",
      "x_t torch.Size([2, 300])\n",
      "================================================================================\n",
      "                            Kernel Shape    Output Shape     Params  Mult-Adds\n",
      "Layer                                                                         \n",
      "0_x_context                  [1324, 300]        [2, 300]     397.5k     397.2k\n",
      "1_lstm                                 -        [2, 512]  1.667072M  1.662976M\n",
      "2_enc_attention.Linear_W_h  [1024, 1024]  [2, 200, 1024]  1.048576M  1.048576M\n",
      "3_enc_attention.Linear_W_s  [1024, 1024]       [2, 1024]    1.0496M  1.048576M\n",
      "4_enc_attention.Linear_v       [1024, 1]     [2, 200, 1]     1.024k     1.024k\n",
      "5_dec_attention                        -        [2, 512]          -          -\n",
      "6_p_gen_linear                 [2860, 1]          [2, 1]     2.861k      2.86k\n",
      "7_V                          [2048, 512]        [2, 512]  1.049088M  1.048576M\n",
      "8_V1                        [512, 50000]      [2, 50000]     25.65M      25.6M\n",
      "--------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params          30.865721M\n",
      "Trainable params      30.865721M\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             30.809788M\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_x_context</th>\n",
       "      <td>[1324, 300]</td>\n",
       "      <td>[2, 300]</td>\n",
       "      <td>397500.0</td>\n",
       "      <td>397200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[2, 512]</td>\n",
       "      <td>1667072.0</td>\n",
       "      <td>1662976.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_enc_attention.Linear_W_h</th>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>[2, 200, 1024]</td>\n",
       "      <td>1048576.0</td>\n",
       "      <td>1048576.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_enc_attention.Linear_W_s</th>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>[2, 1024]</td>\n",
       "      <td>1049600.0</td>\n",
       "      <td>1048576.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_enc_attention.Linear_v</th>\n",
       "      <td>[1024, 1]</td>\n",
       "      <td>[2, 200, 1]</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_dec_attention</th>\n",
       "      <td>-</td>\n",
       "      <td>[2, 512]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_p_gen_linear</th>\n",
       "      <td>[2860, 1]</td>\n",
       "      <td>[2, 1]</td>\n",
       "      <td>2861.0</td>\n",
       "      <td>2860.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_V</th>\n",
       "      <td>[2048, 512]</td>\n",
       "      <td>[2, 512]</td>\n",
       "      <td>1049088.0</td>\n",
       "      <td>1048576.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_V1</th>\n",
       "      <td>[512, 50000]</td>\n",
       "      <td>[2, 50000]</td>\n",
       "      <td>25650000.0</td>\n",
       "      <td>25600000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Kernel Shape    Output Shape      Params  \\\n",
       "Layer                                                                  \n",
       "0_x_context                  [1324, 300]        [2, 300]    397500.0   \n",
       "1_lstm                                 -        [2, 512]   1667072.0   \n",
       "2_enc_attention.Linear_W_h  [1024, 1024]  [2, 200, 1024]   1048576.0   \n",
       "3_enc_attention.Linear_W_s  [1024, 1024]       [2, 1024]   1049600.0   \n",
       "4_enc_attention.Linear_v       [1024, 1]     [2, 200, 1]      1024.0   \n",
       "5_dec_attention                        -        [2, 512]         NaN   \n",
       "6_p_gen_linear                 [2860, 1]          [2, 1]      2861.0   \n",
       "7_V                          [2048, 512]        [2, 512]   1049088.0   \n",
       "8_V1                        [512, 50000]      [2, 50000]  25650000.0   \n",
       "\n",
       "                             Mult-Adds  \n",
       "Layer                                   \n",
       "0_x_context                   397200.0  \n",
       "1_lstm                       1662976.0  \n",
       "2_enc_attention.Linear_W_h   1048576.0  \n",
       "3_enc_attention.Linear_W_s   1048576.0  \n",
       "4_enc_attention.Linear_v        1024.0  \n",
       "5_dec_attention                    NaN  \n",
       "6_p_gen_linear                  2860.0  \n",
       "7_V                          1048576.0  \n",
       "8_V1                        25600000.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Decoder,Model\n",
    "from train_util import *\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# decoder = Decoder().to(device)    \n",
    "\n",
    "model = Model(False,'word2Vec',vocab)\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "print('enc_out',enc_out.shape)\n",
    "print('enc_hidden',enc_hidden[0].shape)\n",
    "\n",
    "# train_batch_MLE\n",
    "dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "step_losses = []\n",
    "s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "# x_t 為decoder每一個time step 的batch input\n",
    "x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "sum_temporal_srcs = None  \n",
    "unk_id = vocab.word2id(data.PAD_TOKEN)\n",
    "# print('x_t',x_t.shape)             \n",
    "# print(vocab._id_to_word)    \n",
    "# print(enc_hidden[0].shape)\n",
    "for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "    use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "    # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "    # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "#     print('--------------------------')\n",
    "#     print('x_t',x_t)\n",
    "#     print('gound_truth_t',use_gound_truth * dec_batch[:, t])\n",
    "#     print('none_gound_truth_t',(1 - use_gound_truth) * x_t)\n",
    "    x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "#     print('x_t',x_t)\n",
    "#     decode_xt = [vocab.id2word(i) for i in x_t.tolist()]\n",
    "#     print('decode_xt',decode_xt)\n",
    "    x_t = model.embeds(x_t)\n",
    "    enc_key_batch = model.embeds(enc_key_batch)\n",
    "#     print('s_t',s_t[0].shape)\n",
    "    print('x_t',x_t.shape)\n",
    "    final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "    x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "    extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "    enc_key_batch, enc_key_lens)\n",
    "    \n",
    "#     target = target_batch[:, t]\n",
    "#     log_probs = T.log(final_dist + config.eps)\n",
    "#     step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=unk_id)\n",
    "#     step_losses.append(step_loss)\n",
    "#     x_t = T.multinomial(final_dist,1).squeeze()  # Sample words from final distribution which can be used as input in next time step\n",
    "    \n",
    "#     is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "#     x_t = (1 - is_oov) * x_t.detach() + (is_oov) * unk_id  # Replace OOVs with [UNK] token\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "    break\n",
    "decoder_summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
