{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from model import Model\n",
    "\n",
    "\n",
    "from data_util import config, data\n",
    "from data_util.batcher import Batcher\n",
    "from data_util.data import Vocab\n",
    "\n",
    "\n",
    "from train_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse\n",
    "import torchsnooper\n",
    "import logging\n",
    "\n",
    "# -------- Test Packages -------\n",
    "from beam_search import *\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Normal embedding\n",
    "config.lr = 0.0001\n",
    "config.batch_size = 8\n",
    "config.key_attention = True\n",
    "config.gound_truth_prob = 0.1\n",
    "\n",
    "# Bert embedding\n",
    "\n",
    "# config.keywords = \"FOP_keywords\"\n",
    "# config.keywords = \"TextRank_summary\" # problem\n",
    "config.keywords = \"TextRank_keywords\"\n",
    "\n",
    "\n",
    "\n",
    "config.max_key_num = 8\n",
    "word_emb_type = 'glove'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def getLogger(loggerName, loggerPath):\n",
    "    # 設置logger\n",
    "    logger = logging.getLogger(loggerName)  # 不加名稱設置root logger\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s: - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    logging.Filter(loggerName)\n",
    "\n",
    "    # 使用FileHandler輸出到文件\n",
    "    directory = os.path.dirname(loggerPath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fh = logging.FileHandler(loggerPath)\n",
    "\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # 使用StreamHandler輸出到屏幕\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    ch.setFormatter(formatter)\n",
    "    # 添加兩個Handler\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    # Handler只啟動一次\n",
    "    # 設置logger\n",
    "    logger.info(u'logger已啟動')\n",
    "    return logger\n",
    "\n",
    "def removeLogger(logger):\n",
    "    logger.info(u'logger已關閉')\n",
    "    handlers = logger.handlers[:]\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch():\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    # with torchsnooper.snoop():\n",
    "    while batch is not None:\n",
    "        example_list = batch.example_list\n",
    "        for ex in example_list:\n",
    "            r = str(ex.original_review)\n",
    "            s = str(ex.original_summary)\n",
    "            k = str(ex.key_words)\n",
    "            sent = ex.original_summary_sents\n",
    "#             print(\"original_review_sents:\", r)\n",
    "            print(\"original_summary_sents : \", s)\n",
    "            print(\"key_words : \", k)\n",
    "            print('------------------------------------------------------------\\n')\n",
    "        batch = batcher.next_batch()        \n",
    "# test_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bin Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 29115\n",
      "\n",
      "test : 3638\n",
      "\n",
      "valid : 3639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(config.bin_info,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    [print(line) for line in lines]\n",
    "    train_num = int(lines[0].split(\":\")[1])\n",
    "    test_num = int(lines[1].split(\":\")[1])\n",
    "    val_num = int(lines[2].split(\":\")[1])\n",
    "    # f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    # f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    # f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "           Kernel Shape  Output Shape   Params  Mult-Adds\n",
      "Layer                                                    \n",
      "0_lstm                -  [3298, 1024]  3334144    3325952\n",
      "1_reduce_h  [1024, 512]      [8, 512]   524800     524288\n",
      "2_reduce_c  [1024, 512]      [8, 512]   524800     524288\n",
      "---------------------------------------------------------\n",
      "                       Totals\n",
      "Total params          4383744\n",
      "Trainable params      4383744\n",
      "Non-trainable params        0\n",
      "Mult-Adds             4374528\n",
      "=========================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[3298, 1024]</td>\n",
       "      <td>3334144</td>\n",
       "      <td>3325952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_reduce_h</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_reduce_c</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kernel Shape  Output Shape   Params  Mult-Adds\n",
       "Layer                                                    \n",
       "0_lstm                -  [3298, 1024]  3334144    3325952\n",
       "1_reduce_h  [1024, 512]      [8, 512]   524800     524288\n",
       "2_reduce_c  [1024, 512]      [8, 512]   524800     524288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Encoder,Model\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "encoder = Encoder().to(device)    \n",
    "\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "summary(encoder, enc_batch, enc_lens) # encoder summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                               Kernel Shape    Output Shape    Params  \\\n",
      "Layer                                                                   \n",
      "0_x_context                     [1324, 300]        [8, 300]    397500   \n",
      "1_lstm                                    -        [8, 512]   1667072   \n",
      "2_enc_attention.Linear_W_h     [1024, 1024]  [8, 519, 1024]   1048576   \n",
      "3_enc_attention.Linear_W_s     [1024, 1024]       [8, 1024]   1049600   \n",
      "4_enc_attention.Linear_W_t      [300, 1024]       [8, 1024]    308224   \n",
      "5_enc_attention.Linear_v          [1024, 1]     [8, 519, 1]      1024   \n",
      "6_dec_attention.Linear_W_prev    [512, 512]     [8, 1, 512]    262144   \n",
      "7_dec_attention.Linear_W_s       [512, 512]        [8, 512]    262656   \n",
      "8_dec_attention.Linear_W_t       [300, 512]        [8, 512]    154112   \n",
      "9_dec_attention.Linear_v           [512, 1]       [8, 1, 1]       512   \n",
      "10_p_gen_linear                   [2860, 1]          [8, 1]      2861   \n",
      "11_V                            [2048, 512]        [8, 512]   1049088   \n",
      "12_V1                          [512, 50000]      [8, 50000]  25650000   \n",
      "\n",
      "                               Mult-Adds  \n",
      "Layer                                     \n",
      "0_x_context                       397200  \n",
      "1_lstm                           1662976  \n",
      "2_enc_attention.Linear_W_h       1048576  \n",
      "3_enc_attention.Linear_W_s       1048576  \n",
      "4_enc_attention.Linear_W_t        307200  \n",
      "5_enc_attention.Linear_v            1024  \n",
      "6_dec_attention.Linear_W_prev     262144  \n",
      "7_dec_attention.Linear_W_s        262144  \n",
      "8_dec_attention.Linear_W_t        153600  \n",
      "9_dec_attention.Linear_v             512  \n",
      "10_p_gen_linear                     2860  \n",
      "11_V                             1048576  \n",
      "12_V1                           25600000  \n",
      "--------------------------------------------------------------------------------\n",
      "                        Totals\n",
      "Total params          31853369\n",
      "Trainable params      31853369\n",
      "Non-trainable params         0\n",
      "Mult-Adds             31795388\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_x_context</th>\n",
       "      <td>[1324, 300]</td>\n",
       "      <td>[8, 300]</td>\n",
       "      <td>397500</td>\n",
       "      <td>397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>1667072</td>\n",
       "      <td>1662976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_enc_attention.Linear_W_h</th>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>[8, 519, 1024]</td>\n",
       "      <td>1048576</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_enc_attention.Linear_W_s</th>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>[8, 1024]</td>\n",
       "      <td>1049600</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_enc_attention.Linear_W_t</th>\n",
       "      <td>[300, 1024]</td>\n",
       "      <td>[8, 1024]</td>\n",
       "      <td>308224</td>\n",
       "      <td>307200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_enc_attention.Linear_v</th>\n",
       "      <td>[1024, 1]</td>\n",
       "      <td>[8, 519, 1]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_dec_attention.Linear_W_prev</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>[8, 1, 512]</td>\n",
       "      <td>262144</td>\n",
       "      <td>262144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_dec_attention.Linear_W_s</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>262656</td>\n",
       "      <td>262144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_dec_attention.Linear_W_t</th>\n",
       "      <td>[300, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>154112</td>\n",
       "      <td>153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9_dec_attention.Linear_v</th>\n",
       "      <td>[512, 1]</td>\n",
       "      <td>[8, 1, 1]</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_p_gen_linear</th>\n",
       "      <td>[2860, 1]</td>\n",
       "      <td>[8, 1]</td>\n",
       "      <td>2861</td>\n",
       "      <td>2860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11_V</th>\n",
       "      <td>[2048, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>1049088</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12_V1</th>\n",
       "      <td>[512, 50000]</td>\n",
       "      <td>[8, 50000]</td>\n",
       "      <td>25650000</td>\n",
       "      <td>25600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Kernel Shape    Output Shape    Params  \\\n",
       "Layer                                                                   \n",
       "0_x_context                     [1324, 300]        [8, 300]    397500   \n",
       "1_lstm                                    -        [8, 512]   1667072   \n",
       "2_enc_attention.Linear_W_h     [1024, 1024]  [8, 519, 1024]   1048576   \n",
       "3_enc_attention.Linear_W_s     [1024, 1024]       [8, 1024]   1049600   \n",
       "4_enc_attention.Linear_W_t      [300, 1024]       [8, 1024]    308224   \n",
       "5_enc_attention.Linear_v          [1024, 1]     [8, 519, 1]      1024   \n",
       "6_dec_attention.Linear_W_prev    [512, 512]     [8, 1, 512]    262144   \n",
       "7_dec_attention.Linear_W_s       [512, 512]        [8, 512]    262656   \n",
       "8_dec_attention.Linear_W_t       [300, 512]        [8, 512]    154112   \n",
       "9_dec_attention.Linear_v           [512, 1]       [8, 1, 1]       512   \n",
       "10_p_gen_linear                   [2860, 1]          [8, 1]      2861   \n",
       "11_V                            [2048, 512]        [8, 512]   1049088   \n",
       "12_V1                          [512, 50000]      [8, 50000]  25650000   \n",
       "\n",
       "                               Mult-Adds  \n",
       "Layer                                     \n",
       "0_x_context                       397200  \n",
       "1_lstm                           1662976  \n",
       "2_enc_attention.Linear_W_h       1048576  \n",
       "3_enc_attention.Linear_W_s       1048576  \n",
       "4_enc_attention.Linear_W_t        307200  \n",
       "5_enc_attention.Linear_v            1024  \n",
       "6_dec_attention.Linear_W_prev     262144  \n",
       "7_dec_attention.Linear_W_s        262144  \n",
       "8_dec_attention.Linear_W_t        153600  \n",
       "9_dec_attention.Linear_v             512  \n",
       "10_p_gen_linear                     2860  \n",
       "11_V                             1048576  \n",
       "12_V1                           25600000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Decoder,Model\n",
    "from train_util import *\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# decoder = Decoder().to(device)    \n",
    "\n",
    "model = Model(False,'word2Vec',vocab)\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "# train_batch_MLE\n",
    "dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "step_losses = []\n",
    "s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "# x_t 為decoder每一個time step 的batch input\n",
    "x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "sum_temporal_srcs = None     \n",
    "             \n",
    "    \n",
    "for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "    use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "    # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "    # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "    x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "    x_t = model.embeds(x_t)\n",
    "    \n",
    "    enc_key_batch = model.embeds(enc_key_batch)\n",
    "    \n",
    "#     final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "    final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "    x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "    extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "    enc_key_batch, enc_key_padding_mask)\n",
    "    \n",
    "    decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "    break\n",
    "decoder_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummaryX import summary\n",
    "class Train(object):\n",
    "    def __init__(self, opt, vocab):\n",
    "#         self.vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "        self.vocab = vocab\n",
    "        self.train_batcher = Batcher(config.train_data_path, self.vocab, mode='train',\n",
    "                               batch_size=config.batch_size, single_pass=False)\n",
    "        self.test_batcher = Batcher(config.test_data_path, self.vocab, mode='eval',\n",
    "                               batch_size=config.batch_size, single_pass=True)\n",
    "        self.opt = opt\n",
    "        self.start_id = self.vocab.word2id(data.START_DECODING)\n",
    "        self.end_id = self.vocab.word2id(data.STOP_DECODING)\n",
    "        self.pad_id = self.vocab.word2id(data.PAD_TOKEN)\n",
    "        self.unk_id = self.vocab.word2id(data.UNKNOWN_TOKEN)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def save_model(self, iter, loss, r_loss):\n",
    "        if not os.path.exists(config.save_model_path):\n",
    "            os.makedirs(config.save_model_path)\n",
    "        file_path = \"/%07d_%.2f_%.2f.tar\" % (iter, loss, r_loss)\n",
    "        save_path = config.save_model_path + '/%s' % (self.opt.word_emb_type)\n",
    "        if not os.path.isdir(save_path): os.mkdir(save_path)\n",
    "        save_path = save_path + file_path\n",
    "        T.save({\n",
    "            \"iter\": iter + 1,\n",
    "            \"model_dict\": self.model.state_dict(),\n",
    "            \"trainer_dict\": self.trainer.state_dict()\n",
    "        }, save_path)\n",
    "        return file_path\n",
    "\n",
    "    def setup_train(self):\n",
    "        self.model = Model(opt.pre_train_emb, opt.word_emb_type, self.vocab)\n",
    "        #         print(\"Model : \",self.model)\n",
    "        #         logger.info(\"Model : \")\n",
    "        logger.info(str(self.model))\n",
    "        #         print(\"Encoder : \",self.model.encoder)\n",
    "        #         print(\"Decoder : \",self.model.decoder)\n",
    "        #         print(\"Embeds : \",self.model.embeds)\n",
    "        self.model = get_cuda(self.model)\n",
    "        device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")  # PyTorch v0.4.0\n",
    "        if opt.multi_device:\n",
    "            if T.cuda.device_count() > 1:\n",
    "                #                 print(\"Let's use\", T.cuda.device_count(), \"GPUs!\")\n",
    "                logger.info(\"Let's use \" + str(T.cuda.device_count()) + \" GPUs!\")\n",
    "                self.model = nn.DataParallel(self.model, list(range(T.cuda.device_count()))).cuda()\n",
    "\n",
    "        if isinstance(self.model, nn.DataParallel):\n",
    "            self.model = self.model.module\n",
    "        self.model.to(device)\n",
    "        #         self.model.eval()\n",
    "\n",
    "        self.trainer = T.optim.Adam(self.model.parameters(), lr=config.lr)\n",
    "        start_iter = 0\n",
    "        if self.opt.load_model is not None:\n",
    "#             load_model_path = os.path.join(config.save_model_path, self.opt.load_model)\n",
    "            load_model_path = config.save_model_path + self.opt.load_model\n",
    "            print(load_model_path)\n",
    "#             print('xxxx')\n",
    "            checkpoint = T.load(load_model_path)\n",
    "            start_iter = checkpoint[\"iter\"]\n",
    "            self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "            self.trainer.load_state_dict(checkpoint[\"trainer_dict\"])\n",
    "            #             print(\"Loaded model at \" + load_model_path)\n",
    "            logger.info(\"Loaded model at \" + load_model_path)\n",
    "        if self.opt.new_lr is not None:\n",
    "            self.trainer = T.optim.Adam(self.model.parameters(), lr=self.opt.new_lr)\n",
    "        return start_iter\n",
    "\n",
    "    def train_batch_MLE(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_padding_mask, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(\n",
    "            batch)  # Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        try:\n",
    "#             print('-----------------')\n",
    "            for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "                use_gound_truth = get_cuda((T.rand(len(enc_out)) > config.gound_truth_prob)).long()  # Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "                x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t  # Select decoder input based on use_ground_truth probabilities\n",
    "                x_t = self.model.embeds(x_t) \n",
    "                final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                          ct_e, extra_zeros,\n",
    "                                                                                          enc_batch_extend_vocab,\n",
    "                                                                                          sum_temporal_srcs, prev_s, enc_key_batch, enc_key_padding_mask)\n",
    "                target = target_batch[:, t]\n",
    "                log_probs = T.log(final_dist + config.eps)\n",
    "                step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=self.pad_id)\n",
    "                step_losses.append(step_loss)\n",
    "                x_t = T.multinomial(final_dist,1).squeeze()  # Sample words from final distribution which can be used as input in next time step\n",
    "#                 print(config.vocab_size)\n",
    "#                 print(x_t)\n",
    "                is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "                x_t = (1 - is_oov) * x_t.detach() + (is_oov) * self.unk_id  # Replace OOVs with [UNK] token\n",
    "        except Exception as e:\n",
    "            logger.error('xxxxxxxxxxx')\n",
    "            traceback = sys.exc_info()[2]\n",
    "            logger.error(sys.exc_info())\n",
    "            logger.error(traceback.tb_lineno)\n",
    "            logger.error(e)\n",
    "#             logger.error(final_dist)\n",
    "            logger.error('xxxxxxxxxxx')\n",
    "#             print(step_loss)\n",
    "\n",
    "                \n",
    "        losses = T.sum(T.stack(step_losses, 1), 1)  # unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens  # Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)  # Average batch loss\n",
    "        return mle_loss\n",
    "\n",
    "    def train_batch_RL(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab,\n",
    "                   review_oovs, enc_key_batch, enc_key_padding_mask, greedy):\n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param review_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        s_t = enc_hidden  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []  # Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []  # Stores padding masks of generated samples\n",
    "        log_probs = []  # Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))  # Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e,\n",
    "                                                                             extra_zeros, enc_batch_extend_vocab,\n",
    "                                                                             sum_temporal_srcs, prev_s, enc_key_batch, enc_key_padding_mask)\n",
    "\n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs)\n",
    "                x_t = multi_dist.sample()  # perform multinomial sampling\n",
    "                log_prob = multi_dist.log_prob(x_t)\n",
    "                log_probs.append(log_prob)\n",
    "            else:\n",
    "                _, x_t = T.max(probs, dim=1)  # perform greedy sampling\n",
    "            x_t = x_t.detach()\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))  # Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1  # If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (\n",
    "            x_t == self.end_id) == 2] = 0  # If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t + (is_oov) * self.unk_id  # Replace OOVs with [UNK] token\n",
    "\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:  # If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1)\n",
    "            log_probs = log_probs * decoder_padding_mask  # Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1)  # Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs, dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy()\n",
    "            oovs = review_oovs[i]\n",
    "            S = data.outputids2words(id_list, self.vocab, oovs)  # Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:  # If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "#         print(log_probs)\n",
    "        return decoded_strs, log_probs \n",
    "\n",
    "    def reward_function(self, decoded_sents, original_sents):\n",
    "        rouge = Rouge()\n",
    "        try:\n",
    "            scores = rouge.get_scores(decoded_sents, original_sents)\n",
    "        except Exception:\n",
    "            #             print(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
    "            logger.info(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
    "            scores = []\n",
    "            for i in range(len(decoded_sents)):\n",
    "                try:\n",
    "                    score = rouge.get_scores(decoded_sents[i], original_sents[i])\n",
    "                except Exception:\n",
    "                    #                     print(\"Error occured at:\")\n",
    "                    #                     print(\"decoded_sents:\", decoded_sents[i])\n",
    "                    #                     print(\"original_sents:\", original_sents[i])\n",
    "                    logger.info(\"Error occured at:\")\n",
    "                    logger.info(\"decoded_sents:\", decoded_sents[i])\n",
    "                    logger.info(\"original_sents:\", original_sents[i])\n",
    "                    score = [{\"rouge-l\": {\"f\": 0.0}}]\n",
    "                scores.append(score[0])\n",
    "        rouge_l_f1 = [score[\"rouge-l\"][\"f\"] for score in scores]\n",
    "        avg_rouge_l_f1 = sum(rouge_l_f1) / len(rouge_l_f1)\n",
    "        rouge_l_f1 = get_cuda(T.FloatTensor(rouge_l_f1))\n",
    "        return rouge_l_f1, scores, avg_rouge_l_f1\n",
    "\n",
    "    # def write_to_file(self, decoded, max, original, sample_r, baseline_r, iter):\n",
    "    #     with open(\"temp.txt\", \"w\") as f:\n",
    "    #         f.write(\"iter:\"+str(iter)+\"\\n\")\n",
    "    #         for i in range(len(original)):\n",
    "    #             f.write(\"dec: \"+decoded[i]+\"\\n\")\n",
    "    #             f.write(\"max: \"+max[i]+\"\\n\")\n",
    "    #             f.write(\"org: \"+original[i]+\"\\n\")\n",
    "    #             f.write(\"Sample_R: %.4f, Baseline_R: %.4f\\n\\n\"%(sample_r[i].item(), baseline_r[i].item()))\n",
    "\n",
    "\n",
    "    def train_one_batch(self, batch,test_batch, iter):\n",
    "        ans_list, batch_scores = None, None\n",
    "        # Train\n",
    "#         enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "\n",
    "        enc_batch, enc_lens, enc_padding_mask, \\\n",
    "        enc_key_batch, enc_key_lens, enc_key_padding_mask,\\\n",
    "        enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    \n",
    "\n",
    "        enc_batch = self.model.embeds(enc_batch)  # Get embeddings for encoder input\n",
    "        enc_key_batch = self.model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "        enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        # Test\n",
    "#         enc_batch2, enc_lens2, enc_padding_mask2, enc_batch_extend_vocab2, extra_zeros2, context2 = get_enc_data(test_batch)\n",
    "        enc_batch2, enc_lens2, enc_padding_mask2, \\\n",
    "        enc_key_batch2, enc_key_lens2, enc_key_padding_mask2,\\\n",
    "        enc_batch_extend_vocab2, extra_zeros2, context2 = get_enc_data(test_batch)\n",
    "    \n",
    "        with T.autograd.no_grad():\n",
    "            enc_batch2 = self.model.embeds(enc_batch2)\n",
    "            enc_key_batch2 = self.model.embeds(enc_key_batch2)\n",
    "            enc_out2, enc_hidden2 = self.model.encoder(enc_batch2, enc_lens2)\n",
    "        # -------------------------------Summarization-----------------------\n",
    "        if self.opt.train_mle == True:  # perform MLE training\n",
    "            mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros,\n",
    "                                            enc_batch_extend_vocab, enc_key_batch, enc_key_padding_mask, batch)\n",
    "            mle_loss_2 = self.train_batch_MLE(enc_out2, enc_hidden2, enc_padding_mask2, context2, extra_zeros2,\n",
    "                                            enc_batch_extend_vocab2, enc_key_batch2, enc_key_padding_mask2, test_batch)\n",
    "        else:\n",
    "            mle_loss = get_cuda(T.FloatTensor([0]))\n",
    "            mle_loss_2 = get_cuda(T.FloatTensor([0]))\n",
    "        # original view\n",
    "#         if opt.view:\n",
    "#             sample_sents, ans_list = self.train_batch_decode(batch, enc_out, enc_hidden, enc_padding_mask, context,\n",
    "#                                                              extra_zeros, enc_batch_extend_vocab, batch.rev_oovs,\n",
    "#                                                              greedy=True)\n",
    "#             rouge_l_f1, batch_scores, avg_rouge_l_f1 = self.reward_function(sample_sents, batch.original_summarys)\n",
    "#             #             writer.add_text('Train/%s'% (iter), ans_list[0]['decoded_str'] , iter)\n",
    "#             #             writer.add_text('Train/%s'% (iter), ans_list[0]['summary'] , iter)\n",
    "#             #             writer.add_text('Train/%s'% (iter), ans_list[0]['review'] , iter)\n",
    "#             writer.add_scalar('Train/avg_rouge_l_f1', avg_rouge_l_f1, iter)\n",
    "            \n",
    "        # --------------RL training-----------------------------------------------------\n",
    "        if self.opt.train_rl == True:  # perform reinforcement learning training\n",
    "            # multinomial sampling\n",
    "            sample_sents, RL_log_probs = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context,\n",
    "                                                             extra_zeros, enc_batch_extend_vocab, batch.rev_oovs,\n",
    "                                                             enc_key_batch, enc_key_padding_mask, greedy=False)\n",
    "            sample_sents2, RL_log_probs2 = self.train_batch_RL(enc_out2, enc_hidden2, enc_padding_mask2, context2,\n",
    "                                                             extra_zeros2, enc_batch_extend_vocab2, test_batch.rev_oovs,\n",
    "                                                             enc_key_batch2, enc_key_padding_mask2, greedy=False)\n",
    "            with T.autograd.no_grad():\n",
    "                # greedy sampling\n",
    "                greedy_sents, _ = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros,\n",
    "                                                      enc_batch_extend_vocab, batch.rev_oovs, enc_key_batch, enc_key_padding_mask, greedy=True)\n",
    "\n",
    "            sample_reward, _, _ = self.reward_function(sample_sents, batch.original_summarys)\n",
    "            baseline_reward, _, _ = self.reward_function(greedy_sents, batch.original_summarys)\n",
    "            # if iter%200 == 0:\n",
    "            #     self.write_to_file(sample_sents, greedy_sents, batch.original_abstracts, sample_reward, baseline_reward, iter)\n",
    "            rl_loss = -(sample_reward - baseline_reward) * RL_log_probs  # Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "            rl_loss = T.mean(rl_loss)\n",
    "\n",
    "            batch_reward = T.mean(sample_reward).item()\n",
    "            writer.add_scalar('RL_Train/rl_loss', rl_loss, iter)\n",
    "        else:\n",
    "            rl_loss = get_cuda(T.FloatTensor([0]))\n",
    "            batch_reward = 0\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        #         if opt.train_mle == True: \n",
    "        self.trainer.zero_grad()\n",
    "        (self.opt.mle_weight * mle_loss + self.opt.rl_weight * rl_loss).backward()\n",
    "        self.trainer.step()\n",
    "        #-----------------------Summarization----------------------------------------------------\n",
    "        if iter % 1000 == 0:\n",
    "            with T.autograd.no_grad():\n",
    "                train_rouge_l_f = self.calc_avg_rouge_result(iter,batch,'Train',enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens)\n",
    "                test_rouge_l_f = self.calc_avg_rouge_result(iter,test_batch,'Test',enc_hidden2, enc_out2, enc_padding_mask2, context2, extra_zeros2, enc_batch_extend_vocab2, enc_key_batch2, enc_key_lens2)\n",
    "                writer.add_scalars('Compare/rouge-l-f',  \n",
    "                   {'train_rouge_l_f': train_rouge_l_f,\n",
    "                    'test_rouge_l_f': test_rouge_l_f\n",
    "                   }, iter)\n",
    "                logger.info('iter: %s train_rouge_l_f: %.3f test_rouge_l_f: %.3f \\n' % (iter, train_rouge_l_f, test_rouge_l_f))\n",
    "                \n",
    "#         return mle_loss.item(), batch_reward, ans_list, batch_scores\n",
    "        return mle_loss.item(),mle_loss_2.item(), batch_reward\n",
    "\n",
    "    def calc_avg_rouge_result(self, iter, batch, mode, enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens):\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens, self.model, self.start_id, self.end_id, self.unk_id)\n",
    "\n",
    "        decoded_sents = []\n",
    "        ref_sents = []\n",
    "        ref_sents2 = []\n",
    "        article_sents = []\n",
    "        keywords_list = []\n",
    "        \n",
    "        summary_len = max_summary_len = long_seq_index = 0\n",
    "        for i in range(len(pred_ids)):            \n",
    "            decoded_words = data.outputids2words(pred_ids[i], self.vocab, batch.rev_oovs[i])\n",
    "            if len(decoded_words) < 2:\n",
    "                decoded_words = \"xxx\"\n",
    "            else:\n",
    "                decoded_words = \" \".join(decoded_words)\n",
    "            decoded_words = decoded_words.replace(\"[UNK]\",\"\")\n",
    "            decoded_sents.append(decoded_words)\n",
    "            summary = batch.original_summarys[i]\n",
    "            summary = summary.replace(\"<s>\",\"\").replace(\"</s>\",\"\")\n",
    "            review = batch.original_reviews[i]\n",
    "            ref_sents.append(summary)\n",
    "            article_sents.append(review) \n",
    "            keywords = batch.key_words[i]\n",
    "            keywords_list.append(str(keywords))\n",
    "            summary_len = len(summary.split(\" \"))\n",
    "            if max_summary_len < summary_len: \n",
    "                max_summary_len = summary_len\n",
    "                long_seq_index = i\n",
    "\n",
    "        rouge = Rouge()    \n",
    "        score = rouge.get_scores(decoded_sents, ref_sents, avg = True)    \n",
    "        writer.add_scalars('%s/rouge-1' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-1']['f'],\n",
    "                'p': score['rouge-1']['p'],\n",
    "                'r': score['rouge-1']['r']}\n",
    "                , iter)\n",
    "        writer.add_scalars('%s/rouge-2' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-2']['f'],\n",
    "                'p': score['rouge-2']['p'],\n",
    "                'r': score['rouge-2']['r']}\n",
    "                , iter)\n",
    "        writer.add_scalars('%s/rouge-l' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-l']['f'],\n",
    "                'p': score['rouge-l']['p'],\n",
    "                'r': score['rouge-l']['r']}\n",
    "                , iter)\n",
    "        for i in range(len(decoded_sents)):\n",
    "            if type(article_sents[i]) != str: continue\n",
    "            if type(ref_sents[i]) != str:  continue\n",
    "            if type(decoded_sents[i]) != str:  continue\n",
    "\n",
    "        writer.add_text('Rouge/%s/%s' % (iter,mode), decoded_sents[long_seq_index], iter)\n",
    "        writer.add_text('Rouge/%s/%s' % (iter,mode), keywords_list[long_seq_index], iter)\n",
    "        writer.add_text('Rouge/%s/%s' % (iter,mode), ref_sents[long_seq_index], iter)\n",
    "        writer.add_text('Rouge/%s/%s' % (iter,mode), article_sents[long_seq_index], iter)\n",
    "        \n",
    "#         for i in range(len(decoded_sents)):\n",
    "# #             writer.add_text('Rouge/%s/%s' % (iter,mode), decoded_sents[i], iter)\n",
    "#             writer.add_text('Rouge/%s/%s' % (iter,mode), ref_sents[i], iter)\n",
    "#             writer.add_text('Rouge/%s/%s' % (iter,mode), article_sents[i], iter)\n",
    "        \n",
    "        bleu_decode_sents = [decode.split(\" \") for decode in decoded_sents]\n",
    "        bleu_ref_sents = [[ref.split(\" \")] for ref in ref_sents]\n",
    "\n",
    "        writer.add_scalars('%s/Individual' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'BLEU-1': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(1, 0, 0, 0)),\n",
    "                'BLEU-2': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0, 1, 0, 0)),\n",
    "                'BLEU-3': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0, 0, 1, 0)),\n",
    "                'BLEU-4': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0, 0, 0, 1))}\n",
    "                , iter)\n",
    "       \n",
    "        writer.add_scalars('%s/Cumulative' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'BLEU-1': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(1, 0, 0, 0)),\n",
    "                'BLEU-2': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0.5, 0.5, 0, 0)),\n",
    "                'BLEU-3': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0.33, 0.33, 0.33, 0)),\n",
    "                'BLEU-4': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0.25, 0.25, 0.25, 0.25))}\n",
    "                , iter)\n",
    "    \n",
    "        return score['rouge-l']['f']\n",
    "    \n",
    "    def get_best_res_score(self, results, scores):\n",
    "        max_score = float(0)\n",
    "        _id = 0\n",
    "        for idx in range(len(results)):\n",
    "            re_matchData = re.compile(r'\\-?\\d{1,10}\\.?\\d{1,10}')\n",
    "            data = re.findall(re_matchData, str(scores[idx]))\n",
    "            score = sum([float(d) for d in data])\n",
    "            if score > max_score:\n",
    "                _id = idx\n",
    "        return results[_id], scores[_id]\n",
    "\n",
    "    def get_lr(self):\n",
    "        for param_group in self.trainer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    def get_weight_decay(self):\n",
    "        for param_group in self.trainer.param_groups:\n",
    "            #             print(param_group)\n",
    "            return param_group['weight_decay']\n",
    "\n",
    "    def trainIters(self):\n",
    "        final_file_path = None\n",
    "        iter = self.setup_train()\n",
    "        epoch = 0\n",
    "        count = test_mle_total = train_mle_total = r_total = 0\n",
    "        logger.info(u'------Training START--------')\n",
    "        test_batch = self.test_batcher.next_batch()\n",
    "        #         while iter <= config.max_iterations:\n",
    "        while epoch <= config.max_epochs:\n",
    "            train_batch = self.train_batcher.next_batch()\n",
    "            try:\n",
    "                train_mle_loss,test_mle_loss, r  = self.train_one_batch(train_batch,test_batch, iter)\n",
    "\n",
    "                writer.add_scalars('Compare/mle_loss' ,  \n",
    "                   {'train_mle_loss': train_mle_loss,\n",
    "                    'test_mle_loss': test_mle_loss\n",
    "                   }, iter)\n",
    "                \n",
    "#             # break\n",
    "            except KeyboardInterrupt:\n",
    "                logger.info(\"-------------------Keyboard Interrupt------------------\")\n",
    "                exit(0)\n",
    "            except Exception as e:                \n",
    "                logger.info(\"-------------------Ignore error------------------\\n%s\\n\" % e)\n",
    "                print(\"Please load final_file_path : %s\" % final_file_path)\n",
    "                traceback = sys.exc_info()[2]\n",
    "                print(sys.exc_info())\n",
    "                print(traceback.tb_lineno)\n",
    "                print(e)\n",
    "                break\n",
    "            # if opt.train_mle == False: break\n",
    "            train_mle_total += train_mle_loss\n",
    "            r_total += r\n",
    "            test_mle_total += test_mle_loss\n",
    "            count += 1\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 1000 == 0:\n",
    "                train_mle_avg = train_mle_total / count\n",
    "                r_avg = r_total / count\n",
    "                test_mle_avg = test_mle_total / count\n",
    "                epoch = int((iter * config.batch_size) / train_num) + 1\n",
    "#                 logger.info('epoch: %s iter: %s train_mle_loss: %.3f test_mle_loss: %.3f reward: %.3f \\n' % (epoch, iter, train_mle_avg, test_mle_avg, r_avg))\n",
    "\n",
    "                count = test_mle_total = train_mle_total = r_total = 0\n",
    "                writer.add_scalar('RL_Train/r_avg', r_avg, iter)\n",
    "                \n",
    "                writer.add_scalars('Compare/mle_avg_loss' ,  \n",
    "                   {'train_mle_avg': train_mle_avg,\n",
    "                    'test_mle_avg': test_mle_avg\n",
    "                   }, iter)\n",
    "            # break\n",
    "            if iter % 5000 == 0:\n",
    "                final_file_path = self.save_model(iter, test_mle_avg, r_avg)\n",
    "#                 if opt.view:\n",
    "#                     best_res, best_score = self.get_best_res_score(ans_list, batch_scores)\n",
    "#                     logger.info('best_res: %s \\n' % (best_res))\n",
    "#                     logger.info('best_score: %s \\n' % (best_score))\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['decoded_str'], iter)\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['summary'], iter)\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['review'], iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_action(opt):\n",
    "    try:       \n",
    "        opt.rl_weight = 1 - opt.mle_weight  \n",
    "\n",
    "        if opt.load_model:\n",
    "            opt.load_model = \"/%s/%s\"%(opt.word_emb_type,opt.load_model)    \n",
    "\n",
    "        logger.info(u'------Training Setting--------')  \n",
    "   \n",
    "        logger.info(\"Traing Type :%s\" %(config.data_type))\n",
    "        if opt.train_mle == True:\n",
    "            logger.info(\"Training mle: %s, mle weight: %.2f\"%(opt.train_mle, opt.mle_weight))\n",
    "\n",
    "        if opt.train_rl == True:\n",
    "            logger.info(\"Training rl: %s, rl weight: %.2f \\n\"%(opt.train_rl, opt.rl_weight))\n",
    "\n",
    "        if opt.word_emb_type == 'bert': config.emb_dim = 768\n",
    "        if opt.pre_train_emb : \n",
    "            logger.info('use pre_train_%s vocab_size %s \\n'%(opt.word_emb_type,config.vocab_size))\n",
    "\n",
    "        else:\n",
    "            logger.info('use %s vocab_size %s \\n'%(opt.word_emb_type,config.vocab_size))\n",
    "\n",
    "        logger.info(\"intra_encoder: %s intra_decoder: %s \\n\"%(config.intra_encoder, config.intra_decoder))\n",
    "        if opt.word_emb_type in ['word2Vec','glove']:\n",
    "            config.vocab_path = config.Data_path + \"Embedding/%s/word.vocab\"%(opt.word_emb_type)            \n",
    "#             config.vocab_size = len(open(config.vocab_path).readlines())\n",
    "            vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "        train_processor = Train(opt,vocab)\n",
    "        train_processor.trainIters()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        traceback = sys.exc_info()[2]\n",
    "        logger.error(sys.exc_info())\n",
    "        logger.error(traceback.tb_lineno)\n",
    "        logger.error(e)\n",
    "    logger.info(u'------Training END--------')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "def write_enc_graph():\n",
    "    encoder_writer = SummaryWriter('runs/Pointer-Generator/word2Vec/Encoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    encoder = Encoder().to(device) \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "#     enc_batch = Variable(torch.rand(enc_batch.shape)).to(device) \n",
    "    enc_lens = torch.from_numpy(enc_lens).to(device) \n",
    "\n",
    "    encoder_writer.add_graph(encoder, (enc_batch, enc_lens), verbose=True)\n",
    "    encoder_writer.close()\n",
    "\n",
    "def write_dec_graph():\n",
    "    decoder_writer = SummaryWriter('runs/Pointer-Generator/word2Vec/Decoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    # decoder = Decoder().to(device)    \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    model = Model(False,'word2Vec',vocab)\n",
    "    \n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "    # train_batch_MLE\n",
    "    dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "    step_losses = []\n",
    "    s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "    # x_t 為decoder每一個time step 的batch input\n",
    "    x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "    prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    sum_temporal_srcs = None     \n",
    "\n",
    "\n",
    "    for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "        use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "        # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "        # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "        if t == 0 :temp_batch = dec_batch[:, t]\n",
    "        x_t = use_gound_truth * temp_batch + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "        x_t = model.embeds(x_t)\n",
    "    #     final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "        final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "        x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "        extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "        enc_key_batch, enc_key_lens)        \n",
    "\n",
    "\n",
    "        #         decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "#         x_t = Variable(torch.rand(x_t.shape)).to(device) \n",
    "        #             s_t = Variable(torch.rand(s_t.shape)).to(device)\n",
    "#         enc_out = Variable(torch.rand(enc_out.shape)).to(device)\n",
    "#         enc_padding_mask = Variable(torch.rand(enc_padding_mask.shape)).to(device,dtype=torch.long)\n",
    "#         context = Variable(torch.rand(context.shape)).to(device)\n",
    "#         extra_zeros = Variable(torch.rand(extra_zeros.shape)).to(device)\n",
    "#         enc_batch_extend_vocab = Variable(torch.rand(enc_batch_extend_vocab.shape)).to(device)\n",
    "        #             sum_temporal_srcs = Variable(torch.rand(sum_temporal_srcs.shape)).to(device)\n",
    "        #             prev_s = Variable(torch.rand(prev_s.shape)).to(device)\n",
    "#         enc_key_batch = Variable(torch.rand(enc_key_batch.shape)).to(device)\n",
    "        enc_key_lens = torch.from_numpy(enc_key_lens).to(device) \n",
    "        \n",
    "        decoder_writer.add_graph(model.decoder, \n",
    "                         (x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens), verbose=True)\n",
    "        decoder_writer.close()\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-18 09:35:42 - Text-Summary - INFO: - logger已啟動\n",
      "2020-02-18 09:35:42 - Text-Summary - INFO: - ------Training Setting--------\n",
      "2020-02-18 09:35:42 - Text-Summary - INFO: - Traing Type :Cameras\n",
      "2020-02-18 09:35:42 - Text-Summary - INFO: - Training mle: True, mle weight: 1.00\n",
      "2020-02-18 09:35:42 - Text-Summary - INFO: - use pre_train_glove vocab_size 50000 \n",
      "\n",
      "2020-02-18 09:35:42 - Text-Summary - INFO: - intra_encoder: True intra_decoder: True \n",
      "\n",
      "2020-02-18 09:36:02 - Text-Summary - INFO: - Model(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
      "    (reduce_h): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (reduce_c): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (enc_attention): encoder_attention(\n",
      "      (W_h): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (W_s): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (W_t): Linear(in_features=300, out_features=1024, bias=True)\n",
      "      (v): Linear(in_features=1024, out_features=1, bias=False)\n",
      "    )\n",
      "    (dec_attention): decoder_attention(\n",
      "      (W_prev): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (W_s): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (W_t): Linear(in_features=300, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (x_context): Linear(in_features=1324, out_features=300, bias=True)\n",
      "    (x_key_context): Linear(in_features=1624, out_features=300, bias=True)\n",
      "    (lstm): LSTMCell(300, 512)\n",
      "    (p_gen_linear): Linear(in_features=2860, out_features=1, bias=True)\n",
      "    (V): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (V1): Linear(in_features=512, out_features=50000, bias=True)\n",
      "  )\n",
      "  (embeds): Embedding(50000, 300)\n",
      ")\n",
      "2020-02-18 09:36:02 - Text-Summary - INFO: - ------Training START--------\n",
      "2020-02-18 09:36:11 - Text-Summary - INFO: - iter: 0 train_rouge_l_f: 0.000 test_rouge_l_f: 0.012 \n",
      "\n",
      "2020-02-18 09:43:15 - Text-Summary - INFO: - iter: 1000 train_rouge_l_f: 0.202 test_rouge_l_f: 0.057 \n",
      "\n",
      "2020-02-18 09:50:16 - Text-Summary - INFO: - iter: 2000 train_rouge_l_f: 0.170 test_rouge_l_f: 0.114 \n",
      "\n",
      "2020-02-18 09:57:05 - Text-Summary - INFO: - iter: 3000 train_rouge_l_f: 0.127 test_rouge_l_f: 0.175 \n",
      "\n",
      "2020-02-18 10:03:59 - Text-Summary - INFO: - iter: 4000 train_rouge_l_f: 0.054 test_rouge_l_f: 0.195 \n",
      "\n",
      "2020-02-18 10:11:12 - Text-Summary - INFO: - iter: 5000 train_rouge_l_f: 0.308 test_rouge_l_f: 0.243 \n",
      "\n",
      "2020-02-18 10:18:08 - Text-Summary - INFO: - iter: 6000 train_rouge_l_f: 0.137 test_rouge_l_f: 0.154 \n",
      "\n",
      "2020-02-18 10:25:01 - Text-Summary - INFO: - iter: 7000 train_rouge_l_f: 0.159 test_rouge_l_f: 0.159 \n",
      "\n",
      "2020-02-18 10:31:55 - Text-Summary - INFO: - iter: 8000 train_rouge_l_f: 0.336 test_rouge_l_f: 0.395 \n",
      "\n",
      "2020-02-18 10:38:56 - Text-Summary - INFO: - iter: 9000 train_rouge_l_f: 0.140 test_rouge_l_f: 0.499 \n",
      "\n",
      "2020-02-18 10:46:06 - Text-Summary - INFO: - iter: 10000 train_rouge_l_f: 0.221 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 10:53:09 - Text-Summary - INFO: - iter: 11000 train_rouge_l_f: 0.115 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 11:00:14 - Text-Summary - INFO: - iter: 12000 train_rouge_l_f: 0.126 test_rouge_l_f: 0.421 \n",
      "\n",
      "2020-02-18 11:07:11 - Text-Summary - INFO: - iter: 13000 train_rouge_l_f: 0.275 test_rouge_l_f: 0.589 \n",
      "\n",
      "2020-02-18 11:14:12 - Text-Summary - INFO: - iter: 14000 train_rouge_l_f: 0.379 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 11:21:13 - Text-Summary - INFO: - iter: 15000 train_rouge_l_f: 0.179 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 11:28:21 - Text-Summary - INFO: - iter: 16000 train_rouge_l_f: 0.427 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 11:35:21 - Text-Summary - INFO: - iter: 17000 train_rouge_l_f: 0.587 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 11:42:17 - Text-Summary - INFO: - iter: 18000 train_rouge_l_f: 0.211 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 11:49:17 - Text-Summary - INFO: - iter: 19000 train_rouge_l_f: 0.176 test_rouge_l_f: 0.503 \n",
      "\n",
      "2020-02-18 11:56:20 - Text-Summary - INFO: - iter: 20000 train_rouge_l_f: 0.212 test_rouge_l_f: 0.328 \n",
      "\n",
      "2020-02-18 12:03:10 - Text-Summary - INFO: - iter: 21000 train_rouge_l_f: 0.427 test_rouge_l_f: 0.585 \n",
      "\n",
      "2020-02-18 12:10:12 - Text-Summary - INFO: - iter: 22000 train_rouge_l_f: 0.435 test_rouge_l_f: 0.465 \n",
      "\n",
      "2020-02-18 12:17:13 - Text-Summary - INFO: - iter: 23000 train_rouge_l_f: 0.233 test_rouge_l_f: 0.598 \n",
      "\n",
      "2020-02-18 12:24:04 - Text-Summary - INFO: - iter: 24000 train_rouge_l_f: 0.185 test_rouge_l_f: 0.511 \n",
      "\n",
      "2020-02-18 12:31:00 - Text-Summary - INFO: - iter: 25000 train_rouge_l_f: 0.212 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 12:38:11 - Text-Summary - INFO: - iter: 26000 train_rouge_l_f: 0.197 test_rouge_l_f: 0.519 \n",
      "\n",
      "2020-02-18 12:45:07 - Text-Summary - INFO: - iter: 27000 train_rouge_l_f: 0.096 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 12:52:17 - Text-Summary - INFO: - iter: 28000 train_rouge_l_f: 0.550 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 12:59:18 - Text-Summary - INFO: - iter: 29000 train_rouge_l_f: 0.465 test_rouge_l_f: 0.394 \n",
      "\n",
      "2020-02-18 13:06:14 - Text-Summary - INFO: - iter: 30000 train_rouge_l_f: 0.301 test_rouge_l_f: 0.399 \n",
      "\n",
      "2020-02-18 13:13:21 - Text-Summary - INFO: - iter: 31000 train_rouge_l_f: 0.627 test_rouge_l_f: 0.630 \n",
      "\n",
      "2020-02-18 13:20:20 - Text-Summary - INFO: - iter: 32000 train_rouge_l_f: 0.294 test_rouge_l_f: 0.572 \n",
      "\n",
      "2020-02-18 13:27:24 - Text-Summary - INFO: - iter: 33000 train_rouge_l_f: 0.536 test_rouge_l_f: 0.630 \n",
      "\n",
      "2020-02-18 13:34:25 - Text-Summary - INFO: - iter: 34000 train_rouge_l_f: 0.279 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 13:41:26 - Text-Summary - INFO: - iter: 35000 train_rouge_l_f: 0.301 test_rouge_l_f: 0.396 \n",
      "\n",
      "2020-02-18 13:48:13 - Text-Summary - INFO: - iter: 36000 train_rouge_l_f: 0.099 test_rouge_l_f: 0.630 \n",
      "\n",
      "2020-02-18 13:55:18 - Text-Summary - INFO: - iter: 37000 train_rouge_l_f: 0.239 test_rouge_l_f: 0.630 \n",
      "\n",
      "2020-02-18 14:02:27 - Text-Summary - INFO: - iter: 38000 train_rouge_l_f: 0.156 test_rouge_l_f: 0.755 \n",
      "\n",
      "2020-02-18 14:09:24 - Text-Summary - INFO: - iter: 39000 train_rouge_l_f: 0.340 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 14:16:24 - Text-Summary - INFO: - iter: 40000 train_rouge_l_f: 0.508 test_rouge_l_f: 0.591 \n",
      "\n",
      "2020-02-18 14:23:22 - Text-Summary - INFO: - iter: 41000 train_rouge_l_f: 0.404 test_rouge_l_f: 0.640 \n",
      "\n",
      "2020-02-18 14:30:31 - Text-Summary - INFO: - iter: 42000 train_rouge_l_f: 0.250 test_rouge_l_f: 0.460 \n",
      "\n",
      "2020-02-18 14:37:36 - Text-Summary - INFO: - iter: 43000 train_rouge_l_f: 0.454 test_rouge_l_f: 0.500 \n",
      "\n",
      "2020-02-18 14:44:23 - Text-Summary - INFO: - iter: 44000 train_rouge_l_f: 0.434 test_rouge_l_f: 0.755 \n",
      "\n",
      "2020-02-18 14:51:23 - Text-Summary - INFO: - iter: 45000 train_rouge_l_f: 0.351 test_rouge_l_f: 0.521 \n",
      "\n",
      "2020-02-18 14:58:22 - Text-Summary - INFO: - iter: 46000 train_rouge_l_f: 0.394 test_rouge_l_f: 0.633 \n",
      "\n",
      "2020-02-18 15:05:28 - Text-Summary - INFO: - iter: 47000 train_rouge_l_f: 0.728 test_rouge_l_f: 0.633 \n",
      "\n",
      "2020-02-18 15:12:28 - Text-Summary - INFO: - iter: 48000 train_rouge_l_f: 0.280 test_rouge_l_f: 0.633 \n",
      "\n",
      "2020-02-18 15:19:28 - Text-Summary - INFO: - iter: 49000 train_rouge_l_f: 0.075 test_rouge_l_f: 0.649 \n",
      "\n",
      "2020-02-18 15:26:24 - Text-Summary - INFO: - iter: 50000 train_rouge_l_f: 0.334 test_rouge_l_f: 0.717 \n",
      "\n",
      "2020-02-18 15:33:35 - Text-Summary - INFO: - iter: 51000 train_rouge_l_f: 0.433 test_rouge_l_f: 0.546 \n",
      "\n",
      "2020-02-18 15:40:49 - Text-Summary - INFO: - iter: 52000 train_rouge_l_f: 0.783 test_rouge_l_f: 0.754 \n",
      "\n",
      "2020-02-18 15:47:49 - Text-Summary - INFO: - iter: 53000 train_rouge_l_f: 0.506 test_rouge_l_f: 0.772 \n",
      "\n",
      "2020-02-18 15:54:48 - Text-Summary - INFO: - iter: 54000 train_rouge_l_f: 0.441 test_rouge_l_f: 0.685 \n",
      "\n",
      "2020-02-18 16:01:38 - Text-Summary - INFO: - iter: 55000 train_rouge_l_f: 0.295 test_rouge_l_f: 0.609 \n",
      "\n",
      "2020-02-18 16:08:34 - Text-Summary - INFO: - iter: 56000 train_rouge_l_f: 0.408 test_rouge_l_f: 0.783 \n",
      "\n",
      "2020-02-18 16:15:44 - Text-Summary - INFO: - iter: 57000 train_rouge_l_f: 0.206 test_rouge_l_f: 0.806 \n",
      "\n",
      "2020-02-18 16:22:45 - Text-Summary - INFO: - iter: 58000 train_rouge_l_f: 0.388 test_rouge_l_f: 0.633 \n",
      "\n",
      "2020-02-18 16:29:28 - Text-Summary - INFO: - iter: 59000 train_rouge_l_f: 0.297 test_rouge_l_f: 0.553 \n",
      "\n",
      "2020-02-18 16:36:37 - Text-Summary - INFO: - iter: 60000 train_rouge_l_f: 0.419 test_rouge_l_f: 0.654 \n",
      "\n",
      "2020-02-18 16:43:44 - Text-Summary - INFO: - iter: 61000 train_rouge_l_f: 0.243 test_rouge_l_f: 0.739 \n",
      "\n",
      "2020-02-18 16:50:50 - Text-Summary - INFO: - iter: 62000 train_rouge_l_f: 0.541 test_rouge_l_f: 0.427 \n",
      "\n",
      "2020-02-18 16:57:54 - Text-Summary - INFO: - iter: 63000 train_rouge_l_f: 0.150 test_rouge_l_f: 0.608 \n",
      "\n",
      "2020-02-18 17:04:48 - Text-Summary - INFO: - iter: 64000 train_rouge_l_f: 0.225 test_rouge_l_f: 0.783 \n",
      "\n",
      "2020-02-18 17:12:02 - Text-Summary - INFO: - iter: 65000 train_rouge_l_f: 0.643 test_rouge_l_f: 0.602 \n",
      "\n",
      "2020-02-18 17:18:51 - Text-Summary - INFO: - iter: 66000 train_rouge_l_f: 0.228 test_rouge_l_f: 0.546 \n",
      "\n",
      "2020-02-18 17:25:51 - Text-Summary - INFO: - iter: 67000 train_rouge_l_f: 0.190 test_rouge_l_f: 0.490 \n",
      "\n",
      "2020-02-18 17:32:53 - Text-Summary - INFO: - iter: 68000 train_rouge_l_f: 0.310 test_rouge_l_f: 0.506 \n",
      "\n",
      "2020-02-18 17:40:03 - Text-Summary - INFO: - iter: 69000 train_rouge_l_f: 0.387 test_rouge_l_f: 0.783 \n",
      "\n",
      "2020-02-18 17:47:02 - Text-Summary - INFO: - iter: 70000 train_rouge_l_f: 0.317 test_rouge_l_f: 0.609 \n",
      "\n",
      "2020-02-18 17:54:04 - Text-Summary - INFO: - iter: 71000 train_rouge_l_f: 0.526 test_rouge_l_f: 0.619 \n",
      "\n",
      "2020-02-18 18:01:03 - Text-Summary - INFO: - iter: 72000 train_rouge_l_f: 0.157 test_rouge_l_f: 0.686 \n",
      "\n",
      "2020-02-18 18:08:04 - Text-Summary - INFO: - iter: 73000 train_rouge_l_f: 0.575 test_rouge_l_f: 0.556 \n",
      "\n",
      "2020-02-18 18:15:04 - Text-Summary - INFO: - iter: 74000 train_rouge_l_f: 0.378 test_rouge_l_f: 0.522 \n",
      "\n",
      "2020-02-18 18:21:59 - Text-Summary - INFO: - iter: 75000 train_rouge_l_f: 0.450 test_rouge_l_f: 0.353 \n",
      "\n",
      "2020-02-18 18:29:04 - Text-Summary - INFO: - iter: 76000 train_rouge_l_f: 0.240 test_rouge_l_f: 0.573 \n",
      "\n",
      "2020-02-18 18:36:10 - Text-Summary - INFO: - iter: 77000 train_rouge_l_f: 0.582 test_rouge_l_f: 0.451 \n",
      "\n",
      "2020-02-18 18:43:03 - Text-Summary - INFO: - iter: 78000 train_rouge_l_f: 0.723 test_rouge_l_f: 0.639 \n",
      "\n",
      "2020-02-18 18:50:14 - Text-Summary - INFO: - iter: 79000 train_rouge_l_f: 0.336 test_rouge_l_f: 0.735 \n",
      "\n",
      "2020-02-18 18:57:16 - Text-Summary - INFO: - iter: 80000 train_rouge_l_f: 0.396 test_rouge_l_f: 0.665 \n",
      "\n",
      "2020-02-18 19:02:14 - Text-Summary - ERROR: - xxxxxxxxxxx\n",
      "2020-02-18 19:02:14 - Text-Summary - ERROR: - (<class 'RuntimeError'>, RuntimeError('CUDA error: device-side assert triggered',), <traceback object at 0x7fd15a591c08>)\n",
      "2020-02-18 19:02:14 - Text-Summary - ERROR: - 105\n",
      "2020-02-18 19:02:14 - Text-Summary - ERROR: - CUDA error: device-side assert triggered\n",
      "2020-02-18 19:02:14 - Text-Summary - ERROR: - xxxxxxxxxxx\n",
      "2020-02-18 19:02:14 - Text-Summary - INFO: - -------------------Ignore error------------------\n",
      "CUDA error: device-side assert triggered\n",
      "\n",
      "2020-02-18 19:02:14 - Text-Summary - INFO: - ------Training END--------\n",
      "2020-02-18 19:02:14 - Text-Summary - INFO: - logger已關閉\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please load final_file_path : /0080000_1.15_0.00.tar\n",
      "(<class 'RuntimeError'>, RuntimeError('CUDA error: device-side assert triggered',), <traceback object at 0x7fd15a564688>)\n",
      "431\n",
      "CUDA error: device-side assert triggered\n"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/u012869752/article/details/72513141\n",
    "# 由于在jupyter notebook中，args不为空\n",
    "from glob import glob\n",
    "# nvidia-smi -pm 1\n",
    "if __name__ == \"__main__\":   \n",
    "    try:\n",
    "        # --------------------------Training ----------------------------------\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--train_mle', type=bool, default=True)\n",
    "        parser.add_argument('--train_rl', type=bool, default=False)\n",
    "        parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "#         parser.add_argument('--load_model', type=str, default='/0044000_1.91_0.00.tar')\n",
    "        parser.add_argument('--load_model', type=str, default=None)\n",
    "        parser.add_argument('--new_lr', type=float, default=None)\n",
    "        parser.add_argument('--multi_device', type=bool, default=True)\n",
    "        parser.add_argument('--view', type=bool, default=True)\n",
    "        parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "        parser.add_argument('--word_emb_type', type=str, default=word_emb_type)\n",
    "        parser.add_argument('--train_action', type=bool, default=True)\n",
    "        opt = parser.parse_args(args=[])\n",
    "        \n",
    "        today = dt.now()\n",
    "        loggerPath = \"LOG/%s-(%s_%s_%s)-(%s:%s:%s)\"%(opt.word_emb_type,\n",
    "                  today.year,today.month,today.day,\n",
    "                  today.hour,today.minute,today.second)\n",
    "\n",
    "        logger = getLogger(config.loggerName,loggerPath)   \n",
    "        \n",
    "        if opt.load_model == None:\n",
    "            shutil.rmtree('runs/Pointer-Generator/%s_%s'%(word_emb_type,config.keywords), ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/%s_%s/exp-4'%(word_emb_type,config.keywords), ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/%s_%s/Eecoder'%(word_emb_type,config.keywords), ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/%s_%s/Decoder'%(word_emb_type,config.keywords), ignore_errors=True) # clear previous \n",
    "\n",
    "        writer = SummaryWriter('runs/Pointer-Generator/%s_%s/exp-4'%(word_emb_type,config.keywords))\n",
    "#         write_enc_graph()\n",
    "#         write_dec_graph()\n",
    "        if opt.train_action: train_action(opt)\n",
    "        # --------------------------Testing ----------------------------------\n",
    "#         parser = argparse.ArgumentParser()\n",
    "#         parser.add_argument(\"--task\", type=str, default=\"validate\", choices=[\"validate\",\"test\"])\n",
    "#         parser.add_argument(\"--start_from\", type=int, default=\"0020000\")\n",
    "# #         parser.add_argument(\"--load_model\", type=str, default=None)\n",
    "#         parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "#         parser.add_argument('--word_emb_type', type=str, default='bert')\n",
    "#         opt = parser.parse_args(args=[])                \n",
    "#         if opt.word_emb_type == 'bert': config.emb_dim = 768\n",
    "#         test_action(opt)\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback = sys.exc_info()[2]\n",
    "        print(sys.exc_info())\n",
    "        print(traceback.tb_lineno)\n",
    "        print(e)\n",
    "    finally:\n",
    "        removeLogger(logger)\n",
    "        \n",
    "        # export scalar data to JSON for external processing\n",
    "        # tensorboard --logdir /home/eagleuser/Users/leyan/Text-Summarizer-FOP/TensorBoard\n",
    "#         tensorboard --logdir ./runs\n",
    "#         if not os.path.exists('TensorBoard'): os.makedirs('TensorBoard')\n",
    "#         writer.export_scalars_to_json(\"TensorBoard/test.json\")\n",
    "        writer.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
