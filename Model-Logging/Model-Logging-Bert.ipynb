{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0226 13:16:25.251488 140516709062464 file_utils.py:35] PyTorch version 1.3.1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 2 tokens\n",
      "We have added 3 tokens\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from model import Model\n",
    "\n",
    "\n",
    "from data_util import config\n",
    "from data_util import bert_data as data\n",
    "from data_util.bert_batcher import Batcher\n",
    "from data_util.bert_data import Vocab\n",
    "\n",
    "# from data_util import data\n",
    "# from data_util.batcher import Batcher\n",
    "# from data_util.data import Vocab\n",
    "\n",
    "\n",
    "from train_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse\n",
    "import torchsnooper\n",
    "import logging\n",
    "transformers_logger = logging.getLogger(\"transformers.tokenization_utils\")\n",
    "transformers_logger.setLevel(logging.ERROR)\n",
    "transformers_logger.disabled = True\n",
    "\n",
    "# -------- Test Packages -------\n",
    "from bert_beam_search import *\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# from pytorch_pretrained_bert import BertModel\n",
    "from transformers import BertModel, BertTokenizer \n",
    "from transformers import TransfoXLTokenizer, TransfoXLModel, TransfoXLConfig\n",
    "\n",
    "config.batch_size = 4\n",
    "config.emb_dim = 768\n",
    "config.max_enc_steps = 512\n",
    "config.lr = 0.0001 # 0.001\n",
    "\n",
    "# config.keywords = \"TextRank_keywords\"\n",
    "# config.max_key_num = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def getLogger(loggerName, loggerPath):\n",
    "    # 設置logger\n",
    "    logger = logging.getLogger(loggerName)  # 不加名稱設置root logger\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s: - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    logging.Filter(loggerName)\n",
    "\n",
    "    # 使用FileHandler輸出到文件\n",
    "    directory = os.path.dirname(loggerPath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fh = logging.FileHandler(loggerPath)\n",
    "\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # 使用StreamHandler輸出到屏幕\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    ch.setFormatter(formatter)\n",
    "    # 添加兩個Handler\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    # Handler只啟動一次\n",
    "    # 設置logger\n",
    "    logger.info(u'logger已啟動')\n",
    "    return logger\n",
    "\n",
    "def removeLogger(logger):\n",
    "    logger.info(u'logger已關閉')\n",
    "    handlers = logger.handlers[:]\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch():\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    # with torchsnooper.snoop():\n",
    "    while batch is not None:\n",
    "        example_list = batch.example_list\n",
    "        for ex in example_list:\n",
    "            r = str(ex.original_review)\n",
    "            s = str(ex.original_summary)\n",
    "            k = str(ex.key_words)\n",
    "            sent = ex.original_summary_sents\n",
    "#             print(\"original_review_sents:\", r)\n",
    "#             print(\"original_summary_sents : \", s)\n",
    "#             print(\"key_words : \", k)\n",
    "#             print('------------------------------------------------------------\\n')\n",
    "        batch = batcher.next_batch()        \n",
    "        break\n",
    "test_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bin Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 29540\n",
      "\n",
      "test : 5847\n",
      "\n",
      "valid : 4243\n",
      "\n"
     ]
    }
   ],
   "source": [
    " with open(config.bin_info,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    [print(line) for line in lines]\n",
    "    train_num = int(lines[0].split(\":\")[1])\n",
    "    test_num = int(lines[1].split(\":\")[1])\n",
    "    val_num = int(lines[2].split(\":\")[1])\n",
    "    # f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    # f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    # f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View model summary\n",
    "#### 只有torchsummaryX成功\n",
    "#### 日後將以此模擬呈現結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary # 不支援RNN\n",
    "# from model import Encoder,Model\n",
    "# # https://www.cnblogs.com/lindaxin/p/8052043.html\n",
    "# device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# encoder = Encoder().to(device)    \n",
    "\n",
    "# vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "# batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "#                        batch_size=config.batch_size, single_pass=False)\n",
    "# batch = batcher.next_batch()\n",
    "# enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# enc_batch = Model().embeds(enc_batch) # Get embeddings for encoder input\n",
    "\n",
    "# # summary(encoder, enc_batch, enc_lens, show_hierarchical=True) \n",
    "# # summary(encoder, [enc_batch, enc_lens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modelsummary import summary # 未知問題\n",
    "# from model import Encoder,Model\n",
    "# # https://www.cnblogs.com/lindaxin/p/8052043.html\n",
    "# device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# encoder = Encoder().to(device)    \n",
    "\n",
    "# vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "# batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "#                        batch_size=config.batch_size, single_pass=False)\n",
    "# batch = batcher.next_batch()\n",
    "# enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# enc_batch = Model().embeds(enc_batch) # Get embeddings for encoder input\n",
    "\n",
    "# # summary(encoder, enc_batch, enc_lens, show_hierarchical=True) \n",
    "# # summary(encoder, enc_batch, enc_lens, show_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0226 13:16:34.396359 140516709062464 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/eagleuser/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0226 13:16:34.397804 140516709062464 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0226 13:16:35.671444 140516709062464 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/eagleuser/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 214, 768])\n",
      "========================================================\n",
      "           Kernel Shape Output Shape   Params  Mult-Adds\n",
      "Layer                                                   \n",
      "0_lstm                -  [515, 1024]  5251072    5242880\n",
      "1_reduce_h  [1024, 512]     [4, 512]   524800     524288\n",
      "2_reduce_c  [1024, 512]     [4, 512]   524800     524288\n",
      "--------------------------------------------------------\n",
      "                       Totals\n",
      "Total params          6300672\n",
      "Trainable params      6300672\n",
      "Non-trainable params        0\n",
      "Mult-Adds             6291456\n",
      "========================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[515, 1024]</td>\n",
       "      <td>5251072</td>\n",
       "      <td>5242880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_reduce_h</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[4, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_reduce_c</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[4, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kernel Shape Output Shape   Params  Mult-Adds\n",
       "Layer                                                   \n",
       "0_lstm                -  [515, 1024]  5251072    5242880\n",
       "1_reduce_h  [1024, 512]     [4, 512]   524800     524288\n",
       "2_reduce_c  [1024, 512]     [4, 512]   524800     524288"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Encoder,Model\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "encoder = Encoder().to(device)    \n",
    "\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "\n",
    "enc_batch = enc_batch.type(T.LongTensor).cuda() #  `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "     \n",
    "bert_model = get_cuda(BertModel.from_pretrained('bert-base-uncased'))\n",
    "# xl_config = TransfoXLConfig()\n",
    "# xl_config.d_embed = 1024\n",
    "# bert_model = TransfoXLModel(xl_config) # 更改參數以傳入TransfoXLModel\n",
    "\n",
    "# bert_model = get_cuda(TransfoXLModel.from_pretrained('transfo-xl-wt103'))\n",
    "all_hidden_states, _ = bert_model(enc_batch)[-2:]\n",
    "print(all_hidden_states.shape)\n",
    "\n",
    "# enc_batch = self.bert_model(enc_batch)[0][0] ; print(enc_batch.shape)\n",
    "        \n",
    "summary(encoder, all_hidden_states, enc_lens) # encoder summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummaryX import summary\n",
    "class Train(object):\n",
    "    def __init__(self, opt, vocab):\n",
    "#         self.vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "        self.vocab = vocab\n",
    "        self.train_batcher = Batcher(config.train_data_path, self.vocab, mode='train',\n",
    "                               batch_size=config.batch_size, single_pass=False)\n",
    "        self.test_batcher = Batcher(config.test_data_path, self.vocab, mode='eval',\n",
    "                               batch_size=config.batch_size, single_pass=True)\n",
    "        self.opt = opt\n",
    "        self.start_id = self.vocab.word2id(data.START_DECODING)\n",
    "        self.end_id = self.vocab.word2id(data.STOP_DECODING)\n",
    "        self.pad_id = self.vocab.word2id(data.PAD_TOKEN)\n",
    "        self.unk_id = self.vocab.word2id(data.UNKNOWN_TOKEN)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def save_model(self, iter, loss, r_loss):\n",
    "        if not os.path.exists(config.save_model_path):\n",
    "            os.makedirs(config.save_model_path)\n",
    "        file_path = \"/%07d_%.2f_%.2f.tar\" % (iter, loss, r_loss)\n",
    "        save_path = config.save_model_path + '/%s' % (self.opt.word_emb_type)\n",
    "        if not os.path.isdir(save_path): os.mkdir(save_path)\n",
    "        save_path = save_path + file_path\n",
    "        T.save({\n",
    "            \"iter\": iter + 1,\n",
    "            \"model_dict\": self.model.state_dict(),\n",
    "            \"trainer_dict\": self.trainer.state_dict()\n",
    "        }, save_path)\n",
    "        return file_path\n",
    "\n",
    "    def setup_train(self):\n",
    "        # BERT\n",
    "        self.bert_model = get_cuda(BertModel.from_pretrained('bert-base-uncased'))\n",
    "#         self.bert_model = get_cuda(TransfoXLModel.from_pretrained('transfo-xl-wt103'))\n",
    "#         config = TransfoXLConfig()\n",
    "#         config.d_embed = 1025\n",
    "#         self.bert_model = get_cuda(TransfoXLModel(config)) # 更改參數以傳入TransfoXLModel\n",
    "        self.bert_model.resize_token_embeddings(len(bert_data.bert_tokenizer))\n",
    "        \n",
    "        self.model = Model(opt.pre_train_emb, opt.word_emb_type, self.vocab)\n",
    "        #         print(\"Model : \",self.model)\n",
    "        #         logger.info(\"Model : \")\n",
    "        logger.info(str(self.model))\n",
    "        #         print(\"Encoder : \",self.model.encoder)\n",
    "        #         print(\"Decoder : \",self.model.decoder)\n",
    "        #         print(\"Embeds : \",self.model.embeds)\n",
    "        self.model = get_cuda(self.model)\n",
    "        device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")  # PyTorch v0.4.0\n",
    "        if opt.multi_device:\n",
    "            if T.cuda.device_count() > 1:\n",
    "                #                 print(\"Let's use\", T.cuda.device_count(), \"GPUs!\")\n",
    "                logger.info(\"Let's use \" + str(T.cuda.device_count()) + \" GPUs!\")\n",
    "                self.model = nn.DataParallel(self.model, list(range(T.cuda.device_count()))).cuda()\n",
    "\n",
    "        if isinstance(self.model, nn.DataParallel):\n",
    "            self.model = self.model.module\n",
    "        self.model.to(device)\n",
    "        #         self.model.eval()\n",
    "\n",
    "        self.trainer = T.optim.Adam(self.model.parameters(), lr=config.lr)\n",
    "        start_iter = 0\n",
    "        if self.opt.load_model is not None:\n",
    "#             load_model_path = os.path.join(config.save_model_path, self.opt.load_model)\n",
    "            load_model_path = config.save_model_path + self.opt.load_model\n",
    "            print(load_model_path)\n",
    "#             print('xxxx')\n",
    "            checkpoint = T.load(load_model_path)\n",
    "            start_iter = checkpoint[\"iter\"]\n",
    "            self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "            self.trainer.load_state_dict(checkpoint[\"trainer_dict\"])\n",
    "            #             print(\"Loaded model at \" + load_model_path)\n",
    "            logger.info(\"Loaded model at \" + load_model_path)\n",
    "        if self.opt.new_lr is not None:\n",
    "            self.trainer = T.optim.Adam(self.model.parameters(), lr=self.opt.new_lr)\n",
    "        return start_iter\n",
    "\n",
    "    def train_batch_MLE(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(\n",
    "            batch)  # Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        try:\n",
    "#             print('-----------------')\n",
    "            for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "                use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()  # Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "                x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t  # Select decoder input based on use_ground_truth probabilities\n",
    "                x_t = self.model.embeds(x_t) \n",
    "#                 x_t = x_t.unsqueeze(1).contiguous()\n",
    "#                 x_t = self.bert_model(x_t)[-2:][0]\n",
    "#                 x_t = x_t.squeeze(-2)\n",
    "\n",
    "                final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                          ct_e, extra_zeros,\n",
    "                                                                                          enc_batch_extend_vocab,\n",
    "                                                                                          sum_temporal_srcs, prev_s, enc_key_batch, enc_key_lens)\n",
    "                target = target_batch[:, t]\n",
    "                log_probs = T.log(final_dist + config.eps)\n",
    "                step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=self.pad_id)\n",
    "                step_losses.append(step_loss)\n",
    "                x_t = T.multinomial(final_dist,1).squeeze()  # Sample words from final distribution which can be used as input in next time step\n",
    "#                 print(config.vocab_size)\n",
    "#                 print(x_t)\n",
    "                is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "                x_t = (1 - is_oov) * x_t.detach() + (is_oov) * self.unk_id  # Replace OOVs with [UNK] token\n",
    "        except Exception as e:\n",
    "            logger.error('xxxxxxxxxxx')\n",
    "            traceback = sys.exc_info()[2]\n",
    "            logger.error(sys.exc_info())\n",
    "            logger.error(traceback.tb_lineno)\n",
    "            logger.error(e)\n",
    "            logger.error('xxxxxxxxxxx')\n",
    "\n",
    "                \n",
    "        losses = T.sum(T.stack(step_losses, 1), 1)  # unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens  # Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)  # Average batch loss\n",
    "        return mle_loss\n",
    "\n",
    "    def train_batch_RL(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab,\n",
    "                       review_oovs, greedy):\n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param review_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        s_t = enc_hidden  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []  # Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []  # Stores padding masks of generated samples\n",
    "        log_probs = []  # Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(\n",
    "            1))  # Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "\n",
    "        for t in range(config.max_dec_steps):\n",
    "#             x_t = self.model.embeds(x_t)\n",
    "            x_t = self.bert_model(x_t)[-2:][0]\n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e,\n",
    "                                                                             extra_zeros, enc_batch_extend_vocab,\n",
    "                                                                             sum_temporal_srcs, prev_s)\n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs)\n",
    "                x_t = multi_dist.sample()  # perform multinomial sampling\n",
    "                log_prob = multi_dist.log_prob(x_t)\n",
    "                log_probs.append(log_prob)\n",
    "            else:\n",
    "                _, x_t = T.max(probs, dim=1)  # perform greedy sampling\n",
    "            x_t = x_t.detach()\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))  # Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1  # If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (\n",
    "            x_t == self.end_id) == 2] = 0  # If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t + (is_oov) * self.unk_id  # Replace OOVs with [UNK] token\n",
    "\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:  # If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1)\n",
    "            log_probs = log_probs * decoder_padding_mask  # Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1)  # Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs,\n",
    "                              dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy()\n",
    "            oovs = review_oovs[i]\n",
    "            S = data.outputids2words(id_list, self.vocab, oovs)  # Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:  # If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "\n",
    "        return decoded_strs, log_probs \n",
    "\n",
    "    def reward_function(self, decoded_sents, original_sents):\n",
    "        rouge = Rouge()\n",
    "        try:\n",
    "            scores = rouge.get_scores(decoded_sents, original_sents)\n",
    "        except Exception:\n",
    "            #             print(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
    "            logger.info(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
    "            scores = []\n",
    "            for i in range(len(decoded_sents)):\n",
    "                try:\n",
    "                    score = rouge.get_scores(decoded_sents[i], original_sents[i])\n",
    "                except Exception:\n",
    "                    #                     print(\"Error occured at:\")\n",
    "                    #                     print(\"decoded_sents:\", decoded_sents[i])\n",
    "                    #                     print(\"original_sents:\", original_sents[i])\n",
    "                    logger.info(\"Error occured at:\")\n",
    "                    logger.info(\"decoded_sents:\", decoded_sents[i])\n",
    "                    logger.info(\"original_sents:\", original_sents[i])\n",
    "                    score = [{\"rouge-l\": {\"f\": 0.0}}]\n",
    "                scores.append(score[0])\n",
    "        rouge_l_f1 = [score[\"rouge-l\"][\"f\"] for score in scores]\n",
    "        avg_rouge_l_f1 = sum(rouge_l_f1) / len(rouge_l_f1)\n",
    "        rouge_l_f1 = get_cuda(T.FloatTensor(rouge_l_f1))\n",
    "        return rouge_l_f1, scores, avg_rouge_l_f1\n",
    "\n",
    "    # def write_to_file(self, decoded, max, original, sample_r, baseline_r, iter):\n",
    "    #     with open(\"temp.txt\", \"w\") as f:\n",
    "    #         f.write(\"iter:\"+str(iter)+\"\\n\")\n",
    "    #         for i in range(len(original)):\n",
    "    #             f.write(\"dec: \"+decoded[i]+\"\\n\")\n",
    "    #             f.write(\"max: \"+max[i]+\"\\n\")\n",
    "    #             f.write(\"org: \"+original[i]+\"\\n\")\n",
    "    #             f.write(\"Sample_R: %.4f, Baseline_R: %.4f\\n\\n\"%(sample_r[i].item(), baseline_r[i].item()))\n",
    "\n",
    "\n",
    "    def train_one_batch(self, batch,test_batch, iter):\n",
    "        ans_list, batch_scores = None, None\n",
    "        # Train\n",
    "        enc_batch, enc_lens, enc_padding_mask, \\\n",
    "        enc_key_batch, enc_key_lens, enc_key_padding_mask,\\\n",
    "        enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)     \n",
    "        \n",
    "        enc_batch = enc_batch.type(T.LongTensor).cuda() #  `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "        enc_key_batch = enc_key_batch.type(T.LongTensor).cuda() #  `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "        enc_padding_mask = enc_padding_mask.type(T.LongTensor).cuda() #  `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "        enc_key_padding_mask = enc_key_padding_mask.type(T.LongTensor).cuda() #  `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "        \n",
    "        \n",
    "        # enc_padding_mask  `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length]\n",
    "        # `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length]\n",
    "#         enc_batch = self.bert_model(enc_batch, attention_mask = enc_padding_mask)[0][0]\n",
    "#         enc_key_batch = self.bert_model(enc_key_batch, attention_mask = enc_key_padding_mask)[0][0]  \n",
    "            \n",
    "#         all_hidden_states = self.bert_model(enc_batch)[-2:][0]\n",
    "#         print(all_hidden_states.shape)\n",
    "#         print('enc_batch',enc_batch.shape)\n",
    "        enc_batch = self.bert_model(enc_batch, attention_mask = enc_padding_mask)[-2:][0] \n",
    "        enc_key_batch = self.bert_model(enc_key_batch, attention_mask = enc_key_padding_mask)[-2:][0]  \n",
    "        \n",
    "        \n",
    "        # enc_batch = self.model.embeds(enc_batch)  # Get embeddings for encoder input\n",
    "        # enc_key_batch = self.model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "        enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        # Test\n",
    "#         enc_batch2, enc_lens2, enc_padding_mask2, enc_batch_extend_vocab2, extra_zeros2, context2 = get_enc_data(test_batch)\n",
    "        enc_batch2, enc_lens2, enc_padding_mask2, \\\n",
    "        enc_key_batch2, enc_key_lens2, enc_key_padding_mask2,\\\n",
    "        enc_batch_extend_vocab2, extra_zeros2, context2 = get_enc_data(test_batch)\n",
    "        \n",
    "        enc_batch2 = enc_batch2.type(T.LongTensor).cuda() #  `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "        enc_key_batch2 = enc_key_batch2.type(T.LongTensor).cuda() #  `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "        enc_padding_mask2 = enc_padding_mask2.type(T.LongTensor).cuda() #  `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "        enc_key_padding_mask2 = enc_key_padding_mask2.type(T.LongTensor).cuda() #  `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "        \n",
    "        \n",
    "        with T.autograd.no_grad():\n",
    "#             enc_batch2 = self.bert_model(enc_batch2, attention_mask = enc_padding_mask2)[0][0]\n",
    "#             enc_key_batch2 = self.bert_model(enc_key_batch2, attention_mask = enc_key_padding_mask2)[0][0]  \n",
    "            \n",
    "            enc_batch2 = self.bert_model(enc_batch2, attention_mask = enc_padding_mask2)[-2:][0]\n",
    "            enc_key_batch2 = self.bert_model(enc_key_batch2, attention_mask = enc_key_padding_mask2)[-2:][0]  \n",
    "                \n",
    "            enc_out2, enc_hidden2 = self.model.encoder(enc_batch2, enc_lens2)\n",
    "        # -------------------------------Summarization-----------------------\n",
    "        if self.opt.train_mle == True:  # perform MLE training\n",
    "            mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros,\n",
    "                                            enc_batch_extend_vocab, enc_key_batch, enc_key_lens, batch)\n",
    "            mle_loss_2 = self.train_batch_MLE(enc_out2, enc_hidden2, enc_padding_mask2, context2, extra_zeros2,\n",
    "                                            enc_batch_extend_vocab2, enc_key_batch2, enc_key_lens2, test_batch)\n",
    "        else:\n",
    "            mle_loss = get_cuda(T.FloatTensor([0]))\n",
    "            mle_loss_2 = get_cuda(T.FloatTensor([0]))\n",
    "            \n",
    "        # --------------RL training-----------------------------------------------------\n",
    "        if self.opt.train_rl == True:  # perform reinforcement learning training\n",
    "            # multinomial sampling\n",
    "            sample_sents, RL_log_probs = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context,\n",
    "                                                             extra_zeros, enc_batch_extend_vocab, batch.rev_oovs,\n",
    "                                                             greedy=False)\n",
    "            sample_sents2, RL_log_probs2 = self.train_batch_RL(enc_out2, enc_hidden2, enc_padding_mask2, context2,\n",
    "                                                             extra_zeros2, enc_batch_extend_vocab2, test_batch.rev_oovs,\n",
    "                                                             greedy=False)\n",
    "            with T.autograd.no_grad():\n",
    "                # greedy sampling\n",
    "                greedy_sents, _ = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros,\n",
    "                                                      enc_batch_extend_vocab, batch.rev_oovs, greedy=True)\n",
    "\n",
    "            sample_reward, _, _ = self.reward_function(sample_sents, batch.original_summarys)\n",
    "            baseline_reward, _, _ = self.reward_function(greedy_sents, batch.original_summarys)\n",
    "            # if iter%200 == 0:\n",
    "            #     self.write_to_file(sample_sents, greedy_sents, batch.original_abstracts, sample_reward, baseline_reward, iter)\n",
    "            rl_loss = -(sample_reward - baseline_reward) * RL_log_probs  # Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "            rl_loss = T.mean(rl_loss)\n",
    "\n",
    "            batch_reward = T.mean(sample_reward).item()\n",
    "            writer.add_scalar('Train_RL/RL_log_probs', RL_log_probs, iter)\n",
    "        else:\n",
    "            rl_loss = get_cuda(T.FloatTensor([0]))\n",
    "            batch_reward = 0\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        #         if opt.train_mle == True: \n",
    "        self.trainer.zero_grad()\n",
    "        (self.opt.mle_weight * mle_loss + self.opt.rl_weight * rl_loss).backward()\n",
    "        self.trainer.step()\n",
    "        #-----------------------Summarization----------------------------------------------------\n",
    "        if iter % 1000 == 0:\n",
    "            with T.autograd.no_grad():\n",
    "                train_rouge_l_f = self.calc_avg_rouge_result(iter,batch,'Train',enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens)\n",
    "                test_rouge_l_f = self.calc_avg_rouge_result(iter,test_batch,'Test',enc_hidden2, enc_out2, enc_padding_mask2, context2, extra_zeros2, enc_batch_extend_vocab2, enc_key_batch2, enc_key_lens2)\n",
    "                writer.add_scalars('Compare/rouge-l-f',  \n",
    "                   {'train_rouge_l_f': train_rouge_l_f,\n",
    "                    'test_rouge_l_f': test_rouge_l_f\n",
    "                   }, iter)\n",
    "                logger.info('iter: %s train_rouge_l_f: %.3f test_rouge_l_f: %.3f \\n' % (iter, train_rouge_l_f, test_rouge_l_f))\n",
    "                \n",
    "#         return mle_loss.item(), batch_reward, ans_list, batch_scores\n",
    "        return mle_loss.item(),mle_loss_2.item(), batch_reward\n",
    "\n",
    "    def calc_avg_rouge_result(self, iter, batch, mode, enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens):\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens, self.model, self.start_id, self.end_id, self.unk_id)\n",
    "\n",
    "        decoded_sents = []\n",
    "        ref_sents = []\n",
    "        ref_sents2 = []\n",
    "        article_sents = []\n",
    "        keywords_list = []\n",
    "        \n",
    "        summary_len = max_summary_len = long_seq_index = 0\n",
    "        for i in range(len(pred_ids)):            \n",
    "            decoded_words = data.outputids2words(pred_ids[i], self.vocab, batch.rev_oovs[i])\n",
    "            if len(decoded_words) < 2:\n",
    "                decoded_words = \"xxx\"\n",
    "            else:\n",
    "                decoded_words = \" \".join(decoded_words)\n",
    "            decoded_words = decoded_words.replace(\"[UNK]\",\"\")\n",
    "            decoded_sents.append(decoded_words)\n",
    "            summary = batch.original_summarys[i]\n",
    "            summary = summary.replace(\"<s>\",\"\").replace(\"</s>\",\"\")\n",
    "            review = batch.original_reviews[i]\n",
    "            ref_sents.append(summary)\n",
    "            article_sents.append(review) \n",
    "            keywords = batch.key_words[i]\n",
    "            keywords_list.append(str(keywords))\n",
    "            summary_len = len(summary.split(\" \"))\n",
    "            if max_summary_len < summary_len: \n",
    "                max_summary_len = summary_len\n",
    "                long_seq_index = i\n",
    "\n",
    "        rouge = Rouge()    \n",
    "        score = rouge.get_scores(decoded_sents, ref_sents, avg = True)    \n",
    "        writer.add_scalars('%s/rouge-1' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-1']['f'],\n",
    "                'p': score['rouge-1']['p'],\n",
    "                'r': score['rouge-1']['r']}\n",
    "                , iter)\n",
    "        writer.add_scalars('%s/rouge-2' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-2']['f'],\n",
    "                'p': score['rouge-2']['p'],\n",
    "                'r': score['rouge-2']['r']}\n",
    "                , iter)\n",
    "        writer.add_scalars('%s/rouge-l' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-l']['f'],\n",
    "                'p': score['rouge-l']['p'],\n",
    "                'r': score['rouge-l']['r']}\n",
    "                , iter)\n",
    "        for i in range(len(decoded_sents)):\n",
    "            if type(article_sents[i]) != str: continue\n",
    "            if type(ref_sents[i]) != str:  continue\n",
    "            if type(decoded_sents[i]) != str:  continue\n",
    "\n",
    "        writer.add_text('Rouge/%s/%s' % (iter,mode), decoded_sents[long_seq_index], iter)\n",
    "        writer.add_text('Rouge/%s/%s' % (iter,mode), keywords_list[long_seq_index], iter)\n",
    "        writer.add_text('Rouge/%s/%s' % (iter,mode), ref_sents[long_seq_index], iter)\n",
    "        writer.add_text('Rouge/%s/%s' % (iter,mode), article_sents[long_seq_index], iter)\n",
    "        \n",
    "#         for i in range(len(decoded_sents)):\n",
    "# #             writer.add_text('Rouge/%s/%s' % (iter,mode), decoded_sents[i], iter)\n",
    "#             writer.add_text('Rouge/%s/%s' % (iter,mode), ref_sents[i], iter)\n",
    "#             writer.add_text('Rouge/%s/%s' % (iter,mode), article_sents[i], iter)\n",
    "        \n",
    "        bleu_decode_sents = [decode.split(\" \") for decode in decoded_sents]\n",
    "        bleu_ref_sents = [[ref.split(\" \")] for ref in ref_sents]\n",
    "\n",
    "        writer.add_scalars('%s/Individual' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'BLEU-1': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(1, 0, 0, 0)),\n",
    "                'BLEU-2': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0, 1, 0, 0)),\n",
    "                'BLEU-3': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0, 0, 1, 0)),\n",
    "                'BLEU-4': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0, 0, 0, 1))}\n",
    "                , iter)\n",
    "       \n",
    "        writer.add_scalars('%s/Cumulative' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'BLEU-1': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(1, 0, 0, 0)),\n",
    "                'BLEU-2': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0.5, 0.5, 0, 0)),\n",
    "                'BLEU-3': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0.33, 0.33, 0.33, 0)),\n",
    "                'BLEU-4': corpus_bleu(bleu_ref_sents,bleu_decode_sents, weights=(0.25, 0.25, 0.25, 0.25))}\n",
    "                , iter)\n",
    "    \n",
    "        return score['rouge-l']['f']\n",
    "    \n",
    "    def get_best_res_score(self, results, scores):\n",
    "        max_score = float(0)\n",
    "        _id = 0\n",
    "        for idx in range(len(results)):\n",
    "            re_matchData = re.compile(r'\\-?\\d{1,10}\\.?\\d{1,10}')\n",
    "            data = re.findall(re_matchData, str(scores[idx]))\n",
    "            score = sum([float(d) for d in data])\n",
    "            if score > max_score:\n",
    "                _id = idx\n",
    "        return results[_id], scores[_id]\n",
    "\n",
    "    def get_lr(self):\n",
    "        for param_group in self.trainer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    def get_weight_decay(self):\n",
    "        for param_group in self.trainer.param_groups:\n",
    "            return param_group['weight_decay']\n",
    "\n",
    "    def trainIters(self):\n",
    "        final_file_path = None\n",
    "        iter = self.setup_train()\n",
    "        epoch = 0\n",
    "        count = test_mle_total = train_mle_total = r_total = 0\n",
    "        logger.info(u'------Training START--------')\n",
    "        test_batch = self.test_batcher.next_batch()\n",
    "        #         while iter <= config.max_iterations:\n",
    "        while epoch <= config.max_epochs:\n",
    "            train_batch = self.train_batcher.next_batch()\n",
    "            try:\n",
    "                train_mle_loss,test_mle_loss, r  = self.train_one_batch(train_batch,test_batch, iter)\n",
    "\n",
    "                writer.add_scalar('RL_Train/reward', r, iter)\n",
    "\n",
    "                writer.add_scalars('Compare/mle_loss' ,  \n",
    "                   {'train_mle_loss': train_mle_loss,\n",
    "                    'test_mle_loss': test_mle_loss\n",
    "                   }, iter)\n",
    "                \n",
    "#             # break\n",
    "            except KeyboardInterrupt:\n",
    "                logger.info(\"-------------------Keyboard Interrupt------------------\")\n",
    "                exit(0)\n",
    "#             except Exception as e:                \n",
    "#                 logger.info(\"-------------------Ignore error------------------\\n%s\\n\" % e)\n",
    "#                 print(\"Please load final_file_path : %s\" % final_file_path)\n",
    "#                 traceback = sys.exc_info()[2]\n",
    "#                 print(sys.exc_info())\n",
    "#                 print(traceback.tb_lineno)\n",
    "#                 print(e)\n",
    "#                 break\n",
    "            # if opt.train_mle == False: break\n",
    "            train_mle_total += train_mle_loss\n",
    "            r_total += r\n",
    "            test_mle_total += test_mle_loss\n",
    "            count += 1\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 1000 == 0:\n",
    "                train_mle_avg = train_mle_total / count\n",
    "                r_avg = r_total / count\n",
    "                test_mle_avg = test_mle_total / count\n",
    "                epoch = int((iter * config.batch_size) / train_num) + 1\n",
    "#                 logger.info('epoch: %s iter: %s train_mle_loss: %.3f test_mle_loss: %.3f reward: %.3f \\n' % (epoch, iter, train_mle_avg, test_mle_avg, r_avg))\n",
    "                \n",
    "\n",
    "                count = test_mle_total = train_mle_total = r_total = 0\n",
    "                writer.add_scalar('RL_Train/r_avg', r_avg, iter)\n",
    "                \n",
    "                writer.add_scalars('Compare/mle_avg_loss' ,  \n",
    "                   {'train_mle_avg': train_mle_avg,\n",
    "                    'test_mle_avg': test_mle_avg\n",
    "                   }, iter)\n",
    "            # break\n",
    "            if iter % 5000 == 0:\n",
    "                final_file_path = self.save_model(iter, test_mle_avg, r_avg)\n",
    "#                 if opt.view:\n",
    "#                     best_res, best_score = self.get_best_res_score(ans_list, batch_scores)\n",
    "#                     logger.info('best_res: %s \\n' % (best_res))\n",
    "#                     logger.info('best_score: %s \\n' % (best_score))\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['decoded_str'], iter)\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['summary'], iter)\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['review'], iter)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_action(opt):\n",
    "#     try:       \n",
    "    opt.rl_weight = 1 - opt.mle_weight  \n",
    "\n",
    "    if opt.load_model:\n",
    "        opt.load_model = \"/%s/%s\"%(opt.word_emb_type,opt.load_model)    \n",
    "\n",
    "    logger.info(u'------Training Setting--------')  \n",
    "\n",
    "    logger.info(\"Traing Type :%s\" %(config.data_type))\n",
    "    if opt.train_mle == True:\n",
    "        logger.info(\"Training mle: %s, mle weight: %.2f\"%(opt.train_mle, opt.mle_weight))\n",
    "\n",
    "    if opt.train_rl == True:\n",
    "        logger.info(\"Training rl: %s, rl weight: %.2f \\n\"%(opt.train_rl, opt.rl_weight))\n",
    "\n",
    "#     if opt.word_emb_type == 'bert': config.emb_dim = 768\n",
    "    if opt.pre_train_emb : \n",
    "        logger.info('use pre_train_%s vocab_size %s \\n'%(opt.word_emb_type,config.vocab_size))\n",
    "\n",
    "    else:\n",
    "        logger.info('use %s vocab_size %s \\n'%(opt.word_emb_type,config.vocab_size))\n",
    "\n",
    "    logger.info(\"intra_encoder: %s intra_decoder: %s \\n\"%(config.intra_encoder, config.intra_decoder))\n",
    "    if opt.word_emb_type in ['word2Vec','glove']:\n",
    "    #   config.vocab_path = \"Embedding/%s/%s/word.vocab\"%(config.data_type, opt.word_emb_type)\n",
    "        config.vocab_path = config.Data_path + \"Embedding/%s/word.vocab\"%(opt.word_emb_type)            \n",
    "        config.vocab_size = len(open(config.vocab_path).readlines())\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    train_processor = Train(opt,vocab)\n",
    "    train_processor.trainIters()\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         traceback = sys.exc_info()[2]\n",
    "#         logger.error(sys.exc_info())\n",
    "#         logger.error(traceback.tb_lineno)\n",
    "#         logger.error(e)\n",
    "    logger.info(u'------Training END--------')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "def write_enc_graph():\n",
    "    encoder_writer = SummaryWriter('runs/Pointer-Generator/word2Vec/Encoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    encoder = Encoder().to(device) \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "#     enc_batch = Variable(torch.rand(enc_batch.shape)).to(device) \n",
    "    enc_lens = torch.from_numpy(enc_lens).to(device) \n",
    "\n",
    "    encoder_writer.add_graph(encoder, (enc_batch, enc_lens), verbose=True)\n",
    "    encoder_writer.close()\n",
    "\n",
    "def write_dec_graph():\n",
    "    decoder_writer = SummaryWriter('runs/Pointer-Generator/word2Vec/Decoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    # decoder = Decoder().to(device)    \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    model = Model(False,'word2Vec',vocab)\n",
    "    \n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "    # train_batch_MLE\n",
    "    dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "    step_losses = []\n",
    "    s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "    # x_t 為decoder每一個time step 的batch input\n",
    "    x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "    prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    sum_temporal_srcs = None     \n",
    "\n",
    "\n",
    "    for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "        use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "        # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "        # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "        if t == 0 :temp_batch = dec_batch[:, t]\n",
    "        x_t = use_gound_truth * temp_batch + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "        x_t = model.embeds(x_t)\n",
    "    #     final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "        final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "        x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "        extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "        enc_key_batch, enc_key_lens)        \n",
    "\n",
    "\n",
    "        #         decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "#         x_t = Variable(torch.rand(x_t.shape)).to(device) \n",
    "        #             s_t = Variable(torch.rand(s_t.shape)).to(device)\n",
    "#         enc_out = Variable(torch.rand(enc_out.shape)).to(device)\n",
    "#         enc_padding_mask = Variable(torch.rand(enc_padding_mask.shape)).to(device,dtype=torch.long)\n",
    "#         context = Variable(torch.rand(context.shape)).to(device)\n",
    "#         extra_zeros = Variable(torch.rand(extra_zeros.shape)).to(device)\n",
    "#         enc_batch_extend_vocab = Variable(torch.rand(enc_batch_extend_vocab.shape)).to(device)\n",
    "        #             sum_temporal_srcs = Variable(torch.rand(sum_temporal_srcs.shape)).to(device)\n",
    "        #             prev_s = Variable(torch.rand(prev_s.shape)).to(device)\n",
    "#         enc_key_batch = Variable(torch.rand(enc_key_batch.shape)).to(device)\n",
    "        enc_key_lens = torch.from_numpy(enc_key_lens).to(device) \n",
    "        \n",
    "        decoder_writer.add_graph(model.decoder, \n",
    "                         (x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens), verbose=True)\n",
    "        decoder_writer.close()\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-26 13:16:40 - Text-Summary - INFO: - logger已啟動\n",
      "I0226 13:16:40.433144 140516709062464 <ipython-input-2-00f9962e7fdd>:30] logger已啟動\n",
      "2020-02-26 13:16:40 - Text-Summary - INFO: - ------Training Setting--------\n",
      "I0226 13:16:40.443660 140516709062464 <ipython-input-9-ec6a8d673154>:8] ------Training Setting--------\n",
      "2020-02-26 13:16:40 - Text-Summary - INFO: - Traing Type :Cameras_new\n",
      "I0226 13:16:40.446096 140516709062464 <ipython-input-9-ec6a8d673154>:10] Traing Type :Cameras_new\n",
      "2020-02-26 13:16:40 - Text-Summary - INFO: - Training mle: True, mle weight: 1.00\n",
      "I0226 13:16:40.448261 140516709062464 <ipython-input-9-ec6a8d673154>:12] Training mle: True, mle weight: 1.00\n",
      "2020-02-26 13:16:40 - Text-Summary - INFO: - use pre_train_bert vocab_size 50000 \n",
      "\n",
      "I0226 13:16:40.450251 140516709062464 <ipython-input-9-ec6a8d673154>:19] use pre_train_bert vocab_size 50000 \n",
      "\n",
      "2020-02-26 13:16:40 - Text-Summary - INFO: - intra_encoder: True intra_decoder: True \n",
      "\n",
      "I0226 13:16:40.454775 140516709062464 <ipython-input-9-ec6a8d673154>:24] intra_encoder: True intra_decoder: True \n",
      "\n",
      "I0226 13:16:46.967121 140516709062464 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/eagleuser/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0226 13:16:47.026669 140516709062464 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0226 13:16:48.553483 140516709062464 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/eagleuser/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0226 13:16:54.284042 140516709062464 modeling.py:580] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/eagleuser/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "I0226 13:16:54.287829 140516709062464 modeling.py:588] extracting archive file /home/eagleuser/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmphjbvi2gd\n",
      "I0226 13:16:57.397702 140516709062464 modeling.py:598] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0226 13:16:59.631624 140516709062464 tokenization.py:190] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/eagleuser/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2020-02-26 13:17:00 - Text-Summary - INFO: - Model(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(768, 512, batch_first=True, bidirectional=True)\n",
      "    (reduce_h): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (reduce_c): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (enc_attention): encoder_attention(\n",
      "      (W_h): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (W_s): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (W_t): Linear(in_features=768, out_features=1024, bias=True)\n",
      "      (v): Linear(in_features=1024, out_features=1, bias=False)\n",
      "    )\n",
      "    (dec_attention): decoder_attention(\n",
      "      (W_prev): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (W_s): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (W_t): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (x_context): Linear(in_features=1792, out_features=768, bias=True)\n",
      "    (x_key_context): Linear(in_features=2560, out_features=768, bias=True)\n",
      "    (lstm): LSTMCell(768, 512)\n",
      "    (p_gen_linear): Linear(in_features=3328, out_features=1, bias=True)\n",
      "    (V): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (V1): Linear(in_features=512, out_features=50000, bias=True)\n",
      "  )\n",
      "  (embeds): Embedding(50000, 768)\n",
      ")\n",
      "I0226 13:17:00.731870 140516709062464 <ipython-input-8-f47b80dcf0ce>:43] Model(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(768, 512, batch_first=True, bidirectional=True)\n",
      "    (reduce_h): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (reduce_c): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (enc_attention): encoder_attention(\n",
      "      (W_h): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (W_s): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (W_t): Linear(in_features=768, out_features=1024, bias=True)\n",
      "      (v): Linear(in_features=1024, out_features=1, bias=False)\n",
      "    )\n",
      "    (dec_attention): decoder_attention(\n",
      "      (W_prev): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (W_s): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (W_t): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (x_context): Linear(in_features=1792, out_features=768, bias=True)\n",
      "    (x_key_context): Linear(in_features=2560, out_features=768, bias=True)\n",
      "    (lstm): LSTMCell(768, 512)\n",
      "    (p_gen_linear): Linear(in_features=3328, out_features=1, bias=True)\n",
      "    (V): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (V1): Linear(in_features=512, out_features=50000, bias=True)\n",
      "  )\n",
      "  (embeds): Embedding(50000, 768)\n",
      ")\n",
      "2020-02-26 13:17:00 - Text-Summary - INFO: - ------Training START--------\n",
      "I0226 13:17:00.740042 140516709062464 <ipython-input-8-f47b80dcf0ce>:451] ------Training START--------\n",
      "2020-02-26 13:17:05 - Text-Summary - INFO: - iter: 0 train_rouge_l_f: 0.000 test_rouge_l_f: 0.043 \n",
      "\n",
      "I0226 13:17:05.314255 140516709062464 <ipython-input-8-f47b80dcf0ce>:341] iter: 0 train_rouge_l_f: 0.000 test_rouge_l_f: 0.043 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/u012869752/article/details/72513141\n",
    "# 由于在jupyter notebook中，args不为空\n",
    "from glob import glob\n",
    "# nvidia-smi -pm 1\n",
    "if __name__ == \"__main__\":   \n",
    "    try:\n",
    "        # --------------------------Training ----------------------------------\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--train_mle', type=bool, default=True)\n",
    "        parser.add_argument('--train_rl', type=bool, default=False)\n",
    "        parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "    #         parser.add_argument('--load_model', type=str, default='/0015000_3.29_0.00.tar')\n",
    "        parser.add_argument('--load_model', type=str, default=None)\n",
    "        parser.add_argument('--new_lr', type=float, default=None)\n",
    "        parser.add_argument('--multi_device', type=bool, default=True)\n",
    "        parser.add_argument('--view', type=bool, default=True)\n",
    "        parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "        parser.add_argument('--word_emb_type', type=str, default='bert')\n",
    "        parser.add_argument('--train_action', type=bool, default=True)\n",
    "        opt = parser.parse_args(args=[])\n",
    "\n",
    "        today = dt.now()\n",
    "    #         loggerPath = \"LOG//%s-(%s_%s_%s)-(%s:%s:%s)\"%(opt.word_emb_type,\n",
    "    #                   today.year,today.month,today.day,\n",
    "    #                   today.hour,today.minute,today.second)\n",
    "    #         logger = getLogger(config.loggerName,loggerPath)   \n",
    "\n",
    "        loggerPath = \"LOG//%s\"%(opt.word_emb_type)\n",
    "        logger = getLogger(config.loggerName,loggerPath) \n",
    "\n",
    "        if opt.load_model == None:\n",
    "            shutil.rmtree('runs/Pointer-Generator/bert', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/bert/exp-4', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/bert/Eecoder', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/bert/Decoder', ignore_errors=True) # clear previous \n",
    "\n",
    "        writer = SummaryWriter('runs/Pointer-Generator/bert/exp-4')\n",
    "    #         write_enc_graph()\n",
    "    #         write_dec_graph()\n",
    "\n",
    "        if opt.train_action: train_action(opt)\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback = sys.exc_info()[2]\n",
    "        print(sys.exc_info())\n",
    "        print(traceback.tb_lineno)\n",
    "        print(e)\n",
    "    finally:\n",
    "        removeLogger(logger)\n",
    "        # export scalar data to JSON for external processing\n",
    "        # tensorboard --logdir /home/eagleuser/Users/leyan/Text-Summarizer-FOP/TensorBoard\n",
    "#         tensorboard --logdir ./runs\n",
    "#         if not os.path.exists('TensorBoard'): os.makedirs('TensorBoard')\n",
    "#         writer.export_scalars_to_json(\"TensorBoard/test.json\")\n",
    "        writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
