{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from model import Model\n",
    "\n",
    "\n",
    "from data_util import config, data\n",
    "from data_util.batcher import Batcher\n",
    "from data_util.data import Vocab\n",
    "\n",
    "\n",
    "from train_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse\n",
    "import torchsnooper\n",
    "import logging\n",
    "\n",
    "# -------- Test Packages -------\n",
    "from beam_search import *\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def getLogger(loggerName, loggerPath):\n",
    "    # 設置logger\n",
    "    logger = logging.getLogger(loggerName)  # 不加名稱設置root logger\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s: - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    logging.Filter(loggerName)\n",
    "\n",
    "    # 使用FileHandler輸出到文件\n",
    "    directory = os.path.dirname(loggerPath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fh = logging.FileHandler(loggerPath)\n",
    "\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # 使用StreamHandler輸出到屏幕\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    ch.setFormatter(formatter)\n",
    "    # 添加兩個Handler\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    # Handler只啟動一次\n",
    "    # 設置logger\n",
    "    logger.info(u'logger已啟動')\n",
    "    return logger\n",
    "\n",
    "def removeLogger(logger):\n",
    "    logger.info(u'logger已關閉')\n",
    "    handlers = logger.handlers[:]\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch():\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    # with torchsnooper.snoop():\n",
    "    while batch is not None:\n",
    "        example_list = batch.example_list\n",
    "        for ex in example_list:\n",
    "            r = str(ex.original_review)\n",
    "            s = str(ex.original_summary)\n",
    "            k = str(ex.key_words)\n",
    "            sent = ex.original_summary_sents\n",
    "#             print(\"original_review_sents:\", r)\n",
    "            print(\"original_summary_sents : \", s)\n",
    "            print(\"key_words : \", k)\n",
    "            print('------------------------------------------------------------\\n')\n",
    "        batch = batcher.next_batch()        \n",
    "# test_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bin Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 19261\n",
      "\n",
      "test : 6419\n",
      "\n",
      "valid : 6420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"bin/%s/bin-info.txt\"%(config.data_type),'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    [print(line) for line in lines]\n",
    "    train_num = int(lines[0].split(\":\")[1])\n",
    "    test_num = int(lines[1].split(\":\")[1])\n",
    "    val_num = int(lines[2].split(\":\")[1])\n",
    "    # f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    # f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    # f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View model summary\n",
    "#### 只有torchsummaryX成功\n",
    "#### 日後將以此模擬呈現結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary # 不支援RNN\n",
    "# from model import Encoder,Model\n",
    "# # https://www.cnblogs.com/lindaxin/p/8052043.html\n",
    "# device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# encoder = Encoder().to(device)    \n",
    "\n",
    "# vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "# batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "#                        batch_size=config.batch_size, single_pass=False)\n",
    "# batch = batcher.next_batch()\n",
    "# enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# enc_batch = Model().embeds(enc_batch) # Get embeddings for encoder input\n",
    "\n",
    "# # summary(encoder, enc_batch, enc_lens, show_hierarchical=True) \n",
    "# # summary(encoder, [enc_batch, enc_lens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modelsummary import summary # 未知問題\n",
    "# from model import Encoder,Model\n",
    "# # https://www.cnblogs.com/lindaxin/p/8052043.html\n",
    "# device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# encoder = Encoder().to(device)    \n",
    "\n",
    "# vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "# batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "#                        batch_size=config.batch_size, single_pass=False)\n",
    "# batch = batcher.next_batch()\n",
    "# enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# enc_batch = Model().embeds(enc_batch) # Get embeddings for encoder input\n",
    "\n",
    "# # summary(encoder, enc_batch, enc_lens, show_hierarchical=True) \n",
    "# # summary(encoder, enc_batch, enc_lens, show_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "           Kernel Shape  Output Shape   Params  Mult-Adds\n",
      "Layer                                                    \n",
      "0_lstm                -  [3473, 1024]  3334144    3325952\n",
      "1_reduce_h  [1024, 512]      [8, 512]   524800     524288\n",
      "2_reduce_c  [1024, 512]      [8, 512]   524800     524288\n",
      "---------------------------------------------------------\n",
      "                       Totals\n",
      "Total params          4383744\n",
      "Trainable params      4383744\n",
      "Non-trainable params        0\n",
      "Mult-Adds             4374528\n",
      "=========================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[3473, 1024]</td>\n",
       "      <td>3334144</td>\n",
       "      <td>3325952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_reduce_h</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_reduce_c</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kernel Shape  Output Shape   Params  Mult-Adds\n",
       "Layer                                                    \n",
       "0_lstm                -  [3473, 1024]  3334144    3325952\n",
       "1_reduce_h  [1024, 512]      [8, 512]   524800     524288\n",
       "2_reduce_c  [1024, 512]      [8, 512]   524800     524288"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Encoder,Model\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "encoder = Encoder().to(device)    \n",
    "\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "summary(encoder, enc_batch, enc_lens) # encoder summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummaryX import summary\n",
    "class Train(object):\n",
    "    def __init__(self, opt, vocab):\n",
    "#         self.vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "        self.vocab = vocab\n",
    "        self.train_batcher = Batcher(config.train_data_path, self.vocab, mode='train',\n",
    "                               batch_size=config.batch_size, single_pass=False)\n",
    "        self.test_batcher = Batcher(config.test_data_path, self.vocab, mode='eval',\n",
    "                               batch_size=config.batch_size, single_pass=True)\n",
    "        self.opt = opt\n",
    "        self.start_id = self.vocab.word2id(data.START_DECODING)\n",
    "        self.end_id = self.vocab.word2id(data.STOP_DECODING)\n",
    "        self.pad_id = self.vocab.word2id(data.PAD_TOKEN)\n",
    "        self.unk_id = self.vocab.word2id(data.UNKNOWN_TOKEN)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def save_model(self, iter, loss, r_loss):\n",
    "        if not os.path.exists(config.save_model_path):\n",
    "            os.makedirs(config.save_model_path)\n",
    "        file_path = \"/%07d_%.2f_%.2f.tar\" % (iter, loss, r_loss)\n",
    "        save_path = config.save_model_path + '/%s' % (self.opt.word_emb_type)\n",
    "        if not os.path.isdir(save_path): os.mkdir(save_path)\n",
    "        save_path = save_path + file_path\n",
    "        T.save({\n",
    "            \"iter\": iter + 1,\n",
    "            \"model_dict\": self.model.state_dict(),\n",
    "            \"trainer_dict\": self.trainer.state_dict()\n",
    "        }, save_path)\n",
    "        return file_path\n",
    "\n",
    "    def setup_train(self):\n",
    "        self.model = Model(opt.pre_train_emb, opt.word_emb_type, self.vocab)\n",
    "        #         print(\"Model : \",self.model)\n",
    "        #         logger.info(\"Model : \")\n",
    "        logger.info(str(self.model))\n",
    "        #         print(\"Encoder : \",self.model.encoder)\n",
    "        #         print(\"Decoder : \",self.model.decoder)\n",
    "        #         print(\"Embeds : \",self.model.embeds)\n",
    "        self.model = get_cuda(self.model)\n",
    "        device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")  # PyTorch v0.4.0\n",
    "        if opt.multi_device:\n",
    "            if T.cuda.device_count() > 1:\n",
    "                #                 print(\"Let's use\", T.cuda.device_count(), \"GPUs!\")\n",
    "                logger.info(\"Let's use \" + str(T.cuda.device_count()) + \" GPUs!\")\n",
    "                self.model = nn.DataParallel(self.model, list(range(T.cuda.device_count()))).cuda()\n",
    "\n",
    "        if isinstance(self.model, nn.DataParallel):\n",
    "            self.model = self.model.module\n",
    "        self.model.to(device)\n",
    "        #         self.model.eval()\n",
    "\n",
    "        self.trainer = T.optim.Adam(self.model.parameters(), lr=config.lr)\n",
    "        start_iter = 0\n",
    "        if self.opt.load_model is not None:\n",
    "#             load_model_path = os.path.join(config.save_model_path, self.opt.load_model)\n",
    "            load_model_path = config.save_model_path + self.opt.load_model\n",
    "            print(load_model_path)\n",
    "#             print('xxxx')\n",
    "            checkpoint = T.load(load_model_path)\n",
    "            start_iter = checkpoint[\"iter\"]\n",
    "            self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "            self.trainer.load_state_dict(checkpoint[\"trainer_dict\"])\n",
    "            #             print(\"Loaded model at \" + load_model_path)\n",
    "            logger.info(\"Loaded model at \" + load_model_path)\n",
    "        if self.opt.new_lr is not None:\n",
    "            self.trainer = T.optim.Adam(self.model.parameters(), lr=self.opt.new_lr)\n",
    "        return start_iter\n",
    "\n",
    "    def train_batch_MLE(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(\n",
    "            batch)  # Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "            use_gound_truth = get_cuda((T.rand(len(\n",
    "                enc_out)) > 0.25)).long()  # Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "            x_t = use_gound_truth * dec_batch[:, t] + (\n",
    "                                                      1 - use_gound_truth) * x_t  # Select decoder input based on use_ground_truth probabilities\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                  ct_e, extra_zeros,\n",
    "                                                                                  enc_batch_extend_vocab,\n",
    "                                                                                  sum_temporal_srcs, prev_s)\n",
    "            target = target_batch[:, t]\n",
    "            log_probs = T.log(final_dist + config.eps)\n",
    "            step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=self.pad_id)\n",
    "            step_losses.append(step_loss)\n",
    "            x_t = T.multinomial(final_dist,\n",
    "                                1).squeeze()  # Sample words from final distribution which can be used as input in next time step\n",
    "            is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t.detach() + (is_oov) * self.unk_id  # Replace OOVs with [UNK] token\n",
    "\n",
    "        losses = T.sum(T.stack(step_losses, 1), 1)  # unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens  # Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)  # Average batch loss\n",
    "        return mle_loss\n",
    "\n",
    "    def train_batch_RL(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab,\n",
    "                       review_oovs, greedy):\n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param review_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        s_t = enc_hidden  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []  # Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []  # Stores padding masks of generated samples\n",
    "        log_probs = []  # Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(\n",
    "            1))  # Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e,\n",
    "                                                                             extra_zeros, enc_batch_extend_vocab,\n",
    "                                                                             sum_temporal_srcs, prev_s)\n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs)\n",
    "                x_t = multi_dist.sample()  # perform multinomial sampling\n",
    "                log_prob = multi_dist.log_prob(x_t)\n",
    "                log_probs.append(log_prob)\n",
    "            else:\n",
    "                _, x_t = T.max(probs, dim=1)  # perform greedy sampling\n",
    "            x_t = x_t.detach()\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))  # Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1  # If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (\n",
    "            x_t == self.end_id) == 2] = 0  # If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t + (is_oov) * self.unk_id  # Replace OOVs with [UNK] token\n",
    "\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:  # If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1)\n",
    "            log_probs = log_probs * decoder_padding_mask  # Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1)  # Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs,\n",
    "                              dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy()\n",
    "            oovs = review_oovs[i]\n",
    "            S = data.outputids2words(id_list, self.vocab, oovs)  # Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:  # If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "\n",
    "        return decoded_strs, log_probs\n",
    "\n",
    "    def train_batch_decode(self, batch, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros,\n",
    "                           enc_batch_extend_vocab, review_oovs, greedy):\n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param review_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        s_t = enc_hidden  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []  # Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []  # Stores padding masks of generated samples\n",
    "        log_probs = []  # Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))  # Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            #             print('x_t')\n",
    "            #             print(x_t)\n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e,\n",
    "                                                                             extra_zeros, enc_batch_extend_vocab,\n",
    "                                                                             sum_temporal_srcs, prev_s)\n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs)\n",
    "                x_t = multi_dist.sample()  # perform multinomial sampling\n",
    "            # log_prob = multi_dist.log_prob(x_t)\n",
    "            #                 log_probs.append(log_prob)\n",
    "            else:\n",
    "                _, x_t = T.max(probs, dim=1)  # perform greedy sampling\n",
    "            x_t = x_t.detach()\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))  # Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1  # If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            #             mask[(mask == 1) + (x_t == self.end_id) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "        # is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
    "        #             x_t = (1-is_oov)*x_t + (is_oov)*self.unk_id                                             #Replace OOVs with [UNK] token\n",
    "\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        #         if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
    "        #             log_probs = T.stack(log_probs, dim=1)\n",
    "        #             log_probs = log_probs * decoder_padding_mask                                            #Not considering sampled words with padding mask = 0\n",
    "        #             lens = T.sum(decoder_padding_mask, dim=1)                                               #Length of sampled sentence\n",
    "        #             log_probs = T.sum(log_probs, dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        ans_list = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy()\n",
    "            oovs = review_oovs[i]\n",
    "            S = data.outputids2words(id_list, self.vocab, oovs)  # Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:  # If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "            ans_dict = {\n",
    "                'review': batch.original_reviews[i],\n",
    "                'key_words': batch.key_words[i],\n",
    "                'summary': batch.original_summarys[i],\n",
    "                'decoded_str': S\n",
    "            }\n",
    "            ans_list.append(ans_dict)\n",
    "\n",
    "        return decoded_strs, ans_list\n",
    "\n",
    "    def reward_function(self, decoded_sents, original_sents):\n",
    "        rouge = Rouge()\n",
    "        try:\n",
    "            scores = rouge.get_scores(decoded_sents, original_sents)\n",
    "        except Exception:\n",
    "            #             print(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
    "            logger.info(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
    "            scores = []\n",
    "            for i in range(len(decoded_sents)):\n",
    "                try:\n",
    "                    score = rouge.get_scores(decoded_sents[i], original_sents[i])\n",
    "                except Exception:\n",
    "                    #                     print(\"Error occured at:\")\n",
    "                    #                     print(\"decoded_sents:\", decoded_sents[i])\n",
    "                    #                     print(\"original_sents:\", original_sents[i])\n",
    "                    logger.info(\"Error occured at:\")\n",
    "                    logger.info(\"decoded_sents:\", decoded_sents[i])\n",
    "                    logger.info(\"original_sents:\", original_sents[i])\n",
    "                    score = [{\"rouge-l\": {\"f\": 0.0}}]\n",
    "                scores.append(score[0])\n",
    "        rouge_l_f1 = [score[\"rouge-l\"][\"f\"] for score in scores]\n",
    "        avg_rouge_l_f1 = sum(rouge_l_f1) / len(rouge_l_f1)\n",
    "        rouge_l_f1 = get_cuda(T.FloatTensor(rouge_l_f1))\n",
    "        return rouge_l_f1, scores, avg_rouge_l_f1\n",
    "\n",
    "    # def write_to_file(self, decoded, max, original, sample_r, baseline_r, iter):\n",
    "    #     with open(\"temp.txt\", \"w\") as f:\n",
    "    #         f.write(\"iter:\"+str(iter)+\"\\n\")\n",
    "    #         for i in range(len(original)):\n",
    "    #             f.write(\"dec: \"+decoded[i]+\"\\n\")\n",
    "    #             f.write(\"max: \"+max[i]+\"\\n\")\n",
    "    #             f.write(\"org: \"+original[i]+\"\\n\")\n",
    "    #             f.write(\"Sample_R: %.4f, Baseline_R: %.4f\\n\\n\"%(sample_r[i].item(), baseline_r[i].item()))\n",
    "\n",
    "\n",
    "    def train_one_batch(self, batch,test_batch, iter):\n",
    "        ans_list, batch_scores = None, None\n",
    "        # Train\n",
    "        enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "\n",
    "        enc_batch = self.model.embeds(enc_batch)  # Get embeddings for encoder input\n",
    "        enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "        # Test\n",
    "        enc_batch2, enc_lens2, enc_padding_mask2, enc_batch_extend_vocab2, extra_zeros2, context2 = get_enc_data(test_batch)\n",
    "        with T.autograd.no_grad():\n",
    "            enc_batch2 = self.model.embeds(enc_batch2)\n",
    "            enc_out2, enc_hidden2 = self.model.encoder(enc_batch2, enc_lens2)\n",
    "        # -------------------------------Summarization-----------------------\n",
    "        if self.opt.train_mle == True:  # perform MLE training\n",
    "            mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros,\n",
    "                                            enc_batch_extend_vocab, batch)\n",
    "            mle_loss_2 = self.train_batch_MLE(enc_out2, enc_hidden2, enc_padding_mask2, context2, extra_zeros2,\n",
    "                                            enc_batch_extend_vocab2, test_batch)\n",
    "        else:\n",
    "            mle_loss = get_cuda(T.FloatTensor([0]))\n",
    "            mle_loss_2 = get_cuda(T.FloatTensor([0]))\n",
    "        # original view\n",
    "#         if opt.view:\n",
    "#             sample_sents, ans_list = self.train_batch_decode(batch, enc_out, enc_hidden, enc_padding_mask, context,\n",
    "#                                                              extra_zeros, enc_batch_extend_vocab, batch.rev_oovs,\n",
    "#                                                              greedy=True)\n",
    "#             rouge_l_f1, batch_scores, avg_rouge_l_f1 = self.reward_function(sample_sents, batch.original_summarys)\n",
    "#             #             writer.add_text('Train/%s'% (iter), ans_list[0]['decoded_str'] , iter)\n",
    "#             #             writer.add_text('Train/%s'% (iter), ans_list[0]['summary'] , iter)\n",
    "#             #             writer.add_text('Train/%s'% (iter), ans_list[0]['review'] , iter)\n",
    "#             writer.add_scalar('Train/avg_rouge_l_f1', avg_rouge_l_f1, iter)\n",
    "            \n",
    "        # --------------RL training-----------------------------------------------------\n",
    "        if self.opt.train_rl == True:  # perform reinforcement learning training\n",
    "            # multinomial sampling\n",
    "            sample_sents, RL_log_probs = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context,\n",
    "                                                             extra_zeros, enc_batch_extend_vocab, batch.rev_oovs,\n",
    "                                                             greedy=False)\n",
    "            sample_sents2, RL_log_probs2 = self.train_batch_RL(enc_out2, enc_hidden2, enc_padding_mask2, context2,\n",
    "                                                             extra_zeros2, enc_batch_extend_vocab2, test_batch.rev_oovs,\n",
    "                                                             greedy=False)\n",
    "            with T.autograd.no_grad():\n",
    "                # greedy sampling\n",
    "                greedy_sents, _ = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros,\n",
    "                                                      enc_batch_extend_vocab, batch.rev_oovs, greedy=True)\n",
    "\n",
    "            sample_reward, _, _ = self.reward_function(sample_sents, batch.original_summarys)\n",
    "            baseline_reward, _, _ = self.reward_function(greedy_sents, batch.original_summarys)\n",
    "            # if iter%200 == 0:\n",
    "            #     self.write_to_file(sample_sents, greedy_sents, batch.original_abstracts, sample_reward, baseline_reward, iter)\n",
    "            rl_loss = -(sample_reward - baseline_reward) * RL_log_probs  # Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "            rl_loss = T.mean(rl_loss)\n",
    "\n",
    "            batch_reward = T.mean(sample_reward).item()\n",
    "            writer.add_scalar('Train_RL/RL_log_probs', RL_log_probs, iter)\n",
    "        else:\n",
    "            rl_loss = get_cuda(T.FloatTensor([0]))\n",
    "            batch_reward = 0\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        #         if opt.train_mle == True: \n",
    "        self.trainer.zero_grad()\n",
    "        (self.opt.mle_weight * mle_loss + self.opt.rl_weight * rl_loss).backward()\n",
    "        self.trainer.step()\n",
    "        #-----------------------Summarization----------------------------------------------------\n",
    "        if iter % 5000 == 0:\n",
    "            with T.autograd.no_grad():\n",
    "                train_rouge_l_f = self.calc_avg_rouge_result(iter,batch,'Train',enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab)\n",
    "                test_rouge_l_f = self.calc_avg_rouge_result(iter,test_batch,'Test',enc_hidden2, enc_out2, enc_padding_mask2, context2, extra_zeros2, enc_batch_extend_vocab2)\n",
    "                writer.add_scalars('Compare/rouge-l-f',  \n",
    "                   {'train_rouge_l_f': train_rouge_l_f,\n",
    "                    'test_rouge_l_f': test_rouge_l_f\n",
    "                   }, iter)\n",
    "                \n",
    "#         return mle_loss.item(), batch_reward, ans_list, batch_scores\n",
    "        return mle_loss.item(),mle_loss_2.item(), batch_reward\n",
    "\n",
    "    def calc_avg_rouge_result(self,iter,batch,mode, enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab):\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, self.model, self.start_id, self.end_id, self.unk_id)\n",
    "\n",
    "        decoded_sents = []\n",
    "        ref_sents = []\n",
    "        article_sents = []\n",
    "\n",
    "        for i in range(len(pred_ids)):            \n",
    "            decoded_words = data.outputids2words(pred_ids[i], self.vocab, batch.rev_oovs[i])\n",
    "            if len(decoded_words) < 2:\n",
    "                decoded_words = \"xxx\"\n",
    "            else:\n",
    "                decoded_words = \" \".join(decoded_words)\n",
    "            decoded_sents.append(decoded_words)\n",
    "            summary = batch.original_summarys[i]\n",
    "            review = batch.original_reviews[i]\n",
    "            ref_sents.append(summary)\n",
    "            article_sents.append(review) \n",
    "\n",
    "        rouge = Rouge()    \n",
    "        score = rouge.get_scores(decoded_sents, ref_sents, avg = True)    \n",
    "        writer.add_scalars('%s/rouge-1' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-1']['f'],\n",
    "                'p': score['rouge-1']['p'],\n",
    "                'r': score['rouge-1']['r']}\n",
    "                , iter)\n",
    "        writer.add_scalars('%s/rouge-2' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-2']['f'],\n",
    "                'p': score['rouge-2']['p'],\n",
    "                'r': score['rouge-2']['r']}\n",
    "                , iter)\n",
    "        writer.add_scalars('%s/rouge-l' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-l']['f'],\n",
    "                'p': score['rouge-l']['p'],\n",
    "                'r': score['rouge-l']['r']}\n",
    "                , iter)\n",
    "#         for i in range(len(decoded_sents)):\n",
    "#             if type(article_sents[i]) != str: continue\n",
    "#             if type(ref_sents[i]) != str:  continue\n",
    "#             if type(decoded_sents[i]) != str:  continue\n",
    "        writer.add_text('Rouge/%s/%s' % (iter,mode), decoded_sents[0], iter)\n",
    "        writer.add_text('Rouge/%s/%s' % (iter,mode), ref_sents[0], iter)\n",
    "        writer.add_text('Rouge/%s/%s' % (iter,mode), article_sents[0], iter)\n",
    "        return score['rouge-l']['f']\n",
    "    \n",
    "    def get_best_res_score(self, results, scores):\n",
    "        max_score = float(0)\n",
    "        _id = 0\n",
    "        for idx in range(len(results)):\n",
    "            re_matchData = re.compile(r'\\-?\\d{1,10}\\.?\\d{1,10}')\n",
    "            data = re.findall(re_matchData, str(scores[idx]))\n",
    "            score = sum([float(d) for d in data])\n",
    "            if score > max_score:\n",
    "                _id = idx\n",
    "        return results[_id], scores[_id]\n",
    "\n",
    "    def get_lr(self):\n",
    "        for param_group in self.trainer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    def get_weight_decay(self):\n",
    "        for param_group in self.trainer.param_groups:\n",
    "            #             print(param_group)\n",
    "            return param_group['weight_decay']\n",
    "\n",
    "    def trainIters(self):\n",
    "        final_file_path = None\n",
    "        iter = self.setup_train()\n",
    "        epoch = 0\n",
    "        count = test_mle_total = train_mle_total = r_total = 0\n",
    "        logger.info(u'------Training START--------')\n",
    "        test_batch = self.test_batcher.next_batch()\n",
    "        #         while iter <= config.max_iterations:\n",
    "        while epoch <= config.max_epochs:\n",
    "            train_batch = self.train_batcher.next_batch()\n",
    "            try:\n",
    "#                 train_mle_loss, train_r, ans_list, batch_scores = self.train_one_batch(train_batch,test_batch, iter)\n",
    "                train_mle_loss,test_mle_loss, r = self.train_one_batch(train_batch,test_batch, iter)\n",
    "\n",
    "                #                 writer.add_scalar('lr', self.get_lr(), iter)\n",
    "#                 writer.add_scalar('Train/mle_loss', train_mle_loss, iter)\n",
    "                writer.add_scalar('RL_Train/reward', r, iter)\n",
    "                \n",
    "#                 writer.add_scalar('Test/mle_loss', test_mle_loss, iter)\n",
    "                \n",
    "                writer.add_scalars('Compare/mle_loss' ,  \n",
    "                   {'train_mle_loss': train_mle_loss,\n",
    "                    'test_mle_loss': test_mle_loss\n",
    "                   }, iter)\n",
    "                \n",
    "            # break\n",
    "            except KeyboardInterrupt:\n",
    "                logger.info(\"-------------------Keyboard Interrupt------------------\")\n",
    "                exit(0)\n",
    "            except Exception as e:\n",
    "                logger.info(\"-------------------Ignore error------------------\\n%s\\n\" % e)\n",
    "                print(\"Please load final_file_path : %s\" % final_file_path)\n",
    "                break\n",
    "            # if opt.train_mle == False: break\n",
    "            train_mle_total += train_mle_loss\n",
    "            r_total += r\n",
    "            test_mle_total += test_mle_loss\n",
    "            count += 1\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 1000 == 0:\n",
    "                train_mle_avg = train_mle_total / count\n",
    "                r_avg = r_total / count\n",
    "                test_mle_avg = test_mle_total / count\n",
    "                epoch = int((iter * config.batch_size) / train_num) + 1\n",
    "                logger.info('epoch: %s iter: %s train_mle_loss: %.3f test_mle_loss: %.3f reward: %.3f \\n' % (epoch, iter, train_mle_avg, test_mle_avg, r_avg))\n",
    "\n",
    "                count = test_mle_total = train_mle_total = r_total = 0\n",
    "#                 writer.add_scalar('Train/mle_avg_loss', train_mle_avg, iter)\n",
    "#                 writer.add_scalar('Test/mle_avg_loss', test_mle_avg, iter)\n",
    "                writer.add_scalar('RL_Train/r_avg', r_avg, iter)\n",
    "                \n",
    "                writer.add_scalars('Compare/mle_avg_loss' ,  \n",
    "                   {'train_mle_avg': train_mle_avg,\n",
    "                    'test_mle_avg': test_mle_avg\n",
    "                   }, iter)\n",
    "            # break\n",
    "            if iter % 5000 == 0:\n",
    "                final_file_path = self.save_model(iter, test_mle_avg, r_avg)\n",
    "#                 if opt.view:\n",
    "#                     best_res, best_score = self.get_best_res_score(ans_list, batch_scores)\n",
    "#                     logger.info('best_res: %s \\n' % (best_res))\n",
    "#                     logger.info('best_score: %s \\n' % (best_score))\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['decoded_str'], iter)\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['summary'], iter)\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['review'], iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_action(opt):\n",
    "    try:       \n",
    "        opt.rl_weight = 1 - opt.mle_weight  \n",
    "\n",
    "        if opt.load_model:\n",
    "            opt.load_model = \"/%s/%s\"%(opt.word_emb_type,opt.load_model)    \n",
    "\n",
    "        logger.info(u'------Training Setting--------')  \n",
    "   \n",
    "        logger.info(\"Traing Type :%s\" %(config.data_type))\n",
    "        if opt.train_mle == True:\n",
    "            logger.info(\"Training mle: %s, mle weight: %.2f\"%(opt.train_mle, opt.mle_weight))\n",
    "\n",
    "        if opt.train_rl == True:\n",
    "            logger.info(\"Training rl: %s, rl weight: %.2f \\n\"%(opt.train_rl, opt.rl_weight))\n",
    "\n",
    "        if opt.word_emb_type == 'bert': config.emb_dim = 768\n",
    "        if opt.pre_train_emb : \n",
    "            logger.info('use pre_train_%s vocab_size %s \\n'%(opt.word_emb_type,config.vocab_size))\n",
    "\n",
    "        else:\n",
    "            logger.info('use %s vocab_size %s \\n'%(opt.word_emb_type,config.vocab_size))\n",
    "\n",
    "        logger.info(\"intra_encoder: %s intra_decoder: %s \\n\"%(config.intra_encoder, config.intra_decoder))\n",
    "        if opt.word_emb_type in ['word2Vec','glove']:\n",
    "            config.vocab_path = \"Embedding/%s/%s/word.vocab\"%(config.data_type, opt.word_emb_type)\n",
    "            config.vocab_size = len(open(config.vocab_path).readlines())\n",
    "            vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "        train_processor = Train(opt,vocab)\n",
    "        train_processor.trainIters()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        traceback = sys.exc_info()[2]\n",
    "        logger.error(sys.exc_info())\n",
    "        logger.error(traceback.tb_lineno)\n",
    "        logger.error(e)\n",
    "    logger.info(u'------Training END--------')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-04 12:42:25 - Text-Summary - INFO: - logger已啟動\n",
      "2020-01-04 12:42:25 - Text-Summary - INFO: - ------Training Setting--------\n",
      "2020-01-04 12:42:25 - Text-Summary - INFO: - Traing Type :category\n",
      "2020-01-04 12:42:25 - Text-Summary - INFO: - Training mle: True, mle weight: 1.00\n",
      "2020-01-04 12:42:25 - Text-Summary - INFO: - use pre_train_glove vocab_size 50000 \n",
      "\n",
      "2020-01-04 12:42:25 - Text-Summary - INFO: - intra_encoder: True intra_decoder: True \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-04 12:42:38 - Text-Summary - INFO: - Model(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
      "    (reduce_h): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (reduce_c): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (enc_attention): encoder_attention(\n",
      "      (W_h): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (W_s): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v): Linear(in_features=1024, out_features=1, bias=False)\n",
      "    )\n",
      "    (dec_attention): decoder_attention(\n",
      "      (W_prev): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (W_s): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (x_context): Linear(in_features=1324, out_features=300, bias=True)\n",
      "    (lstm): LSTMCell(300, 512)\n",
      "    (p_gen_linear): Linear(in_features=2860, out_features=1, bias=True)\n",
      "    (V): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (V1): Linear(in_features=512, out_features=50000, bias=True)\n",
      "  )\n",
      "  (embeds): Embedding(50000, 300)\n",
      ")\n",
      "2020-01-04 12:42:38 - Text-Summary - INFO: - ------Training START--------\n",
      "2020-01-04 13:01:58 - Text-Summary - INFO: - epoch: 1 iter: 1000 train_mle_loss: 4.111 test_mle_loss: 4.046 reward: 0.000 \n",
      "\n",
      "2020-01-04 13:21:44 - Text-Summary - INFO: - epoch: 1 iter: 2000 train_mle_loss: 3.698 test_mle_loss: 3.661 reward: 0.000 \n",
      "\n",
      "2020-01-04 13:41:39 - Text-Summary - INFO: - epoch: 2 iter: 3000 train_mle_loss: 3.598 test_mle_loss: 3.582 reward: 0.000 \n",
      "\n",
      "2020-01-04 13:55:43 - Text-Summary - INFO: - epoch: 2 iter: 4000 train_mle_loss: 3.480 test_mle_loss: 3.518 reward: 0.000 \n",
      "\n",
      "2020-01-04 14:02:15 - Text-Summary - INFO: - epoch: 3 iter: 5000 train_mle_loss: 3.437 test_mle_loss: 3.400 reward: 0.000 \n",
      "\n",
      "2020-01-04 14:08:46 - Text-Summary - INFO: - epoch: 3 iter: 6000 train_mle_loss: 3.395 test_mle_loss: 3.300 reward: 0.000 \n",
      "\n",
      "2020-01-04 14:15:14 - Text-Summary - INFO: - epoch: 3 iter: 7000 train_mle_loss: 3.334 test_mle_loss: 3.207 reward: 0.000 \n",
      "\n",
      "2020-01-04 14:21:39 - Text-Summary - INFO: - epoch: 4 iter: 8000 train_mle_loss: 3.221 test_mle_loss: 3.170 reward: 0.000 \n",
      "\n",
      "2020-01-04 14:28:13 - Text-Summary - INFO: - epoch: 4 iter: 9000 train_mle_loss: 3.169 test_mle_loss: 3.072 reward: 0.000 \n",
      "\n",
      "2020-01-04 14:34:44 - Text-Summary - INFO: - epoch: 5 iter: 10000 train_mle_loss: 3.193 test_mle_loss: 2.930 reward: 0.000 \n",
      "\n",
      "2020-01-04 14:41:29 - Text-Summary - INFO: - epoch: 5 iter: 11000 train_mle_loss: 3.187 test_mle_loss: 2.994 reward: 0.000 \n",
      "\n",
      "2020-01-04 14:48:15 - Text-Summary - INFO: - epoch: 5 iter: 12000 train_mle_loss: 3.099 test_mle_loss: 2.848 reward: 0.000 \n",
      "\n",
      "2020-01-04 14:54:42 - Text-Summary - INFO: - epoch: 6 iter: 13000 train_mle_loss: 3.058 test_mle_loss: 2.757 reward: 0.000 \n",
      "\n",
      "2020-01-04 15:01:16 - Text-Summary - INFO: - epoch: 6 iter: 14000 train_mle_loss: 3.047 test_mle_loss: 2.739 reward: 0.000 \n",
      "\n",
      "2020-01-04 15:07:56 - Text-Summary - INFO: - epoch: 7 iter: 15000 train_mle_loss: 2.927 test_mle_loss: 2.534 reward: 0.000 \n",
      "\n",
      "2020-01-04 15:14:23 - Text-Summary - INFO: - epoch: 7 iter: 16000 train_mle_loss: 2.999 test_mle_loss: 2.586 reward: 0.000 \n",
      "\n",
      "2020-01-04 15:20:41 - Text-Summary - INFO: - epoch: 8 iter: 17000 train_mle_loss: 3.011 test_mle_loss: 2.554 reward: 0.000 \n",
      "\n",
      "2020-01-04 15:26:59 - Text-Summary - INFO: - epoch: 8 iter: 18000 train_mle_loss: 2.840 test_mle_loss: 2.507 reward: 0.000 \n",
      "\n",
      "2020-01-04 15:33:35 - Text-Summary - INFO: - epoch: 8 iter: 19000 train_mle_loss: 2.877 test_mle_loss: 2.348 reward: 0.000 \n",
      "\n",
      "2020-01-04 15:39:58 - Text-Summary - INFO: - epoch: 9 iter: 20000 train_mle_loss: 2.837 test_mle_loss: 2.385 reward: 0.000 \n",
      "\n",
      "2020-01-04 15:46:34 - Text-Summary - INFO: - epoch: 9 iter: 21000 train_mle_loss: 2.805 test_mle_loss: 2.179 reward: 0.000 \n",
      "\n",
      "2020-01-04 15:53:04 - Text-Summary - INFO: - epoch: 10 iter: 22000 train_mle_loss: 2.889 test_mle_loss: 2.270 reward: 0.000 \n",
      "\n",
      "2020-01-04 15:59:29 - Text-Summary - INFO: - epoch: 10 iter: 23000 train_mle_loss: 2.772 test_mle_loss: 2.242 reward: 0.000 \n",
      "\n",
      "2020-01-04 16:06:00 - Text-Summary - INFO: - epoch: 10 iter: 24000 train_mle_loss: 2.782 test_mle_loss: 2.247 reward: 0.000 \n",
      "\n",
      "2020-01-04 16:12:22 - Text-Summary - INFO: - epoch: 11 iter: 25000 train_mle_loss: 2.735 test_mle_loss: 2.091 reward: 0.000 \n",
      "\n",
      "2020-01-04 16:19:01 - Text-Summary - INFO: - epoch: 11 iter: 26000 train_mle_loss: 2.766 test_mle_loss: 2.106 reward: 0.000 \n",
      "\n",
      "2020-01-04 16:25:26 - Text-Summary - INFO: - epoch: 12 iter: 27000 train_mle_loss: 2.691 test_mle_loss: 2.061 reward: 0.000 \n",
      "\n",
      "2020-01-04 16:32:05 - Text-Summary - INFO: - epoch: 12 iter: 28000 train_mle_loss: 2.604 test_mle_loss: 2.008 reward: 0.000 \n",
      "\n",
      "2020-01-04 16:38:39 - Text-Summary - INFO: - epoch: 13 iter: 29000 train_mle_loss: 2.654 test_mle_loss: 2.008 reward: 0.000 \n",
      "\n",
      "2020-01-04 16:45:06 - Text-Summary - INFO: - epoch: 13 iter: 30000 train_mle_loss: 2.626 test_mle_loss: 2.043 reward: 0.000 \n",
      "\n",
      "2020-01-04 16:51:50 - Text-Summary - INFO: - epoch: 13 iter: 31000 train_mle_loss: 2.516 test_mle_loss: 1.924 reward: 0.000 \n",
      "\n",
      "2020-01-04 16:58:29 - Text-Summary - INFO: - epoch: 14 iter: 32000 train_mle_loss: 2.559 test_mle_loss: 1.913 reward: 0.000 \n",
      "\n",
      "2020-01-04 17:04:47 - Text-Summary - INFO: - epoch: 14 iter: 33000 train_mle_loss: 2.526 test_mle_loss: 1.944 reward: 0.000 \n",
      "\n",
      "2020-01-04 17:11:23 - Text-Summary - INFO: - epoch: 15 iter: 34000 train_mle_loss: 2.593 test_mle_loss: 1.805 reward: 0.000 \n",
      "\n",
      "2020-01-04 17:17:40 - Text-Summary - INFO: - epoch: 15 iter: 35000 train_mle_loss: 2.465 test_mle_loss: 1.915 reward: 0.000 \n",
      "\n",
      "2020-01-04 17:24:22 - Text-Summary - INFO: - epoch: 15 iter: 36000 train_mle_loss: 2.411 test_mle_loss: 1.868 reward: 0.000 \n",
      "\n",
      "2020-01-04 17:31:09 - Text-Summary - INFO: - epoch: 16 iter: 37000 train_mle_loss: 2.429 test_mle_loss: 1.678 reward: 0.000 \n",
      "\n",
      "2020-01-04 17:37:34 - Text-Summary - INFO: - epoch: 16 iter: 38000 train_mle_loss: 2.216 test_mle_loss: 1.779 reward: 0.000 \n",
      "\n",
      "2020-01-04 17:44:22 - Text-Summary - INFO: - epoch: 17 iter: 39000 train_mle_loss: 2.469 test_mle_loss: 1.654 reward: 0.000 \n",
      "\n",
      "2020-01-04 17:50:58 - Text-Summary - INFO: - epoch: 17 iter: 40000 train_mle_loss: 2.119 test_mle_loss: 1.636 reward: 0.000 \n",
      "\n",
      "2020-01-04 17:57:34 - Text-Summary - INFO: - epoch: 18 iter: 41000 train_mle_loss: 2.418 test_mle_loss: 1.602 reward: 0.000 \n",
      "\n",
      "2020-01-04 18:04:11 - Text-Summary - INFO: - epoch: 18 iter: 42000 train_mle_loss: 2.233 test_mle_loss: 1.563 reward: 0.000 \n",
      "\n",
      "2020-01-04 18:10:33 - Text-Summary - INFO: - epoch: 18 iter: 43000 train_mle_loss: 2.163 test_mle_loss: 1.608 reward: 0.000 \n",
      "\n",
      "2020-01-04 18:17:02 - Text-Summary - INFO: - epoch: 19 iter: 44000 train_mle_loss: 2.155 test_mle_loss: 1.573 reward: 0.000 \n",
      "\n",
      "2020-01-04 18:23:29 - Text-Summary - INFO: - epoch: 19 iter: 45000 train_mle_loss: 2.111 test_mle_loss: 1.672 reward: 0.000 \n",
      "\n",
      "2020-01-04 18:30:02 - Text-Summary - INFO: - epoch: 20 iter: 46000 train_mle_loss: 2.122 test_mle_loss: 1.593 reward: 0.000 \n",
      "\n",
      "2020-01-04 18:36:46 - Text-Summary - INFO: - epoch: 20 iter: 47000 train_mle_loss: 2.065 test_mle_loss: 1.454 reward: 0.000 \n",
      "\n",
      "2020-01-04 18:43:24 - Text-Summary - INFO: - epoch: 20 iter: 48000 train_mle_loss: 2.031 test_mle_loss: 1.515 reward: 0.000 \n",
      "\n",
      "2020-01-04 18:49:57 - Text-Summary - INFO: - epoch: 21 iter: 49000 train_mle_loss: 2.016 test_mle_loss: 1.566 reward: 0.000 \n",
      "\n",
      "2020-01-04 18:56:43 - Text-Summary - INFO: - epoch: 21 iter: 50000 train_mle_loss: 1.956 test_mle_loss: 1.555 reward: 0.000 \n",
      "\n",
      "2020-01-04 19:03:34 - Text-Summary - INFO: - epoch: 22 iter: 51000 train_mle_loss: 1.869 test_mle_loss: 1.411 reward: 0.000 \n",
      "\n",
      "2020-01-04 19:09:50 - Text-Summary - INFO: - epoch: 22 iter: 52000 train_mle_loss: 1.782 test_mle_loss: 1.495 reward: 0.000 \n",
      "\n",
      "2020-01-04 19:16:22 - Text-Summary - INFO: - epoch: 23 iter: 53000 train_mle_loss: 1.974 test_mle_loss: 1.375 reward: 0.000 \n",
      "\n",
      "2020-01-04 19:22:58 - Text-Summary - INFO: - epoch: 23 iter: 54000 train_mle_loss: 1.855 test_mle_loss: 1.470 reward: 0.000 \n",
      "\n",
      "2020-01-04 19:29:22 - Text-Summary - INFO: - epoch: 23 iter: 55000 train_mle_loss: 1.655 test_mle_loss: 1.367 reward: 0.000 \n",
      "\n",
      "2020-01-04 19:35:52 - Text-Summary - INFO: - epoch: 24 iter: 56000 train_mle_loss: 1.822 test_mle_loss: 1.377 reward: 0.000 \n",
      "\n",
      "2020-01-04 19:42:19 - Text-Summary - INFO: - epoch: 24 iter: 57000 train_mle_loss: 1.605 test_mle_loss: 1.390 reward: 0.000 \n",
      "\n",
      "2020-01-04 19:48:45 - Text-Summary - INFO: - epoch: 25 iter: 58000 train_mle_loss: 1.742 test_mle_loss: 1.414 reward: 0.000 \n",
      "\n",
      "2020-01-04 19:55:24 - Text-Summary - INFO: - epoch: 25 iter: 59000 train_mle_loss: 1.546 test_mle_loss: 1.417 reward: 0.000 \n",
      "\n",
      "2020-01-04 20:01:51 - Text-Summary - INFO: - epoch: 25 iter: 60000 train_mle_loss: 1.496 test_mle_loss: 1.277 reward: 0.000 \n",
      "\n",
      "2020-01-04 20:08:19 - Text-Summary - INFO: - epoch: 26 iter: 61000 train_mle_loss: 1.750 test_mle_loss: 1.259 reward: 0.000 \n",
      "\n",
      "2020-01-04 20:14:52 - Text-Summary - INFO: - epoch: 26 iter: 62000 train_mle_loss: 1.491 test_mle_loss: 1.284 reward: 0.000 \n",
      "\n",
      "2020-01-04 20:21:19 - Text-Summary - INFO: - epoch: 27 iter: 63000 train_mle_loss: 1.451 test_mle_loss: 1.195 reward: 0.000 \n",
      "\n",
      "2020-01-04 20:22:11 - Text-Summary - INFO: - -------------------Ignore error------------------\n",
      "CUDA error: device-side assert triggered\n",
      "\n",
      "2020-01-04 20:22:11 - Text-Summary - INFO: - ------Training END--------\n",
      "2020-01-04 20:22:11 - Text-Summary - INFO: - logger已關閉\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please load final_file_path : /0060000_1.28_0.00.tar\n"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/u012869752/article/details/72513141\n",
    "# 由于在jupyter notebook中，args不为空\n",
    "from glob import glob\n",
    "# nvidia-smi -pm 1\n",
    "if __name__ == \"__main__\":   \n",
    "    try:\n",
    "        # --------------------------Training ----------------------------------\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--train_mle', type=bool, default=True)\n",
    "        parser.add_argument('--train_rl', type=bool, default=False)\n",
    "        parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "#         parser.add_argument('--load_model', type=str, default='/0065000_1.64_0.00.tar')\n",
    "        parser.add_argument('--load_model', type=str, default=None)\n",
    "        parser.add_argument('--new_lr', type=float, default=None)\n",
    "        parser.add_argument('--multi_device', type=bool, default=True)\n",
    "        parser.add_argument('--view', type=bool, default=True)\n",
    "        parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "        parser.add_argument('--word_emb_type', type=str, default='glove')\n",
    "        parser.add_argument('--train_action', type=bool, default=True)\n",
    "        opt = parser.parse_args(args=[])\n",
    "        \n",
    "        today = dt.now()\n",
    "        loggerPath = \"LOG/%s-(%s_%s_%s)-(%s:%s:%s)\"%(opt.word_emb_type,\n",
    "                  today.year,today.month,today.day,\n",
    "                  today.hour,today.minute,today.second)\n",
    "\n",
    "        logger = getLogger(config.loggerName,loggerPath)   \n",
    "        if not opt.load_model:\n",
    "            print('clear history')\n",
    "            shutil.rmtree('runs/Pointer-Generator/glove/exp-3', ignore_errors=True) # clear previous \n",
    "        \n",
    "        writer = SummaryWriter('runs/Pointer-Generator/glove/exp-3')\n",
    "        if opt.train_action: train_action(opt)\n",
    "        \n",
    "#         if not opt.load_model:\n",
    "#             shutil.rmtree('runs/Pointer-Generator/bert', ignore_errors=True) # clear previous \n",
    "#         # --------------------------Testing ----------------------------------\n",
    "#         parser = argparse.ArgumentParser()\n",
    "#         parser.add_argument(\"--task\", type=str, default=\"validate\", choices=[\"validate\",\"test\"])\n",
    "#         parser.add_argument(\"--start_from\", type=int, default=\"0020000\")\n",
    "# #         parser.add_argument(\"--load_model\", type=str, default=None)\n",
    "#         parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "#         parser.add_argument('--word_emb_type', type=str, default='bert')\n",
    "#         opt = parser.parse_args(args=[])                \n",
    "#         if opt.word_emb_type == 'bert': config.emb_dim = 768\n",
    "#         test_action(opt)\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback = sys.exc_info()[2]\n",
    "        print(sys.exc_info())\n",
    "        print(traceback.tb_lineno)\n",
    "        print(e)\n",
    "    finally:\n",
    "        removeLogger(logger)\n",
    "        # export scalar data to JSON for external processing\n",
    "        # tensorboard --logdir /home/eagleuser/Users/leyan/Text-Summarizer-FOP/TensorBoard\n",
    "#         tensorboard --logdir ./runs\n",
    "#         if not os.path.exists('TensorBoard'): os.makedirs('TensorBoard')\n",
    "#         writer.export_scalars_to_json(\"TensorBoard/test.json\")\n",
    "        writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Evaluate(object):\n",
    "#     def __init__(self, data_path, opt, batch_size = config.batch_size):\n",
    "#         self.vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "#         self.batcher = Batcher(data_path, self.vocab, mode='eval',\n",
    "#                                batch_size=batch_size, single_pass=True)\n",
    "#         self.opt = opt\n",
    "#         time.sleep(5)\n",
    "\n",
    "#     def setup_valid(self):\n",
    "#         self.model = Model(opt.pre_train_emb,opt.word_emb_type,self.vocab)\n",
    "#         self.model = get_cuda(self.model)\n",
    "#         checkpoint = T.load(os.path.join(config.save_model_path, self.opt.load_model))\n",
    "#         self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "\n",
    "\n",
    "# #     def print_original_predicted(self, decoded_sents, ref_sents, article_sents, loadfile):\n",
    "# #         filename = \"test_\"+loadfile.split(\".\")[0]+\".txt\"\n",
    "    \n",
    "# #         with open(os.path.join(\"bin\",filename), \"w\") as f:\n",
    "# #             for i in range(len(decoded_sents)):\n",
    "# #                 f.write(\"article: \"+article_sents[i] + \"\\n\")\n",
    "# #                 f.write(\"ref: \" + ref_sents[i] + \"\\n\")\n",
    "# #                 f.write(\"dec: \" + decoded_sents[i] + \"\\n\\n\")\n",
    "\n",
    "#     def evaluate_batch(self, model_iter,print_sents = False):\n",
    "\n",
    "#         self.setup_valid()\n",
    "#         batch = self.batcher.next_batch()\n",
    "#         start_id = self.vocab.word2id(data.START_DECODING)\n",
    "#         end_id = self.vocab.word2id(data.STOP_DECODING)\n",
    "#         unk_id = self.vocab.word2id(data.UNKNOWN_TOKEN)\n",
    "#         decoded_sents = []\n",
    "#         ref_sents = []\n",
    "#         article_sents = []\n",
    "#         rouge = Rouge()\n",
    "#         batch_id = 0\n",
    "#         while batch is not None:\n",
    "#             enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, ct_e = get_enc_data(batch)\n",
    "\n",
    "#             with T.autograd.no_grad():\n",
    "#                 enc_batch = self.model.embeds(enc_batch)\n",
    "#                 enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "# #                 writer.add_graph(self.model.encoder, (enc_batch, enc_lens))\n",
    "\n",
    "\n",
    "#             #-----------------------Summarization----------------------------------------------------\n",
    "#             with T.autograd.no_grad():\n",
    "#                 pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, self.model, start_id, end_id, unk_id)\n",
    "\n",
    "#             for i in range(len(pred_ids)):\n",
    "#                 decoded_words = data.outputids2words(pred_ids[i], self.vocab, batch.rev_oovs[i])\n",
    "#                 if len(decoded_words) < 2:\n",
    "#                     decoded_words = \"xxx\"\n",
    "#                 else:\n",
    "#                     decoded_words = \" \".join(decoded_words)\n",
    "#                 decoded_sents.append(decoded_words)\n",
    "#                 summary = batch.original_summarys[i]\n",
    "#                 review = batch.original_reviews[i]\n",
    "#                 ref_sents.append(summary)\n",
    "#                 article_sents.append(review)\n",
    "# #                 logger.info('----------------- batch decode %s-----------------'%(i))\n",
    "# #                 logger.info('review :\\n%s \\n'%(review))\n",
    "# #                 logger.info('summary :\\n%s \\n'%(summary))\n",
    "# #                 logger.info('decoded_words :\\n%s \\n'%(decoded_words))\n",
    "# #                 logger.info('batch decode %s ...'%(i))\n",
    "    \n",
    "#             writer.add_text('Test/model_iter_%s/'% (int(model_iter)), decoded_sents[batch_id] , batch_id)\n",
    "#             writer.add_text('Test/model_iter_%s/'% (int(model_iter)), ref_sents[batch_id] , batch_id)\n",
    "#             writer.add_text('Test/model_iter_%s/'% (int(model_iter)), article_sents[batch_id] , batch_id)\n",
    "\n",
    "\n",
    "#             batch = self.batcher.next_batch()\n",
    "#             batch_id += 1\n",
    "#         load_file = self.opt.load_model\n",
    "\n",
    "# #         if print_sents:\n",
    "# #             self.print_original_predicted(decoded_sents, ref_sents, article_sents, load_file)\n",
    "\n",
    "#         score = rouge.get_scores(decoded_sents, ref_sents, avg = True)\n",
    "#         if self.opt.task == \"test\":\n",
    "# #             print(load_file, \"score:\", score)\n",
    "#             logger.info(load_file + \" score: \" + score)\n",
    "#         else:\n",
    "#             logger.info('--------------- %s -----------------'%(load_file))\n",
    "#             logger.info('\\nROUGE-1: f:%4f p:%4f r:%4f , \\nROUGE-2: f:%4f p:%4f r:%4f , \\nROUGE-L: f:%4f p:%4f r:%4f \\n' % ( \\\n",
    "#             score['rouge-1']['f'], score['rouge-1']['p'], score['rouge-1']['r'], \\\n",
    "#             score['rouge-2']['f'], score['rouge-2']['p'], score['rouge-2']['r'], \\\n",
    "#             score['rouge-l']['f'], score['rouge-l']['p'], score['rouge-l']['r']))\n",
    "#             logger.info('------------------------------------')\n",
    "#             writer.add_scalars('Validate/rouge-1',  # 'rouge-2' , 'rouge-l'\n",
    "#                    {'f': score['rouge-1']['f'],\n",
    "#                     'p': score['rouge-1']['p'],\n",
    "#                     'r': score['rouge-1']['r']}\n",
    "#                     , int(model_iter))\n",
    "#             writer.add_scalars('Validate/rouge-2',  # 'rouge-2' , 'rouge-l'\n",
    "#                    {'f': score['rouge-2']['f'],\n",
    "#                     'p': score['rouge-2']['p'],\n",
    "#                     'r': score['rouge-2']['r']}\n",
    "#                     , int(model_iter))\n",
    "#             writer.add_scalars('Validate/rouge-l',  # 'rouge-2' , 'rouge-l'\n",
    "#                    {'f': score['rouge-l']['f'],\n",
    "#                     'p': score['rouge-l']['p'],\n",
    "#                     'r': score['rouge-l']['r']}\n",
    "#                     , int(model_iter))         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_action(opt):\n",
    "# #     opt.iter = int(opt.start_from)\n",
    "#     if opt.task == \"validate\":\n",
    "#         logger.info(u'------Validate START--------')  \n",
    "# #             saved_models = os.listdir(config.save_model_path + \"/%s\"%(opt.word_emb_type))\n",
    "#         saved_models = glob(config.save_model_path + \"/%s/*.tar\"%(opt.word_emb_type))\n",
    "#         saved_models = sorted(saved_models)\n",
    "#         saved_models_dict = {}\n",
    "#         for model in saved_models:\n",
    "#             model_iter = model.split(\"_\")[1].split(\"/\")[-1]                \n",
    "#             if int(model_iter) < int(opt.start_from): continue\n",
    "#             model = \"/\".join(model.split(\"/\")[2:])\n",
    "#             saved_models_dict[model_iter] = model   \n",
    "\n",
    "#         for iter,f in saved_models_dict.items():\n",
    "#             opt.load_model = f\n",
    "#             eval_processor = Evaluate(config.valid_data_path, opt)\n",
    "#             eval_processor.evaluate_batch(iter)\n",
    "# #                 break\n",
    "#         logger.info(u'------Validate END--------')  \n",
    "#     else:   #test\n",
    "#         logger.info(u'------Test START--------')  \n",
    "#         eval_processor = Evaluate(config.test_data_path, opt)\n",
    "#         eval_processor.evaluate_batch()\n",
    "#         logger.info(u'------Test END--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
