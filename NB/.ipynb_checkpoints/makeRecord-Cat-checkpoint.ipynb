{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# from product import *\n",
    "# from data_util.product import *\n",
    "from data_util.mainCat import *\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "import nltk\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "# from product import *\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "CHUNK_SIZE = 1000  # num examples per chunk, for the chunked data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key word Attention DataSet 讀取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLSX/main_cat/Health & Personal Care_key.xlsx Read finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_ID</th>\n",
       "      <th>review</th>\n",
       "      <th>summary</th>\n",
       "      <th>big_categories</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>small_categories</th>\n",
       "      <th>lemm_review</th>\n",
       "      <th>lemm_summary</th>\n",
       "      <th>lemm_review_len</th>\n",
       "      <th>lemm_summary_len</th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>total_keyword</th>\n",
       "      <th>FOP_sents</th>\n",
       "      <th>total_mention_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1392940800</td>\n",
       "      <td>I have 2 Crane Humidifiers at home. This filte...</td>\n",
       "      <td>Great filter for both old and new Crane Design</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>have crane humidifier at home.\\nthis filter wi...</td>\n",
       "      <td>&lt;s&gt; great filter for both old and new crane de...</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>model new,model old</td>\n",
       "      <td>this filter will fit both the new and old model.</td>\n",
       "      <td>humidifier crane filter model fit base water h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1211932800</td>\n",
       "      <td>We have the \"Hello Kitty\" humidifier and I bou...</td>\n",
       "      <td>Only made things worse.</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>have the hello and buy this filter the hope re...</td>\n",
       "      <td>&lt;s&gt; only make thing worse &lt;/s&gt;</td>\n",
       "      <td>93</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>room brown,color dingy,color room,room dingy,w...</td>\n",
       "      <td>have the hello and buy this filter the hope re...</td>\n",
       "      <td>yellow color turn room hope brown white water ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1304208000</td>\n",
       "      <td>Very helpful if you have hard water or humidif...</td>\n",
       "      <td>Good product</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>very helpful if have hard water humidifier cre...</td>\n",
       "      <td>&lt;s&gt; good product &lt;/s&gt;</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cleaning water humidifier build create mainten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1296950400</td>\n",
       "      <td>These work great....leave a lot less residue i...</td>\n",
       "      <td>Am I really reviewing a filter?</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>these work great leave lot less residue humidi...</td>\n",
       "      <td>&lt;s&gt; really review filter &lt;/s&gt;</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>lot great href normal,heel high,link more righ...</td>\n",
       "      <td>these work great leave lot less residue humidi...</td>\n",
       "      <td>room residue humidifier leave kid lot find clo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1235001600</td>\n",
       "      <td>I bought this with the Penquin humidifier beca...</td>\n",
       "      <td>Watch out!</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>buy this with the humidifier because with the ...</td>\n",
       "      <td>&lt;s&gt; watch out &lt;/s&gt;</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>humidifier buy set purchase model watch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_ID                                             review  \\\n",
       "0  1392940800  I have 2 Crane Humidifiers at home. This filte...   \n",
       "1  1211932800  We have the \"Hello Kitty\" humidifier and I bou...   \n",
       "2  1304208000  Very helpful if you have hard water or humidif...   \n",
       "3  1296950400  These work great....leave a lot less residue i...   \n",
       "4  1235001600  I bought this with the Penquin humidifier beca...   \n",
       "\n",
       "                                          summary big_categories  \\\n",
       "0  Great filter for both old and new Crane Design     Appliances   \n",
       "1                         Only made things worse.     Appliances   \n",
       "2                                    Good product     Appliances   \n",
       "3                 Am I really reviewing a filter?     Appliances   \n",
       "4                                      Watch out!     Appliances   \n",
       "\n",
       "                 main_cat     small_categories  \\\n",
       "0  Health & Personal Care  Parts & Accessories   \n",
       "1  Health & Personal Care  Parts & Accessories   \n",
       "2  Health & Personal Care  Parts & Accessories   \n",
       "3  Health & Personal Care  Parts & Accessories   \n",
       "4  Health & Personal Care  Parts & Accessories   \n",
       "\n",
       "                                         lemm_review  \\\n",
       "0  have crane humidifier at home.\\nthis filter wi...   \n",
       "1  have the hello and buy this filter the hope re...   \n",
       "2  very helpful if have hard water humidifier cre...   \n",
       "3  these work great leave lot less residue humidi...   \n",
       "4  buy this with the humidifier because with the ...   \n",
       "\n",
       "                                        lemm_summary  lemm_review_len  \\\n",
       "0  <s> great filter for both old and new crane de...               45   \n",
       "1                     <s> only make thing worse </s>               93   \n",
       "2                              <s> good product </s>               66   \n",
       "3                      <s> really review filter </s>               49   \n",
       "4                                 <s> watch out </s>               25   \n",
       "\n",
       "   lemm_summary_len  overall  vote  \\\n",
       "0                11        5     4   \n",
       "1                 6        1     7   \n",
       "2                 4        4     2   \n",
       "3                 5        5     3   \n",
       "4                 4        1     3   \n",
       "\n",
       "                                       total_keyword  \\\n",
       "0                                model new,model old   \n",
       "1  room brown,color dingy,color room,room dingy,w...   \n",
       "2                                                NaN   \n",
       "3  lot great href normal,heel high,link more righ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                           FOP_sents  \\\n",
       "0   this filter will fit both the new and old model.   \n",
       "1  have the hello and buy this filter the hope re...   \n",
       "2                                                NaN   \n",
       "3  these work great leave lot less residue humidi...   \n",
       "4                                                NaN   \n",
       "\n",
       "                              total_mention_features  \n",
       "0  humidifier crane filter model fit base water h...  \n",
       "1  yellow color turn room hope brown white water ...  \n",
       "2  cleaning water humidifier build create mainten...  \n",
       "3  room residue humidifier leave kid lot find clo...  \n",
       "4            humidifier buy set purchase model watch  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main_cat = All_Electronics().getAttr()\n",
    "# main_cat = Pet_Supplies().getAttr()\n",
    "# main_cat = Sports_Outdoors().getAttr()\n",
    "main_cat = Health_personal_Care().getAttr()\n",
    "\n",
    "xlsx_path = \"XLSX/main_cat/%s_key.xlsx\"%(main_cat)\n",
    "# df.to_csv(csv_path) #默认dt是DataFrame的一个实例，参数解释如下\n",
    "# key_train_df.to_excel(csv_path, encoding='utf8')\n",
    "orign_key_df = pd.read_excel(xlsx_path)\n",
    "print(xlsx_path + \" Read finished\")\n",
    "len(orign_key_df)\n",
    "orign_key_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key word load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'FOP-View/%s_keywords2.txt' % (main_cat)\n",
    "print('load %s keywords...' % (fn))\n",
    "total_keywords = set()\n",
    "with open(fn, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        k, v = line.split(\":\")\n",
    "        total_keywords.add(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_lexicon = {}\n",
    "for filename in os.listdir('opinion-lexicon-English/'):      \n",
    "    if \"txt\" not in filename: continue\n",
    "    print(filename)\n",
    "    with open('opinion-lexicon-English/'+filename,'r') as f_input:\n",
    "        lexion = []\n",
    "        for line in f_input:\n",
    "            if line.startswith(\";\"):\n",
    "                continue\n",
    "            word = line.replace(\"\\n\",\"\")\n",
    "            if word != \"\" : lexion.append(word)\n",
    "        pos = filename.replace(\".txt\",\"\")\n",
    "        opinion_lexicon[pos] = lexion\n",
    "\n",
    "opinion_lexicon[\"total-words\"] = opinion_lexicon[\"negative-words\"] + opinion_lexicon[\"positive-words\"]\n",
    "print(\"total-words 已取得\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary 資料清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_summary(x):\n",
    "    x = x.replace(\"\\n\",\"\").replace(\"</s>\",\"\").replace(\"<s>\",\"\")\n",
    "    x = \"<s>\" + x + \"</s>\"  \n",
    "    x = \" \".join([str(token) for token in nlp(x) if (\" \" not in str(token)) and \\\n",
    "                  (str(token).isalpha()) and \\\n",
    "                  (len(str(token)) > 1)   ])\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_custom_tokenizer(nlp):\n",
    "    prefix_re = re.compile(r'[0-9]\\.')\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search)\n",
    "\n",
    "def calc_summary_len(x):\n",
    "#     tokens = [token for token in nlp(x)]\n",
    "#     print(tokens)\n",
    "#     print([len(t) for t in tokens])\n",
    "#     return len(tokens)\n",
    "    return len(x.split(\" \"))\n",
    "\n",
    "nlp.tokenizer = create_custom_tokenizer(nlp)\n",
    "\n",
    "orign_key_df['lemm_summary'] = orign_key_df['lemm_summary'].apply(compose_summary)\n",
    "orign_key_df['lemm_summary_len'] = orign_key_df['lemm_summary'].apply(calc_summary_len)\n",
    "\n",
    "\n",
    "amount = len(orign_key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "\n",
    "orign_key_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# review 多句合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def compose_review(x):\n",
    "    x = eval(x)\n",
    "    x = \"\\n\".join(x)\n",
    "    x = x.replace(\"\\n\",\" \")    \n",
    "    tokens = [str(token) for token in x.split(\" \") if (\" \" not in str(token))and (str(token) == '.' or str(token).isalpha())]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def calc_review_len(x):\n",
    "#     tokens = [token for token in nlp(x)]\n",
    "#     print(tokens)\n",
    "#     print([len(t) for t in tokens])\n",
    "#     return len(tokens)\n",
    "    return len(x.split(\" \"))\n",
    "key_df = deepcopy(orign_key_df)\n",
    "key_df['lemm_review'] = key_df['lemm_review'].apply(compose_review)\n",
    "key_df['lemm_review_len'] = key_df['lemm_review'].apply(calc_review_len)\n",
    "key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orign_key_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 過濾不合適的訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(text):\n",
    "    keywords = set()\n",
    "    for words in text.split(\",\"):\n",
    "        for word in words.split(\" \"):\n",
    "            keywords.add(word)\n",
    "    keywords = \" \".join(keywords)\n",
    "    return keywords\n",
    "\n",
    "def calc_keyword_num(x):\n",
    "    return len(x.split(\" \"))\n",
    "\n",
    "# and(key_df.lemm_review_len>20)\n",
    "flit_key_df = key_df[(key_df.lemm_summary_len>=4) ] # 過濾single word summary\n",
    "flit_key_df = flit_key_df[(flit_key_df.lemm_review_len <= 1000) ] # 過濾single word summary\n",
    "\n",
    "flit_key_df = flit_key_df.dropna(\n",
    "    axis=0,     # 0: 对行进行操作; 1: 对列进行操作\n",
    "    how='any'   # 'any': 只要存在 NaN 就 drop 掉; 'all': 必须全部是 NaN 才 drop \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOP_keywords 資料整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flit_key_df['FOP_keywords'] = flit_key_df['total_keyword']\n",
    "flit_key_df['FOP_keywords'] = flit_key_df['FOP_keywords'].apply(to_words)\n",
    "flit_key_df['FOP_keywords_num'] = flit_key_df['FOP_keywords'].apply(calc_keyword_num)\n",
    "flit_key_df = flit_key_df[(flit_key_df.FOP_keywords_num>=2) ] # 過濾single word summary\n",
    "flit_key_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flit_key_df['Cheat'] = False \n",
    "\n",
    "# flit_key_df.head()\n",
    "from tqdm import tqdm\n",
    "# 非符號alpha word重疊數\n",
    "with tqdm(total=len(flit_key_df)) as pbar:\n",
    "    for i ,row in flit_key_df.iterrows():\n",
    "        rev_tokens = set(row['lemm_review'].split(\" \"))\n",
    "#         if 's' in rev_tokens: rev_tokens.remove('s')\n",
    "#         if '.' in rev_tokens: rev_tokens.remove('.')\n",
    "#         if 'i' in rev_tokens: rev_tokens.remove('i')\n",
    "#         if 'a' in rev_tokens: rev_tokens.remove('a')\n",
    "#         if 'the' in rev_tokens: rev_tokens.remove('the')\n",
    "        \n",
    "        summ_tokens = set(row['lemm_summary'].split(\" \"))\n",
    "        key_sets = rev_tokens & summ_tokens & (total_keywords| set(opinion_lexicon[\"total-words\"]))\n",
    "        if len(key_sets) > 2: \n",
    "#             print(True)\n",
    "            flit_key_df.loc[i,'Cheat'] = True\n",
    "\n",
    "#         print(rev_tokens)\n",
    "#         print(summ_tokens)\n",
    "#         print([len(t) for t in summ_tokens])\n",
    "        pbar.update(1)\n",
    "    \n",
    "flit_key_df = flit_key_df[(flit_key_df.Cheat == True) ] # 過濾single word summary\n",
    "amount = len(flit_key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "flit_key_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank_keywords 資料整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords as TextRank\n",
    "from summa.summarizer import summarize\n",
    "def textrank_keys(text):\n",
    "    keywords1 = list()\n",
    "    for words in TextRank.keywords(text).split('\\n'):\n",
    "        keywords1.extend(words.split(\" \"))\n",
    "    keywords1 = set(keywords1)    \n",
    "    \n",
    "    return \" \".join(list(keywords1))\n",
    "\n",
    "def textrank_summ_keys(text): \n",
    "    keywords2 = list()\n",
    "    for words in summarize(text, words=8).split('\\n'):\n",
    "        keywords2.extend(words.split(\" \"))\n",
    "    keywords2 = set(keywords2)\n",
    "    \n",
    "    return \" \".join(list(keywords2))\n",
    "\n",
    "# flit_key_df['TextRank_keywords'] = flit_key_df['lemm_review'].apply(textrank_to_words)\n",
    "# flit_key_df['TextRank_keywords_num'] = flit_key_df['TextRank_keywords'].apply(calc_num)\n",
    "flit_key_df['TextRank_keywords'] = flit_key_df['FOP_keywords'] \n",
    "flit_key_df['TextRank_keywords_num'] = flit_key_df['FOP_keywords_num'] \n",
    "flit_key_df.loc[:,'TextRank_keywords'] = ''\n",
    "flit_key_df.loc[:,'TextRank_keywords_num'] = 0\n",
    "flit_key_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "with tqdm(total=len(flit_key_df)) as pbar:\n",
    "    for i ,row in flit_key_df.iterrows():\n",
    "        TextRank_keywords = textrank_keys(row['lemm_review'])\n",
    "    #     TextRank_keywords = textrank_summ_keys(row['lemm_review'])  \n",
    "        num = calc_keyword_num(TextRank_keywords)\n",
    "        flit_key_df.loc[i,'TextRank_keywords'] = TextRank_keywords\n",
    "        flit_key_df.loc[i,'TextRank_keywords_num'] = num\n",
    "        pbar.update(1)\n",
    "        \n",
    "flit_key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 輸出統計長度資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "if not os.path.exists('XLSX/statistic'):\n",
    "    os.makedirs('XLSX/statistic')\n",
    "with open('XLSX/statistic/%s_info.txt'%(main_cat),'w') as f:\n",
    "    max_rev_len = flit_key_df['lemm_review_len'].max()\n",
    "    min_rev_len = flit_key_df['lemm_review_len'].min()\n",
    "    mean_rev_len = flit_key_df['lemm_review_len'].mean()\n",
    "    median_rev_len = flit_key_df['lemm_review_len'].median()\n",
    "\n",
    "    f.write('max_rev_len :%s \\n'%(max_rev_len))\n",
    "    f.write('min_rev_len :%s \\n'%(min_rev_len))\n",
    "    f.write('mean_rev_len :%s \\n'%(mean_rev_len))\n",
    "    f.write('median_rev_len :%s \\n'%(median_rev_len))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_summary_len = flit_key_df['lemm_summary_len'].max()\n",
    "    min_summary_len = flit_key_df['lemm_summary_len'].min()\n",
    "    mean_summary_len = flit_key_df['lemm_summary_len'].mean()\n",
    "    median_summary_len = flit_key_df['lemm_summary_len'].median()\n",
    "\n",
    "    f.write('max_summary_len :%s \\n'%(max_summary_len))\n",
    "    f.write('min_summary_len :%s \\n'%(min_summary_len))\n",
    "    f.write('mean_summary_len :%s \\n'%(mean_summary_len))\n",
    "    f.write('median_summary_len :%s \\n'%(median_summary_len))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_FOP_keywords_num = flit_key_df['FOP_keywords_num'].max()\n",
    "    min_FOP_keywords_num = flit_key_df['FOP_keywords_num'].min()\n",
    "    mean_FOP_keywords_num = flit_key_df['FOP_keywords_num'].mean()\n",
    "    median_FOP_keywords_num = flit_key_df['FOP_keywords_num'].median()\n",
    "\n",
    "    f.write('max_FOP_keywords_num :%s \\n'%(max_FOP_keywords_num))\n",
    "    f.write('min_FOP_keywords_num :%s \\n'%(min_FOP_keywords_num))\n",
    "    f.write('mean_FOP_keywords_num :%s \\n'%(mean_FOP_keywords_num))\n",
    "    f.write('median_FOP_keywords_num :%s \\n'%(median_FOP_keywords_num))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].max()\n",
    "    min_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].min()\n",
    "    mean_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].mean()\n",
    "    median_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].median()\n",
    "\n",
    "    f.write('max_TextRank_keywords_num :%s \\n'%(max_TextRank_keywords_num))\n",
    "    f.write('min_TextRank_keywords_num :%s \\n'%(min_TextRank_keywords_num))\n",
    "    f.write('mean_TextRank_keywords_num :%s \\n'%(mean_TextRank_keywords_num))\n",
    "    f.write('median_TextRank_keywords_num :%s \\n'%(median_TextRank_keywords_num))\n",
    "\n",
    "    \n",
    "\n",
    "# plt.xlim(xmax = mean_rev_len)\n",
    "# plt.ylim(ymax = flit_key_df['lemm_review_len'].value_counts().max())\n",
    "\n",
    "flit_key_df['lemm_review_len'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/review_len_%s.png'%(main_cat))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# plt.xlim(xmax = max_summary_len)\n",
    "# plt.ylim(ymax = flit_key_df['lemm_summary_len'].value_counts().max())\n",
    "flit_key_df['lemm_summary_len'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/summary_len_%s.png'%(main_cat))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# plt.xlim(xmax = mean_keyword_num)\n",
    "flit_key_df['FOP_keywords_num'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/FOP_keywords_num_%s.png'%(main_cat))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "flit_key_df['TextRank_keywords_num'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/TextRank_keywords_num_%s.png'%(main_cat))\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 製作record bin檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if os.path.exists('bin'):\n",
    "    shutil.rmtree('/bin', ignore_errors=True)\n",
    "\n",
    "if not os.path.exists('bin/main_cat/chunked'):\n",
    "    os.makedirs('bin/main_cat/chunked')\n",
    "\n",
    "makevocab = True\n",
    "if makevocab:\n",
    "    vocab_counter = collections.Counter()\n",
    "    \n",
    "# train_file\n",
    "flit_key_train_df = flit_key_df.iloc[:int(amount*0.6)]\n",
    "\n",
    "# test_file\n",
    "flit_key_test_df = flit_key_df.iloc[int(amount*0.6)+1:int(amount*0.8)]\n",
    "\n",
    "# vald_file\n",
    "flit_key_valid_df = flit_key_df.iloc[int(amount*0.8)+1:]\n",
    "sentence_start = \"<s>\"\n",
    "sentence_end = \"</s>\"\n",
    "\n",
    "\n",
    "def xlsx2bin(set_name,df):\n",
    "    sents = []\n",
    "    with open(\"bin/main_cat/%s.bin\"%(set_name), 'wb') as file:\n",
    "        i = 0\n",
    "        for idx in tqdm(range(len(df))):\n",
    "            series = df.iloc[idx]\n",
    "            data_dict = series.to_dict()\n",
    "            review_ID , big_categories , small_categories , \\\n",
    "            review , lemm_review , summary , lemm_summary , FOP_keywords ,TextRank_keywords = \\\n",
    "            data_dict['review_ID'],data_dict['big_categories'],data_dict['small_categories'],data_dict['review'],data_dict['lemm_review'], \\\n",
    "            data_dict['summary'],data_dict['lemm_summary'],data_dict['FOP_keywords'] ,data_dict['TextRank_keywords']\n",
    "#             print(FOP_keywords)\n",
    "\n",
    "            # save Embedding/word2Vec calculate sents\n",
    "            for sent in nltk.sent_tokenize(lemm_review):\n",
    "                sent = sent.replace(\".\" ,\"\")\n",
    "#                 sents.append(str(sent).split()) # 切分词汇 \n",
    "\n",
    "            for sent in nltk.sent_tokenize(lemm_summary):\n",
    "                sent = sent.replace(sentence_start ,\"\").replace(sentence_end ,\"\")\n",
    "#                 sents.append(str(sent).split()) # 切分词汇 \n",
    "\n",
    "            lemm_review = lemm_review.replace(\"\\n\",\"\").replace(\".\",\" \")\n",
    "            lemm_summary = lemm_summary.replace(\"\\n\",\"\").replace(\".\",\" \")\n",
    "            lemm_summary = sentence_start + ' '+ lemm_summary + ' ' + sentence_end\n",
    "#             print(lemm_summary)\n",
    "            # Write to tf.Example\n",
    "            tf_example = example_pb2.Example()\n",
    "            try:\n",
    "                tf_example.features.feature['orign_review'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(review, encoding='utf-8')])\n",
    "\n",
    "                tf_example.features.feature['orign_summary'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(summary, encoding='utf-8')])\n",
    "                \n",
    "                tf_example.features.feature['review'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(lemm_review, encoding='utf-8')])\n",
    "\n",
    "                tf_example.features.feature['summary'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(lemm_summary, encoding='utf-8')]) \n",
    "        \n",
    "                tf_example.features.feature['FOP_keywords'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(FOP_keywords, encoding='utf-8')]) \n",
    "            \n",
    "                tf_example.features.feature['TextRank_keywords'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(TextRank_keywords, encoding='utf-8')]) \n",
    "\n",
    "                tf_example_str = tf_example.SerializeToString()\n",
    "                str_len = len(tf_example_str)  \n",
    "                file.write(struct.pack('q', str_len))\n",
    "                file.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "            except Exception as e:\n",
    "#                 print(e)\n",
    "                pass\n",
    "    print(\" %s finished... \"%(file.name))\n",
    "    return sents\n",
    "    \n",
    "    \n",
    "sents1 = xlsx2bin('train',flit_key_train_df)\n",
    "sents2 = xlsx2bin('test',flit_key_test_df)\n",
    "sents3 = xlsx2bin('valid',flit_key_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bin/main_cat/bin-info.txt\",'w',encoding='utf-8') as f :\n",
    "    f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割record bin檔(1000為單位)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(set_name, chunks_dir):\n",
    "    in_file = 'bin/main_cat/%s.bin' % set_name\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "#         chunk_fname = os.path.join('bin', '/%s/%s_%03d.bin' % (chunks_dir,set_name, chunk))  # new chunk\n",
    "        chunk_fname = '%s/%s/%s_%03d.bin' % (chunks_dir,set_name,set_name, chunk)\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(CHUNK_SIZE):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1\n",
    "\n",
    "\n",
    "def chunk_all(chunks_dir = 'bin/main_cat/chunked'):\n",
    "    # Make a dir to hold the chunks\n",
    "    \n",
    "    # Chunk the data\n",
    "    for set_name in ['train', 'valid', 'test']:\n",
    "        if not os.path.isdir(os.path.join(chunks_dir,set_name)):\n",
    "            os.mkdir(os.path.join(chunks_dir,set_name))\n",
    "        print(\"Splitting %s data into chunks...\" % set_name)\n",
    "        chunk_file(set_name, chunks_dir)\n",
    "    print(\"Saved chunked data in %s\" % chunks_dir)\n",
    "    \n",
    "chunk_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_valid():\n",
    "    #Performing rouge evaluation on 1.9 lakh sentences takes lot of time. So, create mini validation set & test set by borrowing 15k samples each from these 1.9 lakh sentences\n",
    "    bin_valid_chuncks = os.listdir('bin/main_cat/chunked/valid')\n",
    "    bin_valid_chuncks.sort()\n",
    "    if not os.path.exists('bin/main_cat/chunked/main_valid'):\n",
    "        os.mkdir('bin/main_cat/chunked/main_valid')\n",
    "        \n",
    "    samples = random.sample(set(bin_valid_chuncks[:-1]), 2)      #Exclude last bin file; contains only 9k sentences\n",
    "    valid_chunk, test_chunk = samples[0], samples[1]\n",
    "    shutil.copyfile(os.path.join('bin/main_cat/chunked/valid', valid_chunk), os.path.join(\"bin/main_cat/chunked/main_valid\", \"valid_00.bin\"))\n",
    "    shutil.copyfile(os.path.join('bin/main_cat/chunked/valid', test_chunk), os.path.join(\"bin/main_cat/chunked/main_valid\", \"test_00.bin\"))\n",
    "main_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [] # total sentence\n",
    "for idx in tqdm(range(len(orign_key_df))):\n",
    "    series = orign_key_df.iloc[idx]\n",
    "    data_dict = series.to_dict()\n",
    "    lemm_review_sents , lemm_summary  = data_dict['lemm_review'],data_dict['lemm_summary'] \n",
    "    lemm_review_sents = eval(lemm_review_sents)\n",
    "    for sent in lemm_review_sents:\n",
    "        sent_tokens = sent.split(\" \")\n",
    "        tokens = [str(token) for token in sent.split() if (\" \" not in str(token))and (str(token) == '.' or str(token).isalpha())]\n",
    "        sentences.append(tokens)     \n",
    "    sentences.append(lemm_summary.split(\" \"))\n",
    "print('word2Vec training sentence finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = sents1 + sents2 + sents3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 word2vec\n",
    "from gensim.models import word2vec\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsnooper\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "vocab_count = 50000\n",
    "# write vocab to file\n",
    "if not os.path.exists('Embedding/main_cat/word2Vec'):\n",
    "    os.makedirs('Embedding/main_cat/word2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"Embedding/main_cat/word2Vec/word2Vec.300d.txt\"):\n",
    "\n",
    "    w2vec = word2vec.Word2Vec(sentences, size=300, min_count=1,max_vocab_size=None,iter=100,\n",
    "                              sorted_vocab=1,max_final_vocab=vocab_count)\n",
    "\n",
    "    \n",
    "\n",
    "    w2vec.wv.save_word2vec_format('Embedding/main_cat/word2Vec/word2Vec.300d.txt', binary=False)\n",
    "\n",
    "    #保存模型，供日後使用\n",
    "    # w2vec.save(\"Embedding/word2Vec/word2vec.model\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型讀取方式\n",
    "# model = word2vec.Word2Vec.load(\"Embedding/word2Vec/word2vec.model\")\n",
    "\n",
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'Embedding/main_cat/word2Vec/word2Vec.300d.txt', binary=False, encoding='utf-8')\n",
    "\n",
    "wvmodel.most_similar(u\"player\", topn=10)\n",
    "# wvmodel.most_similar(['dvd','player','changer','machine','video'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = \"bin/main_cat/word.vocab\"\n",
    "\n",
    "if os.path.exists(vocab_file):\n",
    "    vocab_count = len(wvmodel.wv.index2entity)    \n",
    "\n",
    "    print(\"Writing vocab file...\")\n",
    "    with open(vocab_file, 'w',encoding='utf-8') as writer:\n",
    "        for word in wvmodel.wv.index2entity[:vocab_count]:\n",
    "            # print(word, w2vec.wv.vocab[word].count)\n",
    "            writer.write(word + ' ' + str(wvmodel.wv.vocab[word].count) + '\\n') # Output vocab count\n",
    "    print(\"Finished writing vocab file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = wvmodel.wv.index2entity[25]\n",
    "vector = wvmodel.wv.vectors[25]\n",
    "print(word)\n",
    "# print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data_util.data import Vocab\n",
    "vocab_size = len(wvmodel.vocab) + 1\n",
    "\n",
    "\n",
    "vocab = Vocab('bin/main_cat/word.vocab', vocab_size)\n",
    "\n",
    "embed_size = 300\n",
    "weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "for i in range(len(vocab._id_to_word.keys())):\n",
    "    try:\n",
    "        vocab_word = vocab._id_to_word[i+4]\n",
    "        w2vec_word = w2vec.wv.index2entity[i]\n",
    "    except Exception as e :\n",
    "        continue\n",
    "    if i + 4 > vocab_size: break\n",
    "#     print(vocab_word,w2vec_word)\n",
    "    weight[i+4, :] = torch.from_numpy(w2vec.wv.vectors[i])\n",
    "        \n",
    "embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "embedding.weight.requires_grad = True\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.word2id('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(wvmodel.vocab) + 1\n",
    "\n",
    "vocab = Vocab('Embedding/main_cat/word2Vec/word2Vec.vocab', vocab_size)\n",
    "\n",
    "with open(\"Embedding/glove/glove.6B.300d.txt\", 'r',encoding='utf-8') as f :\n",
    "#     print(vocab_size) \n",
    "    embed_size = 300\n",
    "    weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "    for line in f.readlines():\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word not in vocab._word_to_id.keys(): continue\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        wid = vocab.word2id(word)          \n",
    "        weight[wid, :] = torch.from_numpy(vector)\n",
    "        \n",
    "\n",
    "embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "embedding.weight.requires_grad = True\n",
    "embedding        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "# BERT\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model.eval()\n",
    "model.embeddings.word_embeddings\n",
    "\n",
    "\n",
    "# vocab = Vocab('Embedding/word2Vec/word2Vec.vocab', vocab_size)\n",
    "\n",
    "# embed_size = 300\n",
    "# weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "\n",
    "# embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# # requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "# embedding.weight.requires_grad = True\n",
    "# embedding        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
