{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# from product import *\n",
    "# from data_util.product import *\n",
    "from data_util.mainCat import *\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "import nltk\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "# from product import *\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "CHUNK_SIZE = 1000  # num examples per chunk, for the chunked data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key word Attention DataSet 讀取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLSX/main_cat/Pet Supplies_key.xlsx Read finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_ID</th>\n",
       "      <th>review</th>\n",
       "      <th>summary</th>\n",
       "      <th>big_categories</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>small_categories</th>\n",
       "      <th>lemm_review</th>\n",
       "      <th>lemm_summary</th>\n",
       "      <th>lemm_review_len</th>\n",
       "      <th>lemm_summary_len</th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>total_keyword</th>\n",
       "      <th>FOP_sents</th>\n",
       "      <th>total_mention_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1403740800</td>\n",
       "      <td>It is not clear on the description, but this b...</td>\n",
       "      <td>Be aware of modification needed</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Pet Supplies</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>not clear the description but this bracket mak...</td>\n",
       "      <td>&lt;s&gt; aware modification need &lt;/s&gt;</td>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>axle modify bracket unusable,wheel change</td>\n",
       "      <td>necessary to modify the axle for that and the ...</td>\n",
       "      <td>description hand blaster axle handle rest brac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1428278400</td>\n",
       "      <td>A lot to pay for the wall bracket, but it work...</td>\n",
       "      <td>A lot to pay for the wall bracket, but ...</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Pet Supplies</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>lot to pay for the wall bracket but work.\\nwor...</td>\n",
       "      <td>&lt;s&gt; lot to pay for the wall bracket but &lt;/s&gt;</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>bracket work,bracket wall</td>\n",
       "      <td>lot to pay for the wall bracket but work.</td>\n",
       "      <td>lot pay wall bracket wall blaster mount note f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1394841600</td>\n",
       "      <td>Does what it is supposed to but ridiculously o...</td>\n",
       "      <td>okay - but ridiculous price</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Pet Supplies</td>\n",
       "      <td>Parts &amp; Accessories</td>\n",
       "      <td>what suppose to but ridiculously over price fo...</td>\n",
       "      <td>&lt;s&gt; okay but ridiculous price &lt;/s&gt;</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>metal simple,metal bent</td>\n",
       "      <td>what suppose to but ridiculously over price fo...</td>\n",
       "      <td>piece metal suppose price simple gouge include...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1410739200</td>\n",
       "      <td>Worked well.  I use to do water changes and wa...</td>\n",
       "      <td>Worked well. I use to do water changes and ...</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>Pet Supplies</td>\n",
       "      <td>Water Filters</td>\n",
       "      <td>work well.\\nuse to water change and surprise t...</td>\n",
       "      <td>&lt;s&gt; work well &lt;/s&gt; \\n  &lt;s&gt; use to water change...</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>suction surprise,pump hold</td>\n",
       "      <td>use to water change and surprise the suction c...</td>\n",
       "      <td>water pump suction surprise change cup glass s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1406419200</td>\n",
       "      <td>This ribbon sucks! It tears easily and it's my...</td>\n",
       "      <td>This ribbon sucks! It tears easily and it's my...</td>\n",
       "      <td>Arts, Crafts &amp; Sewing</td>\n",
       "      <td>Pet Supplies</td>\n",
       "      <td>Ribbons</td>\n",
       "      <td>this ribbon suck.\\ntear easily and fault miss ...</td>\n",
       "      <td>&lt;s&gt; this ribbon suck &lt;/s&gt; \\n  &lt;s&gt; tear easily ...</td>\n",
       "      <td>73</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>tie tight</td>\n",
       "      <td>rip when try to tie bow and the bow ugly becau...</td>\n",
       "      <td>ribbon tear read satin satin plastic tight kno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_ID                                             review  \\\n",
       "0  1403740800  It is not clear on the description, but this b...   \n",
       "1  1428278400  A lot to pay for the wall bracket, but it work...   \n",
       "2  1394841600  Does what it is supposed to but ridiculously o...   \n",
       "3  1410739200  Worked well.  I use to do water changes and wa...   \n",
       "4  1406419200  This ribbon sucks! It tears easily and it's my...   \n",
       "\n",
       "                                             summary         big_categories  \\\n",
       "0                    Be aware of modification needed             Appliances   \n",
       "1         A lot to pay for the wall bracket, but ...             Appliances   \n",
       "2                        okay - but ridiculous price             Appliances   \n",
       "3     Worked well. I use to do water changes and ...             Appliances   \n",
       "4  This ribbon sucks! It tears easily and it's my...  Arts, Crafts & Sewing   \n",
       "\n",
       "       main_cat     small_categories  \\\n",
       "0  Pet Supplies  Parts & Accessories   \n",
       "1  Pet Supplies  Parts & Accessories   \n",
       "2  Pet Supplies  Parts & Accessories   \n",
       "3  Pet Supplies        Water Filters   \n",
       "4  Pet Supplies              Ribbons   \n",
       "\n",
       "                                         lemm_review  \\\n",
       "0  not clear the description but this bracket mak...   \n",
       "1  lot to pay for the wall bracket but work.\\nwor...   \n",
       "2  what suppose to but ridiculously over price fo...   \n",
       "3  work well.\\nuse to water change and surprise t...   \n",
       "4  this ribbon suck.\\ntear easily and fault miss ...   \n",
       "\n",
       "                                        lemm_summary  lemm_review_len  \\\n",
       "0                   <s> aware modification need </s>               82   \n",
       "1       <s> lot to pay for the wall bracket but </s>               58   \n",
       "2                 <s> okay but ridiculous price </s>               32   \n",
       "3  <s> work well </s> \\n  <s> use to water change...               19   \n",
       "4  <s> this ribbon suck </s> \\n  <s> tear easily ...               73   \n",
       "\n",
       "   lemm_summary_len  overall  vote                              total_keyword  \\\n",
       "0                 5        3     4  axle modify bracket unusable,wheel change   \n",
       "1                10        4     2                  bracket work,bracket wall   \n",
       "2                 6        3     8                    metal simple,metal bent   \n",
       "3                12        5     2                 suction surprise,pump hold   \n",
       "4                12        1     3                                  tie tight   \n",
       "\n",
       "                                           FOP_sents  \\\n",
       "0  necessary to modify the axle for that and the ...   \n",
       "1          lot to pay for the wall bracket but work.   \n",
       "2  what suppose to but ridiculously over price fo...   \n",
       "3  use to water change and surprise the suction c...   \n",
       "4  rip when try to tie bow and the bow ugly becau...   \n",
       "\n",
       "                              total_mention_features  \n",
       "0  description hand blaster axle handle rest brac...  \n",
       "1  lot pay wall bracket wall blaster mount note f...  \n",
       "2  piece metal suppose price simple gouge include...  \n",
       "3  water pump suction surprise change cup glass s...  \n",
       "4  ribbon tear read satin satin plastic tight kno...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main_cat = All_Electronics().getAttr()\n",
    "main_cat = Pet_Supplies().getAttr()\n",
    "# main_cat = Sports_Outdoors().getAttr()\n",
    "# main_cat = Health_personal_Care().getAttr()\n",
    "\n",
    "xlsx_path = \"XLSX/main_cat/%s_key.xlsx\"%(main_cat)\n",
    "# df.to_csv(csv_path) #默认dt是DataFrame的一个实例，参数解释如下\n",
    "# key_train_df.to_excel(csv_path, encoding='utf8')\n",
    "orign_key_df = pd.read_excel(xlsx_path)\n",
    "print(xlsx_path + \" Read finished\")\n",
    "len(orign_key_df)\n",
    "orign_key_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key word load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load FOP-View/Pet Supplies_keywords2.txt keywords...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FOP-View/Pet Supplies_keywords2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-821043ce500e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load %s keywords...'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FOP-View/Pet Supplies_keywords2.txt'"
     ]
    }
   ],
   "source": [
    "fn = 'FOP-View/%s_keywords2.txt' % (main_cat)\n",
    "print('load %s keywords...' % (fn))\n",
    "total_keywords = set()\n",
    "with open(fn, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        k, v = line.split(\":\")\n",
    "        total_keywords.add(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_lexicon = {}\n",
    "for filename in os.listdir('opinion-lexicon-English/'):      \n",
    "    if \"txt\" not in filename: continue\n",
    "    print(filename)\n",
    "    with open('opinion-lexicon-English/'+filename,'r') as f_input:\n",
    "        lexion = []\n",
    "        for line in f_input:\n",
    "            if line.startswith(\";\"):\n",
    "                continue\n",
    "            word = line.replace(\"\\n\",\"\")\n",
    "            if word != \"\" : lexion.append(word)\n",
    "        pos = filename.replace(\".txt\",\"\")\n",
    "        opinion_lexicon[pos] = lexion\n",
    "\n",
    "opinion_lexicon[\"total-words\"] = opinion_lexicon[\"negative-words\"] + opinion_lexicon[\"positive-words\"]\n",
    "print(\"total-words 已取得\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary 資料清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_summary(x):\n",
    "    x = x.replace(\"\\n\",\"\").replace(\"</s>\",\"\").replace(\"<s>\",\"\")\n",
    "    x = \"<s>\" + x + \"</s>\"  \n",
    "    x = \" \".join([str(token) for token in nlp(x) if (\" \" not in str(token)) and \\\n",
    "                  (str(token).isalpha()) and \\\n",
    "                  (len(str(token)) > 1)   ])\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_custom_tokenizer(nlp):\n",
    "    prefix_re = re.compile(r'[0-9]\\.')\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search)\n",
    "\n",
    "def calc_summary_len(x):\n",
    "#     tokens = [token for token in nlp(x)]\n",
    "#     print(tokens)\n",
    "#     print([len(t) for t in tokens])\n",
    "#     return len(tokens)\n",
    "    return len(x.split(\" \"))\n",
    "\n",
    "nlp.tokenizer = create_custom_tokenizer(nlp)\n",
    "\n",
    "orign_key_df['lemm_summary'] = orign_key_df['lemm_summary'].apply(compose_summary)\n",
    "orign_key_df['lemm_summary_len'] = orign_key_df['lemm_summary'].apply(calc_summary_len)\n",
    "\n",
    "\n",
    "amount = len(orign_key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "\n",
    "orign_key_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# review 多句合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def compose_review(x):\n",
    "    x = eval(x)\n",
    "    x = \"\\n\".join(x)\n",
    "    x = x.replace(\"\\n\",\" \")    \n",
    "    tokens = [str(token) for token in x.split(\" \") if (\" \" not in str(token))and (str(token) == '.' or str(token).isalpha())]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def calc_review_len(x):\n",
    "#     tokens = [token for token in nlp(x)]\n",
    "#     print(tokens)\n",
    "#     print([len(t) for t in tokens])\n",
    "#     return len(tokens)\n",
    "    return len(x.split(\" \"))\n",
    "key_df = deepcopy(orign_key_df)\n",
    "key_df['lemm_review'] = key_df['lemm_review'].apply(compose_review)\n",
    "key_df['lemm_review_len'] = key_df['lemm_review'].apply(calc_review_len)\n",
    "key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orign_key_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 過濾不合適的訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(text):\n",
    "    keywords = set()\n",
    "    for words in text.split(\",\"):\n",
    "        for word in words.split(\" \"):\n",
    "            keywords.add(word)\n",
    "    keywords = \" \".join(keywords)\n",
    "    return keywords\n",
    "\n",
    "def calc_keyword_num(x):\n",
    "    return len(x.split(\" \"))\n",
    "\n",
    "# and(key_df.lemm_review_len>20)\n",
    "flit_key_df = key_df[(key_df.lemm_summary_len>=4) ] # 過濾single word summary\n",
    "flit_key_df = flit_key_df[(flit_key_df.lemm_review_len <= 1000) ] # 過濾single word summary\n",
    "\n",
    "flit_key_df = flit_key_df.dropna(\n",
    "    axis=0,     # 0: 对行进行操作; 1: 对列进行操作\n",
    "    how='any'   # 'any': 只要存在 NaN 就 drop 掉; 'all': 必须全部是 NaN 才 drop \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOP_keywords 資料整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flit_key_df['FOP_keywords'] = flit_key_df['total_keyword']\n",
    "flit_key_df['FOP_keywords'] = flit_key_df['FOP_keywords'].apply(to_words)\n",
    "flit_key_df['FOP_keywords_num'] = flit_key_df['FOP_keywords'].apply(calc_keyword_num)\n",
    "flit_key_df = flit_key_df[(flit_key_df.FOP_keywords_num>=2) ] # 過濾single word summary\n",
    "flit_key_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flit_key_df['Cheat'] = False \n",
    "\n",
    "# flit_key_df.head()\n",
    "from tqdm import tqdm\n",
    "# 非符號alpha word重疊數\n",
    "with tqdm(total=len(flit_key_df)) as pbar:\n",
    "    for i ,row in flit_key_df.iterrows():\n",
    "        rev_tokens = set(row['lemm_review'].split(\" \"))\n",
    "#         if 's' in rev_tokens: rev_tokens.remove('s')\n",
    "#         if '.' in rev_tokens: rev_tokens.remove('.')\n",
    "#         if 'i' in rev_tokens: rev_tokens.remove('i')\n",
    "#         if 'a' in rev_tokens: rev_tokens.remove('a')\n",
    "#         if 'the' in rev_tokens: rev_tokens.remove('the')\n",
    "        \n",
    "        summ_tokens = set(row['lemm_summary'].split(\" \"))\n",
    "        key_sets = rev_tokens & summ_tokens & (total_keywords| set(opinion_lexicon[\"total-words\"]))\n",
    "        if len(key_sets) > 2: \n",
    "#             print(True)\n",
    "            flit_key_df.loc[i,'Cheat'] = True\n",
    "\n",
    "#         print(rev_tokens)\n",
    "#         print(summ_tokens)\n",
    "#         print([len(t) for t in summ_tokens])\n",
    "        pbar.update(1)\n",
    "    \n",
    "flit_key_df = flit_key_df[(flit_key_df.Cheat == True) ] # 過濾single word summary\n",
    "amount = len(flit_key_df)\n",
    "print('Total data : %s'%(amount))\n",
    "flit_key_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank_keywords 資料整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords as TextRank\n",
    "from summa.summarizer import summarize\n",
    "def textrank_keys(text):\n",
    "    keywords1 = list()\n",
    "    for words in TextRank.keywords(text).split('\\n'):\n",
    "        keywords1.extend(words.split(\" \"))\n",
    "    keywords1 = set(keywords1)    \n",
    "    \n",
    "    return \" \".join(list(keywords1))\n",
    "\n",
    "def textrank_summ_keys(text): \n",
    "    keywords2 = list()\n",
    "    for words in summarize(text, words=8).split('\\n'):\n",
    "        keywords2.extend(words.split(\" \"))\n",
    "    keywords2 = set(keywords2)\n",
    "    \n",
    "    return \" \".join(list(keywords2))\n",
    "\n",
    "# flit_key_df['TextRank_keywords'] = flit_key_df['lemm_review'].apply(textrank_to_words)\n",
    "# flit_key_df['TextRank_keywords_num'] = flit_key_df['TextRank_keywords'].apply(calc_num)\n",
    "flit_key_df['TextRank_keywords'] = flit_key_df['FOP_keywords'] \n",
    "flit_key_df['TextRank_keywords_num'] = flit_key_df['FOP_keywords_num'] \n",
    "flit_key_df.loc[:,'TextRank_keywords'] = ''\n",
    "flit_key_df.loc[:,'TextRank_keywords_num'] = 0\n",
    "flit_key_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "with tqdm(total=len(flit_key_df)) as pbar:\n",
    "    for i ,row in flit_key_df.iterrows():\n",
    "        TextRank_keywords = textrank_keys(row['lemm_review'])\n",
    "    #     TextRank_keywords = textrank_summ_keys(row['lemm_review'])  \n",
    "        num = calc_keyword_num(TextRank_keywords)\n",
    "        flit_key_df.loc[i,'TextRank_keywords'] = TextRank_keywords\n",
    "        flit_key_df.loc[i,'TextRank_keywords_num'] = num\n",
    "        pbar.update(1)\n",
    "        \n",
    "flit_key_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 輸出統計長度資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "if not os.path.exists('XLSX/statistic'):\n",
    "    os.makedirs('XLSX/statistic')\n",
    "with open('XLSX/statistic/%s_info.txt'%(main_cat),'w') as f:\n",
    "    max_rev_len = flit_key_df['lemm_review_len'].max()\n",
    "    min_rev_len = flit_key_df['lemm_review_len'].min()\n",
    "    mean_rev_len = flit_key_df['lemm_review_len'].mean()\n",
    "    median_rev_len = flit_key_df['lemm_review_len'].median()\n",
    "\n",
    "    f.write('max_rev_len :%s \\n'%(max_rev_len))\n",
    "    f.write('min_rev_len :%s \\n'%(min_rev_len))\n",
    "    f.write('mean_rev_len :%s \\n'%(mean_rev_len))\n",
    "    f.write('median_rev_len :%s \\n'%(median_rev_len))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_summary_len = flit_key_df['lemm_summary_len'].max()\n",
    "    min_summary_len = flit_key_df['lemm_summary_len'].min()\n",
    "    mean_summary_len = flit_key_df['lemm_summary_len'].mean()\n",
    "    median_summary_len = flit_key_df['lemm_summary_len'].median()\n",
    "\n",
    "    f.write('max_summary_len :%s \\n'%(max_summary_len))\n",
    "    f.write('min_summary_len :%s \\n'%(min_summary_len))\n",
    "    f.write('mean_summary_len :%s \\n'%(mean_summary_len))\n",
    "    f.write('median_summary_len :%s \\n'%(median_summary_len))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_FOP_keywords_num = flit_key_df['FOP_keywords_num'].max()\n",
    "    min_FOP_keywords_num = flit_key_df['FOP_keywords_num'].min()\n",
    "    mean_FOP_keywords_num = flit_key_df['FOP_keywords_num'].mean()\n",
    "    median_FOP_keywords_num = flit_key_df['FOP_keywords_num'].median()\n",
    "\n",
    "    f.write('max_FOP_keywords_num :%s \\n'%(max_FOP_keywords_num))\n",
    "    f.write('min_FOP_keywords_num :%s \\n'%(min_FOP_keywords_num))\n",
    "    f.write('mean_FOP_keywords_num :%s \\n'%(mean_FOP_keywords_num))\n",
    "    f.write('median_FOP_keywords_num :%s \\n'%(median_FOP_keywords_num))\n",
    "    \n",
    "    f.write('\\n\\n\\n')\n",
    "    max_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].max()\n",
    "    min_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].min()\n",
    "    mean_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].mean()\n",
    "    median_TextRank_keywords_num = flit_key_df['TextRank_keywords_num'].median()\n",
    "\n",
    "    f.write('max_TextRank_keywords_num :%s \\n'%(max_TextRank_keywords_num))\n",
    "    f.write('min_TextRank_keywords_num :%s \\n'%(min_TextRank_keywords_num))\n",
    "    f.write('mean_TextRank_keywords_num :%s \\n'%(mean_TextRank_keywords_num))\n",
    "    f.write('median_TextRank_keywords_num :%s \\n'%(median_TextRank_keywords_num))\n",
    "\n",
    "    \n",
    "\n",
    "# plt.xlim(xmax = mean_rev_len)\n",
    "# plt.ylim(ymax = flit_key_df['lemm_review_len'].value_counts().max())\n",
    "\n",
    "flit_key_df['lemm_review_len'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/review_len_%s.png'%(main_cat))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# plt.xlim(xmax = max_summary_len)\n",
    "# plt.ylim(ymax = flit_key_df['lemm_summary_len'].value_counts().max())\n",
    "flit_key_df['lemm_summary_len'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/summary_len_%s.png'%(main_cat))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# plt.xlim(xmax = mean_keyword_num)\n",
    "flit_key_df['FOP_keywords_num'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/FOP_keywords_num_%s.png'%(main_cat))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "flit_key_df['TextRank_keywords_num'].value_counts().hist()\n",
    "plt.savefig('XLSX/statistic/TextRank_keywords_num_%s.png'%(main_cat))\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 製作record bin檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if os.path.exists('bin'):\n",
    "    shutil.rmtree('/bin', ignore_errors=True)\n",
    "\n",
    "if not os.path.exists('bin/main_cat/chunked'):\n",
    "    os.makedirs('bin/main_cat/chunked')\n",
    "\n",
    "makevocab = True\n",
    "if makevocab:\n",
    "    vocab_counter = collections.Counter()\n",
    "    \n",
    "# train_file\n",
    "flit_key_train_df = flit_key_df.iloc[:int(amount*0.6)]\n",
    "\n",
    "# test_file\n",
    "flit_key_test_df = flit_key_df.iloc[int(amount*0.6)+1:int(amount*0.8)]\n",
    "\n",
    "# vald_file\n",
    "flit_key_valid_df = flit_key_df.iloc[int(amount*0.8)+1:]\n",
    "sentence_start = \"<s>\"\n",
    "sentence_end = \"</s>\"\n",
    "\n",
    "\n",
    "def xlsx2bin(set_name,df):\n",
    "    sents = []\n",
    "    with open(\"bin/main_cat/%s.bin\"%(set_name), 'wb') as file:\n",
    "        i = 0\n",
    "        for idx in tqdm(range(len(df))):\n",
    "            series = df.iloc[idx]\n",
    "            data_dict = series.to_dict()\n",
    "            review_ID , big_categories , small_categories , \\\n",
    "            review , lemm_review , summary , lemm_summary , FOP_keywords ,TextRank_keywords = \\\n",
    "            data_dict['review_ID'],data_dict['big_categories'],data_dict['small_categories'],data_dict['review'],data_dict['lemm_review'], \\\n",
    "            data_dict['summary'],data_dict['lemm_summary'],data_dict['FOP_keywords'] ,data_dict['TextRank_keywords']\n",
    "#             print(FOP_keywords)\n",
    "\n",
    "            # save Embedding/word2Vec calculate sents\n",
    "            for sent in nltk.sent_tokenize(lemm_review):\n",
    "                sent = sent.replace(\".\" ,\"\")\n",
    "#                 sents.append(str(sent).split()) # 切分词汇 \n",
    "\n",
    "            for sent in nltk.sent_tokenize(lemm_summary):\n",
    "                sent = sent.replace(sentence_start ,\"\").replace(sentence_end ,\"\")\n",
    "#                 sents.append(str(sent).split()) # 切分词汇 \n",
    "\n",
    "            lemm_review = lemm_review.replace(\"\\n\",\"\").replace(\".\",\" \")\n",
    "            lemm_summary = lemm_summary.replace(\"\\n\",\"\").replace(\".\",\" \")\n",
    "            lemm_summary = sentence_start + ' '+ lemm_summary + ' ' + sentence_end\n",
    "#             print(lemm_summary)\n",
    "            # Write to tf.Example\n",
    "            tf_example = example_pb2.Example()\n",
    "            try:\n",
    "                tf_example.features.feature['orign_review'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(review, encoding='utf-8')])\n",
    "\n",
    "                tf_example.features.feature['orign_summary'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(summary, encoding='utf-8')])\n",
    "                \n",
    "                tf_example.features.feature['review'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(lemm_review, encoding='utf-8')])\n",
    "\n",
    "                tf_example.features.feature['summary'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(lemm_summary, encoding='utf-8')]) \n",
    "        \n",
    "                tf_example.features.feature['FOP_keywords'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(FOP_keywords, encoding='utf-8')]) \n",
    "            \n",
    "                tf_example.features.feature['TextRank_keywords'].bytes_list.value.extend(\n",
    "                    [tf.compat.as_bytes(TextRank_keywords, encoding='utf-8')]) \n",
    "\n",
    "                tf_example_str = tf_example.SerializeToString()\n",
    "                str_len = len(tf_example_str)  \n",
    "                file.write(struct.pack('q', str_len))\n",
    "                file.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "            except Exception as e:\n",
    "#                 print(e)\n",
    "                pass\n",
    "    print(\" %s finished... \"%(file.name))\n",
    "    return sents\n",
    "    \n",
    "    \n",
    "sents1 = xlsx2bin('train',flit_key_train_df)\n",
    "sents2 = xlsx2bin('test',flit_key_test_df)\n",
    "sents3 = xlsx2bin('valid',flit_key_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bin/main_cat/bin-info.txt\",'w',encoding='utf-8') as f :\n",
    "    f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割record bin檔(1000為單位)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(set_name, chunks_dir):\n",
    "    in_file = 'bin/main_cat/%s.bin' % set_name\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "#         chunk_fname = os.path.join('bin', '/%s/%s_%03d.bin' % (chunks_dir,set_name, chunk))  # new chunk\n",
    "        chunk_fname = '%s/%s/%s_%03d.bin' % (chunks_dir,set_name,set_name, chunk)\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(CHUNK_SIZE):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1\n",
    "\n",
    "\n",
    "def chunk_all(chunks_dir = 'bin/main_cat/chunked'):\n",
    "    # Make a dir to hold the chunks\n",
    "    \n",
    "    # Chunk the data\n",
    "    for set_name in ['train', 'valid', 'test']:\n",
    "        if not os.path.isdir(os.path.join(chunks_dir,set_name)):\n",
    "            os.mkdir(os.path.join(chunks_dir,set_name))\n",
    "        print(\"Splitting %s data into chunks...\" % set_name)\n",
    "        chunk_file(set_name, chunks_dir)\n",
    "    print(\"Saved chunked data in %s\" % chunks_dir)\n",
    "    \n",
    "chunk_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_valid():\n",
    "    #Performing rouge evaluation on 1.9 lakh sentences takes lot of time. So, create mini validation set & test set by borrowing 15k samples each from these 1.9 lakh sentences\n",
    "    bin_valid_chuncks = os.listdir('bin/main_cat/chunked/valid')\n",
    "    bin_valid_chuncks.sort()\n",
    "    if not os.path.exists('bin/main_cat/chunked/main_valid'):\n",
    "        os.mkdir('bin/main_cat/chunked/main_valid')\n",
    "        \n",
    "    samples = random.sample(set(bin_valid_chuncks[:-1]), 2)      #Exclude last bin file; contains only 9k sentences\n",
    "    valid_chunk, test_chunk = samples[0], samples[1]\n",
    "    shutil.copyfile(os.path.join('bin/main_cat/chunked/valid', valid_chunk), os.path.join(\"bin/main_cat/chunked/main_valid\", \"valid_00.bin\"))\n",
    "    shutil.copyfile(os.path.join('bin/main_cat/chunked/valid', test_chunk), os.path.join(\"bin/main_cat/chunked/main_valid\", \"test_00.bin\"))\n",
    "main_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [] # total sentence\n",
    "for idx in tqdm(range(len(orign_key_df))):\n",
    "    series = orign_key_df.iloc[idx]\n",
    "    data_dict = series.to_dict()\n",
    "    lemm_review_sents , lemm_summary  = data_dict['lemm_review'],data_dict['lemm_summary'] \n",
    "    lemm_review_sents = eval(lemm_review_sents)\n",
    "    for sent in lemm_review_sents:\n",
    "        sent_tokens = sent.split(\" \")\n",
    "        tokens = [str(token) for token in sent.split() if (\" \" not in str(token))and (str(token) == '.' or str(token).isalpha())]\n",
    "        sentences.append(tokens)     \n",
    "    sentences.append(lemm_summary.split(\" \"))\n",
    "print('word2Vec training sentence finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = sents1 + sents2 + sents3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 word2vec\n",
    "from gensim.models import word2vec\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsnooper\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "vocab_count = 50000\n",
    "# write vocab to file\n",
    "if not os.path.exists('Embedding/main_cat/word2Vec'):\n",
    "    os.makedirs('Embedding/main_cat/word2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"Embedding/main_cat/word2Vec/word2Vec.300d.txt\"):\n",
    "\n",
    "    w2vec = word2vec.Word2Vec(sentences, size=300, min_count=1,max_vocab_size=None,iter=100,\n",
    "                              sorted_vocab=1,max_final_vocab=vocab_count)\n",
    "\n",
    "    \n",
    "\n",
    "    w2vec.wv.save_word2vec_format('Embedding/main_cat/word2Vec/word2Vec.300d.txt', binary=False)\n",
    "\n",
    "    #保存模型，供日後使用\n",
    "    # w2vec.save(\"Embedding/word2Vec/word2vec.model\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型讀取方式\n",
    "# model = word2vec.Word2Vec.load(\"Embedding/word2Vec/word2vec.model\")\n",
    "\n",
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'Embedding/main_cat/word2Vec/word2Vec.300d.txt', binary=False, encoding='utf-8')\n",
    "\n",
    "wvmodel.most_similar(u\"player\", topn=10)\n",
    "# wvmodel.most_similar(['dvd','player','changer','machine','video'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = \"bin/main_cat/word.vocab\"\n",
    "\n",
    "if os.path.exists(vocab_file):\n",
    "    vocab_count = len(wvmodel.wv.index2entity)    \n",
    "\n",
    "    print(\"Writing vocab file...\")\n",
    "    with open(vocab_file, 'w',encoding='utf-8') as writer:\n",
    "        for word in wvmodel.wv.index2entity[:vocab_count]:\n",
    "            # print(word, w2vec.wv.vocab[word].count)\n",
    "            writer.write(word + ' ' + str(wvmodel.wv.vocab[word].count) + '\\n') # Output vocab count\n",
    "    print(\"Finished writing vocab file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = wvmodel.wv.index2entity[25]\n",
    "vector = wvmodel.wv.vectors[25]\n",
    "print(word)\n",
    "# print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data_util.data import Vocab\n",
    "vocab_size = len(wvmodel.vocab) + 1\n",
    "\n",
    "\n",
    "vocab = Vocab('bin/main_cat/word.vocab', vocab_size)\n",
    "\n",
    "embed_size = 300\n",
    "weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "for i in range(len(vocab._id_to_word.keys())):\n",
    "    try:\n",
    "        vocab_word = vocab._id_to_word[i+4]\n",
    "        w2vec_word = w2vec.wv.index2entity[i]\n",
    "    except Exception as e :\n",
    "        continue\n",
    "    if i + 4 > vocab_size: break\n",
    "#     print(vocab_word,w2vec_word)\n",
    "    weight[i+4, :] = torch.from_numpy(w2vec.wv.vectors[i])\n",
    "        \n",
    "embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "embedding.weight.requires_grad = True\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.word2id('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(wvmodel.vocab) + 1\n",
    "\n",
    "vocab = Vocab('Embedding/main_cat/word2Vec/word2Vec.vocab', vocab_size)\n",
    "\n",
    "with open(\"Embedding/glove/glove.6B.300d.txt\", 'r',encoding='utf-8') as f :\n",
    "#     print(vocab_size) \n",
    "    embed_size = 300\n",
    "    weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "    for line in f.readlines():\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word not in vocab._word_to_id.keys(): continue\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        wid = vocab.word2id(word)          \n",
    "        weight[wid, :] = torch.from_numpy(vector)\n",
    "        \n",
    "\n",
    "embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "embedding.weight.requires_grad = True\n",
    "embedding        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding/Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "# BERT\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model.eval()\n",
    "model.embeddings.word_embeddings\n",
    "\n",
    "\n",
    "# vocab = Vocab('Embedding/word2Vec/word2Vec.vocab', vocab_size)\n",
    "\n",
    "# embed_size = 300\n",
    "# weight = torch.zeros(vocab_size, embed_size)\n",
    "\n",
    "\n",
    "# embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    "# # requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "# embedding.weight.requires_grad = True\n",
    "# embedding        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
