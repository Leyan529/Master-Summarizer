{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from model import Model\n",
    "\n",
    "\n",
    "from data_util import config, data\n",
    "from data_util.batcher import Batcher\n",
    "from data_util.data import Vocab\n",
    "\n",
    "\n",
    "from train_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse\n",
    "import torchsnooper\n",
    "import logging\n",
    "\n",
    "# -------- Test Packages -------\n",
    "from beam_search import *\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def getLogger(loggerName, loggerPath):\n",
    "    # 設置logger\n",
    "    logger = logging.getLogger(loggerName)  # 不加名稱設置root logger\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s: - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    logging.Filter(loggerName)\n",
    "\n",
    "    # 使用FileHandler輸出到文件\n",
    "    directory = os.path.dirname(loggerPath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fh = logging.FileHandler(loggerPath)\n",
    "\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # 使用StreamHandler輸出到屏幕\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    ch.setFormatter(formatter)\n",
    "    # 添加兩個Handler\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    # Handler只啟動一次\n",
    "    # 設置logger\n",
    "    logger.info(u'logger已啟動')\n",
    "    return logger\n",
    "\n",
    "def removeLogger(logger):\n",
    "    logger.info(u'logger已關閉')\n",
    "    handlers = logger.handlers[:]\n",
    "    for handler in handlers:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch():\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    # with torchsnooper.snoop():\n",
    "    while batch is not None:\n",
    "        example_list = batch.example_list\n",
    "        for ex in example_list:\n",
    "            r = str(ex.original_review)\n",
    "            s = str(ex.original_summary)\n",
    "            k = str(ex.key_words)\n",
    "            sent = ex.original_summary_sents\n",
    "#             print(\"original_review_sents:\", r)\n",
    "            print(\"original_summary_sents : \", s)\n",
    "            print(\"key_words : \", k)\n",
    "            print('------------------------------------------------------------\\n')\n",
    "        batch = batcher.next_batch()        \n",
    "# test_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bin Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 19241\n",
      "\n",
      "test : 6413\n",
      "\n",
      "valid : 6413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(config.bin_info,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    [print(line) for line in lines]\n",
    "    train_num = int(lines[0].split(\":\")[1])\n",
    "    test_num = int(lines[1].split(\":\")[1])\n",
    "    val_num = int(lines[2].split(\":\")[1])\n",
    "    # f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    # f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    # f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "           Kernel Shape  Output Shape   Params  Mult-Adds\n",
      "Layer                                                    \n",
      "0_lstm                -  [3950, 1024]  3334144    3325952\n",
      "1_reduce_h  [1024, 512]     [16, 512]   524800     524288\n",
      "2_reduce_c  [1024, 512]     [16, 512]   524800     524288\n",
      "---------------------------------------------------------\n",
      "                       Totals\n",
      "Total params          4383744\n",
      "Trainable params      4383744\n",
      "Non-trainable params        0\n",
      "Mult-Adds             4374528\n",
      "=========================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[3950, 1024]</td>\n",
       "      <td>3334144</td>\n",
       "      <td>3325952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_reduce_h</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[16, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_reduce_c</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[16, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kernel Shape  Output Shape   Params  Mult-Adds\n",
       "Layer                                                    \n",
       "0_lstm                -  [3950, 1024]  3334144    3325952\n",
       "1_reduce_h  [1024, 512]     [16, 512]   524800     524288\n",
       "2_reduce_c  [1024, 512]     [16, 512]   524800     524288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Encoder,Model\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "encoder = Encoder().to(device)    \n",
    "\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "summary(encoder, enc_batch, enc_lens) # encoder summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================\n",
      "                               Kernel Shape     Output Shape    Params  \\\n",
      "Layer                                                                    \n",
      "0_x_context                     [1324, 300]        [16, 300]    397500   \n",
      "1_lstm                                    -        [16, 512]   1667072   \n",
      "2_enc_attention.Linear_W_h     [1024, 1024]  [16, 845, 1024]   1048576   \n",
      "3_enc_attention.Linear_W_s     [1024, 1024]       [16, 1024]   1049600   \n",
      "4_enc_attention.Linear_v          [1024, 1]     [16, 845, 1]      1024   \n",
      "5_dec_attention.Linear_W_prev    [512, 512]     [16, 1, 512]    262144   \n",
      "6_dec_attention.Linear_W_s       [512, 512]        [16, 512]    262656   \n",
      "7_dec_attention.Linear_v           [512, 1]       [16, 1, 1]       512   \n",
      "8_p_gen_linear                    [2860, 1]          [16, 1]      2861   \n",
      "9_V                             [2048, 512]        [16, 512]   1049088   \n",
      "10_V1                          [512, 50000]      [16, 50000]  25650000   \n",
      "\n",
      "                               Mult-Adds  \n",
      "Layer                                     \n",
      "0_x_context                       397200  \n",
      "1_lstm                           1662976  \n",
      "2_enc_attention.Linear_W_h       1048576  \n",
      "3_enc_attention.Linear_W_s       1048576  \n",
      "4_enc_attention.Linear_v            1024  \n",
      "5_dec_attention.Linear_W_prev     262144  \n",
      "6_dec_attention.Linear_W_s        262144  \n",
      "7_dec_attention.Linear_v             512  \n",
      "8_p_gen_linear                      2860  \n",
      "9_V                              1048576  \n",
      "10_V1                           25600000  \n",
      "---------------------------------------------------------------------------------\n",
      "                        Totals\n",
      "Total params          31391033\n",
      "Trainable params      31391033\n",
      "Non-trainable params         0\n",
      "Mult-Adds             31334588\n",
      "=================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_x_context</th>\n",
       "      <td>[1324, 300]</td>\n",
       "      <td>[16, 300]</td>\n",
       "      <td>397500</td>\n",
       "      <td>397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[16, 512]</td>\n",
       "      <td>1667072</td>\n",
       "      <td>1662976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_enc_attention.Linear_W_h</th>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>[16, 845, 1024]</td>\n",
       "      <td>1048576</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_enc_attention.Linear_W_s</th>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>[16, 1024]</td>\n",
       "      <td>1049600</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_enc_attention.Linear_v</th>\n",
       "      <td>[1024, 1]</td>\n",
       "      <td>[16, 845, 1]</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_dec_attention.Linear_W_prev</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>[16, 1, 512]</td>\n",
       "      <td>262144</td>\n",
       "      <td>262144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_dec_attention.Linear_W_s</th>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>[16, 512]</td>\n",
       "      <td>262656</td>\n",
       "      <td>262144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_dec_attention.Linear_v</th>\n",
       "      <td>[512, 1]</td>\n",
       "      <td>[16, 1, 1]</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_p_gen_linear</th>\n",
       "      <td>[2860, 1]</td>\n",
       "      <td>[16, 1]</td>\n",
       "      <td>2861</td>\n",
       "      <td>2860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9_V</th>\n",
       "      <td>[2048, 512]</td>\n",
       "      <td>[16, 512]</td>\n",
       "      <td>1049088</td>\n",
       "      <td>1048576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_V1</th>\n",
       "      <td>[512, 50000]</td>\n",
       "      <td>[16, 50000]</td>\n",
       "      <td>25650000</td>\n",
       "      <td>25600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Kernel Shape     Output Shape    Params  \\\n",
       "Layer                                                                    \n",
       "0_x_context                     [1324, 300]        [16, 300]    397500   \n",
       "1_lstm                                    -        [16, 512]   1667072   \n",
       "2_enc_attention.Linear_W_h     [1024, 1024]  [16, 845, 1024]   1048576   \n",
       "3_enc_attention.Linear_W_s     [1024, 1024]       [16, 1024]   1049600   \n",
       "4_enc_attention.Linear_v          [1024, 1]     [16, 845, 1]      1024   \n",
       "5_dec_attention.Linear_W_prev    [512, 512]     [16, 1, 512]    262144   \n",
       "6_dec_attention.Linear_W_s       [512, 512]        [16, 512]    262656   \n",
       "7_dec_attention.Linear_v           [512, 1]       [16, 1, 1]       512   \n",
       "8_p_gen_linear                    [2860, 1]          [16, 1]      2861   \n",
       "9_V                             [2048, 512]        [16, 512]   1049088   \n",
       "10_V1                          [512, 50000]      [16, 50000]  25650000   \n",
       "\n",
       "                               Mult-Adds  \n",
       "Layer                                     \n",
       "0_x_context                       397200  \n",
       "1_lstm                           1662976  \n",
       "2_enc_attention.Linear_W_h       1048576  \n",
       "3_enc_attention.Linear_W_s       1048576  \n",
       "4_enc_attention.Linear_v            1024  \n",
       "5_dec_attention.Linear_W_prev     262144  \n",
       "6_dec_attention.Linear_W_s        262144  \n",
       "7_dec_attention.Linear_v             512  \n",
       "8_p_gen_linear                      2860  \n",
       "9_V                              1048576  \n",
       "10_V1                           25600000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Decoder,Model\n",
    "from train_util import *\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# decoder = Decoder().to(device)    \n",
    "\n",
    "model = Model(False,'word2Vec',vocab)\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "# train_batch_MLE\n",
    "dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "step_losses = []\n",
    "s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "# x_t 為decoder每一個time step 的batch input\n",
    "x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "sum_temporal_srcs = None     \n",
    "             \n",
    "    \n",
    "for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "    use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "    # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "    # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "    x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "    x_t = model.embeds(x_t)\n",
    "#     final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "    final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "    x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "    extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "    enc_key_batch, enc_key_lens)\n",
    "    \n",
    "    decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "    break\n",
    "decoder_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummaryX import summary\n",
    "class Train(object):\n",
    "    def __init__(self, opt, vocab):\n",
    "#         self.vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "        self.vocab = vocab\n",
    "        self.train_batcher = Batcher(config.train_data_path, self.vocab, mode='train',\n",
    "                               batch_size=config.batch_size, single_pass=False)\n",
    "        self.test_batcher = Batcher(config.test_data_path, self.vocab, mode='eval',\n",
    "                               batch_size=config.batch_size, single_pass=True)\n",
    "        self.opt = opt\n",
    "        self.start_id = self.vocab.word2id(data.START_DECODING)\n",
    "        self.end_id = self.vocab.word2id(data.STOP_DECODING)\n",
    "        self.pad_id = self.vocab.word2id(data.PAD_TOKEN)\n",
    "        self.unk_id = self.vocab.word2id(data.UNKNOWN_TOKEN)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def save_model(self, iter, loss, r_loss):\n",
    "        if not os.path.exists(config.save_model_path):\n",
    "            os.makedirs(config.save_model_path)\n",
    "        file_path = \"/%07d_%.2f_%.2f.tar\" % (iter, loss, r_loss)\n",
    "        save_path = config.save_model_path + '/%s' % (self.opt.word_emb_type)\n",
    "        if not os.path.isdir(save_path): os.mkdir(save_path)\n",
    "        save_path = save_path + file_path\n",
    "        T.save({\n",
    "            \"iter\": iter + 1,\n",
    "            \"model_dict\": self.model.state_dict(),\n",
    "            \"trainer_dict\": self.trainer.state_dict()\n",
    "        }, save_path)\n",
    "        return file_path\n",
    "\n",
    "    def setup_train(self):\n",
    "        self.model = Model(opt.pre_train_emb, opt.word_emb_type, self.vocab)\n",
    "        #         print(\"Model : \",self.model)\n",
    "        #         logger.info(\"Model : \")\n",
    "        logger.info(str(self.model))\n",
    "        #         print(\"Encoder : \",self.model.encoder)\n",
    "        #         print(\"Decoder : \",self.model.decoder)\n",
    "        #         print(\"Embeds : \",self.model.embeds)\n",
    "        self.model = get_cuda(self.model)\n",
    "        device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")  # PyTorch v0.4.0\n",
    "        if opt.multi_device:\n",
    "            if T.cuda.device_count() > 1:\n",
    "                #                 print(\"Let's use\", T.cuda.device_count(), \"GPUs!\")\n",
    "                logger.info(\"Let's use \" + str(T.cuda.device_count()) + \" GPUs!\")\n",
    "                self.model = nn.DataParallel(self.model, list(range(T.cuda.device_count()))).cuda()\n",
    "\n",
    "        if isinstance(self.model, nn.DataParallel):\n",
    "            self.model = self.model.module\n",
    "        self.model.to(device)\n",
    "        #         self.model.eval()\n",
    "\n",
    "        self.trainer = T.optim.Adam(self.model.parameters(), lr=config.lr)\n",
    "        start_iter = 0\n",
    "        if self.opt.load_model is not None:\n",
    "#             load_model_path = os.path.join(config.save_model_path, self.opt.load_model)\n",
    "            load_model_path = config.save_model_path + self.opt.load_model\n",
    "            print(load_model_path)\n",
    "#             print('xxxx')\n",
    "            checkpoint = T.load(load_model_path)\n",
    "            start_iter = checkpoint[\"iter\"]\n",
    "            self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "            self.trainer.load_state_dict(checkpoint[\"trainer_dict\"])\n",
    "            #             print(\"Loaded model at \" + load_model_path)\n",
    "            logger.info(\"Loaded model at \" + load_model_path)\n",
    "        if self.opt.new_lr is not None:\n",
    "            self.trainer = T.optim.Adam(self.model.parameters(), lr=self.opt.new_lr)\n",
    "        return start_iter\n",
    "\n",
    "    def train_batch_MLE(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(\n",
    "            batch)  # Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        try:\n",
    "#             print('-----------------')\n",
    "            for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "                use_gound_truth = get_cuda((T.rand(len(enc_out)) > config.gound_truth_prob)).long()  # Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "                x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t  # Select decoder input based on use_ground_truth probabilities\n",
    "                x_t = self.model.embeds(x_t)                \n",
    "                final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask,\n",
    "                                                                                          ct_e, extra_zeros,\n",
    "                                                                                          enc_batch_extend_vocab,\n",
    "                                                                                          sum_temporal_srcs, prev_s, enc_key_batch, enc_key_lens)\n",
    "                target = target_batch[:, t]\n",
    "                log_probs = T.log(final_dist + config.eps)\n",
    "                step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=self.pad_id)\n",
    "                step_losses.append(step_loss)\n",
    "                x_t = T.multinomial(final_dist,1).squeeze()  # Sample words from final distribution which can be used as input in next time step\n",
    "#                 print(config.vocab_size)\n",
    "#                 print(x_t)\n",
    "                is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "                x_t = (1 - is_oov) * x_t.detach() + (is_oov) * self.unk_id  # Replace OOVs with [UNK] token\n",
    "        except Exception as e:\n",
    "            logger.error('xxxxxxxxxxx')\n",
    "            traceback = sys.exc_info()[2]\n",
    "            logger.error(sys.exc_info())\n",
    "            logger.error(traceback.tb_lineno)\n",
    "            logger.error(e)\n",
    "#             logger.error(final_dist)\n",
    "            logger.error('xxxxxxxxxxx')\n",
    "#             print(step_loss)\n",
    "\n",
    "                \n",
    "        losses = T.sum(T.stack(step_losses, 1), 1)  # unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens  # Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)  # Average batch loss\n",
    "        return mle_loss\n",
    "\n",
    "    def train_batch_RL(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab,\n",
    "                       review_oovs, greedy):\n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param review_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        s_t = enc_hidden  # Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []  # Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []  # Stores padding masks of generated samples\n",
    "        log_probs = []  # Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(\n",
    "            1))  # Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e,\n",
    "                                                                             extra_zeros, enc_batch_extend_vocab,\n",
    "                                                                             sum_temporal_srcs, prev_s)\n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs)\n",
    "                x_t = multi_dist.sample()  # perform multinomial sampling\n",
    "                log_prob = multi_dist.log_prob(x_t)\n",
    "                log_probs.append(log_prob)\n",
    "            else:\n",
    "                _, x_t = T.max(probs, dim=1)  # perform greedy sampling\n",
    "            x_t = x_t.detach()\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))  # Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1  # If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (\n",
    "            x_t == self.end_id) == 2] = 0  # If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t >= config.vocab_size).long()  # Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t + (is_oov) * self.unk_id  # Replace OOVs with [UNK] token\n",
    "\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:  # If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1)\n",
    "            log_probs = log_probs * decoder_padding_mask  # Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1)  # Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs,\n",
    "                              dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy()\n",
    "            oovs = review_oovs[i]\n",
    "            S = data.outputids2words(id_list, self.vocab, oovs)  # Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:  # If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "\n",
    "        return decoded_strs, log_probs \n",
    "\n",
    "    def reward_function(self, decoded_sents, original_sents):\n",
    "        rouge = Rouge()\n",
    "        try:\n",
    "            scores = rouge.get_scores(decoded_sents, original_sents)\n",
    "        except Exception:\n",
    "            #             print(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
    "            logger.info(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
    "            scores = []\n",
    "            for i in range(len(decoded_sents)):\n",
    "                try:\n",
    "                    score = rouge.get_scores(decoded_sents[i], original_sents[i])\n",
    "                except Exception:\n",
    "                    #                     print(\"Error occured at:\")\n",
    "                    #                     print(\"decoded_sents:\", decoded_sents[i])\n",
    "                    #                     print(\"original_sents:\", original_sents[i])\n",
    "                    logger.info(\"Error occured at:\")\n",
    "                    logger.info(\"decoded_sents:\", decoded_sents[i])\n",
    "                    logger.info(\"original_sents:\", original_sents[i])\n",
    "                    score = [{\"rouge-l\": {\"f\": 0.0}}]\n",
    "                scores.append(score[0])\n",
    "        rouge_l_f1 = [score[\"rouge-l\"][\"f\"] for score in scores]\n",
    "        avg_rouge_l_f1 = sum(rouge_l_f1) / len(rouge_l_f1)\n",
    "        rouge_l_f1 = get_cuda(T.FloatTensor(rouge_l_f1))\n",
    "        return rouge_l_f1, scores, avg_rouge_l_f1\n",
    "\n",
    "    # def write_to_file(self, decoded, max, original, sample_r, baseline_r, iter):\n",
    "    #     with open(\"temp.txt\", \"w\") as f:\n",
    "    #         f.write(\"iter:\"+str(iter)+\"\\n\")\n",
    "    #         for i in range(len(original)):\n",
    "    #             f.write(\"dec: \"+decoded[i]+\"\\n\")\n",
    "    #             f.write(\"max: \"+max[i]+\"\\n\")\n",
    "    #             f.write(\"org: \"+original[i]+\"\\n\")\n",
    "    #             f.write(\"Sample_R: %.4f, Baseline_R: %.4f\\n\\n\"%(sample_r[i].item(), baseline_r[i].item()))\n",
    "\n",
    "\n",
    "    def train_one_batch(self, batch,test_batch, iter):\n",
    "        ans_list, batch_scores = None, None\n",
    "        # Train\n",
    "#         enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "\n",
    "        enc_batch, enc_lens, enc_padding_mask, \\\n",
    "        enc_key_batch, enc_key_lens, enc_key_padding_mask,\\\n",
    "        enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "\n",
    "        enc_batch = self.model.embeds(enc_batch)  # Get embeddings for encoder input\n",
    "        enc_key_batch = self.model.embeds(enc_key_batch)  # Get key embeddings for encoder input\n",
    "        enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        # Test\n",
    "#         enc_batch2, enc_lens2, enc_padding_mask2, enc_batch_extend_vocab2, extra_zeros2, context2 = get_enc_data(test_batch)\n",
    "        enc_batch2, enc_lens2, enc_padding_mask2, \\\n",
    "        enc_key_batch2, enc_key_lens2, enc_key_padding_mask2,\\\n",
    "        enc_batch_extend_vocab2, extra_zeros2, context2 = get_enc_data(test_batch)\n",
    "    \n",
    "        with T.autograd.no_grad():\n",
    "            enc_batch2 = self.model.embeds(enc_batch2)\n",
    "            enc_key_batch2 = self.model.embeds(enc_key_batch2)\n",
    "            enc_out2, enc_hidden2 = self.model.encoder(enc_batch2, enc_lens2)\n",
    "        # -------------------------------Summarization-----------------------\n",
    "        if self.opt.train_mle == True:  # perform MLE training\n",
    "            mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros,\n",
    "                                            enc_batch_extend_vocab, enc_key_batch, enc_key_lens, batch)\n",
    "            mle_loss_2 = self.train_batch_MLE(enc_out2, enc_hidden2, enc_padding_mask2, context2, extra_zeros2,\n",
    "                                            enc_batch_extend_vocab2, enc_key_batch2, enc_key_lens2, test_batch)\n",
    "        else:\n",
    "            mle_loss = get_cuda(T.FloatTensor([0]))\n",
    "            mle_loss_2 = get_cuda(T.FloatTensor([0]))\n",
    "        # original view\n",
    "#         if opt.view:\n",
    "#             sample_sents, ans_list = self.train_batch_decode(batch, enc_out, enc_hidden, enc_padding_mask, context,\n",
    "#                                                              extra_zeros, enc_batch_extend_vocab, batch.rev_oovs,\n",
    "#                                                              greedy=True)\n",
    "#             rouge_l_f1, batch_scores, avg_rouge_l_f1 = self.reward_function(sample_sents, batch.original_summarys)\n",
    "#             #             writer.add_text('Train/%s'% (iter), ans_list[0]['decoded_str'] , iter)\n",
    "#             #             writer.add_text('Train/%s'% (iter), ans_list[0]['summary'] , iter)\n",
    "#             #             writer.add_text('Train/%s'% (iter), ans_list[0]['review'] , iter)\n",
    "#             writer.add_scalar('Train/avg_rouge_l_f1', avg_rouge_l_f1, iter)\n",
    "            \n",
    "        # --------------RL training-----------------------------------------------------\n",
    "        if self.opt.train_rl == True:  # perform reinforcement learning training\n",
    "            # multinomial sampling\n",
    "            sample_sents, RL_log_probs = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context,\n",
    "                                                             extra_zeros, enc_batch_extend_vocab, batch.rev_oovs,\n",
    "                                                             greedy=False)\n",
    "            sample_sents2, RL_log_probs2 = self.train_batch_RL(enc_out2, enc_hidden2, enc_padding_mask2, context2,\n",
    "                                                             extra_zeros2, enc_batch_extend_vocab2, test_batch.rev_oovs,\n",
    "                                                             greedy=False)\n",
    "            with T.autograd.no_grad():\n",
    "                # greedy sampling\n",
    "                greedy_sents, _ = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros,\n",
    "                                                      enc_batch_extend_vocab, batch.rev_oovs, greedy=True)\n",
    "\n",
    "            sample_reward, _, _ = self.reward_function(sample_sents, batch.original_summarys)\n",
    "            baseline_reward, _, _ = self.reward_function(greedy_sents, batch.original_summarys)\n",
    "            # if iter%200 == 0:\n",
    "            #     self.write_to_file(sample_sents, greedy_sents, batch.original_abstracts, sample_reward, baseline_reward, iter)\n",
    "            rl_loss = -(sample_reward - baseline_reward) * RL_log_probs  # Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "            rl_loss = T.mean(rl_loss)\n",
    "\n",
    "            batch_reward = T.mean(sample_reward).item()\n",
    "            writer.add_scalar('Train_RL/RL_log_probs', RL_log_probs, iter)\n",
    "        else:\n",
    "            rl_loss = get_cuda(T.FloatTensor([0]))\n",
    "            batch_reward = 0\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        #         if opt.train_mle == True: \n",
    "        self.trainer.zero_grad()\n",
    "        (self.opt.mle_weight * mle_loss + self.opt.rl_weight * rl_loss).backward()\n",
    "        self.trainer.step()\n",
    "        #-----------------------Summarization----------------------------------------------------\n",
    "        if iter % 5000 == 0:\n",
    "            with T.autograd.no_grad():\n",
    "                train_rouge_l_f = self.calc_avg_rouge_result(iter,batch,'Train',enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens)\n",
    "                test_rouge_l_f = self.calc_avg_rouge_result(iter,test_batch,'Test',enc_hidden2, enc_out2, enc_padding_mask2, context2, extra_zeros2, enc_batch_extend_vocab2, enc_key_batch2, enc_key_lens2)\n",
    "                writer.add_scalars('Compare/rouge-l-f',  \n",
    "                   {'train_rouge_l_f': train_rouge_l_f,\n",
    "                    'test_rouge_l_f': test_rouge_l_f\n",
    "                   }, iter)\n",
    "                \n",
    "#         return mle_loss.item(), batch_reward, ans_list, batch_scores\n",
    "        return mle_loss.item(),mle_loss_2.item(), batch_reward\n",
    "\n",
    "    def calc_avg_rouge_result(self, iter, batch, mode, enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens):\n",
    "        pred_ids = beam_search(enc_hidden, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, enc_key_batch, enc_key_lens, self.model, self.start_id, self.end_id, self.unk_id)\n",
    "\n",
    "        decoded_sents = []\n",
    "        ref_sents = []\n",
    "        ref_sents2 = []\n",
    "        article_sents = []\n",
    "        \n",
    "        summary_len = max_summary_len = long_seq_index = 0\n",
    "        for i in range(len(pred_ids)):            \n",
    "            decoded_words = data.outputids2words(pred_ids[i], self.vocab, batch.rev_oovs[i])\n",
    "            if len(decoded_words) < 2:\n",
    "                decoded_words = \"xxx\"\n",
    "            else:\n",
    "                decoded_words = \" \".join(decoded_words)\n",
    "            decoded_sents.append(decoded_words)\n",
    "            summary = batch.original_summarys[i]\n",
    "            summary2 = batch.original_summary_sents[i]\n",
    "            review = batch.original_reviews[i]\n",
    "            ref_sents.append(summary)\n",
    "            ref_sents2.append(summary2)\n",
    "            article_sents.append(review) \n",
    "            summary_len = len(summary.split(\" \"))\n",
    "            if max_summary_len < summary_len: \n",
    "                max_summary_len = summary_len\n",
    "                long_seq_index = i\n",
    "\n",
    "        rouge = Rouge()    \n",
    "        score = rouge.get_scores(decoded_sents, ref_sents, avg = True)    \n",
    "        writer.add_scalars('%s/rouge-1' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-1']['f'],\n",
    "                'p': score['rouge-1']['p'],\n",
    "                'r': score['rouge-1']['r']}\n",
    "                , iter)\n",
    "        writer.add_scalars('%s/rouge-2' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-2']['f'],\n",
    "                'p': score['rouge-2']['p'],\n",
    "                'r': score['rouge-2']['r']}\n",
    "                , iter)\n",
    "        writer.add_scalars('%s/rouge-l' % mode,  # 'rouge-2' , 'rouge-l'\n",
    "               {'f': score['rouge-l']['f'],\n",
    "                'p': score['rouge-l']['p'],\n",
    "                'r': score['rouge-l']['r']}\n",
    "                , iter)\n",
    "#         for i in range(len(decoded_sents)):\n",
    "#             if type(article_sents[i]) != str: continue\n",
    "#             if type(ref_sents[i]) != str:  continue\n",
    "#             if type(decoded_sents[i]) != str:  continue\n",
    "\n",
    "#         writer.add_text('Rouge/%s/%s' % (iter,mode), decoded_sents[long_seq_index], iter)\n",
    "#         writer.add_text('Rouge/%s/%s' % (iter,mode), ref_sents[long_seq_index], iter)\n",
    "#         #writer.add_text('Rouge/%s/%s' % (iter,mode), ref_sents2[long_seq_index], iter)\n",
    "#         writer.add_text('Rouge/%s/%s' % (iter,mode), article_sents[long_seq_index], iter)\n",
    "        \n",
    "        for i in range(len(decoded_sents)):\n",
    "#             writer.add_text('Rouge/%s/%s' % (iter,mode), decoded_sents[i], iter)\n",
    "            writer.add_text('Rouge/%s/%s' % (iter,mode), ref_sents[i], iter)\n",
    "            #writer.add_text('Rouge/%s/%s' % (iter,mode), ref_sents2[i], iter)\n",
    "            writer.add_text('Rouge/%s/%s' % (iter,mode), article_sents[i], iter)\n",
    "        \n",
    "        return score['rouge-l']['f']\n",
    "    \n",
    "    def get_best_res_score(self, results, scores):\n",
    "        max_score = float(0)\n",
    "        _id = 0\n",
    "        for idx in range(len(results)):\n",
    "            re_matchData = re.compile(r'\\-?\\d{1,10}\\.?\\d{1,10}')\n",
    "            data = re.findall(re_matchData, str(scores[idx]))\n",
    "            score = sum([float(d) for d in data])\n",
    "            if score > max_score:\n",
    "                _id = idx\n",
    "        return results[_id], scores[_id]\n",
    "\n",
    "    def get_lr(self):\n",
    "        for param_group in self.trainer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    def get_weight_decay(self):\n",
    "        for param_group in self.trainer.param_groups:\n",
    "            #             print(param_group)\n",
    "            return param_group['weight_decay']\n",
    "\n",
    "    def trainIters(self):\n",
    "        final_file_path = None\n",
    "        iter = self.setup_train()\n",
    "        epoch = 0\n",
    "        count = test_mle_total = train_mle_total = r_total = 0\n",
    "        logger.info(u'------Training START--------')\n",
    "        test_batch = self.test_batcher.next_batch()\n",
    "        #         while iter <= config.max_iterations:\n",
    "        while epoch <= config.max_epochs:\n",
    "            train_batch = self.train_batcher.next_batch()\n",
    "            try:\n",
    "    #                 train_mle_loss, train_r, ans_list, batch_scores = self.train_one_batch(train_batch,test_batch, iter)\n",
    "                train_mle_loss,test_mle_loss, r = self.train_one_batch(train_batch,test_batch, iter)\n",
    "\n",
    "                #                 writer.add_scalar('lr', self.get_lr(), iter)\n",
    "    #                 writer.add_scalar('Train/mle_loss', train_mle_loss, iter)\n",
    "                writer.add_scalar('RL_Train/reward', r, iter)\n",
    "\n",
    "    #                 writer.add_scalar('Test/mle_loss', test_mle_loss, iter)\n",
    "\n",
    "                writer.add_scalars('Compare/mle_loss' ,  \n",
    "                   {'train_mle_loss': train_mle_loss,\n",
    "                    'test_mle_loss': test_mle_loss\n",
    "                   }, iter)\n",
    "                \n",
    "#             # break\n",
    "            except KeyboardInterrupt:\n",
    "                logger.info(\"-------------------Keyboard Interrupt------------------\")\n",
    "                exit(0)\n",
    "            except Exception as e:                \n",
    "                logger.info(\"-------------------Ignore error------------------\\n%s\\n\" % e)\n",
    "                print(\"Please load final_file_path : %s\" % final_file_path)\n",
    "                traceback = sys.exc_info()[2]\n",
    "                print(sys.exc_info())\n",
    "                print(traceback.tb_lineno)\n",
    "                print(e)\n",
    "                break\n",
    "            # if opt.train_mle == False: break\n",
    "            train_mle_total += train_mle_loss\n",
    "            r_total += r\n",
    "            test_mle_total += test_mle_loss\n",
    "            count += 1\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 1000 == 0:\n",
    "                train_mle_avg = train_mle_total / count\n",
    "                r_avg = r_total / count\n",
    "                test_mle_avg = test_mle_total / count\n",
    "                epoch = int((iter * config.batch_size) / train_num) + 1\n",
    "                logger.info('epoch: %s iter: %s train_mle_loss: %.3f test_mle_loss: %.3f reward: %.3f \\n' % (epoch, iter, train_mle_avg, test_mle_avg, r_avg))\n",
    "\n",
    "                count = test_mle_total = train_mle_total = r_total = 0\n",
    "#                 writer.add_scalar('Train/mle_avg_loss', train_mle_avg, iter)\n",
    "#                 writer.add_scalar('Test/mle_avg_loss', test_mle_avg, iter)\n",
    "                writer.add_scalar('RL_Train/r_avg', r_avg, iter)\n",
    "                \n",
    "                writer.add_scalars('Compare/mle_avg_loss' ,  \n",
    "                   {'train_mle_avg': train_mle_avg,\n",
    "                    'test_mle_avg': test_mle_avg\n",
    "                   }, iter)\n",
    "            # break\n",
    "            if iter % 5000 == 0:\n",
    "                final_file_path = self.save_model(iter, test_mle_avg, r_avg)\n",
    "#                 if opt.view:\n",
    "#                     best_res, best_score = self.get_best_res_score(ans_list, batch_scores)\n",
    "#                     logger.info('best_res: %s \\n' % (best_res))\n",
    "#                     logger.info('best_score: %s \\n' % (best_score))\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['decoded_str'], iter)\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['summary'], iter)\n",
    "#                     writer.add_text('Train/%s' % (iter), best_res['review'], iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_action(opt):\n",
    "    try:       \n",
    "        opt.rl_weight = 1 - opt.mle_weight  \n",
    "\n",
    "        if opt.load_model:\n",
    "            opt.load_model = \"/%s/%s\"%(opt.word_emb_type,opt.load_model)    \n",
    "\n",
    "        logger.info(u'------Training Setting--------')  \n",
    "   \n",
    "        logger.info(\"Traing Type :%s\" %(config.data_type))\n",
    "        if opt.train_mle == True:\n",
    "            logger.info(\"Training mle: %s, mle weight: %.2f\"%(opt.train_mle, opt.mle_weight))\n",
    "\n",
    "        if opt.train_rl == True:\n",
    "            logger.info(\"Training rl: %s, rl weight: %.2f \\n\"%(opt.train_rl, opt.rl_weight))\n",
    "\n",
    "        if opt.word_emb_type == 'bert': config.emb_dim = 768\n",
    "        if opt.pre_train_emb : \n",
    "            logger.info('use pre_train_%s vocab_size %s \\n'%(opt.word_emb_type,config.vocab_size))\n",
    "\n",
    "        else:\n",
    "            logger.info('use %s vocab_size %s \\n'%(opt.word_emb_type,config.vocab_size))\n",
    "\n",
    "        logger.info(\"intra_encoder: %s intra_decoder: %s \\n\"%(config.intra_encoder, config.intra_decoder))\n",
    "        if opt.word_emb_type in ['word2Vec','glove']:\n",
    "            config.vocab_path = config.Data_path + \"Embedding/%s/word.vocab\"%(opt.word_emb_type)            \n",
    "#             config.vocab_size = len(open(config.vocab_path).readlines())\n",
    "            vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "        train_processor = Train(opt,vocab)\n",
    "        train_processor.trainIters()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        traceback = sys.exc_info()[2]\n",
    "        logger.error(sys.exc_info())\n",
    "        logger.error(traceback.tb_lineno)\n",
    "        logger.error(e)\n",
    "    logger.info(u'------Training END--------')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "def write_enc_graph():\n",
    "    encoder_writer = SummaryWriter('runs/Pointer-Generator/glove/Encoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    encoder = Encoder().to(device) \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "#     enc_batch = Variable(torch.rand(enc_batch.shape)).to(device) \n",
    "    enc_lens = torch.from_numpy(enc_lens).to(device) \n",
    "\n",
    "    encoder_writer.add_graph(encoder, (enc_batch, enc_lens), verbose=True)\n",
    "    encoder_writer.close()\n",
    "\n",
    "def write_dec_graph():\n",
    "    decoder_writer = SummaryWriter('runs/Pointer-Generator/glove/Decoder')\n",
    "    device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "    # decoder = Decoder().to(device)    \n",
    "\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    model = Model(False,'word2Vec',vocab)\n",
    "    \n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    enc_batch, enc_lens, enc_padding_mask, enc_key_batch, enc_key_lens, enc_key_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "    enc_batch = model.embeds(enc_batch) #Get embeddings for encoder input\n",
    "    enc_out, enc_hidden = model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "    # train_batch_MLE\n",
    "    dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "    step_losses = []\n",
    "    s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "    # x_t 為decoder每一個time step 的batch input\n",
    "    x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(2))                             #Input to the decoder\n",
    "    prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in DEEP REINFORCED MODEL - https://arxiv.org/pdf/1705.04304.pdf)\n",
    "    sum_temporal_srcs = None     \n",
    "\n",
    "\n",
    "    for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "        use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "        # use_gound_truth * dec_batch[:, t] : 為ground true time step token\n",
    "        # (1 - use_gound_truth) * x_t : 為previous time step token\n",
    "        if t == 0 :temp_batch = dec_batch[:, t]\n",
    "        x_t = use_gound_truth * temp_batch + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "        x_t = model.embeds(x_t)\n",
    "    #     final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "        final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = model.decoder(\n",
    "        x_t, s_t, enc_out, enc_padding_mask,context, \n",
    "        extra_zeros,enc_batch_extend_vocab,sum_temporal_srcs, prev_s, \n",
    "        enc_key_batch, enc_key_lens)   \n",
    "        \n",
    "#         (, , , , , , , , ,, ), verbose=True)\n",
    "        '''\n",
    "        intra_encoder = True   intra_decoder = True\n",
    "        <class 'torch.Tensor'> <class 'tuple'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
    "        <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
    "        <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'numpy.ndarray'>\n",
    "        '''\n",
    "        print(type(x_t),type(s_t),type(enc_out),type(enc_padding_mask))\n",
    "        print(type(context),type(extra_zeros),type(enc_batch_extend_vocab))\n",
    "        print(type(sum_temporal_srcs),type(prev_s),type(enc_key_batch),type(enc_key_lens))\n",
    "\n",
    "        #         decoder_summary = summary(model.decoder, x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens) # encoder summary\n",
    "#         x_t = Variable(torch.rand(x_t.shape)).to(device) \n",
    "        #             s_t = Variable(torch.rand(s_t.shape)).to(device)\n",
    "#         enc_out = Variable(torch.rand(enc_out.shape)).to(device)\n",
    "#         enc_padding_mask = Variable(torch.rand(enc_padding_mask.shape)).to(device,dtype=torch.long)\n",
    "#         context = Variable(torch.rand(context.shape)).to(device)\n",
    "#         extra_zeros = Variable(torch.rand(extra_zeros.shape)).to(device)\n",
    "#         enc_batch_extend_vocab = Variable(torch.rand(enc_batch_extend_vocab.shape)).to(device)\n",
    "        #             sum_temporal_srcs = Variable(torch.rand(sum_temporal_srcs.shape)).to(device)\n",
    "        #             prev_s = Variable(torch.rand(prev_s.shape)).to(device)\n",
    "#         enc_key_batch = Variable(torch.rand(enc_key_batch.shape)).to(device)\n",
    "        enc_key_lens = torch.from_numpy(enc_key_lens).to(device) \n",
    "        \n",
    "        decoder_writer.add_graph(model.decoder, \n",
    "                         (x_t, s_t, enc_out, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s,enc_key_batch, enc_key_lens), verbose=True)\n",
    "        \n",
    "        decoder_writer.close()\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-23 16:32:12 - Text-Summary - INFO: - logger已啟動\n",
      "2020-01-23 16:32:12 - Text-Summary - INFO: - ------Training Setting--------\n",
      "2020-01-23 16:32:12 - Text-Summary - INFO: - Traing Type :Cameras\n",
      "2020-01-23 16:32:12 - Text-Summary - INFO: - Training mle: True, mle weight: 1.00\n",
      "2020-01-23 16:32:13 - Text-Summary - INFO: - use pre_train_glove vocab_size 50000 \n",
      "\n",
      "2020-01-23 16:32:13 - Text-Summary - INFO: - intra_encoder: True intra_decoder: True \n",
      "\n",
      "2020-01-23 16:32:27 - Text-Summary - INFO: - Model(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
      "    (reduce_h): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (reduce_c): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (enc_attention): encoder_attention(\n",
      "      (W_h): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (W_s): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v): Linear(in_features=1024, out_features=1, bias=False)\n",
      "    )\n",
      "    (dec_attention): decoder_attention(\n",
      "      (W_prev): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (W_s): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (x_context): Linear(in_features=1324, out_features=300, bias=True)\n",
      "    (lstm): LSTMCell(300, 512)\n",
      "    (p_gen_linear): Linear(in_features=2860, out_features=1, bias=True)\n",
      "    (V): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (V1): Linear(in_features=512, out_features=50000, bias=True)\n",
      "  )\n",
      "  (embeds): Embedding(50000, 300)\n",
      ")\n",
      "2020-01-23 16:32:27 - Text-Summary - INFO: - ------Training START--------\n",
      "2020-01-23 16:44:01 - Text-Summary - INFO: - epoch: 1 iter: 1000 train_mle_loss: 4.040 test_mle_loss: 4.272 reward: 0.000 \n",
      "\n",
      "2020-01-23 16:54:49 - Text-Summary - INFO: - epoch: 2 iter: 2000 train_mle_loss: 3.573 test_mle_loss: 3.910 reward: 0.000 \n",
      "\n",
      "2020-01-23 17:06:20 - Text-Summary - INFO: - epoch: 3 iter: 3000 train_mle_loss: 3.522 test_mle_loss: 3.822 reward: 0.000 \n",
      "\n",
      "2020-01-23 17:17:45 - Text-Summary - INFO: - epoch: 4 iter: 4000 train_mle_loss: 3.387 test_mle_loss: 3.740 reward: 0.000 \n",
      "\n",
      "2020-01-23 17:29:30 - Text-Summary - INFO: - epoch: 5 iter: 5000 train_mle_loss: 3.336 test_mle_loss: 3.669 reward: 0.000 \n",
      "\n",
      "2020-01-23 17:41:20 - Text-Summary - INFO: - epoch: 5 iter: 6000 train_mle_loss: 3.263 test_mle_loss: 3.609 reward: 0.000 \n",
      "\n",
      "2020-01-23 17:53:49 - Text-Summary - INFO: - epoch: 6 iter: 7000 train_mle_loss: 3.201 test_mle_loss: 3.553 reward: 0.000 \n",
      "\n",
      "2020-01-23 18:04:37 - Text-Summary - INFO: - epoch: 7 iter: 8000 train_mle_loss: 3.111 test_mle_loss: 3.519 reward: 0.000 \n",
      "\n",
      "2020-01-23 18:15:27 - Text-Summary - INFO: - epoch: 8 iter: 9000 train_mle_loss: 3.093 test_mle_loss: 3.469 reward: 0.000 \n",
      "\n",
      "2020-01-23 18:26:18 - Text-Summary - INFO: - epoch: 9 iter: 10000 train_mle_loss: 3.029 test_mle_loss: 3.444 reward: 0.000 \n",
      "\n",
      "2020-01-23 18:37:04 - Text-Summary - INFO: - epoch: 10 iter: 11000 train_mle_loss: 2.984 test_mle_loss: 3.436 reward: 0.000 \n",
      "\n",
      "2020-01-23 18:47:47 - Text-Summary - INFO: - epoch: 10 iter: 12000 train_mle_loss: 2.958 test_mle_loss: 3.394 reward: 0.000 \n",
      "\n",
      "2020-01-23 18:58:36 - Text-Summary - INFO: - epoch: 11 iter: 13000 train_mle_loss: 2.907 test_mle_loss: 3.373 reward: 0.000 \n",
      "\n",
      "2020-01-23 19:09:17 - Text-Summary - INFO: - epoch: 12 iter: 14000 train_mle_loss: 2.852 test_mle_loss: 3.366 reward: 0.000 \n",
      "\n",
      "2020-01-23 19:20:02 - Text-Summary - INFO: - epoch: 13 iter: 15000 train_mle_loss: 2.805 test_mle_loss: 3.344 reward: 0.000 \n",
      "\n",
      "2020-01-23 19:30:47 - Text-Summary - INFO: - epoch: 14 iter: 16000 train_mle_loss: 2.782 test_mle_loss: 3.344 reward: 0.000 \n",
      "\n",
      "2020-01-23 19:41:45 - Text-Summary - INFO: - epoch: 15 iter: 17000 train_mle_loss: 2.768 test_mle_loss: 3.318 reward: 0.000 \n",
      "\n",
      "2020-01-23 19:52:30 - Text-Summary - INFO: - epoch: 15 iter: 18000 train_mle_loss: 2.704 test_mle_loss: 3.299 reward: 0.000 \n",
      "\n",
      "2020-01-23 20:03:21 - Text-Summary - INFO: - epoch: 16 iter: 19000 train_mle_loss: 2.654 test_mle_loss: 3.272 reward: 0.000 \n",
      "\n",
      "2020-01-23 20:14:05 - Text-Summary - INFO: - epoch: 17 iter: 20000 train_mle_loss: 2.633 test_mle_loss: 3.285 reward: 0.000 \n",
      "\n",
      "2020-01-23 20:24:58 - Text-Summary - INFO: - epoch: 18 iter: 21000 train_mle_loss: 2.586 test_mle_loss: 3.273 reward: 0.000 \n",
      "\n",
      "2020-01-23 20:35:42 - Text-Summary - INFO: - epoch: 19 iter: 22000 train_mle_loss: 2.502 test_mle_loss: 3.301 reward: 0.000 \n",
      "\n",
      "2020-01-23 20:46:32 - Text-Summary - INFO: - epoch: 20 iter: 23000 train_mle_loss: 2.493 test_mle_loss: 3.295 reward: 0.000 \n",
      "\n",
      "2020-01-23 20:57:14 - Text-Summary - INFO: - epoch: 20 iter: 24000 train_mle_loss: 2.418 test_mle_loss: 3.304 reward: 0.000 \n",
      "\n",
      "2020-01-23 21:08:00 - Text-Summary - INFO: - epoch: 21 iter: 25000 train_mle_loss: 2.346 test_mle_loss: 3.354 reward: 0.000 \n",
      "\n",
      "2020-01-23 21:19:00 - Text-Summary - INFO: - epoch: 22 iter: 26000 train_mle_loss: 2.315 test_mle_loss: 3.347 reward: 0.000 \n",
      "\n",
      "2020-01-23 21:29:44 - Text-Summary - INFO: - epoch: 23 iter: 27000 train_mle_loss: 2.299 test_mle_loss: 3.333 reward: 0.000 \n",
      "\n",
      "2020-01-23 21:40:24 - Text-Summary - INFO: - epoch: 24 iter: 28000 train_mle_loss: 2.143 test_mle_loss: 3.369 reward: 0.000 \n",
      "\n",
      "2020-01-23 21:51:18 - Text-Summary - INFO: - epoch: 25 iter: 29000 train_mle_loss: 2.148 test_mle_loss: 3.321 reward: 0.000 \n",
      "\n",
      "2020-01-23 22:01:55 - Text-Summary - INFO: - epoch: 25 iter: 30000 train_mle_loss: 2.070 test_mle_loss: 3.421 reward: 0.000 \n",
      "\n",
      "2020-01-23 22:12:53 - Text-Summary - INFO: - epoch: 26 iter: 31000 train_mle_loss: 2.013 test_mle_loss: 3.400 reward: 0.000 \n",
      "\n",
      "2020-01-23 22:15:52 - Text-Summary - ERROR: - xxxxxxxxxxx\n",
      "2020-01-23 22:15:52 - Text-Summary - ERROR: - (<class 'RuntimeError'>, RuntimeError('CUDA error: device-side assert triggered',), <traceback object at 0x7fa1c04b53c8>)\n",
      "2020-01-23 22:15:52 - Text-Summary - ERROR: - 105\n",
      "2020-01-23 22:15:52 - Text-Summary - ERROR: - CUDA error: device-side assert triggered\n",
      "2020-01-23 22:15:52 - Text-Summary - ERROR: - xxxxxxxxxxx\n",
      "2020-01-23 22:15:52 - Text-Summary - INFO: - -------------------Ignore error------------------\n",
      "CUDA error: device-side assert triggered\n",
      "\n",
      "2020-01-23 22:15:52 - Text-Summary - INFO: - ------Training END--------\n",
      "2020-01-23 22:15:52 - Text-Summary - INFO: - logger已關閉\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please load final_file_path : /0030000_3.42_0.00.tar\n",
      "(<class 'RuntimeError'>, RuntimeError('CUDA error: device-side assert triggered',), <traceback object at 0x7fa1c010aa08>)\n",
      "412\n",
      "CUDA error: device-side assert triggered\n"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/u012869752/article/details/72513141\n",
    "# 由于在jupyter notebook中，args不为空\n",
    "from glob import glob\n",
    "# nvidia-smi -pm 1\n",
    "if __name__ == \"__main__\":   \n",
    "    try:\n",
    "        # --------------------------Training ----------------------------------\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--train_mle', type=bool, default=True)\n",
    "        parser.add_argument('--train_rl', type=bool, default=False)\n",
    "        parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "#         parser.add_argument('--load_model', type=str, default='/0044000_1.91_0.00.tar')\n",
    "        parser.add_argument('--load_model', type=str, default=None)\n",
    "        parser.add_argument('--new_lr', type=float, default=None)\n",
    "        parser.add_argument('--multi_device', type=bool, default=True)\n",
    "        parser.add_argument('--view', type=bool, default=True)\n",
    "        parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "        parser.add_argument('--word_emb_type', type=str, default='glove')\n",
    "        parser.add_argument('--train_action', type=bool, default=True)\n",
    "        opt = parser.parse_args(args=[])\n",
    "        \n",
    "        today = dt.now()\n",
    "        loggerPath = \"LOG/%s-(%s_%s_%s)-(%s:%s:%s)\"%(opt.word_emb_type,\n",
    "                  today.year,today.month,today.day,\n",
    "                  today.hour,today.minute,today.second)\n",
    "\n",
    "        logger = getLogger(config.loggerName,loggerPath)   \n",
    "        \n",
    "        if opt.load_model == None:\n",
    "            shutil.rmtree('runs/Pointer-Generator/glove', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/glove/exp-4', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/glove/Eecoder', ignore_errors=True) # clear previous \n",
    "            shutil.rmtree('runs/Pointer-Generator/glove/Decoder', ignore_errors=True) # clear previous \n",
    "\n",
    "                \n",
    "        writer = SummaryWriter('runs/Pointer-Generator/glove/exp-4')\n",
    "        \n",
    "#         write_enc_graph()\n",
    "#         write_dec_graph()\n",
    "        if opt.train_action: train_action(opt)\n",
    "        # --------------------------Testing ----------------------------------\n",
    "#         parser = argparse.ArgumentParser()\n",
    "#         parser.add_argument(\"--task\", type=str, default=\"validate\", choices=[\"validate\",\"test\"])\n",
    "#         parser.add_argument(\"--start_from\", type=int, default=\"0020000\")\n",
    "# #         parser.add_argument(\"--load_model\", type=str, default=None)\n",
    "#         parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "#         parser.add_argument('--word_emb_type', type=str, default='bert')\n",
    "#         opt = parser.parse_args(args=[])                \n",
    "#         if opt.word_emb_type == 'bert': config.emb_dim = 768\n",
    "#         test_action(opt)\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback = sys.exc_info()[2]\n",
    "        print(sys.exc_info())\n",
    "        print(traceback.tb_lineno)\n",
    "        print(e)\n",
    "    finally:\n",
    "        removeLogger(logger)\n",
    "        # export scalar data to JSON for external processing\n",
    "        # tensorboard --logdir /home/eagleuser/Users/leyan/Text-Summarizer-FOP/TensorBoard\n",
    "#         tensorboard --logdir ./runs\n",
    "#         if not os.path.exists('TensorBoard'): os.makedirs('TensorBoard')\n",
    "#         writer.export_scalars_to_json(\"TensorBoard/test.json\")\n",
    "        writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
