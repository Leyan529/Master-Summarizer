{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "import torch as T\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from model import Model\n",
    "\n",
    "\n",
    "from data_util import config, data\n",
    "from data_util.batcher import Batcher\n",
    "from data_util.data import Vocab\n",
    "\n",
    "\n",
    "from train_util import *\n",
    "from torch.distributions import Categorical\n",
    "from rouge import Rouge\n",
    "from numpy import random\n",
    "import argparse\n",
    "import torchsnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(123)\n",
    "# T.manual_seed(123)\n",
    "# if T.cuda.is_available():\n",
    "#     T.cuda.manual_seed_all(123)\n",
    "#     print(\"is_available\", T.cuda.is_available())\n",
    "#     print(\"use_cuda\", use_cuda)\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# T.cuda.set_device(1)\n",
    "\n",
    "# use_cuda = config.use_gpu and torch.cuda.is_available()\n",
    "# print(\"use_cuda\", T.cuda.is_available())\n",
    "\n",
    "# print(\"use_cuda\", use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch():\n",
    "    vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "    batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                           batch_size=config.batch_size, single_pass=False)\n",
    "    batch = batcher.next_batch()\n",
    "    # with torchsnooper.snoop():\n",
    "    while batch is not None:\n",
    "        example_list = batch.example_list\n",
    "        for ex in example_list:\n",
    "            r = str(ex.original_review)\n",
    "            s = str(ex.original_summary)\n",
    "            k = str(ex.key_words)\n",
    "            sent = ex.original_summary_sents\n",
    "#             print(\"original_review_sents:\", r)\n",
    "#             print(\"original_summary_sents : \", s)\n",
    "#             print(\"review_key_words : \", k)\n",
    "#             print('------------------------------------------------------------\\n')\n",
    "        batch = batcher.next_batch()        \n",
    "# test_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bin Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 26716\n",
      "\n",
      "test : 8905\n",
      "\n",
      "valid : 8905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"bin/bin-info.txt\",'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    [print(line) for line in lines]\n",
    "    train_num = int(lines[0].split(\":\")[1])\n",
    "    test_num = int(lines[1].split(\":\")[1])\n",
    "    val_num = int(lines[2].split(\":\")[1])\n",
    "    # f.write(\"train : %s\\n\"%(len(flit_key_train_df)))\n",
    "    # f.write(\"test : %s\\n\"%(len(flit_key_test_df)))\n",
    "    # f.write(\"valid : %s\\n\"%(len(flit_key_valid_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View model summary\n",
    "#### 只有torchsummaryX成功\n",
    "#### 日後將以此模擬呈現結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary # 不支援RNN\n",
    "# from model import Encoder,Model\n",
    "# # https://www.cnblogs.com/lindaxin/p/8052043.html\n",
    "# device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# encoder = Encoder().to(device)    \n",
    "\n",
    "# vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "# batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "#                        batch_size=config.batch_size, single_pass=False)\n",
    "# batch = batcher.next_batch()\n",
    "# enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# enc_batch = Model().embeds(enc_batch) # Get embeddings for encoder input\n",
    "\n",
    "# # summary(encoder, enc_batch, enc_lens, show_hierarchical=True) \n",
    "# # summary(encoder, [enc_batch, enc_lens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modelsummary import summary # 未知問題\n",
    "# from model import Encoder,Model\n",
    "# # https://www.cnblogs.com/lindaxin/p/8052043.html\n",
    "# device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# encoder = Encoder().to(device)    \n",
    "\n",
    "# vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "# batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "#                        batch_size=config.batch_size, single_pass=False)\n",
    "# batch = batcher.next_batch()\n",
    "# enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "# enc_batch = Model().embeds(enc_batch) # Get embeddings for encoder input\n",
    "\n",
    "# # summary(encoder, enc_batch, enc_lens, show_hierarchical=True) \n",
    "# # summary(encoder, enc_batch, enc_lens, show_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "           Kernel Shape Output Shape   Params  Mult-Adds\n",
      "Layer                                                   \n",
      "0_lstm                -  [690, 1024]  3334144    3325952\n",
      "1_reduce_h  [1024, 512]     [8, 512]   524800     524288\n",
      "2_reduce_c  [1024, 512]     [8, 512]   524800     524288\n",
      "--------------------------------------------------------\n",
      "                       Totals\n",
      "Total params          4383744\n",
      "Trainable params      4383744\n",
      "Non-trainable params        0\n",
      "Mult-Adds             4374528\n",
      "========================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_lstm</th>\n",
       "      <td>-</td>\n",
       "      <td>[690, 1024]</td>\n",
       "      <td>3334144</td>\n",
       "      <td>3325952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_reduce_h</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_reduce_c</th>\n",
       "      <td>[1024, 512]</td>\n",
       "      <td>[8, 512]</td>\n",
       "      <td>524800</td>\n",
       "      <td>524288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kernel Shape Output Shape   Params  Mult-Adds\n",
       "Layer                                                   \n",
       "0_lstm                -  [690, 1024]  3334144    3325952\n",
       "1_reduce_h  [1024, 512]     [8, 512]   524800     524288\n",
       "2_reduce_c  [1024, 512]     [8, 512]   524800     524288"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "from model import Encoder,Model\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "encoder = Encoder().to(device)    \n",
    "\n",
    "vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "batcher = Batcher(config.train_data_path, vocab, mode='train',\n",
    "                       batch_size=config.batch_size, single_pass=False)\n",
    "batch = batcher.next_batch()\n",
    "enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "enc_batch = Model(False,'word2Vec',vocab).embeds(enc_batch) #Get embeddings for encoder input\n",
    "\n",
    "summary(encoder, enc_batch, enc_lens) # encoder summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummaryX import summary\n",
    "class Train(object):\n",
    "    def __init__(self, opt):\n",
    "        self.vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "#         print('use %s vocab_size %s '%(opt.word_emb_type,config.vocab_size))\n",
    "\n",
    "        self.batcher = Batcher(config.train_data_path, self.vocab, mode='train',\n",
    "                               batch_size=config.batch_size, single_pass=False)\n",
    "        self.opt = opt\n",
    "        self.start_id = self.vocab.word2id(data.START_DECODING)\n",
    "        self.end_id = self.vocab.word2id(data.STOP_DECODING)\n",
    "        self.pad_id = self.vocab.word2id(data.PAD_TOKEN)\n",
    "        self.unk_id = self.vocab.word2id(data.UNKNOWN_TOKEN)\n",
    "        time.sleep(5)\n",
    "\n",
    "    def save_model(self, iter):\n",
    "        if not os.path.exists(config.save_model_path):\n",
    "            os.makedirs(config.save_model_path)\n",
    "        save_path = config.save_model_path + \"/%07d.tar\" % iter\n",
    "        T.save({\n",
    "            \"iter\": iter + 1,\n",
    "            \"model_dict\": self.model.state_dict(),\n",
    "            \"trainer_dict\": self.trainer.state_dict()\n",
    "        }, save_path)\n",
    "\n",
    "    def setup_train(self):\n",
    "        self.model = Model(opt.pre_train_emb,opt.word_emb_type,self.vocab) \n",
    "#         print(\"Model : \",self.model)\n",
    "#         print(\"Encoder : \",self.model.encoder)\n",
    "#         print(\"Decoder : \",self.model.decoder)\n",
    "#         print(\"Embeds : \",self.model.embeds)\n",
    "        self.model = get_cuda(self.model)\n",
    "        device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "        if opt.multi_device:\n",
    "            if T.cuda.device_count() > 1:\n",
    "                print(\"Let's use\", T.cuda.device_count(), \"GPUs!\")\n",
    "                self.model = nn.DataParallel(self.model, list(range(T.cuda.device_count()))).cuda()    \n",
    "                \n",
    "                \n",
    "        if isinstance(self.model,nn.DataParallel):\n",
    "            self.model = self.model.module\n",
    "        self.model.to(device)\n",
    "#         self.model.eval()\n",
    "        \n",
    "        self.trainer = T.optim.Adam(self.model.parameters(), lr=config.lr)\n",
    "        start_iter = 0\n",
    "        if self.opt.load_model is not None:\n",
    "            load_model_path = os.path.join(config.save_model_path, self.opt.load_model)\n",
    "            checkpoint = T.load(load_model_path)\n",
    "            start_iter = checkpoint[\"iter\"]\n",
    "            self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "            self.trainer.load_state_dict(checkpoint[\"trainer_dict\"])\n",
    "            print(\"Loaded model at \" + load_model_path)\n",
    "        if self.opt.new_lr is not None:\n",
    "            self.trainer = T.optim.Adam(self.model.parameters(), lr=self.opt.new_lr)\n",
    "        return start_iter\n",
    "\n",
    "    def train_batch_MLE(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, batch):\n",
    "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
    "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
    "        Args:\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param batch: batch object\n",
    "        '''\n",
    "        dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "            use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "            x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "            target = target_batch[:, t]\n",
    "            log_probs = T.log(final_dist + config.eps)\n",
    "            step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=self.pad_id)\n",
    "            step_losses.append(step_loss)\n",
    "            x_t = T.multinomial(final_dist, 1).squeeze()                                            #Sample words from final distribution which can be used as input in next time step\n",
    "            is_oov = (x_t >= config.vocab_size).long()                                              #Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t.detach() + (is_oov) * self.unk_id                              #Replace OOVs with [UNK] token\n",
    "\n",
    "        losses = T.sum(T.stack(step_losses, 1), 1)                                                  #unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens                                                          #Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)                                                           #Average batch loss\n",
    "        return mle_loss\n",
    "\n",
    "    def train_batch_RL(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, article_oovs, greedy):\n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param article_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        s_t = enc_hidden                                                                            #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []                                                                                   #Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []                                                                   #Stores padding masks of generated samples\n",
    "        log_probs = []                                                                              #Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs)\n",
    "                x_t = multi_dist.sample()                                                           #perform multinomial sampling\n",
    "                log_prob = multi_dist.log_prob(x_t)\n",
    "                log_probs.append(log_prob)\n",
    "            else:\n",
    "                _, x_t = T.max(probs, dim=1)                                                        #perform greedy sampling\n",
    "            x_t = x_t.detach()\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "            mask[(mask == 1) + (x_t == self.end_id) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "            is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
    "            x_t = (1-is_oov)*x_t + (is_oov)*self.unk_id                                             #Replace OOVs with [UNK] token\n",
    "\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "        if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
    "            log_probs = T.stack(log_probs, dim=1)\n",
    "            log_probs = log_probs * decoder_padding_mask                                            #Not considering sampled words with padding mask = 0\n",
    "            lens = T.sum(decoder_padding_mask, dim=1)                                               #Length of sampled sentence\n",
    "            log_probs = T.sum(log_probs, dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy()\n",
    "            oovs = article_oovs[i]\n",
    "            S = data.outputids2words(id_list, self.vocab, oovs)                                     #Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:                                                                           #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "\n",
    "        return decoded_strs, log_probs\n",
    "    \n",
    "    def train_batch_decode(self,batch, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, article_oovs, greedy):\n",
    "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
    "        Args\n",
    "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
    "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
    "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
    "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
    "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
    "        :param article_oovs: Batch containing list of OOVs in each example\n",
    "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
    "        Returns:\n",
    "        :decoded_strs: List of decoded sentences\n",
    "        :log_probs: Log probabilities of sampled words\n",
    "        '''\n",
    "        s_t = enc_hidden                                                                            #Decoder hidden states\n",
    "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n",
    "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        inds = []                                                                                   #Stores sampled indices for each time step\n",
    "        decoder_padding_mask = []                                                                   #Stores padding masks of generated samples\n",
    "        log_probs = []                                                                              #Stores log probabilites of generated samples\n",
    "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
    "\n",
    "        for t in range(config.max_dec_steps):\n",
    "            x_t = self.model.embeds(x_t)\n",
    "#             print('x_t')\n",
    "#             print(x_t)\n",
    "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
    "            if greedy is False:\n",
    "                multi_dist = Categorical(probs)\n",
    "                x_t = multi_dist.sample()                                                           #perform multinomial sampling\n",
    "#                 log_prob = multi_dist.log_prob(x_t)\n",
    "#                 log_probs.append(log_prob)\n",
    "            else:\n",
    "                _, x_t = T.max(probs, dim=1)                                                        #perform greedy sampling\n",
    "            x_t = x_t.detach()\n",
    "            inds.append(x_t)\n",
    "            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n",
    "            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
    "#             mask[(mask == 1) + (x_t == self.end_id) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
    "            decoder_padding_mask.append(mask_t)\n",
    "#             is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
    "#             x_t = (1-is_oov)*x_t + (is_oov)*self.unk_id                                             #Replace OOVs with [UNK] token\n",
    "\n",
    "        inds = T.stack(inds, dim=1)\n",
    "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
    "#         if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
    "#             log_probs = T.stack(log_probs, dim=1)\n",
    "#             log_probs = log_probs * decoder_padding_mask                                            #Not considering sampled words with padding mask = 0\n",
    "#             lens = T.sum(decoder_padding_mask, dim=1)                                               #Length of sampled sentence\n",
    "#             log_probs = T.sum(log_probs, dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n",
    "        decoded_strs = []\n",
    "        ans_list = []\n",
    "        for i in range(len(enc_out)):\n",
    "            id_list = inds[i].cpu().numpy()\n",
    "            oovs = article_oovs[i]\n",
    "            S = data.outputids2words(id_list, self.vocab, oovs)                                     #Generate sentence corresponding to sampled words\n",
    "            try:\n",
    "                end_idx = S.index(data.STOP_DECODING)\n",
    "                S = S[:end_idx]\n",
    "            except ValueError:\n",
    "                S = S\n",
    "            if len(S) < 2:                                                                           #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
    "                S = [\"xxx\"]\n",
    "            S = \" \".join(S)\n",
    "            decoded_strs.append(S)\n",
    "            ans_dict = {\n",
    "                'review':batch.original_reviews[i],\n",
    "                'key_words':batch.key_words[i],\n",
    "                'summary':batch.original_summarys[i],                \n",
    "                'decoded_str':S                \n",
    "            }\n",
    "            ans_list.append(ans_dict)\n",
    "        return decoded_strs , ans_list\n",
    "\n",
    "    def reward_function(self, decoded_sents, original_sents):\n",
    "        rouge = Rouge()\n",
    "        try:\n",
    "            scores = rouge.get_scores(decoded_sents, original_sents)\n",
    "        except Exception:\n",
    "            print(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
    "            scores = []\n",
    "            for i in range(len(decoded_sents)):\n",
    "                try:\n",
    "                    score = rouge.get_scores(decoded_sents[i], original_sents[i])\n",
    "                except Exception:\n",
    "                    print(\"Error occured at:\")\n",
    "                    print(\"decoded_sents:\", decoded_sents[i])\n",
    "                    print(\"original_sents:\", original_sents[i])\n",
    "                    score = [{\"rouge-l\":{\"f\":0.0}}]\n",
    "                scores.append(score[0])\n",
    "        rouge_l_f1 = [score[\"rouge-l\"][\"f\"] for score in scores]\n",
    "        rouge_l_f1 = get_cuda(T.FloatTensor(rouge_l_f1))\n",
    "        return rouge_l_f1 , scores\n",
    "\n",
    "    # def write_to_file(self, decoded, max, original, sample_r, baseline_r, iter):\n",
    "    #     with open(\"temp.txt\", \"w\") as f:\n",
    "    #         f.write(\"iter:\"+str(iter)+\"\\n\")\n",
    "    #         for i in range(len(original)):\n",
    "    #             f.write(\"dec: \"+decoded[i]+\"\\n\")\n",
    "    #             f.write(\"max: \"+max[i]+\"\\n\")\n",
    "    #             f.write(\"org: \"+original[i]+\"\\n\")\n",
    "    #             f.write(\"Sample_R: %.4f, Baseline_R: %.4f\\n\\n\"%(sample_r[i].item(), baseline_r[i].item()))\n",
    "\n",
    "\n",
    "    def train_one_batch(self, batch, iter):\n",
    "        ans_list, batch_scores = None , None\n",
    "        enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
    "\n",
    "        enc_batch = self.model.embeds(enc_batch)                                                    #Get embeddings for encoder input\n",
    "        enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "        # -------------------------------Summarization-----------------------\n",
    "        if self.opt.train_mle == True:                                                             #perform MLE training\n",
    "            mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch)\n",
    "        else:\n",
    "            mle_loss = get_cuda(T.FloatTensor([0]))\n",
    "            \n",
    "        if opt.view:\n",
    "            sample_sents,ans_list = self.train_batch_decode(batch, enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=True)\n",
    "            _ , batch_scores = self.reward_function(sample_sents, batch.original_summarys)\n",
    "        # --------------RL training-----------------------------------------------------\n",
    "        if self.opt.train_rl == True:                                                              #perform reinforcement learning training\n",
    "            # multinomial sampling\n",
    "            sample_sents, RL_log_probs = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=False)\n",
    "            with T.autograd.no_grad():\n",
    "                # greedy sampling\n",
    "                greedy_sents, _ = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=True)\n",
    "\n",
    "            sample_reward , _ = self.reward_function(sample_sents, batch.original_summarys)\n",
    "            baseline_reward , _ = self.reward_function(greedy_sents, batch.original_summarys)\n",
    "            # if iter%200 == 0:\n",
    "            #     self.write_to_file(sample_sents, greedy_sents, batch.original_abstracts, sample_reward, baseline_reward, iter)\n",
    "            rl_loss = -(sample_reward - baseline_reward) * RL_log_probs                             #Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "            rl_loss = T.mean(rl_loss)\n",
    "\n",
    "            batch_reward = T.mean(sample_reward).item()\n",
    "        else:\n",
    "            rl_loss = get_cuda(T.FloatTensor([0]))\n",
    "            batch_reward = 0\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "#         if opt.train_mle == True: \n",
    "        self.trainer.zero_grad()\n",
    "        (self.opt.mle_weight * mle_loss + self.opt.rl_weight * rl_loss).backward()\n",
    "        self.trainer.step()       \n",
    "        \n",
    "        return mle_loss.item(), batch_reward, ans_list, batch_scores\n",
    "        \n",
    "        \n",
    "    def get_best_res_score(self,results,scores):\n",
    "        max_score = float(0)\n",
    "        _id = 0\n",
    "        for idx in range(len(results)):       \n",
    "            re_matchData = re.compile(r'\\-?\\d{1,10}\\.?\\d{1,10}')\n",
    "            data = re.findall(re_matchData, str(scores[idx]))\n",
    "            score = sum([float(d) for d in data])\n",
    "            if score > max_score: \n",
    "                _id = idx\n",
    "        return results[_id],scores[_id]\n",
    "            \n",
    "        \n",
    "    \n",
    "    def trainIters(self):\n",
    "        iter = self.setup_train()\n",
    "        epoch = 0\n",
    "        count = mle_total = r_total = 0\n",
    "        print(\"Start training.....\")\n",
    "        while iter <= config.max_iterations:\n",
    "#         while iter <= config.max_epochs:\n",
    "            f = open('training_log.txt','a+',encoding='utf-8')\n",
    "            batch = self.batcher.next_batch()\n",
    "            try:\n",
    "                mle_loss, r , ans_list, batch_scores = self.train_one_batch(batch, iter)\n",
    "#                 print(iter,mle_loss)\n",
    "#                 break\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"-------------------Keyboard Interrupt------------------\")\n",
    "                exit(0)\n",
    "#             if opt.train_mle == False: break\n",
    "            mle_total += mle_loss\n",
    "            r_total += r\n",
    "            count += 1\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 1000 == 0:\n",
    "                mle_avg = mle_total / count\n",
    "                r_avg = r_total / count\n",
    "                epoch = int((iter * config.batch_size) / train_num) + 1\n",
    "                print('epoch:',epoch ,\"iter:\", iter, \"mle_loss:\", \"%.3f\" % mle_avg, \"reward:\", \"%.4f\" % r_avg)\n",
    "                f.write('epoch: %s iter: %s mle_loss: %.3f reward: %.3f \\n'%(epoch,iter,mle_avg,r_avg))\n",
    "                if opt.view :\n",
    "                    best_res , best_score = self.get_best_res_score(ans_list,batch_scores)\n",
    "                    print('best_res:',best_res); f.write('best_res: %s \\n' %(best_res))\n",
    "                    print('best_score:',best_score); f.write('best_score: %s \\n' %(best_score))\n",
    "                count = mle_total = r_total = 0\n",
    "                f.close()\n",
    "#                 break\n",
    "\n",
    "            if iter % 5000 == 0:\n",
    "                self.save_model(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mle: True, mle weight: 1.00\n",
      "use pre_train_glove vocab_size 50000 \n",
      "intra_encoder: True intra_decoder: True\n",
      "Let's use 2 GPUs!\n",
      "Start training.....\n",
      "epoch: 1 iter: 1000 mle_loss: 3.643 reward: 0.0000\n",
      "best_res: {'review': 'this small case but fit both nikon coolpix and husband olympus snugly without too tight water resistant which great for when to the beach waterpark and nicely pad inside to help protect the camera the color bright and vibrant make easier to see at the bottom beach bag purse can just toss and have put the camera strap to keep from catch the zipper and so far work great ', 'key_words': ['beach', 'protect', 'camera', 'bottom', 'purse'], 'summary': '<s> cute little case </s>', 'decoded_str': '[UNK] great case [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.24999999531250006, 'p': 0.3333333333333333, 'r': 0.2}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.22368421052631188, 'p': 0.3333333333333333, 'r': 0.2}}\n",
      "epoch: 1 iter: 2000 mle_loss: 3.539 reward: 0.0000\n",
      "best_res: {'review': 'great picture out this small camera test several location and so far so good inexpensive way to keep eye thing run this dvr system for big company good for blindspot building warehouse ', 'key_words': ['small', 'great', 'picture', 'camera', 'company', 'big'], 'summary': '<s> awesome picture from small camera </s>', 'decoded_str': '[UNK] great camera [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.19999999580000008, 'p': 0.3333333333333333, 'r': 0.14285714285714285}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.15675675675685707, 'p': 0.3333333333333333, 'r': 0.14285714285714285}}\n",
      "epoch: 1 iter: 3000 mle_loss: 3.577 reward: 0.0000\n",
      "best_res: {'review': 'blow away with the new canon mk iii realize that review right the heel purchasing so may update the future as for now will post what out most the ability amazing dead accurate even the dark take some picture with bulb light up the room and would still focus pretty much anything the high iso capability and quality at high isos nothing short astounding not only the high iso quality great but love love love the quality the noise have read several time and concur the noise the iii film like look more like grain than noise have provide link with raw image show phenomenal high iso capability also compare dynamic range vs the nikon would have no hesitation shoot at iso that good as the following link will show with little clean up the image very very usable imo admittedly still have lot to discover and high iso at the very top list so naturally that what explore when come to photography professional pretty old school bell and whistle not matter to as much as image quality so will discover more pro and con as learn and explore more but as far as the meat and bone the camera build and image quality could not more satisfied this the the series that consider true professional camera rather than prosumer at this time can honestly say the canon iii exceed all previous hope and expectation will update this review as well as image the link above the future ', 'key_words': ['update', 'expectation', 'iso', 'top', 'high', 'quality', 'film', 'raw', 'very', 'image', 'camera', 'isos', 'future', 'range', 'usable', 'dynamic', 'realize', 'iii', 'prosumer', 'true', 'professional', 'previous', 'hope', 'imo', 'review', 'list'], 'summary': '<s> camera available until </s>', 'decoded_str': '[UNK] great camera [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.24999999531250006, 'p': 0.3333333333333333, 'r': 0.2}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.22368421052631188, 'p': 0.3333333333333333, 'r': 0.2}}\n",
      "epoch: 2 iter: 4000 mle_loss: 3.465 reward: 0.0000\n",
      "best_res: {'review': 'this camera good device by fuji have own other fuji camera and have quite pleased with this camera certainly different and therefore has some interesting quality which will discuss here physically the red color great the surface the camera high gloss not pick up normal fingerprint and seem bit scratch resistant the door for the battery and memory card compartment not close as cleanly as would prefer so this worry there door which conceal the usb connection but will need to open the battery cover every time want to charge the battery however the door that hide the port only secure with strip plastic and could easily break off and get lose the camera also seem heavier than most other that have use touchscreen this camera has touchscreen with no button to control menu function everything via the touchscreen the screen large and bright when use the camera the image surround the left and right edge by option so the image not fill the entire screen while shoot picture the touchscreen somewhat responsive although this not the type touchscreen would find phone music device need to apply just bit pressure to activate section the screen other word sense pressure and not the static charge finger there orientation sensor the camera that will sense which way hold the camera landscape and will rotate the menu option right side up accordingly have notice however that the screen very fingerprint prone would also call the touchscreen bit bouncy when touch not like touch glass like touch plastic sheet probably without say but this camera deserve case to protect especially the touchscreen functionality to turn the camera have to pull the protective door down this really counter intuitive since not the way hand work when hold camera much easier to pull slide door to the left right once this take for the device to ready to take picture as soon as the image appear the screen can press the shutter release and take picture within if leave the camera auto then almost ready to the camera will automatically shut down after and not easy to restart quickly the lens at the upper left when hold the camera there no indentation that prevent finger from get the way have have several shot that include the edge finger other word have to hold this camera just right to keep finger out the image there no independent viewfinder so must everything through the display the menu system simple common function available while take picture some option however bury level below the top level so not always quick to get to the setting need there general setup menu that allow to choose global setting such as screen brightness sound level etc image quality auto mode can definitely see the camera work will actively compensate for difference lighting and will choose appropriate flash setting can course override everything and select specific shooting mode the camera seem to perform fairly well low light situation but still incapable capture distant object dark setting other word will not able to sit the middle the auditorium and capture child during recital particularly glaring obstacle how the camera decide to auto focus while use zoom if center object then zoom the image incredibly out focus until zooming and re center the object unfortunately very easy to lose the subject when this happen megapixel really make difference this camera not only feature megapixel but also has exr mode which force the camera to give priority over resolution and noise reduction all this very apparent have never easily impress with higher megapixel number but this camera has convince otherwise for example if take picture from across the room dvr and use the camera edit feature to crop very small region image can clearly discern the very tiny print the control button dvr what this mean well if take picture without use zoom can edit the picture and zoom crop without lose any quality have quite impressed with what can the camera without use natural light picture some the best have ever take with this camera prefer image without flash but even when use the flash this camera not saturate the image with ghastly white glow general the camera very good job at detect lighting level and compensate accordingly video can record high definition unfortunately the focus become fix focus and also can not use the zoom functionality high definition capability aside feel like the video recording very casual nature and not mean as substitute for separate video recording device battery life battery life average the camera use proprietary fuji battery so if travel and need spare will need to buy the battery must remove from the camera to charge include wall charger can probably expect to use the camera substantially over the course before require charge unfortunately the battery level indicator the camera only has bar and seem once get to the middle bar head downward very quickly not very good job at warn when the battery start to drain use higher function exr flash face detection zoom only drain the battery even faster would nice if the battery charge indicator had more segment so could better determine remain charge when turn red not know if can take more picture for example overall very pleased with this camera like how construct and especially like the touchscreen the image quality superb unfortunately everything relate to the battery from the door to the indicator keep the product from perfect fortunately though would recommend not only this camera but any fuji camera base what have experience so far ', 'key_words': ['choose', 'subject', 'sound', 'high', 'large', 'upper', 'lose', 'edit', 'open', 'quality', 'interesting', 'situation', 'low', 'independent', 'zoom', 'specific', 'shutter', 'crop', 'press', 'function', 'incapable', 'touchscreen', 'menu', 'proprietary', 'dark', 'level', 'job', 'device', 'definition', 'focus', 'hold', 'natural', 'number', 'battery', 'charge', 'remove', 'color', 'screen', 'record', 'out', 'very', 'left', 'rotate', 'select', 'common', 'light', 'incredibly', 'leave', 'setting', 'image', 'picture', 'call', 'camera', 'higher', 'good', 'red', 'mode', 'still', 'global', 'viewfinder', 'capture'], 'summary': '<s> stylish camera with excellent picture quality </s>', 'decoded_str': '[UNK] great camera but not the battery [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.13333332835555575, 'p': 0.14285714285714285, 'r': 0.125}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.13216374268970296, 'p': 0.14285714285714285, 'r': 0.125}}\n",
      "epoch: 2 iter: 5000 mle_loss: 3.504 reward: 0.0000\n",
      "best_res: {'review': 'purchase this from as use actually brand new judge by the manual and component still seal original wrapping love this camera especially the dual ovf and evf for frame and view photo the evf the highest resolution for camera this price the ovf allow to see as through the naked eye love to able to view photo through the lcd as well as the evf also like to take photo with the ovf word caution not alarm when use the macro mode for focus will lost the ovf this way will not mislead by the paralex extreme close framing make sense by the way even purchase from as use the limited edition certificate still come the box as new purchase thank for extremely good deal otherwise can not afford this limited edition at the list price update have shoot photo with this camera and love the quality the image particularly satisfy to view the slide show the evf as has higher resolution than the lcd look through the also has lovely effect not there with the lcd ', 'key_words': ['component', 'lcd', 'eye', 'dual', 'manual', 'evf', 'wrapping', 'original', 'seal', 'view', 'camera', 'naked', 'edition', 'love', 'higher', 'certificate', 'resolution', 'ovf', 'limited', 'slide'], 'summary': '<s> love this limited edition at significant price reduction </s>', 'decoded_str': '[UNK] great camera for the price [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.12499999531250018, 'p': 0.16666666666666666, 'r': 0.1}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.11184210526315402, 'p': 0.16666666666666666, 'r': 0.1}}\n",
      "epoch: 2 iter: 6000 mle_loss: 3.405 reward: 0.0000\n",
      "best_res: {'review': 'purchase this camera look for one that smaller and more portable than dslr but also capable great image pro the image quality great well low light small and easy to handle intuitive operation flash for snapshot con bit slow to save image to disk not seem to as well with action shot no viewfinder but know that when buy will probably add the vf viewfinder accessory have also purchase the fotodiox adapter that allow use slr manual lense with this body summary great quality good camera for family photo sightsee etc ', 'key_words': ['light', 'low', 'great', 'purchase', 'operation', 'image', 'camera', 'lense', 'manual', 'adapter', 'intuitive', 'fotodiox', 'quality', 'good', 'smaller'], 'summary': '<s> small camera great image </s>', 'decoded_str': '[UNK] great camera [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.44444444000000005, 'p': 0.6666666666666666, 'r': 0.3333333333333333}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.18518518518525923, 'p': 0.3333333333333333, 'r': 0.16666666666666666}}\n",
      "epoch: 3 iter: 7000 mle_loss: 3.245 reward: 0.0000\n",
      "best_res: {'review': 'have not love with camera since canon that had high school that until have put the through the ringer for now and has perform like champ beautiful design and stunning performance this the best digital camera have ever use ', 'key_words': ['beautiful', 'design', 'digital', 'camera', 'stunning', 'performance'], 'summary': '<s> best digital camera yet </s>', 'decoded_str': '[UNK] great camera [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.22222221777777784, 'p': 0.3333333333333333, 'r': 0.16666666666666666}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.18518518518525923, 'p': 0.3333333333333333, 'r': 0.16666666666666666}}\n",
      "epoch: 3 iter: 8000 mle_loss: 3.070 reward: 0.0000\n",
      "best_res: {'review': 'this great camera especially for kid the off slide protect the lens take great picture and video buy for like so much buy another one ', 'key_words': ['protect', 'great', 'picture', 'camera', 'video', 'lens'], 'summary': '<s> great camera for the price </s>', 'decoded_str': '[UNK] great camera [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.3999999958, 'p': 0.6666666666666666, 'r': 0.2857142857142857}, 'rouge-2': {'f': 0.22222221777777784, 'p': 0.3333333333333333, 'r': 0.16666666666666666}, 'rouge-l': {'f': 0.3135135135136138, 'p': 0.6666666666666666, 'r': 0.2857142857142857}}\n",
      "epoch: 3 iter: 9000 mle_loss: 3.362 reward: 0.0000\n",
      "best_res: {'review': 'have very satisfied with this product the picture quality simply wonderful at iso setting at and below have recommend this camera and nikon to many friend and anticipate to use nikon camera the future easy to use take great picture this what know about this camera after use for now the and seem to improvement this good design ', 'key_words': ['great', 'nikon', 'design', 'picture', 'camera', 'recommend', 'good'], 'summary': '<s> great camera for some who appreciate photography </s>', 'decoded_str': '[UNK] great camera [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.33333332958333334, 'p': 0.6666666666666666, 'r': 0.2222222222222222}, 'rouge-2': {'f': 0.18181817785123974, 'p': 0.3333333333333333, 'r': 0.125}, 'rouge-l': {'f': 0.23809523809533759, 'p': 0.6666666666666666, 'r': 0.2222222222222222}}\n",
      "epoch: 3 iter: 10000 mle_loss: 3.142 reward: 0.0000\n",
      "best_res: {'review': 'this great camera for the money take great photo video and the lens very good the telephoto lens zoom very clear and has great zoom distance ', 'key_words': ['zoom', 'great', 'distance', 'camera', 'money', 'video', 'lens', 'photo'], 'summary': '<s> well make camera for the money </s>', 'decoded_str': '[UNK] great camera [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.18181817785123974, 'p': 0.3333333333333333, 'r': 0.125}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.13543599257895406, 'p': 0.3333333333333333, 'r': 0.125}}\n",
      "epoch: 4 iter: 11000 mle_loss: 3.251 reward: 0.0000\n",
      "best_res: {'review': 'space small space top name valuehttps image ssl image image ckgnpuxjs mp classvideo urlinput typehidden name valuehttps image ssl image image riftzbls png classvideo slate img urlnbspthis video take camera phone video test powershot at at frame per ', 'key_words': ['small', 'urlnbspthis', 'video', 'space'], 'summary': '<s> best camera ever </s>', 'decoded_str': '[UNK] great camera [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.24999999531250006, 'p': 0.3333333333333333, 'r': 0.2}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.22368421052631188, 'p': 0.3333333333333333, 'r': 0.2}}\n",
      "epoch: 4 iter: 12000 mle_loss: 3.141 reward: 0.0000\n",
      "best_res: {'review': 'this camera unbelievable crisp photo and high definition video not to mention very easy to use and navigate through the setting highly recommend for novice photographer professional ', 'key_words': ['crisp', 'definition', 'high', 'video', 'photo'], 'summary': '<s> awesome camera </s>', 'decoded_str': '[UNK] great camera [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.2857142808163266, 'p': 0.3333333333333333, 'r': 0.25}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.2747252747250878, 'p': 0.3333333333333333, 'r': 0.25}}\n",
      "epoch: 4 iter: 13000 mle_loss: 2.916 reward: 0.0000\n",
      "best_res: {'review': 'the control terrific dial for aperture dial for shutter speed dial for exposure compensation and function button that can map to iso the image quality great love the tone get from this this probably the best beginner camera because force to stick to very good prime lens and encourage to easily learn to use aperture and shutter speed also has the best control any camera have ever use sell nikon dslr because not see much purpose for compare to this for if want video and faster will better want small inconspicuous silent camera almost no shutter sound at all to have with everywhere make high shot even low light the these thing very well ', 'key_words': ['prime', 'exposure', 'silent', 'inconspicuous', 'small', 'high', 'camera', 'lens', 'good', 'speed', 'love', 'button', 'low', 'shot', 'tone', 'even', 'compensation', 'light', 'aperture'], 'summary': '<s> fantastic camera </s>   <s> favorite camera have ever use </s>   <s> sell dslr </s>', 'decoded_str': '[UNK] great camera for the price [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.12499999531250018, 'p': 0.16666666666666666, 'r': 0.1}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.11184210526315402, 'p': 0.16666666666666666, 'r': 0.1}}\n",
      "epoch: 5 iter: 14000 mle_loss: 2.929 reward: 0.0000\n",
      "best_res: {'review': 'nice camera for the price like the auto smile feature lightweight and easy to use able to recover photo after have delete would recommend this camera buy pink one for daughter and blue one for son both enjoy use these camera ', 'key_words': ['recommend', 'camera', 'nice'], 'summary': '<s> decent camera </s>', 'decoded_str': '[UNK] nice camera [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.2857142808163266, 'p': 0.3333333333333333, 'r': 0.25}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.2747252747250878, 'p': 0.3333333333333333, 'r': 0.25}}\n",
      "epoch: 5 iter: 15000 mle_loss: 3.380 reward: 0.0000\n",
      "best_res: {'review': 'this take really great picture maybe just have beautiful kid nah the camera seriously not want to just give star sit and think about possible drawback but have not find any last digital camera something like megapixel and the telephoto lens thingy get stick and never get unstuck that will never happen with this one and the picture so sharp and so beautiful and love the touchscreen control had little trouble with the menu but not good with high tech device buy cell phone that not anything but make phone call because can not handle the camera and other add ons figure out which mean must really easy come with large gig card easy to charge hold the charge long time love the battery life have not test with vacation yet but base how often have have to recharge think will just fine easy to load the picture the computer never even instal the software that come with easy to use even figure out and have very unenthusiastic about digital camera still keep film camera easy reach but get less and less use now the digital revolution finally embroil and force to take side and have to admit the freedom not use film exhilarate really like this camera not worthy not know what can not give one those technical review except turn the little flat knob the ring around the top right leave for telephoto wide angle counterintuitive but that how work and if the wrong mode hit the other top and switch but work spite have take some fantastic picture dread review this because like so much sound sycophantic but would unfair not to review just because like lot and can not find anything negative to say no matter how try must pretty sturdy because hold up to family use have have for and had no problem any sort even though drop hear that find out what fuji right husband nikon not survive small fall onto carpet but this has drop outside and fine finally red always secretly want bright red digital camera easier to keep track because that and look very sharp and stylish so that totally non technical review from someone who terrify menu and button and gadget if like not scare very easy and if have problem find small child to help that what ', 'key_words': ['beautiful', 'technical', 'great', 'reach', 'picture', 'easy', 'camera', 'bright', 'gadget', 'touchscreen', 'really', 'menu', 'control', 'button', 'digital', 'review', 'kid', 'fantastic', 'load'], 'summary': '<s> review for those who not good with high tech gadget </s>', 'decoded_str': '[UNK] great camera but not so so so so so so so so so so so [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.11111110666666683, 'p': 0.16666666666666666, 'r': 0.08333333333333333}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.09259259259266667, 'p': 0.16666666666666666, 'r': 0.08333333333333333}}\n",
      "epoch: 5 iter: 16000 mle_loss: 3.209 reward: 0.0000\n",
      "best_res: {'review': 'excellent less camera the nex menu system easy to use have fewer button actually make easier to navigate as the menu provide description each option for newer to average user shoot mostly nature and portrait this camera simply stunning such crisp image from the kit lense and such great preview screen that fully tilt able ', 'key_words': ['nature', 'crisp', 'kit', 'system', 'button', 'excellent', 'portrait', 'menu', 'user', 'camera', 'average', 'easier', 'great', 'screen', 'preview', 'image', 'lense', 'fewer', 'navigate', 'easy'], 'summary': '<s> great aps camera easy to use amazing quality </s>', 'decoded_str': '[UNK] excellent camera [UNK]'}\n",
      "best_score: {'rouge-1': {'f': 0.15384615029585808, 'p': 0.3333333333333333, 'r': 0.1}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.10613437195724833, 'p': 0.3333333333333333, 'r': 0.1}}\n"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/u012869752/article/details/72513141\n",
    "# 由于在jupyter notebook中，args不为空\n",
    "\n",
    "# nvidia-smi -pm 1\n",
    "\n",
    "if __name__ == \"__main__\":   \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_mle', type=bool, default=True)\n",
    "    parser.add_argument('--train_rl', type=bool, default=False)\n",
    "    parser.add_argument('--mle_weight', type=float, default=1.0)\n",
    "    parser.add_argument('--load_model', type=str, default=None)\n",
    "    parser.add_argument('--new_lr', type=float, default=None)\n",
    "    parser.add_argument('--multi_device', type=bool, default=True)\n",
    "    parser.add_argument('--view', type=bool, default=True)\n",
    "    parser.add_argument('--pre_train_emb', type=bool, default=True)\n",
    "    parser.add_argument('--word_emb_type', type=str, default='word2Vec')\n",
    "    parser.add_argument('--truncate', type=bool, default=False)\n",
    "    opt = parser.parse_args(args=[])\n",
    "\n",
    "    opt.rl_weight = 1 - opt.mle_weight  \n",
    "    \n",
    "#     config.vocab_path = \"Embedding/%s/%s.vocab\"%(opt.word_emb_type,opt.word_emb_type)\n",
    "#     if config.word_emb_type == \"glove\": \n",
    "#         config.vocab_size = len(open(config.vocab_path, 'r',encoding='utf-8').readlines())\n",
    "    \n",
    "    if opt.truncate :\n",
    "        if os.path.exists('training_log.txt'):\n",
    "            with open('training_log.txt','r+',encoding='utf-8') as q :\n",
    "                q.truncate()     \n",
    "        else:\n",
    "            open('training_log.txt','w').close()                \n",
    "\n",
    "    f = open('training_log.txt','a+',encoding='utf-8')\n",
    "    if opt.train_rl == True:\n",
    "        print(\"Training mle: %s, Training rl: %s, mle weight: %.2f, rl weight: %.2f\"%(opt.train_mle, opt.train_rl, opt.mle_weight, opt.rl_weight))\n",
    "        f.write(\"Training mle: %s, Training rl: %s, mle weight: %.2f, rl weight: %.2f \\n\"%(opt.train_mle, opt.train_rl, opt.mle_weight, opt.rl_weight))\n",
    "    else:\n",
    "        print(\"Training mle: %s, mle weight: %.2f\"%(opt.train_mle, opt.mle_weight))\n",
    "        f.write(\"Training mle: %s, mle weight: %.2f \\n\"%(opt.train_mle, opt.mle_weight))\n",
    "\n",
    "    if opt.pre_train_emb :\n",
    "#         with open(config.vocab_path,'r',encoding='utf-8') as q :\n",
    "#             config.vocab_size = len(q.readlines())   \n",
    "        print('use pre_train_%s vocab_size %s '%(opt.word_emb_type,config.vocab_size))\n",
    "        f.write('use pre_train_%s vocab_size %s \\n'%(opt.word_emb_type,config.vocab_size))\n",
    "        \n",
    "    else:\n",
    "        print('use %s %s vocab_size %s '%(opt.word_emb_type,config.vocab_size))\n",
    "        f.write('use %s %s vocab_size %s \\n'%(opt.word_emb_type,config.vocab_size))\n",
    "\n",
    "    print(\"intra_encoder:\", config.intra_encoder, \"intra_decoder:\", config.intra_decoder)\n",
    "    f.write(\"intra_encoder: %s intra_decoder: %s \\n\"%(config.intra_encoder, config.intra_decoder))\n",
    "    f.close()\n",
    "    \n",
    "    with open('training_log.txt','a+',encoding='utf-8') as f :\n",
    "        f.write('----------------------------------------------------------------\\n')\n",
    "    train_processor = Train(opt)\n",
    "    train_processor.trainIters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if self.opt.train_rl == \"yes\":\n",
    "# ~/Users/leyan/Text-Summarizer-FOP/data_util/data.py in outputids2words(id_list, vocab, review_oovs)\n",
    "#     141       review_oov_idx = i - vocab.size()\n",
    "#     142       try:\n",
    "# --> 143         w = review_oovs[review_oov_idx]\n",
    "#     144       except ValueError as e: # i doesn't correspond to an review oov\n",
    "#     145         raise ValueError('Error: model produced word ID %i which corresponds to review OOV %i but this example only has %i review OOVs' % (i, review_oov_idx, len(review_oovs)))\n",
    "\n",
    "# IndexError: list index out of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
