{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pprint import pprint\n",
    "nlp = en_core_web_sm.load()\n",
    "import re\n",
    "from stopwords import *\n",
    "import nltk\n",
    "from preprocess import *\n",
    "\n",
    "from spacy import displacy\n",
    "from textblob import TextBlob\n",
    "import collections\n",
    "pattern_counter = collections.Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然語言處理 -- Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 45 could: 40 may: 47 might: 26 must: 19 will: 61 "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import WordNetLemmatizer\n",
    "from math import log \n",
    "wnl=WordNetLemmatizer()\n",
    "\n",
    "# 其中 _Fdist 是單字出現的頻率 , _Sents 是文章中所有的句子\n",
    "_Fdist = nltk.FreqDist([wnl.lemmatize(w.lower()) for w in brown.words(categories='news')])\n",
    "\n",
    "_Sents = [[wnl.lemmatize(j.lower()) for j in i] for i in brown.sents(categories='news')]\n",
    "\n",
    "\n",
    "\n",
    "def p(x): # p(x) 計算單字 x 出現的機率\n",
    "       return _Fdist[x]/float(len(_Fdist))\n",
    "\n",
    "def pxy(x,y): # pxy(x,y) 計算單字 x 和單字 y 出現在同一個句子的機率\n",
    "       return (len(filter(lambda s :  x in s and y in s ,_Sents))+1)/ float(len(_Sents) )\n",
    "\n",
    "def pmi(x,y): # pmi(x,y) 計算單字 x 和單字 y 的 Pointwise Mutual Information\n",
    "       return  log(pxy(x,y)/(p(x)*p(y)),2) \n",
    "    \n",
    "# 在評論類文本裡面，計算情境助動詞的個數\n",
    "reviews_text = brown.words(categories='reviews')\n",
    "fdist = nltk.FreqDist(w.lower() for w in reviews_text)\n",
    "genres = ['reviews', 'hobbies', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m], end=' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brown.words(categories='news')\n",
    "# _Fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picture quality on this machine was substandard would not recommend this machine to anyone unless you enjoy getting angry and frustrated i have to assume that the lip synch problem referred to inearlier reviews must have been taken care of because haven tencountered the slightest problem\n",
      "polarity :  -0.26666666666666666\n",
      "verb_phrases :  quality on this machine was substandard\n",
      "[('picture', 'NOUN'), ('quality', 'NOUN'), ('on', 'ADP'), ('this', 'DET'), ('machine', 'NOUN'), ('was', 'AUX'), ('substandard', 'ADJ'), ('would', 'AUX'), ('not', 'PART'), ('recommend', 'VERB'), ('this', 'DET'), ('machine', 'NOUN'), ('to', 'ADP'), ('anyone', 'PRON'), ('unless', 'SCONJ'), ('you', 'PRON'), ('enjoy', 'VERB'), ('getting', 'VERB'), ('angry', 'ADJ'), ('and', 'CCONJ'), ('frustrated', 'ADJ'), ('i', 'PRON'), ('have', 'AUX'), ('to', 'PART'), ('assume', 'VERB'), ('that', 'SCONJ'), ('the', 'DET'), ('lip', 'NOUN'), ('synch', 'NOUN'), ('problem', 'NOUN'), ('referred', 'VERB'), ('to', 'ADP'), ('inearlier', 'NOUN'), ('reviews', 'NOUN'), ('must', 'AUX'), ('have', 'AUX'), ('been', 'AUX'), ('taken', 'VERB'), ('care', 'NOUN'), ('of', 'ADP'), ('because', 'SCONJ'), ('haven', 'PROPN'), ('tencountered', 'VERB'), ('the', 'DET'), ('slightest', 'ADJ'), ('problem', 'NOUN')]\n",
      "the video quality is simply astonishing won my wife over by doing direct comparison with the laserdisc and dvd versions of gone with the wind she is sold now\n",
      "polarity :  0.3\n",
      "verb_phrases :  video quality is simply astonishing\n",
      "[('the', 'DET'), ('video', 'NOUN'), ('quality', 'NOUN'), ('is', 'AUX'), ('simply', 'ADV'), ('astonishing', 'ADJ'), ('won', 'VERB'), ('my', 'PRON'), ('wife', 'NOUN'), ('over', 'ADP'), ('by', 'ADP'), ('doing', 'VERB'), ('direct', 'ADJ'), ('comparison', 'NOUN'), ('with', 'ADP'), ('the', 'DET'), ('laserdisc', 'NOUN'), ('and', 'CCONJ'), ('dvd', 'NOUN'), ('versions', 'NOUN'), ('of', 'ADP'), ('gone', 'VERB'), ('with', 'ADP'), ('the', 'DET'), ('wind', 'NOUN'), ('she', 'PRON'), ('is', 'AUX'), ('sold', 'VERB'), ('now', 'ADV')]\n",
      "there s enough flexibility that you can upgrade to component video capable tv if you don t have one now i m using the video output and that blows me away\n",
      "polarity :  0.1\n",
      "verb_phrases :  video capable tv\n",
      "[('there', 'PRON'), ('s', 'VERB'), ('enough', 'ADJ'), ('flexibility', 'NOUN'), ('that', 'PRON'), ('you', 'PRON'), ('can', 'AUX'), ('upgrade', 'VERB'), ('to', 'PART'), ('component', 'VERB'), ('video', 'NOUN'), ('capable', 'ADJ'), ('tv', 'NOUN'), ('if', 'SCONJ'), ('you', 'PRON'), ('don', 'VERB'), ('t', 'PROPN'), ('have', 'AUX'), ('one', 'NUM'), ('now', 'ADV'), ('i', 'PRON'), ('m', 'X'), ('using', 'VERB'), ('the', 'DET'), ('video', 'NOUN'), ('output', 'NOUN'), ('and', 'CCONJ'), ('that', 'DET'), ('blows', 'VERB'), ('me', 'PRON'), ('away', 'ADV')]\n",
      "the remote has excellent human factors engineering have no trouble using it in dimly lit room for the usual functions pause slow forward backwards fast etc\n",
      "polarity :  0.09375\n",
      "[('the', 'DET'), ('remote', 'ADJ'), ('has', 'AUX'), ('excellent', 'ADJ'), ('human', 'ADJ'), ('factors', 'NOUN'), ('engineering', 'NOUN'), ('have', 'AUX'), ('no', 'DET'), ('trouble', 'NOUN'), ('using', 'VERB'), ('it', 'PRON'), ('in', 'ADP'), ('dimly', 'ADV'), ('lit', 'VERB'), ('room', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('usual', 'ADJ'), ('functions', 'NOUN'), ('pause', 'VERB'), ('slow', 'ADJ'), ('forward', 'ADV'), ('backwards', 'ADV'), ('fast', 'ADV'), ('etc', 'X')]\n",
      "plus some handy features like how much time is left on the movie\n",
      "polarity :  0.26666666666666666\n",
      "[('plus', 'CCONJ'), ('some', 'DET'), ('handy', 'ADJ'), ('features', 'NOUN'), ('like', 'SCONJ'), ('how', 'ADV'), ('much', 'ADJ'), ('time', 'NOUN'), ('is', 'AUX'), ('left', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('movie', 'NOUN')]\n",
      "full outputs and dts dolby digital if you can use them and host of options for the two three speaker folks\n",
      "polarity :  0.175\n",
      "[('full', 'ADJ'), ('outputs', 'NOUN'), ('and', 'CCONJ'), ('dts', 'PROPN'), ('dolby', 'PROPN'), ('digital', 'PROPN'), ('if', 'SCONJ'), ('you', 'PRON'), ('can', 'AUX'), ('use', 'VERB'), ('them', 'PRON'), ('and', 'CCONJ'), ('host', 'NOUN'), ('of', 'ADP'), ('options', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('two', 'NUM'), ('three', 'NUM'), ('speaker', 'NOUN'), ('folks', 'NOUN')]\n",
      "really this has everything you could ask for in player in this price range don t know who selected the toshiba as amazon s pick but this is really the one you want\n",
      "polarity :  0.2\n",
      "[('really', 'ADV'), ('this', 'DET'), ('has', 'AUX'), ('everything', 'PRON'), ('you', 'PRON'), ('could', 'AUX'), ('ask', 'VERB'), ('for', 'ADP'), ('in', 'ADP'), ('player', 'NOUN'), ('in', 'ADP'), ('this', 'DET'), ('price', 'NOUN'), ('range', 'NOUN'), ('don', 'PROPN'), ('t', 'PROPN'), ('know', 'VERB'), ('who', 'PRON'), ('selected', 'VERB'), ('the', 'DET'), ('toshiba', 'NOUN'), ('as', 'SCONJ'), ('amazon', 'PROPN'), ('s', 'PART'), ('pick', 'NOUN'), ('but', 'CCONJ'), ('this', 'DET'), ('is', 'AUX'), ('really', 'ADV'), ('the', 'DET'), ('one', 'NOUN'), ('you', 'PRON'), ('want', 'VERB')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\textacy\\extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\textacy\\extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\textacy\\extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\textacy\\extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\textacy\\extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\textacy\\extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\textacy\\extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import textacy\n",
    "about_talk_text = (\n",
    "\"\"\"\n",
    "picture quality on this machine was substandard, i would not recommend this machine to anyone unless you enjoy getting angry and frustrated\n",
    "\n",
    "I have to assume that the lip-synch problem referred to inearlier reviews must have been taken care of, because I haven'tencountered the slightest problem. The video quality is simply astonishing - I won my wife over by doing a direct comparison with the laserdisc and DVD versions of \"Gone With the Wind\" - she is 100% sold now! There's enough flexibility that you can upgrade to a component-video-capable TV if you don't have one now; I'm using the S-video output and THAT blows me away.  The remote has EXCELLENT human factors engineering - I have no trouble using it in a dimly-lit room for the usual functions (pause, slow forward/backwards, fast, etc.) plus some HANDY features like \"How much time is left on the movie?\" Menu access is simple and intuitive, too - no teeny-tiny pixie-sized buttons to feel around for. Full 5.1 outputs and DTS/Dolby Digital if you can use them and a host of options for the two-three speaker folks. Really, this has everything you could ask for in a player in this price range - I don't know who selected the Toshiba as Amazon's pick, but THIS is REALLY the one you want!\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "pattern = r\"\"\"( \n",
    "(<NOUN><NOUN> | <ADJ><NOUN> | <NOUN><NOUN><NOUN> | <NOUN><ADP><DET><NOUN> | \n",
    "<NOUN> | <ADJ><NOUN><NOUN> | <ADJ><ADJ><NOUN> | <NOUN><PREP><NOUN> | <NOUN><PREP><DET><NOUN> | \n",
    "<VERB><PUNCT><ADP><NOUN><NOUN> | <DET><NOUN> )\n",
    "\n",
    "\n",
    "(<AUX><ADV><ADJ>|<AUX><ADJ>|<ADV><ADJ>|<ADJ><NOUN>) \n",
    ")\n",
    "\"\"\"\n",
    "# about_talk_text = SentProcess(about_talk_text)\n",
    "for sent in nltk.sent_tokenize(about_talk_text):\n",
    "    sent = remove_word2(sent)\n",
    "    sentiment = TextBlob(sent).sentences[0].sentiment\n",
    "    if sentiment.polarity == 0: continue\n",
    "    about_talk_doc = textacy.make_spacy_doc(sent,\n",
    "                                             lang='en_core_web_sm')\n",
    "    verb_phrases = textacy.extract.pos_regex_matches(about_talk_doc, pattern)\n",
    "    # Print all Verb Phrase\n",
    "    print(sent)\n",
    "    print(\"polarity : \",sentiment.polarity)\n",
    "    for chunk in verb_phrases:\n",
    "        print(\"verb_phrases : \" , chunk.text)\n",
    "    #     print([(token.text,token.pos_) for token in nlp(chunk.text)])\n",
    "\n",
    "    print([(token.text,token.pos_) for token in nlp(sent)])\n",
    "# will introduce\n",
    "# Extract Noun Phrase to explain what nouns are involved\n",
    "# for chunk in about_talk_doc.noun_chunks:\n",
    "#      print (chunk)\n",
    "\n",
    "# The talk\n",
    "# reader\n",
    "# Use cases\n",
    "# Natural Language Processing\n",
    "# Fintech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然語言處理 -- Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "freq_words = collections.Counter()\n",
    "_Sents = []\n",
    "# 其中 freq_words 是單字出現的頻率 , _Sents 是文章中所有的句子\n",
    "for sent in _Sents:\n",
    "    tok_sent = []\n",
    "    for token in nlp(sent):\n",
    "        tok_sent.append(wnl.lemmatize(token.text.lower()))\n",
    "#     print(tok_sent)\n",
    "    tok_sent = [t for t in tok_sent if t!=\"\"] # remove empty\n",
    "    freq_words.update(tok_sent)    \n",
    "    _Sents.append([wnl.lemmatize(token.lower()) for token in nltk.word_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(x): # p(x) 計算單字 x 出現的機率\n",
    "    return freq_words[x]/float(len(freq_words))\n",
    "\n",
    "def pxy(x,y): # pxy(x,y) 計算單字 x 和單字 y 出現在同一個句子的機率\n",
    "#     print((lambda s :  x in s and y in s ,_Sents))\n",
    "    return (len(list(filter(lambda s :  x in s and y in s ,_Sents)))+1)/ float(len(_Sents) )\n",
    "\n",
    "def pmi(x,y): # pmi(x,y) 計算單字 x 和單字 y 的 Pointwise Mutual Information\n",
    "    try:\n",
    "        return  log(pxy(x,y)/(p(x)*p(y)),2) \n",
    "    except Exception as e :\n",
    "        return 0\n",
    "    \n",
    "def pmi_fopair(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    noun_,op_ = [],[]\n",
    "    cand_pairs = []\n",
    "    for token in doc:\n",
    "        if token.pos in [ADJ,ADV,VERB]: op_.append(token.text)\n",
    "        if token.pos in [NOUN]: noun_.append(token.text)\n",
    "\n",
    "    for f in noun_:\n",
    "        for o in op_:\n",
    "            pair = (f,o)\n",
    "            score = pmi(f,o)\n",
    "            if score > 10 :\n",
    "                cand_pairs.append((pair,score))\n",
    "    print(sentence)\n",
    "    print(cand_pairs)\n",
    "    return cand_pairs\n",
    "\n",
    "pmi_fopair(msg.sentence) # 效果不好"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
