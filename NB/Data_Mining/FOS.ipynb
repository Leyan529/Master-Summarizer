{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter('ignore')\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "\n",
    "\n",
    "import spacy\n",
    "# gpu = spacy.prefer_gpu()\n",
    "# print('GPU:', gpu)\n",
    "# pip install -U spacy[cuda100]\n",
    "# python -m spacy validate\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "import re\n",
    "from stopwords import *\n",
    "import nltk\n",
    "from preprocess import *\n",
    "# from feature import *\n",
    "\n",
    "from textblob import TextBlob\n",
    "import collections\n",
    "\n",
    "from spacy.symbols import cop, acomp, amod, conj, neg, nn, nsubj, dobj,prep,advmod\n",
    "from spacy.symbols import VERB, NOUN, PROPN, ADJ, ADV, AUX, PART\n",
    "\n",
    "pattern_counter = collections.Counter()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "from MongoDB import MongoDB\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 斷詞辭典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stpwords_list3 = [f.replace(\"\\n\",\"\") for f in open(\"stopwords.txt\",\"r\",encoding = \"utf-8\").readlines()]\n",
    "stpwords_list3.remove(\"not\")\n",
    "stopwords = list(html_escape_table + stpwords_list2) + list(list(stops) + list(stpwords_list1) + list(stpwords_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Opinion\n",
    "# https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n",
    "<!-- Opinion Lexicon: A list of English positive and negative opinion words or sentiment words (around 6800 words). This list was compiled over many years starting from our first paper (Hu and Liu, KDD-2004). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative-words.txt\n",
      "positive-words.txt\n"
     ]
    }
   ],
   "source": [
    "opinion_lexicon = {}\n",
    "for filename in os.listdir('opinion-lexicon-English/'):      \n",
    "    if \"txt\" not in filename: continue\n",
    "    print(filename)\n",
    "    with open('opinion-lexicon-English/'+filename,'r') as f_input:\n",
    "        lexion = []\n",
    "        for line in f_input:\n",
    "            if line.startswith(\";\"):\n",
    "                continue\n",
    "            word = line.replace(\"\\n\",\"\")\n",
    "            if word != \"\" : lexion.append(word)\n",
    "        pos = filename.replace(\".txt\",\"\")\n",
    "        opinion_lexicon[pos] = lexion\n",
    "\n",
    "opinion_lexicon[\"total-words\"] = opinion_lexicon[\"negative-words\"] + opinion_lexicon[\"positive-words\"]\n",
    "# opinion_lexicon[\"total-words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.lm import Vocabulary\n",
    "# vocab = Vocabulary(words, unk_cutoff=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pickle 提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\new_Electronics_DVD Players.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Electronics', 'DVD Players')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload a file to a variable    \n",
    "from glob import glob\n",
    "with open(glob('data/new_*_*.pickle')[1], 'rb') as file:\n",
    "    print(file.name)\n",
    "    PROD_DICT = pickle.load(file)\n",
    "    category1,category2 = file.name.replace(\".pickle\",\"\").split(\"_\")[1:]\n",
    "    \n",
    "category1,category2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句子篩選/切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "from summa import keywords,summarizer\n",
    "\n",
    "def sentence_extract_blob(text):    \n",
    "    extract_sents = []\n",
    "    text = text.replace(\",\",\"\").replace(\"i.e.\",\"\")\n",
    "    text = SentProcess(text)\n",
    "    \n",
    "    for sent in text.split(\"<end>\"):        \n",
    "#         print(sent)\n",
    "        if sent == \"\": continue\n",
    "#         sentiment = TextBlob(sent).sentences[0].sentiment\n",
    "#         if abs(sentiment.polarity) <= 0.5: continue\n",
    "#         if abs(sentiment.subjectivity) <= 0.5: continue\n",
    "#         print(TextBlob(sent).sentences)\n",
    "#         print(\"polarity : \",sentiment.polarity)\n",
    "#         print(\"subjectivity : \",sentiment.subjectivity)\n",
    "# #         print(keywords.keywords(sent))\n",
    "#         print(\"***\")\n",
    "        extract_sents.append(sent)\n",
    "    return extract_sents\n",
    "\n",
    "def extract_cand_pharse(text): \n",
    "    sent_pattern = r\"\"\"( \n",
    "    (<NOUN><NOUN> | <ADJ><NOUN> | <NOUN><NOUN><NOUN> | <NOUN><ADP><DET><NOUN> | \n",
    "    <NOUN> | <ADJ><NOUN><NOUN> | <ADJ><ADJ><NOUN> | <NOUN><PREP><NOUN> | <NOUN><PREP><DET><NOUN> | \n",
    "    <VERB><PUNCT><ADP><NOUN><NOUN> | <DET><NOUN> )\n",
    "\n",
    "\n",
    "    (<AUX><ADV><ADJ>|<AUX><ADJ>|<ADV><ADJ>|<ADJ><NOUN>) \n",
    "    )\n",
    "    \"\"\"\n",
    "#     pattern = r'<PROPN>+ (<PUNCT|CCONJ> <PUNCT|CCONJ>? <PROPN>+)*'\n",
    "    extract_pharse = []\n",
    "    doc = textacy.make_spacy_doc(text,lang='en_core_web_sm')\n",
    "    phrases = textacy.extract.pos_regex_matches(doc, sent_pattern)\n",
    "    # Print all Verb Phrase\n",
    "\n",
    "    for phrase in phrases:\n",
    "#         print(\"verb_phrases : \" , phrase.text)\n",
    "    #     print([(token.text,token.pos_) for token in nlp(chunk.text)])\n",
    "        extract_pharse.append(phrase.text+\"\\n\")           \n",
    "    return extract_pharse        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 規格關鍵字提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeywordFeat(description,feature):  # 搜索關鍵保留字，並重新清理句子       \n",
    "    description_list = []\n",
    "    for sent in description:\n",
    "        if len(TextBlob(sent)) > 1:\n",
    "            sent_list = sent.split(\".\")\n",
    "            description_list.extend(sent_list) \n",
    "        else:\n",
    "            description_list.extend(sent) \n",
    "    description_list = [sent.strip() +\".\" for sent in description_list]\n",
    "    if type(feature) == str:\n",
    "        total_desc_sents = description_list + [feature]\n",
    "    else:\n",
    "        total_desc_sents = description_list + feature    \n",
    " \n",
    "    # Stage 1 取dash feature\n",
    "    keywords = []\n",
    "    newdescription = \"\"\n",
    "    \n",
    "    for sent in total_desc_sents:\n",
    "        keys = []\n",
    "        for token in nlp(sent):            \n",
    "            sent = sent.lower()\n",
    "            for k, v in contractions.items():\n",
    "                if k in sent:\n",
    "                    sent = sent.replace(k, v)\n",
    "\n",
    "            for k in html_escape_table:\n",
    "                if k in sent:\n",
    "                    sent = sent.replace(k, \"\")            \n",
    "            \n",
    "        pattern = re.compile(r\"([\\d\\w\\.-]+[-'//.][\\d\\w\\.-]+)\")  # |  ([\\(](\\w)+[\\)])\n",
    "        keys = pattern.findall(sent)            \n",
    "            \n",
    "        keywords.extend(keys)\n",
    "        cleansent = sent\n",
    "        for k in keys:    \n",
    "            k = k.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "            cleansent = cleansent.replace(k,\"(\" + k + \")\")\n",
    "            cleansent = cleansent.replace(\"((\",\"(\").replace(\"))\",\")\")     \n",
    "            cleansent = cleansent.replace(\"(\" + k + \")\",\"\") # 移除符號特徵\n",
    "            cleansent = cleansent.strip()\n",
    "        \n",
    "#         cleansent = cleansent + \".\\n\"\n",
    "        cleansent = remove_word3(cleansent)\n",
    "        char = \" \"\n",
    "        while char * 2 in cleansent:\n",
    "            cleansent = cleansent.replace(char * 2, char)  \n",
    "        char = \".\"\n",
    "        while char * 2 in cleansent:\n",
    "            cleansent = cleansent.replace(char * 2, char)            \n",
    "         \n",
    "        if len(cleansent) < 5 : continue\n",
    "        newdescription = newdescription + cleansent + \".\\n\"\n",
    "        \n",
    "    newdescription = newdescription.replace(\"..\",\".\")   \n",
    "    newdescription = re.sub(r\"\\([\\w]+\\)\",\"\",newdescription)\n",
    "    newdescription = re.sub(r\"\\(\\)\",\"\",newdescription)   \n",
    "    newdescription = re.sub(r'\\\"',\"\",newdescription)\n",
    "    \n",
    "    # get Noun pharse keyword for newdescription\n",
    "#     for pharse in nlp(newdescription).noun_chunks:\n",
    "#         print(pharse.text)\n",
    "    cand_pf = PF_rule_POS(newdescription).run()\n",
    "#     print(cand_pf)\n",
    "    \n",
    "    keywords2 = set()\n",
    "    for pf in cand_pf:\n",
    "        chunk_pfs = nltk.word_tokenize(pf)\n",
    "        for tok in chunk_pfs:\n",
    "            if tok in stopwords: continue\n",
    "            if tok in opinion_lexicon[\"total-words\"]: continue\n",
    "            keywords2.add(tok)\n",
    "\n",
    "#     print(len(keywords2),keywords2)\n",
    "    newkeywords = list()\n",
    "    for key in list(keywords) + list(keywords2):\n",
    "        clean_key = remove_word2(key)\n",
    "        clean_key = clean_key.split(\" \")            \n",
    "        newkeywords.extend(clean_key) \n",
    "    newkeywords = list(set([key for key in newkeywords if len(key) >=2 and key not in stopwords and key not in opinion_lexicon[\"total-words\"]]))\n",
    "    \n",
    "    return keywords,keywords2,newkeywords,newdescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeywordFeat_2(article):  # 搜索關鍵保留字，並重新清理句子       \n",
    "   \n",
    "    total_desc_sents = sentence_extract_blob(article)\n",
    "    # Stage 1 取dash feature\n",
    "    keywords = []\n",
    "    newarticle = \"\"\n",
    "    \n",
    "    for sent in total_desc_sents:\n",
    "        keys = []\n",
    "        for token in nlp(sent):            \n",
    "            sent = sent.lower()\n",
    "            for k, v in contractions.items():\n",
    "                if k in sent:\n",
    "                    sent = sent.replace(k, v)\n",
    "\n",
    "            for k in html_escape_table:\n",
    "                if k in sent:\n",
    "                    sent = sent.replace(k, \"\")            \n",
    "            \n",
    "        pattern = re.compile(r\"([\\d\\w\\.-]+[-'//.][\\d\\w\\.-]+)\")  # |  ([\\(](\\w)+[\\)])\n",
    "        keys = pattern.findall(sent)            \n",
    "            \n",
    "        keywords.extend(keys)\n",
    "        cleansent = sent\n",
    "        for k in keys:    \n",
    "            k = k.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "            cleansent = cleansent.replace(k,\"(\" + k + \")\")\n",
    "            cleansent = cleansent.replace(\"((\",\"(\").replace(\"))\",\")\")     \n",
    "            cleansent = cleansent.replace(\"(\" + k + \")\",\"\") # 移除符號特徵\n",
    "            cleansent = cleansent.strip()\n",
    "        \n",
    "#         cleansent = cleansent + \".\\n\"\n",
    "        cleansent = remove_word3(cleansent)\n",
    "        char = \" \"\n",
    "        while char * 2 in cleansent:\n",
    "            cleansent = cleansent.replace(char * 2, char)  \n",
    "        char = \".\"\n",
    "        while char * 2 in cleansent:\n",
    "            cleansent = cleansent.replace(char * 2, char)            \n",
    "         \n",
    "        if len(cleansent) < 5 : continue\n",
    "        newarticle = newarticle + cleansent + \".\\n\"\n",
    "        \n",
    "    newarticle = newarticle.replace(\"..\",\".\")   \n",
    "    newarticle = re.sub(r\"\\([\\w]+\\)\",\"\",newarticle)\n",
    "    newarticle = re.sub(r\"\\(\\)\",\"\",newarticle)   \n",
    "    newarticle = re.sub(r'\\\"',\"\",newarticle)\n",
    "    \n",
    "    # get Noun pharse keyword for newarticle\n",
    "#     for pharse in nlp(newarticle).noun_chunks:\n",
    "#         print(pharse.text)\n",
    "    cand_pf = PF_rule_POS(newarticle).run()\n",
    "#     print(cand_pf)\n",
    "    \n",
    "    keywords2 = set()\n",
    "    for pf in cand_pf:\n",
    "        chunk_pfs = nltk.word_tokenize(pf)\n",
    "        for tok in chunk_pfs:\n",
    "            if tok in stopwords: continue\n",
    "            if tok in opinion_lexicon[\"total-words\"]: continue\n",
    "            keywords2.add(tok)\n",
    "            \n",
    "    newkeywords = list()\n",
    "    for key in list(keywords) + list(keywords2):\n",
    "        clean_key = remove_word2(key)\n",
    "        clean_key = clean_key.split(\" \")            \n",
    "        newkeywords.extend(clean_key) \n",
    "    newkeywords = list(set([key for key in newkeywords if len(key) >=2 and key not in stopwords and key not in opinion_lexicon[\"total-words\"]]))\n",
    "    \n",
    "\n",
    "#     print(len(keywords2),keywords2)\n",
    "    return keywords,keywords2,newkeywords,newarticle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評論濾除符號-英文特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanReview(review):  # 搜索關鍵保留字，並重新清理句子     \n",
    "    # Stage 1 取dash feature\n",
    "    keywords = []\n",
    "    newReview = \"\"\n",
    "    for sent in nltk.sent_tokenize(review):\n",
    "        for token in nlp(sent):\n",
    "            sent = sent.lower()\n",
    "            for k, v in contractions.items():\n",
    "                if k in sent:\n",
    "                    sent = sent.replace(k, v)\n",
    "\n",
    "            for k in html_escape_table:\n",
    "                if k in sent:\n",
    "                    sent = sent.replace(k, \"\")\n",
    "            \n",
    "            \n",
    "            pattern = re.compile(r\"([\\d\\w\\.-]+[-'//.][\\d\\w\\.-]+)\")  # |  ([\\(](\\w)+[\\)])\n",
    "            keys = pattern.findall(sent)\n",
    "            \n",
    "            \n",
    "        keywords.extend(keys)\n",
    "        cleansent = sent\n",
    "        for k in keys:    \n",
    "            k = k.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "            cleansent = cleansent.replace(k,\"(\" + k + \")\")\n",
    "            cleansent = cleansent.replace(\"((\",\"(\").replace(\"))\",\")\")     \n",
    "            cleansent = cleansent.replace(\"(\" + k + \")\",\"\") # 移除符號特徵\n",
    "            cleansent = cleansent.strip()\n",
    "        \n",
    "#         cleansent = cleansent + \".\\n\"\n",
    "        cleansent = remove_word3(cleansent)\n",
    "        char = \" \"\n",
    "        while char * 2 in cleansent:\n",
    "            cleansent = cleansent.replace(char * 2, char)  \n",
    "        char = \".\"\n",
    "        while char * 2 in cleansent:\n",
    "            cleansent = cleansent.replace(char * 2, char)            \n",
    "         \n",
    "        if len(cleansent) < 5 : continue\n",
    "        newReview = newReview + cleansent + \".\\n\"\n",
    "        \n",
    "    newReview = newReview.replace(\"..\",\".\")   \n",
    "    newReview = re.sub(r\"\\([\\w]+\\)\",\"\",newReview)\n",
    "    newReview = re.sub(r\"\\(\\)\",\"\",newReview)   \n",
    "    newReview = re.sub(r'\\\"',\"\",newReview)\n",
    "    \n",
    "    # get Noun pharse keyword for newdescription\n",
    "#     cand_pf = PF_rule_POS(newReview).run()\n",
    "    \n",
    "#     keywords2 = set()\n",
    "#     for pf in cand_pf:\n",
    "#         chunk_pfs = nltk.word_tokenize(pf)\n",
    "#         for tok in chunk_pfs:\n",
    "#             if tok in stopwords: continue\n",
    "#             if tok in opinion_lexicon[\"total-words\"]: continue\n",
    "#             keywords2.add(tok)\n",
    "\n",
    "#     print(len(keywords2),keywords2)\n",
    "    return keywords, newReview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was happy sd owner until month ago when began getting the dreaded error.\n",
      "it would not recognize any dvd put in even brand new ones.\n",
      "did web search and was amazed to find that many many people who have bought this player and sony players in general have had exactly the same problem after year to year and half.\n",
      "sony refuses to do recall and have since bought new player not sony settled on jvc but thought should post my experience here.\n",
      "buyer beware on this player it has very high rate of terminal failure.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "I was a happy S530D owner until a month ago when I began getting the dreaded \\\"C13 00\\\" error.  It would not recognize any DVD I put in; even brand new ones.\\n\\nI did a web search and was amazed to find that many, many people who have bought this player (and Sony players in general) have had exactly the same problem after a year to a year and half.\\n\\nSony refuses to do a recall and I've since bought a new player (not a Sony; I settled on a JVC XV-FA90), but I thought I should post my experience here.  Buyer beware on this player; it has a very high rate of terminal failure.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "rev_dash_keywords, newreview = cleanReview(review)\n",
    "print(newreview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PF-Extraction-Rule(POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -NN, \n",
    "# -NN NN, \n",
    "# JJ NN \n",
    "# -NN NN NN, \n",
    "# JJ NN NN, \n",
    "# JJ JJ NN, \n",
    "# NN IN NN \n",
    "# -NN IN DT NN\n",
    "\n",
    "class PF_rule_POS():\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "        self.matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "    def collect_sents(self, matcher, doc, i, matches):\n",
    "        match_id, start, end = matches[i]\n",
    "        span = doc[start:end]  # Matched span\n",
    "        sent = span.sent  # Sentence containing matched span\n",
    "        # Append mock entity for match in displaCy style to matched_sents\n",
    "        # get the match span by ofsetting the start and end of the span with the\n",
    "        # start and end of the sentence in the doc\n",
    "        match_ents = [{\n",
    "            \"start\": span.start_char - sent.start_char,\n",
    "            \"end\": span.end_char - sent.start_char,\n",
    "            \"label\": \"MATCH\",\n",
    "        }]\n",
    "        self.matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "    def match_pattern(self, sent, flit_keyword=None):\n",
    "        res = []\n",
    "        #         ('Dolby', 'PROPN'), ('Digital', 'PROPN')\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        #         matcher.add(\"pf1\", self.collect_sents, [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True},\n",
    "        #                                                    {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf2\", self.collect_sents,\n",
    "        #                     [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                      {'POS': 'SYM'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         #         matcher.add(\"specn3\", self.collect_sents, [{'POS': 'ADJ'}, {'POS': 'NOUN'}])  # add pattern\n",
    "        #         matcher.add(\"pf3\", self.collect_sents,\n",
    "        #                     [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf4\", self.collect_sents,\n",
    "        #                     [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                      {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf5\", self.collect_sents, [{'POS': 'ADJ'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                                                    {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf6\", self.collect_sents,\n",
    "        #                     [{'POS': 'ADJ'}, {'POS': 'ADJ'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         #         ('inches', 'NOUN'), ('(', 'PUNCT'), ('W', 'NOUN'), ('x', 'SYM'), ('H', 'NOUN'), ('x', 'SYM'), ('D', 'NOUN'), (')', 'PUNCT')\n",
    "        #         matcher.add(\"pf6.1\", self.collect_sents,\n",
    "        #                     [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                      {'POS': 'SYM'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'POS': 'SYM'},\n",
    "        #                      {'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True}])  # inches (W x H x D)\n",
    "        #         matcher.add(\"pf6.2\", self.collect_sents,\n",
    "        #                     [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                      {'POS': 'SYM'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True}])  # inches (W x H)\n",
    "        #         matcher.add(\"pf7\", self.collect_sents, [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'POS': 'PREP'},\n",
    "        #                                                    {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf8\", self.collect_sents, [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'POS': 'PREP'}, {'POS': 'DET'},\n",
    "        #                                                    {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf9\", self.collect_sents,\n",
    "        #                     [{'POS': 'VERB'}, {'POS': 'PUNCT'}, {'POS': 'ADP'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                      {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "\n",
    "#         matcher.add(\"pf1\", self.collect_sents, [{'TAG': {\"IN\": ['NN']}}])  # add pattern\n",
    "        matcher.add(\"pf2\", self.collect_sents, [{'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['NN']}}])  # add pattern\n",
    "        matcher.add(\"pf3\", self.collect_sents, [{'TAG': {\"IN\": ['JJ']}}, {'TAG': {\"IN\": ['NN']}}])  # add pattern    \n",
    "        matcher.add(\"pf4\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['NN']}}])  # add pattern\n",
    "        matcher.add(\"pf5\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['JJ']}}, {'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['NN']}}])\n",
    "        matcher.add(\"pf6\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['JJ']}}, {'TAG': {\"IN\": ['JJ']}}, {'TAG': {\"IN\": ['NN']}}])\n",
    "        matcher.add(\"pf7\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['IN']}}, {'TAG': {\"IN\": ['NN']}}])\n",
    "        matcher.add(\"pf8\", self.collect_sents, [{'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['IN']}},\n",
    "                                                {'TAG': {\"IN\": ['DT']}}, {'TAG': {\"IN\": ['NN']}}])\n",
    "\n",
    "        doc = nlp(sent)\n",
    "        matches = matcher(doc)\n",
    "        # Serve visualization of sentences containing match with displaCy\n",
    "        # set manual=True to make displaCy render straight from a dictionary\n",
    "        # (if you're not running the code within a Jupyer environment, you can\n",
    "        # use displacy.serve instead)\n",
    "        #         displacy.render(self.matched_sents, style=\"ent\", manual=True)\n",
    "        for match_id, start, end in matches:\n",
    "            # Get the string representation\n",
    "            string_id = nlp.vocab.strings[match_id]\n",
    "            span = doc[start:end]  # The matched span\n",
    "            #             print(match_id, string_id, start, end, span.text)\n",
    "            if flit_keyword:\n",
    "                found = False\n",
    "                spanStr = span.text\n",
    "                for word in spanStr.split(\" \"):\n",
    "                    if word in flit_keyword: found = True; break\n",
    "                if found: res.append(span.text)\n",
    "            else:\n",
    "                res.append(span.text)\n",
    "        return res\n",
    "\n",
    "    def run(self):\n",
    "        total_res = []\n",
    "        for sent in nltk.sent_tokenize(self.article):\n",
    "            sent = sent.lower()\n",
    "            sent = sent.replace(\",\", \" \").replace(\":\", \" \").replace(\"(\", \" \").replace(\")\", \" \").replace(\"multi-\",\n",
    "                                                                                                        \"multiple \")\n",
    "            ress = self.match_pattern(sent)\n",
    "            total_res.extend(ress)\n",
    "\n",
    "        total_res = list(set(total_res))\n",
    "        #         print(\"final res\",total_res)\n",
    "        return total_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FO-Rule(POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FO_rule_POS():\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "        self.matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "    def collect_sents(self, matcher, doc, i, matches):\n",
    "        match_id, start, end = matches[i]\n",
    "        span = doc[start:end]  # Matched span\n",
    "        sent = span.sent  # Sentence containing matched span\n",
    "        # Append mock entity for match in displaCy style to matched_sents\n",
    "        # get the match span by ofsetting the start and end of the span with the\n",
    "        # start and end of the sentence in the doc\n",
    "        match_ents = [{\n",
    "            \"start\": span.start_char - sent.start_char,\n",
    "            \"end\": span.end_char - sent.start_char,\n",
    "            \"label\": \"MATCH\",\n",
    "        }]\n",
    "        self.matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "    def match_pattern(self, sent, flit_keyword=None):\n",
    "        res = []\n",
    "        #         ('Dolby', 'PROPN'), ('Digital', 'PROPN')\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        matcher.add(\"fos1\", self.collect_sents, [{'TAG': {\"IN\": ['JJ']}},\n",
    "                                                 {'TAG': {\"IN\": ['NN']}}])  # add pattern JJ[O] NN[F] \n",
    "        matcher.add(\"fos1.1\", self.collect_sents, [{'TAG': {\"IN\": ['NN']}}, # picture/NN quality/NN great/JJ\n",
    "                                                 {'TAG': {\"IN\": ['NN']}},\n",
    "                                                  {'TAG': {\"IN\": ['JJ']}}])  # add pattern JJ[O] NN[F] \n",
    "               \n",
    "        matcher.add(\"fos2\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['VB']}},\n",
    "                     {'TAG': {\"IN\": ['IN']}},\n",
    "                     {'TAG': 'DT'},\n",
    "                     {'TAG': {\"IN\": ['NN']}}])  # add pattern VB[O] NN[F]\n",
    "\n",
    "        matcher.add(\"fos3\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['JJ']}},\n",
    "                     {'TAG': {\"IN\": ['CC']}},\n",
    "                     {'TAG': {\"IN\": ['JJ']}},\n",
    "                     {'TAG': {\"IN\": ['NN']}}])  # add pattern JJ[O] JJ[O] NN[F]\n",
    "\n",
    "        matcher.add(\"fos4\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['NN']}},\n",
    "                     {'TAG': {\"IN\": ['VB']}},\n",
    "                     {'TAG': {\"IN\": ['JJ']}}])  # add pattern NN[F] JJ[O]\n",
    "\n",
    "        matcher.add(\"fos5\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['NN']}},\n",
    "                     {'TAG': {\"IN\": ['VB']}},\n",
    "                     {'TAG': {\"IN\": ['RB']}}])  # add pattern NN[F] RB[O] \n",
    "\n",
    "        matcher.add(\"fos6\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['NN']}},\n",
    "                     {'TAG': {\"IN\": ['VB']}},\n",
    "                     {'TAG': {\"IN\": ['RB']}},\n",
    "                     {'TAG': {\"IN\": ['RB']}}])  # add pattern NN[F] RB[O]\n",
    "\n",
    "        matcher.add(\"fos7\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['NN']}},\n",
    "                     {'TAG': {\"IN\": ['MD']}},\n",
    "                     {'TAG': {\"IN\": ['RB']}},\n",
    "                     {'TAG': {\"IN\": ['VB']}}])  # add pattern NN[F] VB[O]        \n",
    "\n",
    "        matcher.add(\"fos8\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['VB']}},\n",
    "                     {'TAG': {\"IN\": ['DT']}},\n",
    "                     {'TAG': {\"IN\": ['NN']}}])  # add pattern NN[F] VB[O]\n",
    "\n",
    "        matcher.add(\"fos9\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['NN']}},\n",
    "                     {'TAG': {\"IN\": ['VBZ']}},\n",
    "                     {'TAG': {\"IN\": ['JJR']}},\n",
    "                     {'TAG': {\"IN\": ['IN']}},\n",
    "                     {'TAG': {\"IN\": ['NN']}}])  # add pattern NN[F] JJR[O]\n",
    "\n",
    "        matcher.add(\"fos10\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['VB']}},\n",
    "                     {'TAG': {\"IN\": ['NN']}}])  # add pattern NN[F] VB[O]    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        matcher.add(\"p1\", self.collect_sents, [{'TAG': {\"IN\": ['JJ']}},\n",
    "                                               {'TAG': {\"IN\": ['NN',\"NNS\"]}}\n",
    "                                               ])  \n",
    "        matcher.add(\"p2\", self.collect_sents, [{'TAG': {\"IN\": ['JJ']}},\n",
    "                                               {'TAG': {\"IN\": ['NN',\"NNS\"]}},\n",
    "                                               {'TAG': {\"IN\": ['NN',\"NNS\"]}}\n",
    "                                               ])  \n",
    "        matcher.add(\"p3\", self.collect_sents, [{'TAG': {\"IN\": ['RB','RBR','RBS']}},\n",
    "                                               {'TAG': {\"IN\": ['JJ']}},\n",
    "                                               {'TAG': {\"IN\": ['NN',\"NNS\"]}}\n",
    "                                               ]) \n",
    "#         matcher.add(\"p4\", self.collect_sents, [{'TAG': {\"IN\": ['RB','RBR','RBS']}},\n",
    "#                                                {'TAG': {\"IN\": ['RB','RBR','RBS']}},\n",
    "#                                                {'TAG': {\"IN\": ['NN',\"NNS\"]}}\n",
    "#                                                ])\n",
    "        matcher.add(\"p5\", self.collect_sents, [{'TAG': {\"IN\": ['RB','RBR','RBS']}},\n",
    "                                               {'TAG': {\"IN\": ['RB','RBR','RBS']}},\n",
    "                                               {'TAG': {\"IN\": ['JJ']}},\n",
    "                                               {'TAG': {\"IN\": ['NN',\"NNS\"]}}\n",
    "                                               ])\n",
    "#         matcher.add(\"p6\", self.collect_sents, [{'TAG': {\"IN\": ['RB','RBR','RBS']}},\n",
    "#                                                {'TAG': {\"IN\": ['VBN','VBD','VB']}}                                               \n",
    "#                                                ])\n",
    "        matcher.add(\"p7\", self.collect_sents, [{'TAG': {\"IN\": ['VBN','VBD','VB']}},\n",
    "                                               {'TAG': {\"IN\": ['JJ']}}                                               \n",
    "                                               ])    \n",
    "#         matcher.add(\"p8\", self.collect_sents, [{'TAG': {\"IN\": ['RB','RBR','RBS']}},\n",
    "#                                                {'TAG': {\"IN\": ['RB','RBR','RBS']}},\n",
    "#                                                {'TAG': {\"IN\": ['JJ']}}                                               \n",
    "#                                                ])  \n",
    "        matcher.add(\"p9\", self.collect_sents, [{'TAG': {\"IN\": ['VBN','VBD','VB']}},\n",
    "                                               {'TAG': {\"IN\": ['RB','RBR','RBS']}},\n",
    "                                               {'TAG': {\"IN\": ['RB','RBR','RBS']}}\n",
    "                                              ])\n",
    "\n",
    "        doc = nlp(sent)\n",
    "        matches = matcher(doc)\n",
    "\n",
    "        for match_id, start, end in matches:\n",
    "            # Get the string representation\n",
    "            pattern_id = nlp.vocab.strings[match_id]\n",
    "            span = doc[start:end]  # The matched span\n",
    "            pos = [(token.text,token.tag_) for token in doc]\n",
    "            span_pos = pos[start:end]\n",
    "#             print(match_id, pattern_id, start, end, span.text)\n",
    "#             print(span_pos)\n",
    "            fos = self.match_fos(pattern_id,span_pos) \n",
    "#             print(fos)\n",
    "            for fo in fos:\n",
    "                if fo not in res: \n",
    "                    res.append(fo)\n",
    "        return res\n",
    "    \n",
    "    def match_fos(self,pattern_id,span_pos):\n",
    "        if pattern_id == \"fos1\": # add pattern JJ[O] NN[F]\n",
    "            o = span_pos[0][0]\n",
    "            f = span_pos[1][0]\n",
    "        if pattern_id == \"fos1.1\": # add pattern JJ[O] NN[F]\n",
    "            o = span_pos[2][0]\n",
    "            f = \"%s %s\"%(span_pos[0][0] ,span_pos[1][0])\n",
    "        elif pattern_id == \"fos2\":\n",
    "            o = span_pos[0][0]\n",
    "            f = span_pos[3][0]\n",
    "        elif pattern_id == \"fos3\":\n",
    "            o1 = span_pos[0][0] \n",
    "            o2 = span_pos[2][0]\n",
    "            f = span_pos[3][0]\n",
    "            return [(f,o1),(f,o2)]\n",
    "        elif pattern_id == \"fos4\":\n",
    "            o = span_pos[2][0]\n",
    "            f = span_pos[0][0]\n",
    "        elif pattern_id == \"fos5\":\n",
    "            o = span_pos[2][0]\n",
    "            f = span_pos[0][0]\n",
    "        elif pattern_id == \"fos6\":\n",
    "            o = span_pos[3][0]\n",
    "            f = span_pos[0][0]\n",
    "        elif pattern_id == \"fos7\":\n",
    "            o = span_pos[0][0]\n",
    "            f = span_pos[2][0]\n",
    "        elif pattern_id == \"fos8\":\n",
    "            o = span_pos[0][0]\n",
    "            f = span_pos[2][0]\n",
    "        elif pattern_id == \"fos9\":\n",
    "            o = span_pos[1][0]\n",
    "            f = span_pos[0][0]\n",
    "        #-----------------------------------------------\n",
    "        elif pattern_id == \"p1\":\n",
    "            o = span_pos[0][0]\n",
    "            f = span_pos[1][0]\n",
    "        elif pattern_id == \"p2\":\n",
    "            o = span_pos[0][0]\n",
    "            f = \"%s %s\"%(span_pos[1][0] ,span_pos[2][0])\n",
    "        elif pattern_id == \"p3\":\n",
    "            o = \"%s %s\"%(span_pos[0][0] ,span_pos[1][0])\n",
    "            f = span_pos[2][0]\n",
    "        elif pattern_id == \"p5\":\n",
    "            o = \"%s %s\"%(span_pos[1][0] ,span_pos[2][0])\n",
    "            f = span_pos[3][0]\n",
    "        elif pattern_id == \"p6\":\n",
    "            o = \"%s %s\"%(span_pos[1][0] ,span_pos[2][0])\n",
    "            f = span_pos[3][0]\n",
    "        elif pattern_id == \"p7\":\n",
    "            o = span_pos[1][0]\n",
    "            f = span_pos[0][0]\n",
    "#         elif pattern_id == \"p8\":\n",
    "#             o = span_pos[1][0]\n",
    "#             f = span_pos[0][0]\n",
    "        elif pattern_id == \"p9\":\n",
    "            o = span_pos[2][0]\n",
    "            f = span_pos[0][0]\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "        if o in stopwords: return ()\n",
    "        else: return [(f,o)]\n",
    "\n",
    "    def run(self):\n",
    "#         for sent in nltk.sent_tokenize(self.article):\n",
    "#             sent = sent.lower()\n",
    "#             sent = sent.replace(\",\", \" \").replace(\":\", \" \").replace(\"(\", \" \").replace(\")\", \" \").replace(\"multi-\",                                                                                                        \"multiple \")\n",
    "        self.article = self.article.replace(\" is \",\" \")\n",
    "        self.article = self.article.replace(\"  \",\" \")\n",
    "        self.article = re.sub(r'(?:^| )\\w(?:$| )', \" \", self.article).strip() # remove single alphbet\n",
    "        ress = self.match_pattern(self.article)\n",
    "        return ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sent_pattern = r\"\"\"<JJ>\"\"\"\n",
    "# #     pattern = r'<PROPN>+ (<PUNCT|CCONJ> <PUNCT|CCONJ>? <PROPN>+)*'\n",
    "\n",
    "\n",
    "# # tag_pattern = \"<DT>?<JJ>*<NN.*>\"\n",
    "# # regexp_pattern = tag_pattern2re_pattern(tag_pattern)\n",
    "# # regexp_pattern\n",
    "# # '(<(DT)>)?(<(JJ)>)*(<(NN[^\\\\{\\\\}<>]*)>)'\n",
    "\n",
    "# rev = \"\"\"\n",
    "# I found this model to be great value for money. It can play DVDs, VCDs, CDs, CD-RW (i.e. burned CDs) as well as MP3 files. It can also convert PAL to NTSC and NTSC to PAL on any TV, which enables you to watch foreign movies. Since it has 110-220 power coversion, you can use it anywhere in the US, Europe and Israel. It also has a secret menu that allows you to play movies from every DVD region number (look on the web, you'll find it...).The downside: seems like the production quality of these machines is problematic. The first unit I got did not work properly. I returned it with no problems, and the one I have now works great.Overall - a great buy, a fully load machine.\n",
    "# \"\"\"\n",
    "\n",
    "# extract_pharse = []\n",
    "# # Print all Verb Phrase\n",
    "# extract_sents = sentence_extract_blob(rev)\n",
    "# for rev_sent in extract_sents:\n",
    "#     print(rev_sent)\n",
    "#     print(get_pos_sequence(rev_sent))\n",
    "#     doc = textacy.make_spacy_doc(rev_sent,lang='en_core_web_sm')\n",
    "#     phrases = textacy.extract.matches(doc, [\n",
    "#         [{\"TAG\": \"NN\"},{\"TAG\": \"NN\",\"OP\": \"+\"}],\n",
    "#         [{\"TAG\": \"NN\"},{\"TAG\": \"VBZ\",\"OP\": \"+\"}]\n",
    "#     ])\n",
    "#     for phrase in phrases:\n",
    "#     #         print(\"verb_phrases : \" , phrase.text)\n",
    "#     #     print([(token.text,token.pos_) for token in nlp(chunk.text)])\n",
    "#         extract_pharse.append(phrase.text+\"\\n\")           \n",
    "# extract_pharse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOP-Rule(Dependency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FOP_rule_Depend():\n",
    "    def __init__(self, review):\n",
    "        self.review = review\n",
    "        self.possible_verb = ['VB','MD','VBG','VBN']\n",
    "        self.possible_adj = [\"JJ\",\"JJR\"]\n",
    "        self.possible_noun = [\"NN\",\"NNP\",\"NNPS\",\"NNS\"]\n",
    "        self.possible_adv = ['RB','RBR','RBS']\n",
    "\n",
    "    def rule1(self, doc):\n",
    "        #         amod(N, A) →< N, A >\n",
    "        res = []\n",
    "        for token_noun in doc:\n",
    "            if token_noun.tag_ in self.possible_noun:\n",
    "                for possible_opinion in token_noun.children:\n",
    "                    if possible_opinion.dep == amod and possible_opinion.tag_ in self.possible_adj:\n",
    "#                         print(token_noun.text,token_noun.tag_, possible_opinion.text,possible_opinion.tag_)\n",
    "                        f,o = token_noun.text,possible_opinion.text\n",
    "                        if o not in stopwords and f != o:\n",
    "                            res.append((f,o))\n",
    "                    #         print(\"rule1\",res)\n",
    "        return res\n",
    "\n",
    "    def rule2(self, doc):\n",
    "        #         acomp(V, A) + nsubj(V, N) →< N, A >\n",
    "        res = []\n",
    "        term1s, term2s = [], []\n",
    "        for token_verb in doc:\n",
    "            if token_verb.tag_ in self.possible_verb:\n",
    "                for possible_opinion in token_verb.children:\n",
    "                    if possible_opinion.dep == acomp and possible_opinion.tag_ in self.possible_adj:\n",
    "#                         print(token_verb.text,token_verb.tag_, possible_opinion.text,possible_opinion.tag_)\n",
    "                        term1s.append([token_verb.text, possible_opinion.text])  # acomp(V, A)\n",
    "                    if possible_opinion.dep == nsubj and possible_opinion.tag_ in self.possible_noun:\n",
    "                        term2s.append([token_verb.text, possible_opinion.text])  # nsubj(V, N)\n",
    "\n",
    "        for t1 in term1s:\n",
    "            V1, A1 = t1\n",
    "            for t2 in term2s:\n",
    "                V2, N2 = t2\n",
    "                if V1 == V2: \n",
    "                    f,o = N2, A1\n",
    "                    if o not in stopwords and f != o:\n",
    "                        res.append((f,o))  # →< N, A >\n",
    "            #         print(\"rule2\",res)\n",
    "        return res\n",
    "    \n",
    "    def rule3(self, doc):\n",
    "        res = []\n",
    "        #         nsubj(aux,noun) + acomp(aux,adj) →< N, A >\n",
    "\n",
    "        term1s, term2s = [], []\n",
    "        for token_AUX in doc:\n",
    "            if token_AUX.pos == AUX:\n",
    "                for possible_head in token_AUX.children:\n",
    "                    if possible_head.dep == nsubj and possible_head.tag_ in self.possible_noun:\n",
    "                        term1s.append([token_AUX.text, possible_head.text])  # nsubj(aux,noun)\n",
    "                    if possible_head.dep == acomp and possible_head.tag_ in self.possible_adj:\n",
    "                        term2s.append([token_AUX.text, possible_head.text])  # acomp(aux,adj)\n",
    "\n",
    "        for t1 in term1s:\n",
    "            Aux1, N1 = t1\n",
    "            for t2 in term2s:\n",
    "                Aux2, A2 = t2\n",
    "                if Aux1 == Aux2 and N1 != Aux1 and Aux1 != \"is\": \n",
    "                    f,o = N1, Aux1\n",
    "                    if o not in stopwords and f != o:\n",
    "                        res.append((f,o))  # →< N, A >\n",
    "            #         print(\"rule3\",res)\n",
    "        v = \"\"\n",
    "#         if len(res) > 0: \n",
    "#             v = [get_pos_sequence(\" \".join(r))[1] for r in res]\n",
    "        return res\n",
    "\n",
    "\n",
    "#     def rule4(self, doc):\n",
    "#         res = []\n",
    "#         #     dobj(V, N) + nsubj(V, N0) →< N, V >\n",
    "#         term1s, term2s = [], []\n",
    "#         for token_verb in doc:\n",
    "#             if token_verb.tag_ in self.possible_verb:\n",
    "#                 for possible_head in token_verb.children:\n",
    "#                     if possible_head.dep == dobj and possible_head.tag_ in self.possible_noun:\n",
    "# #                         print(token_verb.text,token_verb.tag_, possible_head.text,possible_head.tag_)\n",
    "#                         term1s.append([token_verb.text, possible_head.text])  # nsubj(aux,noun)\n",
    "#                     if possible_head.dep == nsubj and possible_head.tag_ in self.possible_noun:\n",
    "#                         term2s.append([token_verb.text, possible_head.text])  # acomp(aux,adj)\n",
    "#         for t1 in term1s:\n",
    "#             V1, N1 = t1\n",
    "#             f,o = V1, N1\n",
    "#             if o not in stopwords and f != o:\n",
    "#                 res.append((f,o))\n",
    "#         # res.extend(term2s)\n",
    "#         #         print(\"rule4\",res)\n",
    "#         return res\n",
    "\n",
    "    def rule5(self, doc):\n",
    "        #         < h1, m > +conj and(h1, h2) →< h2, m >\n",
    "        res = []\n",
    "        term1s, term2s = [], []\n",
    "        for token in doc:\n",
    "            for possible_head in token.children:\n",
    "                if possible_head.tag_ in self.possible_noun + self.possible_adj + self.possible_adv:\n",
    "                    term1s.append([token.text, possible_head.text])  # < h1, m >\n",
    "                if possible_head.dep == conj and possible_head.tag_ in self.possible_noun:\n",
    "                    term2s.append([token.text, possible_head.text])  # conj and(h1, h2)\n",
    "\n",
    "        for t1 in term1s:\n",
    "            h1, m = t1\n",
    "            for t2 in term2s:\n",
    "                h21, h22 = t2\n",
    "                if h1 == h21:\n",
    "#                     if m not in stopwords:\n",
    "                    f,o = h1, m\n",
    "                    if ((o not in stopwords) and (f not in stopwords)) and (f != o): \n",
    "                        res.append((f, o))\n",
    "                    f,o = h22, m\n",
    "                    if ((o not in stopwords) and (f not in stopwords)) and (f != o): \n",
    "                        res.append((f, o))                    \n",
    "                #         print(\"rule5\",res)\n",
    "        return res\n",
    "\n",
    "    def rule6(self, doc):\n",
    "        #         < h, m1 > +conj and(m1, m2) →< h, m2 >\n",
    "        #   nsubj(aux,noun) + acomp(aux,m1) + conj and(m1, m2) →< h, m1 > + < h, m2 >\n",
    "        res = []\n",
    "        term1s, term2s, term3s = [], [], []\n",
    "        for token in doc:\n",
    "            if token.pos == AUX:\n",
    "                token_AUX = token\n",
    "                for possible_head in token_AUX.children:\n",
    "                    if possible_head.dep == nsubj and possible_head.tag_ in self.possible_noun:\n",
    "                        term1s.append([token_AUX.text, possible_head.text])  # nsubj(aux,noun)\n",
    "                    if possible_head.dep == acomp and possible_head.tag_ in self.possible_adj:\n",
    "                        term2s.append([token_AUX.text, possible_head.text])  # acomp(aux,m1)\n",
    "            else:\n",
    "                for possible_m in token.children:\n",
    "                    if possible_m.dep_ == \"conj\": term3s.append([token.text, possible_m.text])\n",
    "\n",
    "        for t1 in term1s:\n",
    "            Aux1, N1 = t1\n",
    "            for t2 in term2s:\n",
    "                Aux2, A2 = t2\n",
    "                if Aux1 == Aux2 and N1 != Aux1:\n",
    "                    #                     res.append([N1,A2]) # →< N, A >\n",
    "                    for t3 in term3s:\n",
    "                        m1, m2 = t3\n",
    "                        if A2 in t3:\n",
    "#                             if m1 not in stopwords and N1 != m1: \n",
    "                                f,o = N1,m1\n",
    "                                if ((o not in stopwords) and (f not in stopwords)) and (f != o):     \n",
    "                                    res.append((f, o))  # →< N, A > \n",
    "                                f,o = N1, m2\n",
    "                                if ((o not in stopwords) and (f not in stopwords)) and (f != o):     \n",
    "                                    res.append((f, o))  # →< N, A > \n",
    "                        #         print(\"rule6\",res)\n",
    "        return res\n",
    "\n",
    "    def rule7(self, doc):\n",
    "        #         < h, m > +neg(m, not) →< h, not + m >\n",
    "        #          acomp(aux,adj) + neg(aux,part) + nsubj(aux,N1) + compound(N1,N2) => <N1 + N2 , part + adj>\n",
    "        res = []\n",
    "        term1s, term2s, term3s, term4s = [], [], [], []\n",
    "        for token in doc:\n",
    "            if token.pos == AUX:\n",
    "                token_AUX = token\n",
    "                for possible_head in token_AUX.children:\n",
    "                    if possible_head.dep == acomp and possible_head.tag_ in self.possible_adj:\n",
    "                        term1s.append([token_AUX.text, possible_head.text])  # acomp(aux,adj)\n",
    "                    if possible_head.dep == neg and possible_head.pos == PART:\n",
    "                        term2s.append([token_AUX.text, possible_head.text])  # neg(aux,part)\n",
    "                    if possible_head.dep == nsubj and possible_head.tag_ in self.possible_noun:\n",
    "                        term3s.append([token_AUX.text, possible_head.text])  # nsubj(aux,N1)\n",
    "            else:\n",
    "                for possible_m in token.children:\n",
    "                    #                     print(token,possible_m,possible_m.dep_)\n",
    "                    if possible_m.dep_ == \"compound\" and possible_m.tag_ in self.possible_noun:  # compound(N1,N2)\n",
    "                        term4s.append([token.text, possible_m.text])\n",
    "\n",
    "        for t1 in term1s:\n",
    "            Aux1, adj = t1\n",
    "            for t2 in term2s:\n",
    "                Aux2, part = t2\n",
    "                if Aux1 == Aux2:\n",
    "                    #                     print(part , adj) # <part , adj>\n",
    "                    for t3 in term3s:\n",
    "                        Aux3, N1 = t3\n",
    "                        if Aux1 == Aux3:\n",
    "                            for t4 in term4s:\n",
    "                                N41, N42 = t4\n",
    "                                if N1 in t4:\n",
    "#                                     res.append([N42, N41, part, adj])  # => <N1 + N2 , part + adj>\n",
    "                                    f,o = \"%s %s\"%(N42, N41),\"%s %s\"%(part, adj)\n",
    "                                    if (o not in stopwords) and (o not in f):\n",
    "                                        res.append((f,o))  # => <N1 + N2 , part + adj>\n",
    "\n",
    "                                #         print(\"rule7\",res)\n",
    "        return res\n",
    "\n",
    "    def rule8(self, doc):\n",
    "        #         < h, m > +nn(h, N) →< N + h, m >\n",
    "        #       compound(h,N1) + nsubj(V1,N1) + acomp(V1,A) -> <h + N1 , A>\n",
    "        res = []\n",
    "        term1s, term2s, term3s = [], [], []\n",
    "        for token in doc:\n",
    "            for possible_head in token.children:\n",
    "                if possible_head.dep_ == \"compound\" and possible_head.tag_ in self.possible_noun:\n",
    "                    term1s.append([token.text, possible_head.text])  # compound(h,N1)\n",
    "                if possible_head.dep == nsubj and token.tag_ in self.possible_verb and possible_head.tag_ in self.possible_noun:\n",
    "                    term2s.append([token.text, possible_head.text])  # nsubj(V1,N1)\n",
    "                if possible_head.dep == acomp and token.tag_ in self.possible_verb and possible_head.tag_ in self.possible_adj:\n",
    "                    term3s.append([token.text, possible_head.text])  # acomp(V1,A)\n",
    "        for t1 in term1s:\n",
    "            N1, N2 = t1\n",
    "            for t2 in term2s:\n",
    "                V21, N22 = t2\n",
    "                if N22 in t1:\n",
    "                    #                     print(N2,N1) # <h + N1>\n",
    "                    for t3 in term3s:\n",
    "                        V31, A = t3\n",
    "                        if V31 == V21:\n",
    "                            f,o = \"%s %s\"%(N2, N1), A\n",
    "                            if (o not in stopwords) and (o not in f):\n",
    "                                res.append((f,o))  # -> <h + N1 , A>\n",
    "\n",
    "                        #         print(\"rule8\",res)\n",
    "        return res\n",
    "\n",
    "    def rule9(self, doc):\n",
    "        #         < h, m > +nn(N, h) →< h + N, m >\n",
    "        #         compound(N1,N2) + dobj(V,N1) -> < N2 + N1, V >\n",
    "        res = []\n",
    "        term1s, term2s = [], []\n",
    "        for token in doc:\n",
    "            for possible_head in token.children:\n",
    "                if possible_head.dep_ == \"compound\" and possible_head.tag_ in self.possible_noun:\n",
    "                    term1s.append([token.text, possible_head.text])  # compound(N1,N2)\n",
    "                if possible_head.dep == dobj and token.tag_ in self.possible_adj + self.possible_verb and possible_head.tag_ in self.possible_noun:\n",
    "#                     print(token.text,token.tag_, possible_head.text,possible_head.tag_)\n",
    "                    term2s.append([token.text, possible_head.text])  # dobj(V,N1)\n",
    "        for t1 in term1s:\n",
    "            N1, N2 = t1\n",
    "            for t2 in term2s:\n",
    "                V, N22 = t2\n",
    "                if N22 in t1:\n",
    "                    f,o = \"%s %s\"%(N2, N1), V\n",
    "                    if (o not in stopwords) and (o not in f):\n",
    "                        res.append((f,o))  # -> <h + N1 , A>\n",
    "\n",
    "                #         print(\"rule9\",res)\n",
    "        return res\n",
    "\n",
    "    def run(self):\n",
    "        self.review = self.review\n",
    "        doc = nlp(self.review)\n",
    "        #         displacy.render(doc, style='dep', jupyter = True) # dependency parse tree\n",
    "        deplist = []\n",
    "        dep1 = self.rule1(doc)\n",
    "        dep2  = self.rule2(doc)\n",
    "        dep3  = self.rule3(doc)\n",
    "#         dep4  = self.rule4(doc)\n",
    "        dep5  = self.rule5(doc)\n",
    "        dep6  = self.rule6(doc)\n",
    "        dep7  = self.rule7(doc)\n",
    "        dep8  = self.rule8(doc)\n",
    "        dep9  = self.rule9(doc)\n",
    "#         print(\"dep1:\",str(dep1))\n",
    "#         print(\"dep2:\",str(dep2))\n",
    "#         print(\"dep3:\",str(dep3))\n",
    "#         print(\"dep4:\",str(dep4))\n",
    "#         print(\"dep5:\",str(dep5))\n",
    "#         print(\"dep6:\",str(dep6))\n",
    "#         print(\"dep7:\",str(dep7))\n",
    "#         print(\"dep8:\",str(dep8))\n",
    "#         print(\"dep9:\",str(dep9))\n",
    "#         print(\"-------------------------------------\")\n",
    "#         print(dep1 , dep2 , dep3 , dep4 , dep5 , dep6 , dep7 , dep8 , dep9)\n",
    "        dep = dep1 + dep2 + dep3 + dep5 + dep6 + dep7 + dep8 + dep9\n",
    "        return dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FOP_rule_Depend2():\n",
    "    def __init__(self, review):\n",
    "        self.review = review\n",
    "        self.possible_verb = ['VB','MD','VBG','VBN']\n",
    "        self.possible_adj = [\"JJ\",\"JJR\"]\n",
    "        self.possible_noun = [\"NN\",\"NNP\",\"NNPS\",\"NNS\"]\n",
    "        self.possible_adv = ['RB','RBR','RBS']\n",
    "        # spacy 是head指向dependecy(child)\n",
    "        # 一個word可以有很多child\n",
    "\n",
    "    def rule10(self, doc):\n",
    "        #  nsubj(f,o)\n",
    "        res = []\n",
    "        for token in doc:\n",
    "            for children in token.children:\n",
    "                if children.dep == nsubj:    \n",
    "                    if children.tag in self.possible_noun:\n",
    "    #                     print(token.text,token.tag_, children.text,children.tag_)\n",
    "                        f,o = children,token\n",
    "                        if (o.text in stopwords) or (f.text in stopwords): continue\n",
    "                        if (o.text == f.text): continue\n",
    "                        res.append((f.text,o.text))\n",
    "                        #         print(\"rule10\",res)\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def rule12(self, doc):\n",
    "        #  amod(f,o)\n",
    "        res = []\n",
    "        for token in doc:\n",
    "            for children in token.children:\n",
    "                if children.dep == amod:                \n",
    "#                     print(token.text,token.tag_, children.text,children.tag_)\n",
    "                    f,o = token,children\n",
    "                    if (o.text in stopwords) or (f.text in stopwords): continue\n",
    "                    if (o.text == f.text): continue\n",
    "                    res.append((f.text,o.text))\n",
    "                    #         print(\"rule12\",res)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def rule13(self, doc):\n",
    "        #  advmod(f,o)\n",
    "        res = []\n",
    "        for token in doc:\n",
    "            for children in token.children:\n",
    "                if children.dep == advmod and token.tag_ in self.possible_adj + self.possible_verb:                \n",
    "#                     print(token.text,token.tag_, children.text,children.tag_)\n",
    "                    f,o = token,children\n",
    "                    if (o.text in stopwords) or (f.text in stopwords): continue\n",
    "                    if (o.text == f.text): continue\n",
    "                    res.append((f.text,o.text))\n",
    "                    #         print(\"rule12\",res)\n",
    "        return res\n",
    "    \n",
    "    def rule14(self, doc):\n",
    "        #  dobj(f,o)\n",
    "        res = []\n",
    "        for token in doc:\n",
    "            for children in token.children:\n",
    "                if children.dep == dobj and children.pos in [NOUN,PROPN] and token.tag_ in self.possible_verb:  \n",
    "                    if (token.tag_ in self.possible_adj + self.possible_adv):#\n",
    "                        f,o = children,token\n",
    "                        if (o.text in stopwords) or (f.text in stopwords): continue\n",
    "                        if (o.text == f.text): continue\n",
    "                        res.append((f.text,o.text))\n",
    "                        #         print(\"rule12\",res)\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        self.review = self.review.replace(\" is \",\" \")\n",
    "        self.review = self.review.replace(\"  \",\" \")\n",
    "        self.review = re.sub(r'(?:^| )\\w(?:$| )', \" \", self.review).strip() # remove single alphbet\n",
    "#         print(self.review)\n",
    "#         print(get_pos_sequence(self.review)[1])    \n",
    "        doc = nlp(self.review)\n",
    "#         displacy.render(doc, style='dep', jupyter = True) # dependency parse tree\n",
    "        deplist = []\n",
    "        dep10 = self.rule10(doc)\n",
    "        dep12  = self.rule12(doc)\n",
    "        dep13  = self.rule13(doc)\n",
    "        dep14  = self.rule14(doc)        \n",
    "#         print(\"dep1:\",str(dep10))\n",
    "#         print(\"dep3:\",str(dep12))\n",
    "#         print(\"dep4:\",str(dep13))\n",
    "#         print(\"dep5:\",str(dep14))\n",
    "        \n",
    "#         print(\"-------------------------------------\")\n",
    "        dep = dep10 + dep12 + dep13 + dep14\n",
    "        return dep\n",
    "    \n",
    "    \n",
    "def TwoRuleFOPDep(rev_sent):\n",
    "    fops1 = FOP_rule_Depend(rev_sent).run() \n",
    "    fops2 = FOP_rule_Depend2(rev_sent).run()\n",
    "    fops = []\n",
    "    [fops.append(fop) for fop in fops1 if fop not in fops]\n",
    "    [fops.append(fop) for fop in fops2 if fop not in fops]    \n",
    "    return fops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_sent = \"\"\"\n",
    "great for those with input receiver but no build decoder the amp\n",
    "\"\"\"\n",
    "DEP_fops = TwoRuleFOPDep(rev_sent)\n",
    "DEP_fops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was happy s d owner until month ago when began getting the dreaded c error.\n",
      "---------------------------------------------\n",
      "it would not recognize any dvd put in even brand new ones.\n",
      "---------------------------------------------\n",
      "did web search and was amazed to find that many many people who have bought this player and sony players in general have had exactly the same problem after year to year and half.\n",
      "---------------------------------------------\n",
      "sony refuses to do recall and i ve since bought new player not sony settled on jvc xv fa but thought should post my experience here.\n",
      "---------------------------------------------\n",
      "buyer beware on this player it has very high rate of terminal failure.\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rev = \"\"\"\n",
    "I was a happy S530D owner until a month ago when I began getting the dreaded \\\"C13 00\\\" error.  It would not recognize any DVD I put in; even brand new ones.\\n\\nI did a web search and was amazed to find that many, many people who have bought this player (and Sony players in general) have had exactly the same problem after a year to a year and half.\\n\\nSony refuses to do a recall and I've since bought a new player (not a Sony; I settled on a JVC XV-FA90), but I thought I should post my experience here.  Buyer beware on this player; it has a very high rate of terminal failure.\n",
    "\"\"\"\n",
    "# rev = \"\"\"\n",
    "# The screen is wide and clear.\n",
    "# \"\"\"\n",
    "\n",
    "# rev = \"\"\"\n",
    "# I just purchased mine 11/5/99. I have had the evil lip-sync problem on only one movie(A bridge to far),Ive played about 8 DVDs. One other note, The universal remote isnt, when it comes to JVC receivers. It would have  been nice to control volume with the 530s remote. On the plus side it does  control my Sony TV perfectly.\n",
    "# \"\"\"\n",
    "extract_sents = sentence_extract_blob(rev)\n",
    "for rev_sent in extract_sents:\n",
    "    POS_fops = FO_rule_POS(rev_sent).run()\n",
    "    DEP_fops = TwoRuleFOPDep(rev_sent)\n",
    "    if (len(POS_fops) == 0) and (len(DEP_fops) == 0): continue\n",
    "    print(rev_sent)\n",
    "#     print(\"POS:\",POS_fops)\n",
    "#     print(\"dep:\",DEP_fops)\n",
    "    print('---------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Extract Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = \"\"\"\n",
    "I found this model to be great value for money. It can play DVDs, VCDs, CDs, CD-RW (i.e. burned CDs) as well as MP3 files. It can also convert PAL to NTSC and NTSC to PAL on any TV, which enables you to watch foreign movies. Since it has 110-220 power coversion, you can use it anywhere in the US, Europe and Israel. It also has a secret menu that allows you to play movies from every DVD region number (look on the web, you'll find it...).The downside: seems like the production quality of these machines is problematic. The first unit I got did not work properly. I returned it with no problems, and the one I have now works great.Overall - a great buy, a fully load machine.\n",
    "\"\"\"\n",
    "\n",
    "# rev = SentProcess(rev)\n",
    "\n",
    "# fops = FO_rule_POS(rev).run()\n",
    "# print(fops,\"\\n\")\n",
    "\n",
    "# FOPS = FOP_rule_Depend(rev).run()\n",
    "# print(FOPS)\n",
    "\n",
    "# extract_sents = sentence_extract(rev)\n",
    "# extract_sents\n",
    "\n",
    "\n",
    "# print(keywords.keywords(rev))\n",
    "# print(summarizer.summarize(rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取所有規格關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1618 Products\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing B01HB7BZ26: 100%|██████████| 1618/1618 [1:40:56<00:00,  3.59s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 963\n",
      "Count : 494\n",
      "Count : 33\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from collections import Counter,OrderedDict\n",
    "\n",
    "feature_counter1 = Counter()\n",
    "feature_counter2 = Counter()\n",
    "feature_counter3 = Counter()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import collections\n",
    "freq_words = collections.Counter()\n",
    "feat_words = collections.Counter()\n",
    "opinion_words = collections.Counter()\n",
    "cooccurs_words = collections.Counter()\n",
    "\n",
    "_Sents = []\n",
    "keywords_list = []\n",
    "# 其中 freq_words 是單字出現的頻率 , _Sents 是文章中所有的句子\n",
    "\n",
    "i = 0\n",
    "\n",
    "# total_keywords = []\n",
    "Products = len(PROD_DICT.items())\n",
    "print(\"%s Products\\n\"%(Products))    \n",
    "\n",
    "if not os.path.exists('View'):\n",
    "    os.makedirs('View')\n",
    "  \n",
    "\n",
    "fn = 'View/%s_%s_keywords.txt'%(category1,category2)\n",
    "if os.path.exists(fn):\n",
    "    with open('View/Prod_keyword.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "        # with open(\"Prod_keyword.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "        with tqdm(total=Products) as pbar:\n",
    "            for asin,DATA_DICT in PROD_DICT.items():\n",
    "                REVIEW_ITEM_LIST = DATA_DICT['REVIEW_ITEM_LIST']\n",
    "                title = DATA_DICT['title']\n",
    "                description = DATA_DICT['description']\n",
    "                feature = DATA_DICT['feature']      \n",
    "                dash_keywords,noun_keywords,newkeywords, newdescription = getKeywordFeat(description,feature)\n",
    "\n",
    "                f.write(\"* asin:\" + asin+\"\\n\")\n",
    "                if \"(new Date()).getTime();\" in title : f.write(\"title:\" + asin+\"\\n\")\n",
    "                else: f.write(\"title:\" + title+\"\\n\")\n",
    "                f.write('dash_keywords:\\n' + str(dash_keywords)+\"\\n\");f.write('noun_keywords2:\\n' + str(noun_keywords)+\"\\n\")\n",
    "#                 keywords = noun_keywords\n",
    "#                 keywords = list(list(dash_keywords1) + list(noun_keywords1))\n",
    "#                 newkeywords1 = list()\n",
    "#                 for key in list(dash_keywords1) + list(noun_keywords1):\n",
    "#                     clean_key = remove_word2(key)\n",
    "#                     clean_key = clean_key.split(\" \")            \n",
    "#                     newkeywords1.extend(clean_key) \n",
    "#                 newkeywords1 = list(set([key for key in newkeywords1 if len(key) >=2 and key not in stopwords and key not in opinion_lexicon[\"total-words\"]]))\n",
    "#                 keywords_list.append(\" \".join(newkeywords1))\n",
    "                f.write('* keywords:\\n' + str(newkeywords)+\"\\n\")\n",
    "                f.write('* newdescription:\\n' + str(newdescription)+\"\\n\")\n",
    "                f.write('* pos_newdescription:\\n' + str(get_pos_sequence(newdescription)[1])+\"\\n\")\n",
    "                f.write('************************************************************************'+\"\\n\")\n",
    "                f.write('************************************************************************'+\"\\n\")      \n",
    "                for pair in REVIEW_ITEM_LIST:\n",
    "                    review = pair[\"review\"]           \n",
    "                    vote = pair[\"vote\"] \n",
    "                    summary = pair[\"summary\"] \n",
    "#                     extract_sents = sentence_extract_blob(review)\n",
    "#                     for rev_sent in extract_sents:            \n",
    "#                         rev_sent,pos_sent = lemm_sent_process2(rev_sent, remove_stopwords=True, summary=False, mode=\"spacy\", withdot=False)\n",
    "#                         if pos_sent == None: continue\n",
    "#                         if len(pos_sent) == 0 : continue \n",
    "#                         tok_sent = set(nltk.word_tokenize(rev_sent)) - set(stopwords)\n",
    "#         #                     tok_sent = [token for token in nltk.word_tokenize(rev_sent) if token not in stopwords]\n",
    "#                         freq_words.update(tok_sent)    \n",
    "#                         _Sents.append(tok_sent)\n",
    "                    dash_keywords2,noun_keywords2,newkeywords2, _ = getKeywordFeat_2(review)                    \n",
    "                    dash_keywords3,noun_keywords3,newkeywords3, _ = getKeywordFeat_2(summary)\n",
    "                feature_counter1.update(newkeywords)\n",
    "                feature_counter2.update(newkeywords2)\n",
    "                feature_counter3.update(newkeywords3)\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(\"Processing %s\" % asin)\n",
    "                \n",
    "    important_features = OrderedDict(sorted(feature_counter1.items(), key=lambda pair: pair[1], reverse=True))\n",
    "    important_features = [(word,important_features[word]) for word in important_features if important_features[word]>5]\n",
    "    print(\"Count : %s\"%(len(important_features)))\n",
    "    \n",
    "    important_features2 = OrderedDict(sorted(feature_counter2.items(), key=lambda pair: pair[1], reverse=True))\n",
    "    important_features2 = [(word,important_features2[word]) for word in important_features2 if important_features2[word]>5]\n",
    "    print(\"Count : %s\"%(len(important_features2)))\n",
    "    \n",
    "    important_features3 = OrderedDict(sorted(feature_counter3.items(), key=lambda pair: pair[1], reverse=True))\n",
    "    important_features3 = [(word,important_features3[word]) for word in important_features3 if important_features3[word]>5]\n",
    "    print(\"Count : %s\"%(len(important_features3)))\n",
    "\n",
    "    fn = 'View/%s_%s_keywords.txt'%(category1,category2)\n",
    "    make = False\n",
    "    if not os.path.exists(fn):\n",
    "        make = True\n",
    "        f = open(fn,\"w\",encoding=\"utf-8\")\n",
    "\n",
    "    total_keywords = set()\n",
    "    # with open(\"total_keywords.txt\",'w',encoding=\"utf-8\") as f:\n",
    "    for word , v in important_features:        \n",
    "        if make: f.write(\"%s:%s \\n\"%(word,v))   \n",
    "        if v > 20 : total_keywords.add(word)  \n",
    "            \n",
    "    fn2 = 'View/%s_%s_keywords2.txt'%(category1,category2)\n",
    "    make = False\n",
    "    if not os.path.exists(fn2):\n",
    "        make = True\n",
    "        f = open(fn2,\"w\",encoding=\"utf-8\")\n",
    "\n",
    "    total_keywords2 = set()\n",
    "    # with open(\"total_keywords.txt\",'w',encoding=\"utf-8\") as f:\n",
    "    for word , v in important_features2:        \n",
    "        if make: f.write(\"%s:%s \\n\"%(word,v))   \n",
    "        if v > 5 : total_keywords2.add(word)  \n",
    "            \n",
    "    fn3 = 'View/%s_%s_keywords3.txt'%(category1,category2)\n",
    "    make = False\n",
    "    if not os.path.exists(fn3):\n",
    "        make = True\n",
    "        f = open(fn3,\"w\",encoding=\"utf-8\")\n",
    "\n",
    "    total_keywords3 = set()\n",
    "    # with open(\"total_keywords.txt\",'w',encoding=\"utf-8\") as f:\n",
    "    for word , v in important_features3:        \n",
    "        if make: f.write(\"%s:%s \\n\"%(word,v))   \n",
    "        if v > 5 : total_keywords3.add(word)  \n",
    "                \n",
    "else:\n",
    "    total_keywords = set()\n",
    "    with open(fn,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            k,v = line.split(\":\")\n",
    "            if int(v) > 20 :\n",
    "                total_keywords.add(k)\n",
    "            \n",
    "# pbar.close()\n",
    "    # total_keywords = set(total_keywords) # 檢查重要程度 篩選字典裡的單字\n",
    "    # open(\"total_keywords.txt\",'w',encoding=\"utf-8\").write(str(total_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 關鍵字濾除 (each newkeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# from sklearn import feature_extraction\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # def Importance(document, sorted_words):\n",
    "# #     scores = {}\n",
    "# #     sent = get_token_sent(document)\n",
    "# #     for word in sent:\n",
    "# #         try:\n",
    "# #             scores[word] = dict(sorted_words)[word]\n",
    "# #         except Exception as e:\n",
    "# #             pass\n",
    "# #     scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "# #     return scores[:20]\n",
    "\n",
    "# vectorizer = CountVectorizer()  # 该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "# transformer = TfidfTransformer()  # 该类会统计每个词语的tf-idf权值\n",
    "# tfidf = transformer.fit_transform(\n",
    "#     vectorizer.fit_transform(keywords_list))  # 第一个fit_transform是计算tf-idf，第二个fit_transform是将文本转为词频矩阵\n",
    "# word = vectorizer.get_feature_names()  # 获取词袋模型中的所有词语\n",
    "# weight = tfidf.toarray()  # 将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重\n",
    "\n",
    "# print(\"Calculate TF-IDF finished\")\n",
    "\n",
    "# scores = {}\n",
    "# for i in range(len(weight)):\n",
    "#     # print (u\"-------这里输出第\",i,u\"类文本的词语tf-idf权重------\")\n",
    "#     for j in range(len(word)):\n",
    "#         # print(word[j],weight[i][j])\n",
    "#         scores[word[j]] = weight[i][j]\n",
    "\n",
    "\n",
    "# # fout = 'TF-IDF/%s_%s.txt'%(big_categories,small_categories)\n",
    "# if not os.path.isdir(\"TF-IDF\"):\n",
    "#     os.mkdir(\"TF-IDF\")\n",
    "    \n",
    "# with open('TF-IDF/%s_%s.txt'%(category1,category2),'w',encoding='utf-8') as f:   \n",
    "#     f.write(\"word\" + '\\t' + \"tf-idf-weight\" + '\\n')\n",
    "#     for k, v in scores.items():\n",
    "#         if float(v) <= 0: continue\n",
    "# #         print(k,v)\n",
    "#         f.write(str(k) + '\\t' + str(v) + '\\n')\n",
    "\n",
    "# # sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "# sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "# sorted_words = [word[0] for word in sorted_words]\n",
    "# print(\"\\nTop 20 Important words\", sorted_words[:20])\n",
    "\n",
    "# # total_keywords = set(sorted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然語言處理 -- Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用total_keywords計算pmi\n",
    "from math import log\n",
    "# need pmi feature words & opinions\n",
    "def p(x): # p(x) 計算單字 x 出現的機率\n",
    "    return freq_words[x]/float(len(freq_words))\n",
    "\n",
    "def pxy(x,y): # pxy(x,y) 計算單字 x 和單字 y 出現在同一個句子的機率\n",
    "#     print((lambda s :  x in s and y in s ,_Sents))\n",
    "#     filter()函數用於過濾序列，過濾掉不符合條件的元素\n",
    "    return (len(list(filter(lambda s :  x in s and y in s ,_Sents)))+1)/ float(len(_Sents) )\n",
    "\n",
    "def pmi(x,y): # pmi(x,y) 計算單字 x 和單字 y 的 Pointwise Mutual Information\n",
    "    try:\n",
    "#         print(\"X :%s Y: %s\"%(x,y))\n",
    "#         print('pxy(x,y)',pxy(x,y))\n",
    "#         print('p(x)',p(x))\n",
    "#         print('p(y)',p(y))\n",
    "#         print('log(pxy(x,y)/(p(x)*p(y)),2)',log(pxy(x,y)/(p(x)*p(y)),2))\n",
    "        return  log(pxy(x,y)/(p(x)*p(y)),2) \n",
    "    except Exception as e :\n",
    "#         print(e)\n",
    "        return 0\n",
    "    \n",
    "def pmi_fopair(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    noun_,op_ = [],[]\n",
    "    cand_pairs = []\n",
    "    cand_pairs_score = []\n",
    "    for token in doc:\n",
    "        if token.pos in [ADJ,ADV,VERB]: op_.append(token.text)\n",
    "        if token.pos in [NOUN]: noun_.append(token.text)\n",
    "\n",
    "    for f in noun_:\n",
    "        for o in op_:\n",
    "            pair = (f,o)\n",
    "            score = pmi(f,o)\n",
    "#             if score > 0: cand_pairs.append((pair,score))\n",
    "            if score > 0: cand_pairs.append(pair)\n",
    "            cand_pairs_score.append((pair,score))\n",
    "\n",
    "#             cand_pairs.append((pair,score))\n",
    "#     print(sentence)\n",
    "#     print(cand_pairs)\n",
    "    return cand_pairs,cand_pairs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _Sents[3]\n",
    "\n",
    "# p('year')\n",
    "# p('close')\n",
    "\n",
    "# pmi_fopair(\"player close year absolutly amazing piece equipment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算FO-PAIR以及句子抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing B01HB7BZ26: 100%|██████████| 1618/1618 [2:48:36<00:00,  6.32s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "i = 0    \n",
    "\n",
    "if not os.path.exists('Train'):\n",
    "    os.makedirs('Train')\n",
    "    \n",
    "pattern_text = open(\"View/review_pattern.txt\",'w',encoding=\"utf-8\")\n",
    "# training_data = open(\"View/%s_%s_train.txt\"%(category1,category2),'w',encoding=\"utf-8\")\n",
    "# training_data.write(\"rev_sent | polarity,subjectivity | f1_o1 f2_o2 f3_o3 | summary\" + '\\n') \n",
    "\n",
    "topic_train_df = {}\n",
    "key_train_df = {}\n",
    "i = 0\n",
    "j = 0\n",
    "with tqdm(total=Products) as pbar:\n",
    "    with open(\"View/result.txt\",'w',encoding=\"utf-8\") as result:\n",
    "        for asin,DATA_DICT in PROD_DICT.items():\n",
    "            REVIEW_ITEM_LIST = DATA_DICT['REVIEW_ITEM_LIST']\n",
    "            title = DATA_DICT['title']\n",
    "#             if \"(new Date()).getTime();\" in title : continue\n",
    "            description = DATA_DICT['description']\n",
    "            feature = DATA_DICT['feature']      \n",
    "            if \"(new Date()).getTime();\" not in title :result.write(\"title:\" + title+\"\\n\")\n",
    "            else: result.write(\"title:\" + asin+\"\\n\")\n",
    "\n",
    "            result.write('************************************************************************'+\"\\n\")      \n",
    "            for pair in REVIEW_ITEM_LIST:\n",
    "                review = pair[\"review\"]  \n",
    "                rev_dash_keywords, newreview = cleanReview(review)\n",
    "#                 f.write('newreview:\\n'+newreview+\"\\n\")\n",
    "                vote = pair[\"vote\"]  \n",
    "                overall = pair[\"overall\"] \n",
    "                summary = pair[\"summary\"]\n",
    "                review_ID = pair[\"review_ID\"]\n",
    "                summ_token_set = set(nltk.word_tokenize(summary))\n",
    "#                 if len(summ_token_set & total_keywords) == 0: continue\n",
    "                result.write('overall:' + str(overall) + \"\\n\")\n",
    "                result.write('review:\\n' + review + \"\\n\")\n",
    "                result.write('summary:\\n' + summary + \"\\n\")\n",
    "                result.write(\"************************************************\\n\")\n",
    "     \n",
    "                extract_sents = sentence_extract_blob(review)\n",
    "                keyword_list = []\n",
    "                rev_summ_row = {}\n",
    "                for rev_sent in extract_sents:\n",
    "                    fop_row = {} # fop train row\n",
    "                    fop_row['overall'] = overall\n",
    "                    sentiment = TextBlob(rev_sent).sentences[0].sentiment\n",
    "                    \n",
    "#                     rev_sent = rev_sent.replace(\"s.\",\"\")\n",
    "#                     rev_sent = re.sub(r'(?:^| )\\w(?:$| )', \" \", rev_sent).strip() # remove single alphbet\n",
    "#                     rev_sent = rev_sent.replace(\" ve \",\" \").replace(\" ha \",\" \")                  \n",
    "                    lemm_sent,pos_sent = lemm_sent_process2(rev_sent, remove_stopwords=False, summary=False, mode=\"spacy\", withdot=False)#                     _,pos_sent = get_pos_sequence(rev_sent,remove_stopwords=False,summary=False,mode = \"spacy\",withdot =True)\n",
    "#                     if len(pos_sent) == 0: continue\n",
    "                    token_set = set(nltk.word_tokenize(rev_sent))\n",
    "                    mention_features = list(token_set & total_keywords)                   \n",
    "                    POS_fops = FO_rule_POS(lemm_sent).run()\n",
    "                    DEP_fops = FOP_rule_Depend(lemm_sent).run()\n",
    "                    POS_fops = [(f,o) for f,o in POS_fops if f in total_keywords]\n",
    "                    DEP_fops = [(f,o) for f,o in DEP_fops if f in total_keywords]\n",
    "#                     PMI_fops,cand_pairs_score = pmi_fopair(lemm_sent) # 皆為空有問題                     \n",
    "\n",
    "                    result.write(\"************************************************\\n\")                     \n",
    "                    result.write('rev_sent:\\n' + rev_sent + \"\\n\") ;fop_row['rev_sent'] = rev_sent\n",
    "                    result.write('lemm_sent:\\n' + lemm_sent) ;fop_row['lemm_sent'] = lemm_sent\n",
    "                    \n",
    "#                     result.write(str(pos_sent)+\"\\n\");pattern_text.write(str(pos_sent)+\"\\n\") ; \n",
    "                    result.write('mention features:' + str(mention_features) + \"\\n\")\n",
    "                    result.write(\"polarity: %s subjectivity: %s \\n\"%(sentiment.polarity,sentiment.subjectivity))\n",
    "                    fop_row['polarity'] = sentiment.polarity;fop_row['subjectivity'] = sentiment.subjectivity\n",
    "                    fop_row['mention_features'] = \",\".join(mention_features)\n",
    "                \n",
    "                    result.write(\"POS_fops:\"+str(POS_fops)+\"\\n\") ;pattern_text.write(\"POS_fops:\"+str(POS_fops)+\"\\n\")\n",
    "                    result.write(\"DEP_fops:\"+str(DEP_fops)+\"\\n\") ;pattern_text.write(\"DEP_fops:\"+str(DEP_fops)+\"\\n\")\n",
    "#                     f.write(\"PMI_fops:\"+str(cand_pairs_score)+\"\\n\")\n",
    "#                     print(cand_pairs_score)\n",
    "                    result.write(\"************************************************\\n\") \n",
    "                    if \"not\" in token_set: continue              \n",
    "                    if overall > 3 and sentiment.polarity < 0: continue\n",
    "                    if overall < 3 and sentiment.polarity > 0: continue\n",
    "                    if (len(POS_fops) == 0) and (len(DEP_fops) == 0): continue\n",
    "                        \n",
    "#                     fop_list = set(PMI_fops + POS_fops + DEP_fops)\n",
    "                    fop_list = set(POS_fops + DEP_fops)\n",
    "                    fop_prob_list = []\n",
    "                    \n",
    "                    for fop in fop_list:\n",
    "                        p1,p2,p3 = 0,0,0; y1,y2,y3 = 0.6,0.2,0.2;\n",
    "                        feat,opinion = fop\n",
    "#                         if fop in PMI_fops: \n",
    "#                             p1 = pmi(feat,opinion)\n",
    "                        if fop in POS_fops: p2 = 1\n",
    "                        if fop in DEP_fops: p3 = 1\n",
    "#                         p = y1*p1 + y2*p2 + y3*p3\n",
    "                        p = y2*p2 + y3*p3\n",
    "                        fop_prob_list.append((fop,p))\n",
    "                    result.write(\"fop_prob_list:\" + str(fop_prob_list)+ \"\\n\")   \n",
    "                    i +=1 \n",
    "                    result.write(\"\\n\");pattern_text.write(\"\\n\")\n",
    "                    # ------------------------------------------------------\n",
    "#                     training_data.write(\"%s\"%(rev_sent) + \"|\") \n",
    "#                     training_data.write(\"%s|%s|\"%(sentiment.polarity,sentiment.subjectivity))\n",
    "#                     if len(token_set & total_keywords) == 0: continue                    \n",
    "                    \n",
    "                    fop_str = \"\"\n",
    "                    for idx,fop_score in enumerate(fop_prob_list):\n",
    "                        fop,score = fop_score\n",
    "                        feat,o = fop                        \n",
    "                        if idx == len(fop_prob_list) - 1:\n",
    "                            fop_str += \"%s %s\"%(feat,o)\n",
    "                        else:     \n",
    "                            fop_str += \"%s %s\\t|\\t\"%(feat,o)\n",
    "                    fop_row[\"fops\"] = fop_str \n",
    "                    keyword_list.append(fop_str)\n",
    "                    fop_row['review_ID'] = review_ID\n",
    "                    fop_row['summary'] = summary\n",
    "                    \n",
    "                    if abs(sentiment.polarity) <= 0.5: continue\n",
    "                    if abs(sentiment.subjectivity) < 0.4: continue\n",
    "                    if len(mention_features) == 0: continue\n",
    "                    topic_train_df[i] = fop_row ;i +=1       \n",
    "                \n",
    "                    \n",
    "            total_keyword = \" \".join(keyword_list)\n",
    "            rev_summ_row['review_ID'] = review_ID\n",
    "            rev_summ_row['review'] = review\n",
    "            rev_summ_row['summary'] = summary\n",
    "            rev_summ_row['overall'] = overall\n",
    "            rev_summ_row['total_keyword'] = total_keyword\n",
    "            key_train_df[j] = rev_summ_row ; j += 1                       \n",
    "#                 f.write(\"---------------------------------------------------\\n\")\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(\"Processing %s\" % asin)\n",
    "#             break  \n",
    "            \n",
    "print(\"finished...\")\n",
    "# training_data.close()\n",
    "topic_train_df = pd.DataFrame.from_dict(topic_train_df, orient='index')\n",
    "key_train_df = pd.DataFrame.from_dict(key_train_df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Electronics_DVD Players_topic.xlsx Write finished\n",
      " Write finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>rev_sent</th>\n",
       "      <th>lemm_sent</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>mention_features</th>\n",
       "      <th>fops</th>\n",
       "      <th>review_ID</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>tried popping in the matrix and the fifth elem...</td>\n",
       "      <td>try pop the matrix and the element and reward ...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>matrix</td>\n",
       "      <td>matrix element\\t|\\tmatrix pop</td>\n",
       "      <td>957225600</td>\n",
       "      <td>Feature-packed with a Brillian Picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>the sd produces just as brilliant sound to com...</td>\n",
       "      <td>the produce just as brilliant sound to complim...</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.70</td>\n",
       "      <td>video,sound</td>\n",
       "      <td>sound brilliant\\t|\\tvideo compliment</td>\n",
       "      <td>957225600</td>\n",
       "      <td>Feature-packed with a Brillian Picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>very nice feature is the hdcd decoder which al...</td>\n",
       "      <td>very nice feature the hdcd decoder which allow...</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.95</td>\n",
       "      <td>feature,decoder</td>\n",
       "      <td>sound superior\\t|\\tfeature very nice\\t|\\tfeatu...</td>\n",
       "      <td>957225600</td>\n",
       "      <td>Feature-packed with a Brillian Picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.0</td>\n",
       "      <td>the progressive scan makes an incredible diffe...</td>\n",
       "      <td>the progressive scan make incredible difference\\n</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>scan,difference</td>\n",
       "      <td>difference incredible</td>\n",
       "      <td>957225600</td>\n",
       "      <td>Feature-packed with a Brillian Picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.0</td>\n",
       "      <td>all can say is that was blown away the picture...</td>\n",
       "      <td>all can say that blow away the picture quality...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>scan,quality,dvd,picture</td>\n",
       "      <td>dvd progressive</td>\n",
       "      <td>956966400</td>\n",
       "      <td>Stunning Picture Quality - Easy To Use</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    overall                                           rev_sent  \\\n",
       "5       5.0  tried popping in the matrix and the fifth elem...   \n",
       "7       5.0  the sd produces just as brilliant sound to com...   \n",
       "9       5.0  very nice feature is the hdcd decoder which al...   \n",
       "16      5.0  the progressive scan makes an incredible diffe...   \n",
       "20      5.0  all can say is that was blown away the picture...   \n",
       "\n",
       "                                            lemm_sent  polarity  subjectivity  \\\n",
       "5   try pop the matrix and the element and reward ...      0.90          0.90   \n",
       "7   the produce just as brilliant sound to complim...      0.65          0.70   \n",
       "9   very nice feature the hdcd decoder which allow...      0.74          0.95   \n",
       "16  the progressive scan make incredible difference\\n      0.90          0.90   \n",
       "20  all can say that blow away the picture quality...      0.90          0.90   \n",
       "\n",
       "            mention_features  \\\n",
       "5                     matrix   \n",
       "7                video,sound   \n",
       "9            feature,decoder   \n",
       "16           scan,difference   \n",
       "20  scan,quality,dvd,picture   \n",
       "\n",
       "                                                 fops  review_ID  \\\n",
       "5                       matrix element\\t|\\tmatrix pop  957225600   \n",
       "7                sound brilliant\\t|\\tvideo compliment  957225600   \n",
       "9   sound superior\\t|\\tfeature very nice\\t|\\tfeatu...  957225600   \n",
       "16                              difference incredible  957225600   \n",
       "20                                    dvd progressive  956966400   \n",
       "\n",
       "                                   summary  \n",
       "5   Feature-packed with a Brillian Picture  \n",
       "7   Feature-packed with a Brillian Picture  \n",
       "9   Feature-packed with a Brillian Picture  \n",
       "16  Feature-packed with a Brillian Picture  \n",
       "20  Stunning Picture Quality - Easy To Use  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"Train/%s_%s_topic.xlsx\"%(category1,category2)\n",
    "# df.to_csv(csv_path) #默认dt是DataFrame的一个实例，参数解释如下\n",
    "topic_train_df.to_excel(csv_path, encoding='utf8')\n",
    "print(csv_path + \" Write finished\")\n",
    "print(\" Write finished\")\n",
    "topic_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Electronics_DVD Players_key.xlsx Write finished\n",
      " Write finished\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"Train/%s_%s_key.xlsx\"%(category1,category2)\n",
    "# df.to_csv(csv_path) #默认dt是DataFrame的一个实例，参数解释如下\n",
    "key_train_df.to_excel(csv_path, encoding='utf8')\n",
    "print(csv_path + \" Write finished\")\n",
    "print(\" Write finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_ID</th>\n",
       "      <th>review</th>\n",
       "      <th>summary</th>\n",
       "      <th>overall</th>\n",
       "      <th>total_keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>956966400</td>\n",
       "      <td>I bought the Toshiba SD-5109 as a Christmas pr...</td>\n",
       "      <td>Stunning Picture Quality - Easy To Use</td>\n",
       "      <td>5.0</td>\n",
       "      <td>toshiba buy functionality progressive\\t|\\toutp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>959385600</td>\n",
       "      <td>This is an awesome DVD player, and I haven't e...</td>\n",
       "      <td>Ultimate DVD Player</td>\n",
       "      <td>5.0</td>\n",
       "      <td>channel tv\\t|\\tvolume channel\\t|\\tvolume tv\\t|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>954201600</td>\n",
       "      <td>Purchased this unit in September 99. It worked...</td>\n",
       "      <td>Great Value</td>\n",
       "      <td>5.0</td>\n",
       "      <td>unit purchase machine lock lock parental\\t|\\tm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>940982400</td>\n",
       "      <td>The Sony DVPS530D DVD Player rocks my world! J...</td>\n",
       "      <td>SonyDVPS530D Rocks My World! Buy Now!!!!!</td>\n",
       "      <td>5.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>964483200</td>\n",
       "      <td>I recieved the Palmtheatre free through a cont...</td>\n",
       "      <td>Anywhere you want</td>\n",
       "      <td>5.0</td>\n",
       "      <td>product nice\\t|\\tproduct convenient</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_ID                                             review  \\\n",
       "0  956966400  I bought the Toshiba SD-5109 as a Christmas pr...   \n",
       "1  959385600  This is an awesome DVD player, and I haven't e...   \n",
       "2  954201600  Purchased this unit in September 99. It worked...   \n",
       "3  940982400  The Sony DVPS530D DVD Player rocks my world! J...   \n",
       "4  964483200  I recieved the Palmtheatre free through a cont...   \n",
       "\n",
       "                                     summary  overall  \\\n",
       "0     Stunning Picture Quality - Easy To Use      5.0   \n",
       "1                        Ultimate DVD Player      5.0   \n",
       "2                                Great Value      5.0   \n",
       "3  SonyDVPS530D Rocks My World! Buy Now!!!!!      5.0   \n",
       "4                          Anywhere you want      5.0   \n",
       "\n",
       "                                       total_keyword  \n",
       "0  toshiba buy functionality progressive\\t|\\toutp...  \n",
       "1  channel tv\\t|\\tvolume channel\\t|\\tvolume tv\\t|...  \n",
       "2  unit purchase machine lock lock parental\\t|\\tm...  \n",
       "3                                                     \n",
       "4                product nice\\t|\\tproduct convenient  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
