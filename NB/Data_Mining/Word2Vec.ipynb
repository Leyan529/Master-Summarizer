{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter('ignore')\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "\n",
    "\n",
    "import spacy\n",
    "# gpu = spacy.prefer_gpu()\n",
    "# print('GPU:', gpu)\n",
    "# pip install -U spacy[cuda100]\n",
    "# python -m spacy validate\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "import re\n",
    "from stopwords import *\n",
    "import nltk\n",
    "from preprocess import *\n",
    "# from feature import *\n",
    "\n",
    "from textblob import TextBlob\n",
    "import collections\n",
    "\n",
    "from spacy.symbols import cop, acomp, amod, conj, neg, nn, nsubj, dobj,prep,advmod\n",
    "from spacy.symbols import VERB, NOUN, PROPN, ADJ, ADV, AUX, PART\n",
    "\n",
    "pattern_counter = collections.Counter()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "from MongoDB import MongoDB\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from collections import Counter,OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 斷詞辭典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stpwords_list3 = [f.replace(\"\\n\",\"\") for f in open(\"stopwords.txt\",\"r\",encoding = \"utf-8\").readlines()]\n",
    "stpwords_list3.remove(\"not\")\n",
    "stopwords = list(html_escape_table + stpwords_list2) + list(list(stops) + list(stpwords_list1) + list(stpwords_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Opinion\n",
    "# https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n",
    "<!-- Opinion Lexicon: A list of English positive and negative opinion words or sentiment words (around 6800 words). This list was compiled over many years starting from our first paper (Hu and Liu, KDD-2004). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative-words.txt\n",
      "positive-words.txt\n"
     ]
    }
   ],
   "source": [
    "opinion_lexicon = {}\n",
    "for filename in os.listdir('opinion-lexicon-English/'):      \n",
    "    if \"txt\" not in filename: continue\n",
    "    print(filename)\n",
    "    with open('opinion-lexicon-English/'+filename,'r') as f_input:\n",
    "        lexion = []\n",
    "        for line in f_input:\n",
    "            if line.startswith(\";\"):\n",
    "                continue\n",
    "            word = line.replace(\"\\n\",\"\")\n",
    "            if word != \"\" : lexion.append(word)\n",
    "        pos = filename.replace(\".txt\",\"\")\n",
    "        opinion_lexicon[pos] = lexion\n",
    "\n",
    "opinion_lexicon[\"total-words\"] = opinion_lexicon[\"negative-words\"] + opinion_lexicon[\"positive-words\"]\n",
    "# opinion_lexicon[\"total-words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pickle 提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_DVD Players.pickle\n"
     ]
    }
   ],
   "source": [
    "# reload a file to a variable\n",
    "from glob import glob\n",
    "with open(glob('new_*.pickle')[0], 'rb') as file:\n",
    "    print(file.name)\n",
    "    PROD_DICT =pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句子篩選/切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "from summa import keywords,summarizer\n",
    "\n",
    "def sentence_extract_blob(text):    \n",
    "    extract_sents = []\n",
    "    text = text.replace(\",\",\"\").replace(\"i.e.\",\"\")\n",
    "    text = SentProcess(text)\n",
    "    \n",
    "    for sent in text.split(\"<end>\"):        \n",
    "#         print(sent)\n",
    "        if sent == \"\": continue\n",
    "        sentiment = TextBlob(sent).sentences[0].sentiment\n",
    "#         if abs(sentiment.polarity) <= 0.5: continue\n",
    "#         if abs(sentiment.subjectivity) <= 0.5: continue\n",
    "#         print(TextBlob(sent).sentences)\n",
    "#         print(\"polarity : \",sentiment.polarity)\n",
    "#         print(\"subjectivity : \",sentiment.subjectivity)\n",
    "# #         print(keywords.keywords(sent))\n",
    "#         print(\"***\")\n",
    "        extract_sents.append(sent)\n",
    "    return extract_sents\n",
    "\n",
    "def extract_cand_pharse(text): \n",
    "    sent_pattern = r\"\"\"( \n",
    "    (<NOUN><NOUN> | <ADJ><NOUN> | <NOUN><NOUN><NOUN> | <NOUN><ADP><DET><NOUN> | \n",
    "    <NOUN> | <ADJ><NOUN><NOUN> | <ADJ><ADJ><NOUN> | <NOUN><PREP><NOUN> | <NOUN><PREP><DET><NOUN> | \n",
    "    <VERB><PUNCT><ADP><NOUN><NOUN> | <DET><NOUN> )\n",
    "\n",
    "\n",
    "    (<AUX><ADV><ADJ>|<AUX><ADJ>|<ADV><ADJ>|<ADJ><NOUN>) \n",
    "    )\n",
    "    \"\"\"\n",
    "#     pattern = r'<PROPN>+ (<PUNCT|CCONJ> <PUNCT|CCONJ>? <PROPN>+)*'\n",
    "    extract_pharse = []\n",
    "    doc = textacy.make_spacy_doc(text,lang='en_core_web_sm')\n",
    "    phrases = textacy.extract.pos_regex_matches(doc, sent_pattern)\n",
    "    # Print all Verb Phrase\n",
    "\n",
    "    for phrase in phrases:\n",
    "#         print(\"verb_phrases : \" , phrase.text)\n",
    "    #     print([(token.text,token.pos_) for token in nlp(chunk.text)])\n",
    "        extract_pharse.append(phrase.text+\"\\n\")           \n",
    "    return extract_pharse        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 規格關鍵字提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeywordFeat(description,feature):  # 搜索關鍵保留字，並重新清理句子       \n",
    "    description_list = []\n",
    "    for sent in description:\n",
    "        if len(TextBlob(sent)) > 1:\n",
    "            sent_list = sent.split(\".\")\n",
    "            description_list.extend(sent_list) \n",
    "        else:\n",
    "            description_list.extend(sent) \n",
    "    description_list = [sent.strip() +\".\" for sent in description_list]\n",
    "    if type(feature) == str:\n",
    "        total_desc_sents = description_list + [feature]\n",
    "    else:\n",
    "        total_desc_sents = description_list + feature    \n",
    " \n",
    "    # Stage 1 取dash feature\n",
    "    keywords = []\n",
    "    newdescription = \"\"\n",
    "    for sent in total_desc_sents:\n",
    "        for token in nlp(sent):\n",
    "            sent = sent.lower()\n",
    "            for k, v in contractions.items():\n",
    "                if k in sent:\n",
    "                    sent = sent.replace(k, v)\n",
    "\n",
    "            for k in html_escape_table:\n",
    "                if k in sent:\n",
    "                    sent = sent.replace(k, \"\")\n",
    "            \n",
    "            \n",
    "            pattern = re.compile(r\"([\\d\\w\\.-]+[-'//.][\\d\\w\\.-]+)\")  # |  ([\\(](\\w)+[\\)])\n",
    "            keys = pattern.findall(sent)\n",
    "            \n",
    "            \n",
    "        keywords.extend(keys)\n",
    "        cleansent = sent\n",
    "        for k in keys:    \n",
    "            k = k.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "            cleansent = cleansent.replace(k,\"(\" + k + \")\")\n",
    "            cleansent = cleansent.replace(\"((\",\"(\").replace(\"))\",\")\")     \n",
    "            cleansent = cleansent.replace(\"(\" + k + \")\",\"\") # 移除符號特徵\n",
    "            cleansent = cleansent.strip()\n",
    "        \n",
    "#         cleansent = cleansent + \".\\n\"\n",
    "        cleansent = remove_word3(cleansent)\n",
    "        char = \" \"\n",
    "        while char * 2 in cleansent:\n",
    "            cleansent = cleansent.replace(char * 2, char)  \n",
    "        char = \".\"\n",
    "        while char * 2 in cleansent:\n",
    "            cleansent = cleansent.replace(char * 2, char)            \n",
    "         \n",
    "        if len(cleansent) < 5 : continue\n",
    "        newdescription = newdescription + cleansent + \".\\n\"\n",
    "        \n",
    "    newdescription = newdescription.replace(\"..\",\".\")   \n",
    "    newdescription = re.sub(r\"\\([\\w]+\\)\",\"\",newdescription)\n",
    "    newdescription = re.sub(r\"\\(\\)\",\"\",newdescription)   \n",
    "    newdescription = re.sub(r'\\\"',\"\",newdescription)\n",
    "    \n",
    "    # get Noun pharse keyword for newdescription\n",
    "#     for pharse in nlp(newdescription).noun_chunks:\n",
    "#         print(pharse.text)\n",
    "    cand_pf = PF_rule_POS(newdescription).run()\n",
    "#     print(cand_pf)\n",
    "    \n",
    "    keywords2 = set()\n",
    "    for pf in cand_pf:\n",
    "        chunk_pfs = nltk.word_tokenize(pf)\n",
    "        for tok in chunk_pfs:\n",
    "            if tok in stopwords: continue\n",
    "            if tok in opinion_lexicon[\"total-words\"]: continue\n",
    "            keywords2.add(tok)\n",
    "\n",
    "#     print(len(keywords2),keywords2)\n",
    "    return keywords,keywords2, newdescription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評論濾除符號-英文特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanReview(review):  # 搜索關鍵保留字，並重新清理句子     \n",
    "    # Stage 1 取dash feature\n",
    "    keywords = []\n",
    "    newReview = \"\"\n",
    "    for sent in nltk.sent_tokenize(review):\n",
    "        for token in nlp(sent):\n",
    "            sent = sent.lower()\n",
    "            for k, v in contractions.items():\n",
    "                if k in sent:\n",
    "                    sent = sent.replace(k, v)\n",
    "\n",
    "            for k in html_escape_table:\n",
    "                if k in sent:\n",
    "                    sent = sent.replace(k, \"\")\n",
    "            \n",
    "            \n",
    "            pattern = re.compile(r\"([\\d\\w\\.-]+[-'//.][\\d\\w\\.-]+)\")  # |  ([\\(](\\w)+[\\)])\n",
    "            keys = pattern.findall(sent)\n",
    "            \n",
    "            \n",
    "        keywords.extend(keys)\n",
    "        cleansent = sent\n",
    "        for k in keys:    \n",
    "            k = k.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "            cleansent = cleansent.replace(k,\"(\" + k + \")\")\n",
    "            cleansent = cleansent.replace(\"((\",\"(\").replace(\"))\",\")\")     \n",
    "            cleansent = cleansent.replace(\"(\" + k + \")\",\"\") # 移除符號特徵\n",
    "            cleansent = cleansent.strip()\n",
    "        \n",
    "#         cleansent = cleansent + \".\\n\"\n",
    "        cleansent = remove_word3(cleansent)\n",
    "        char = \" \"\n",
    "        while char * 2 in cleansent:\n",
    "            cleansent = cleansent.replace(char * 2, char)  \n",
    "        char = \".\"\n",
    "        while char * 2 in cleansent:\n",
    "            cleansent = cleansent.replace(char * 2, char)            \n",
    "         \n",
    "        if len(cleansent) < 5 : continue\n",
    "        newReview = newReview + cleansent + \".\\n\"\n",
    "        \n",
    "    newReview = newReview.replace(\"..\",\".\")   \n",
    "    newReview = re.sub(r\"\\([\\w]+\\)\",\"\",newReview)\n",
    "    newReview = re.sub(r\"\\(\\)\",\"\",newReview)   \n",
    "    newReview = re.sub(r'\\\"',\"\",newReview)\n",
    "    \n",
    "    # get Noun pharse keyword for newdescription\n",
    "#     cand_pf = PF_rule_POS(newReview).run()\n",
    "    \n",
    "#     keywords2 = set()\n",
    "#     for pf in cand_pf:\n",
    "#         chunk_pfs = nltk.word_tokenize(pf)\n",
    "#         for tok in chunk_pfs:\n",
    "#             if tok in stopwords: continue\n",
    "#             if tok in opinion_lexicon[\"total-words\"]: continue\n",
    "#             keywords2.add(tok)\n",
    "\n",
    "#     print(len(keywords2),keywords2)\n",
    "    return keywords, newReview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PF-Extraction-Rule(POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -NN, \n",
    "# -NN NN, \n",
    "# JJ NN \n",
    "# -NN NN NN, \n",
    "# JJ NN NN, \n",
    "# JJ JJ NN, \n",
    "# NN IN NN \n",
    "# -NN IN DT NN\n",
    "\n",
    "class PF_rule_POS():\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "        self.matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "    def collect_sents(self, matcher, doc, i, matches):\n",
    "        match_id, start, end = matches[i]\n",
    "        span = doc[start:end]  # Matched span\n",
    "        sent = span.sent  # Sentence containing matched span\n",
    "        # Append mock entity for match in displaCy style to matched_sents\n",
    "        # get the match span by ofsetting the start and end of the span with the\n",
    "        # start and end of the sentence in the doc\n",
    "        match_ents = [{\n",
    "            \"start\": span.start_char - sent.start_char,\n",
    "            \"end\": span.end_char - sent.start_char,\n",
    "            \"label\": \"MATCH\",\n",
    "        }]\n",
    "        self.matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "    def match_pattern(self, sent, flit_keyword=None):\n",
    "        res = []\n",
    "        #         ('Dolby', 'PROPN'), ('Digital', 'PROPN')\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        #         matcher.add(\"pf1\", self.collect_sents, [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True},\n",
    "        #                                                    {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf2\", self.collect_sents,\n",
    "        #                     [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                      {'POS': 'SYM'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         #         matcher.add(\"specn3\", self.collect_sents, [{'POS': 'ADJ'}, {'POS': 'NOUN'}])  # add pattern\n",
    "        #         matcher.add(\"pf3\", self.collect_sents,\n",
    "        #                     [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf4\", self.collect_sents,\n",
    "        #                     [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                      {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf5\", self.collect_sents, [{'POS': 'ADJ'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                                                    {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf6\", self.collect_sents,\n",
    "        #                     [{'POS': 'ADJ'}, {'POS': 'ADJ'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         #         ('inches', 'NOUN'), ('(', 'PUNCT'), ('W', 'NOUN'), ('x', 'SYM'), ('H', 'NOUN'), ('x', 'SYM'), ('D', 'NOUN'), (')', 'PUNCT')\n",
    "        #         matcher.add(\"pf6.1\", self.collect_sents,\n",
    "        #                     [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                      {'POS': 'SYM'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'POS': 'SYM'},\n",
    "        #                      {'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True}])  # inches (W x H x D)\n",
    "        #         matcher.add(\"pf6.2\", self.collect_sents,\n",
    "        #                     [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                      {'POS': 'SYM'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'IS_PUNCT': True}])  # inches (W x H)\n",
    "        #         matcher.add(\"pf7\", self.collect_sents, [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'POS': 'PREP'},\n",
    "        #                                                    {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf8\", self.collect_sents, [{'POS': {\"IN\": ['NOUN', 'PROPN']}}, {'POS': 'PREP'}, {'POS': 'DET'},\n",
    "        #                                                    {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "        #         matcher.add(\"pf9\", self.collect_sents,\n",
    "        #                     [{'POS': 'VERB'}, {'POS': 'PUNCT'}, {'POS': 'ADP'}, {'POS': {\"IN\": ['NOUN', 'PROPN']}},\n",
    "        #                      {'POS': {\"IN\": ['NOUN', 'PROPN']}}])  # add pattern\n",
    "\n",
    "        matcher.add(\"pf1\", self.collect_sents, [{'TAG': {\"IN\": ['NN']}}])  # add pattern\n",
    "        matcher.add(\"pf2\", self.collect_sents, [{'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['NN']}}])  # add pattern\n",
    "        matcher.add(\"pf3\", self.collect_sents, [{'TAG': {\"IN\": ['JJ']}}, {'TAG': {\"IN\": ['NN']}}])  # add pattern    \n",
    "        matcher.add(\"pf4\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['NN']}}])  # add pattern\n",
    "        matcher.add(\"pf5\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['JJ']}}, {'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['NN']}}])\n",
    "        matcher.add(\"pf6\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['JJ']}}, {'TAG': {\"IN\": ['JJ']}}, {'TAG': {\"IN\": ['NN']}}])\n",
    "        matcher.add(\"pf7\", self.collect_sents,\n",
    "                    [{'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['IN']}}, {'TAG': {\"IN\": ['NN']}}])\n",
    "        matcher.add(\"pf8\", self.collect_sents, [{'TAG': {\"IN\": ['NN']}}, {'TAG': {\"IN\": ['IN']}},\n",
    "                                                {'TAG': {\"IN\": ['DT']}}, {'TAG': {\"IN\": ['NN']}}])\n",
    "\n",
    "        doc = nlp(sent)\n",
    "        matches = matcher(doc)\n",
    "        # Serve visualization of sentences containing match with displaCy\n",
    "        # set manual=True to make displaCy render straight from a dictionary\n",
    "        # (if you're not running the code within a Jupyer environment, you can\n",
    "        # use displacy.serve instead)\n",
    "        #         displacy.render(self.matched_sents, style=\"ent\", manual=True)\n",
    "        for match_id, start, end in matches:\n",
    "            # Get the string representation\n",
    "            string_id = nlp.vocab.strings[match_id]\n",
    "            span = doc[start:end]  # The matched span\n",
    "            #             print(match_id, string_id, start, end, span.text)\n",
    "            if flit_keyword:\n",
    "                found = False\n",
    "                spanStr = span.text\n",
    "                for word in spanStr.split(\" \"):\n",
    "                    if word in flit_keyword: found = True; break\n",
    "                if found: res.append(span.text)\n",
    "            else:\n",
    "                res.append(span.text)\n",
    "        return res\n",
    "\n",
    "    def run(self):\n",
    "        total_res = []\n",
    "        for sent in nltk.sent_tokenize(self.article):\n",
    "            sent = sent.lower()\n",
    "            sent = sent.replace(\",\", \" \").replace(\":\", \" \").replace(\"(\", \" \").replace(\")\", \" \").replace(\"multi-\",\n",
    "                                                                                                        \"multiple \")\n",
    "            ress = self.match_pattern(sent)\n",
    "            total_res.extend(ress)\n",
    "\n",
    "        total_res = list(set(total_res))\n",
    "        #         print(\"final res\",total_res)\n",
    "        return total_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# 引入 word2vec\n",
    "from gensim.models import word2vec\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsnooper\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "# for idx,path in enumerate(raw_path):\n",
    "#     text = open(path,\"r\",encoding=\"utf-8\").readline()\n",
    "#     sentences.append(str(text).split()) # 切分词汇\n",
    "#     if (idx % 10000 ==0 )and (idx > 0): print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485 Products\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing B0145QN4F2: 100%|██████████| 485/485 [07:49<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "Products = len(PROD_DICT.items())\n",
    "print(\"%s Products\\n\"%(Products)) \n",
    "\n",
    "with tqdm(total=Products) as pbar:\n",
    "    for asin,DATA_DICT in PROD_DICT.items():\n",
    "        REVIEW_ITEM_LIST = DATA_DICT['REVIEW_ITEM_LIST']\n",
    "        title = DATA_DICT['title']\n",
    "        description = DATA_DICT['description']\n",
    "        feature = DATA_DICT['feature']      \n",
    "        dash_keywords,noun_keywords, newdescription = getKeywordFeat(description,feature)\n",
    "        for sent in nltk.sent_tokenize(newdescription):\n",
    "            sent = sent.replace(\".\" ,\"\")\n",
    "            sentences.append(str(sent).split()) # 切分词汇\n",
    " \n",
    "        for pair in REVIEW_ITEM_LIST:\n",
    "            review = pair[\"review\"]           \n",
    "            overall = pair[\"vote\"] \n",
    "            rev_dash_keywords, newreview = cleanReview(review)\n",
    "            for sent in nltk.sent_tokenize(newreview):\n",
    "                sent = sent.replace(\".\" ,\"\")\n",
    "                sentences.append(str(sent).split()) # 切分词汇            \n",
    "#             extract_sents = sentence_extract_blob(review)\n",
    "#             for rev_sent in extract_sents:\n",
    "#                 rev_sent = rev_sent.replace(\"s.\",\"\")\n",
    "#                 rev_sent = re.sub(r'(?:^| )\\w(?:$| )', \" \", rev_sent).strip() # remove single alphbet\n",
    "#                 rev_sent = rev_sent.replace(\" ve \",\" \").replace(\" ha \",\" \")                  \n",
    "\n",
    "#                 rev_sent,pos_sent = lemm_sent_process2(rev_sent, remove_stopwords=True, summary=False, mode=\"spacy\", withdot=False)\n",
    "# #                     _,pos_sent = get_pos_sequence(rev_sent,remove_stopwords=False,summary=False,mode = \"spacy\",withdot =True)\n",
    "#                 if pos_sent == None: continue\n",
    "#                 if len(pos_sent) == 0 : continue \n",
    "#                 tok_sent = set(nltk.word_tokenize(rev_sent)) - set(stopwords)\n",
    "# #                     tok_sent = [token for token in nltk.word_tokenize(rev_sent) if token not in stopwords]\n",
    "#                 freq_words.update(tok_sent)    \n",
    "#                 _Sents.append(tok_sent)\n",
    "\n",
    "#         feature_counter.update(newkeywords)\n",
    "#         total_keywords = total_keywords + newkeywords\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\"Processing %s\" % asin)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cat = 'All Electronics'\n",
    "big_categories = 'Electronics'\n",
    "small_categories='DVD Players'\n",
    "vocab_count = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 00:31:38,723 : INFO : collecting all words and their counts\n",
      "2019-12-11 00:31:38,726 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-12-11 00:31:38,776 : INFO : PROGRESS: at sentence #10000, processed 156680 words, keeping 7359 word types\n",
      "2019-12-11 00:31:38,806 : INFO : PROGRESS: at sentence #20000, processed 314535 words, keeping 10630 word types\n",
      "2019-12-11 00:31:38,837 : INFO : PROGRESS: at sentence #30000, processed 474388 words, keeping 12950 word types\n",
      "2019-12-11 00:31:38,864 : INFO : PROGRESS: at sentence #40000, processed 622936 words, keeping 14444 word types\n",
      "2019-12-11 00:31:38,869 : INFO : collected 14583 word types from a corpus of 633779 raw words and 40788 sentences\n",
      "2019-12-11 00:31:38,877 : INFO : max_final_vocab=50000 and min_count=1 resulted in calc_min_count=1, effective_min_count=1\n",
      "2019-12-11 00:31:38,879 : INFO : Loading a fresh vocabulary\n",
      "2019-12-11 00:31:38,901 : INFO : effective_min_count=1 retains 14583 unique words (100% of original 14583, drops 0)\n",
      "2019-12-11 00:31:38,903 : INFO : effective_min_count=1 leaves 633779 word corpus (100% of original 633779, drops 0)\n",
      "2019-12-11 00:31:38,943 : INFO : deleting the raw counts dictionary of 14583 items\n",
      "2019-12-11 00:31:38,946 : INFO : sample=0.001 downsamples 56 most-common words\n",
      "2019-12-11 00:31:38,948 : INFO : downsampling leaves estimated 476991 word corpus (75.3% of prior 633779)\n",
      "2019-12-11 00:31:38,982 : INFO : estimated required memory for 14583 words and 300 dimensions: 42290700 bytes\n",
      "2019-12-11 00:31:38,984 : INFO : resetting layer weights\n",
      "2019-12-11 00:31:42,004 : INFO : training model with 3 workers on 14583 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-12-11 00:31:42,551 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-11 00:31:42,554 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-11 00:31:42,557 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 00:31:42,560 : INFO : EPOCH - 1 : training on 633779 raw words (476478 effective words) took 0.5s, 870699 effective words/s\n",
      "2019-12-11 00:31:43,175 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-11 00:31:43,181 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-11 00:31:43,187 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 00:31:43,189 : INFO : EPOCH - 2 : training on 633779 raw words (476692 effective words) took 0.6s, 772264 effective words/s\n",
      "2019-12-11 00:31:43,802 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-11 00:31:43,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-11 00:31:43,813 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 00:31:43,815 : INFO : EPOCH - 3 : training on 633779 raw words (476834 effective words) took 0.6s, 777436 effective words/s\n",
      "2019-12-11 00:31:44,431 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-11 00:31:44,435 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-11 00:31:44,442 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 00:31:44,444 : INFO : EPOCH - 4 : training on 633779 raw words (476929 effective words) took 0.6s, 776196 effective words/s\n",
      "2019-12-11 00:31:44,996 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-11 00:31:45,007 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-11 00:31:45,015 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 00:31:45,017 : INFO : EPOCH - 5 : training on 633779 raw words (477039 effective words) took 0.6s, 844613 effective words/s\n",
      "2019-12-11 00:31:45,579 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-11 00:31:45,590 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-11 00:31:45,598 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 00:31:45,601 : INFO : EPOCH - 6 : training on 633779 raw words (477198 effective words) took 0.6s, 832641 effective words/s\n",
      "2019-12-11 00:31:46,158 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-11 00:31:46,164 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-11 00:31:46,167 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 00:31:46,169 : INFO : EPOCH - 7 : training on 633779 raw words (477156 effective words) took 0.6s, 853478 effective words/s\n",
      "2019-12-11 00:31:46,810 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-11 00:31:46,820 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-11 00:31:46,827 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 00:31:46,829 : INFO : EPOCH - 8 : training on 633779 raw words (476566 effective words) took 0.6s, 735565 effective words/s\n",
      "2019-12-11 00:31:47,416 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-11 00:31:47,423 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-11 00:31:47,426 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 00:31:47,428 : INFO : EPOCH - 9 : training on 633779 raw words (476521 effective words) took 0.6s, 814499 effective words/s\n",
      "2019-12-11 00:31:48,003 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-12-11 00:31:48,015 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-12-11 00:31:48,022 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 00:31:48,024 : INFO : EPOCH - 10 : training on 633779 raw words (477066 effective words) took 0.6s, 816175 effective words/s\n",
      "2019-12-11 00:31:48,026 : INFO : training on a 6337790 raw words (4768479 effective words) took 6.0s, 792057 effective words/s\n"
     ]
    }
   ],
   "source": [
    "w2vec = word2vec.Word2Vec(sentences, size=300, min_count=1,max_vocab_size=None,iter=10,\n",
    "                          sorted_vocab=1,max_final_vocab=vocab_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vocab file...\n",
      "Finished writing vocab file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 00:31:48,089 : INFO : storing 14583x300 projection weights into word2Vec/gensim_Electronics_DVD Players.300d.txt\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# write vocab to file\n",
    "vocab_file = 'word2Vec/gensim_%s_%s.vocab'%(big_categories,small_categories)\n",
    "if not os.path.isdir(\"word2Vec\"):\n",
    "    os.mkdir(\"word2Vec\")\n",
    "print(\"Writing vocab file...\")\n",
    "with open(vocab_file, 'w',encoding='utf-8') as writer:\n",
    "    for word in w2vec.wv.index2entity[:vocab_count]:\n",
    "        # print(word, w2vec.wv.vocab[word].count)\n",
    "        writer.write(word + ' ' + str(w2vec.wv.vocab[word].count) + '\\n') # Output vocab count\n",
    "print(\"Finished writing vocab file\")\n",
    "\n",
    "w2vec.wv.save_word2vec_format('word2Vec/gensim_%s_%s.300d.txt'%(big_categories,small_categories), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec.corpus_count\n",
    "w2vec.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-12-11 00:31:52,189 : INFO : precomputing L2-norms of word weight vectors\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('recorder', 0.7920495271682739),\n",
       " ('changer', 0.7253953218460083),\n",
       " ('unit', 0.7008682489395142),\n",
       " ('machine', 0.6885756254196167),\n",
       " ('model', 0.593877911567688),\n",
       " ('players', 0.582635760307312),\n",
       " ('modification', 0.5821385979652405),\n",
       " ('deck', 0.5558204650878906),\n",
       " ('device', 0.5466880202293396),\n",
       " ('product', 0.5403242111206055)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec.most_similar(u\"player\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('recorder', 0.7480524778366089),\n",
       " ('deck', 0.6619045734405518),\n",
       " ('unit', 0.6347320079803467),\n",
       " ('device', 0.6124114990234375),\n",
       " ('model', 0.6094457507133484),\n",
       " ('rebate', 0.608971893787384),\n",
       " ('sd', 0.6077044606208801),\n",
       " ('exc', 0.5832141041755676),\n",
       " ('cdrmpjeg', 0.582229495048523),\n",
       " ('writes', 0.5762830376625061),\n",
       " ('dvp', 0.5707143545150757),\n",
       " ('carousel', 0.5653432607650757),\n",
       " ('tablet', 0.5644240379333496),\n",
       " ('format', 0.5600840449333191),\n",
       " ('modification', 0.5533640384674072),\n",
       " ('vcr', 0.5526679158210754),\n",
       " ('hunt', 0.5517515540122986),\n",
       " ('kenwood', 0.5361608266830444),\n",
       " ('noiseless', 0.5309823751449585),\n",
       " ('otherhand', 0.5243709683418274)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec.most_similar(['dvd','player','changer','machine','video'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "# vocab = list(w2vec.vocabulary)\n",
    "\n",
    "vocab = {word: w2vec.wv.vocab[word].count for word in w2vec.wv.index2entity}\n",
    "# vocab\n",
    "# X = list(vocab.values())\n",
    "\n",
    "# tsne = TSNE(n_components=2)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "# df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-ffa467da9265>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
