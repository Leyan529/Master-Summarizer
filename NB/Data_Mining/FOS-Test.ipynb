{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "斷詞辭典 已取得\n",
      "negative-words.txt\n",
      "positive-words.txt\n",
      "total-words 已取得\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter('ignore')\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')\n",
    "\n",
    "\n",
    "import spacy\n",
    "# gpu = spacy.prefer_gpu()\n",
    "# print('GPU:', gpu)\n",
    "# pip install -U spacy[cuda100]\n",
    "# python -m spacy validate\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "import re\n",
    "from stopwords import *\n",
    "import nltk\n",
    "from preprocess import *\n",
    "# from feature import *\n",
    "\n",
    "from textblob import TextBlob\n",
    "import collections\n",
    "\n",
    "# from spacy.symbols import cop, acomp, amod, conj, neg, nn, nsubj, dobj,prep,advmod\n",
    "# from spacy.symbols import VERB, NOUN, PROPN, ADJ, ADV, AUX, PART\n",
    "\n",
    "pattern_counter = collections.Counter()\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# import networkx as nx\n",
    "# from MongoDB import MongoDB\n",
    "import os\n",
    "import pickle\n",
    "from eda import *\n",
    "from glob import glob\n",
    "from product import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 斷詞辭典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2562"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['negative-words', 'positive-words', 'total-words'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_lexicon.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pickle 提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/new_Cell Phones & Accessories_Cell Phones.pickle']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _,category1,category2,_ = DVD_Player().getAttr()\n",
    "# _,category1,category2,_ = Cameras().getAttr()\n",
    "_,category1,category2,_ = Cell_Phones().getAttr()\n",
    "# _,category1,category2,_ = GPS().getAttr()\n",
    "# _,category1,category2,_ = Keyboards().getAttr()\n",
    "\n",
    "glob('data/new_%s_%s.pickle'%(category1,category2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/new_Cell Phones & Accessories_Cell Phones.pickle\n"
     ]
    }
   ],
   "source": [
    "# reload a file to a variable   \n",
    "with open(glob('data/new_%s_%s.pickle'%(category1,category2))[0], 'rb') as file:\n",
    "    print(file.name)\n",
    "    PROD_DICT = pickle.load(file)\n",
    "    category1,category2 = file.name.replace(\".pickle\",\"\").split(\"_\")[1:]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取所有規格關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/107 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 Products\n",
      "\n",
      "Start get Product keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Product : B01GW45SB8 , Processing ID 1485043200: 100%|██████████| 107/107 [06:31<00:00,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 166\n",
      "Count : 787\n",
      "Count : 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from collections import Counter,OrderedDict\n",
    "\n",
    "feature_counter1 = Counter()\n",
    "feature_counter2 = Counter()\n",
    "feature_counter3 = Counter()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import collections\n",
    "freq_words = collections.Counter()\n",
    "feat_words = collections.Counter()\n",
    "opinion_words = collections.Counter()\n",
    "cooccurs_words = collections.Counter()\n",
    "\n",
    "_Sents = []\n",
    "keywords_list = []\n",
    "# 其中 freq_words 是單字出現的頻率 , _Sents 是文章中所有的句子\n",
    "\n",
    "i = 0\n",
    "\n",
    "# total_keywords = []\n",
    "Products = len(PROD_DICT.items())\n",
    "print(\"%s Products\\n\"%(Products))    \n",
    "\n",
    "if not os.path.exists('View'):\n",
    "    os.makedirs('View')\n",
    "  \n",
    "Reviews = 0\n",
    "fn = 'View/%s_%s_keywords.txt'%(category1,category2)\n",
    "if not os.path.exists(fn):\n",
    "    print(\"Start get Product keywords...\")\n",
    "    with open('View/Prod_keyword.txt',\"w\",encoding=\"utf-8\") as f:\n",
    "        # with open(\"Prod_keyword.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "        with tqdm(total=Products) as pbar:\n",
    "            for asin,DATA_DICT in PROD_DICT.items():\n",
    "                REVIEW_ITEM_LIST = DATA_DICT['REVIEW_ITEM_LIST']\n",
    "                title = DATA_DICT['title']\n",
    "                description = DATA_DICT['description']\n",
    "                feature = DATA_DICT['feature']      \n",
    "                dash_keywords,noun_keywords,newkeywords, newdescription = getKeywordFeat(description,feature)\n",
    "#                 print(noun_keywords)\n",
    "                f.write(\"* asin:\" + asin+\"\\n\")\n",
    "                if \"(new Date()).getTime();\" in title : f.write(\"title:\" + asin+\"\\n\")\n",
    "                else: f.write(\"title:\" + title+\"\\n\")\n",
    "                f.write('dash_keywords:\\n' + str(dash_keywords)+\"\\n\");f.write('noun_keywords2:\\n' + str(noun_keywords)+\"\\n\")\n",
    "\n",
    "                f.write('* keywords:\\n' + str(newkeywords)+\"\\n\")\n",
    "#                 f.write('* newdescription:\\n' + str(newdescription)+\"\\n\")\n",
    "#                 f.write('* pos_newdescription:\\n' + str(get_pos_sequence(newdescription)[1])+\"\\n\")\n",
    "                f.write('************************************************************************'+\"\\n\")\n",
    "                f.write('************************************************************************'+\"\\n\")      \n",
    "                for pair in REVIEW_ITEM_LIST:\n",
    "                    review_ID = pair[\"review_ID\"] \n",
    "                    review = pair[\"review\"]  \n",
    "                    lemm_review = pair[\"lemm_review\"] \n",
    "                    vote = pair[\"vote\"] \n",
    "                    summary = pair[\"summary\"] \n",
    "                    pbar.set_description(\"Product : %s , Processing ID %s\" % (asin,review_ID))\n",
    "\n",
    "                    dash_keywords2,noun_keywords2,newkeywords2, _ = getKeywordFeat_2(lemm_review)                    \n",
    "                    dash_keywords3,noun_keywords3,newkeywords3, _ = getKeywordFeat_2(summary)\n",
    "                    feature_counter2.update(newkeywords2)\n",
    "                    feature_counter3.update(newkeywords3)\n",
    "                    Reviews += 1 \n",
    "                    \n",
    "                \n",
    "                feature_counter1.update(newkeywords)\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                \n",
    "    important_features = OrderedDict(sorted(feature_counter1.items(), key=lambda pair: pair[1], reverse=True))\n",
    "    important_features = [(word,important_features[word]) for word in important_features if important_features[word]>5]\n",
    "    print(\"Count : %s\"%(len(important_features)))\n",
    "    \n",
    "    important_features2 = OrderedDict(sorted(feature_counter2.items(), key=lambda pair: pair[1], reverse=True))\n",
    "    important_features2 = [(word,important_features2[word]) for word in important_features2 if important_features2[word]>5]\n",
    "    print(\"Count : %s\"%(len(important_features2)))\n",
    "    \n",
    "    important_features3 = OrderedDict(sorted(feature_counter3.items(), key=lambda pair: pair[1], reverse=True))\n",
    "    important_features3 = [(word,important_features3[word]) for word in important_features3 if important_features3[word]>5]\n",
    "    print(\"Count : %s\"%(len(important_features3)))\n",
    "\n",
    "    fn = 'View/%s_%s_keywords.txt'%(category1,category2)\n",
    "    with open(fn,\"w\",encoding=\"utf-8\") as f :\n",
    "        total_keywords = set()\n",
    "        for word , v in important_features:        \n",
    "            f.write(\"%s:%s \\n\"%(word,v))  \n",
    "            if v > 20 : total_keywords.add(word)  \n",
    "            \n",
    "    fn2 = 'View/%s_%s_keywords2.txt'%(category1,category2)    \n",
    "    with open(fn2,'w',encoding=\"utf-8\") as f:\n",
    "        total_keywords2 = set()\n",
    "        for word , v in important_features2:        \n",
    "            f.write(\"%s:%s \\n\"%(word,v))   \n",
    "            total_keywords2.add(word)  \n",
    "            \n",
    "    fn3 = 'View/%s_%s_keywords3.txt'%(category1,category2)\n",
    "    with open(fn3,'w',encoding=\"utf-8\") as f:\n",
    "        total_keywords3 = set()\n",
    "        for word , v in important_features3:        \n",
    "            f.write(\"%s:%s \\n\"%(word,v))   \n",
    "            total_keywords3.add(word)                   \n",
    "    total_keywords = total_keywords2\n",
    "else:\n",
    "    fn = 'View/%s_%s_keywords2.txt'%(category1,category2)\n",
    "    print('load %s keywords...'%(fn))\n",
    "    total_keywords = set()\n",
    "    with open(fn,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            k,v = line.split(\":\")\n",
    "            total_keywords.add(k)\n",
    "            \n",
    "# pbar.close()\n",
    "    # total_keywords = set(total_keywords) # 檢查重要程度 篩選字典裡的單字\n",
    "    # open(\"total_keywords.txt\",'w',encoding=\"utf-8\").write(str(total_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 關鍵字濾除 (each newkeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# from sklearn import feature_extraction\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # def Importance(document, sorted_words):\n",
    "# #     scores = {}\n",
    "# #     sent = get_token_sent(document)\n",
    "# #     for word in sent:\n",
    "# #         try:\n",
    "# #             scores[word] = dict(sorted_words)[word]\n",
    "# #         except Exception as e:\n",
    "# #             pass\n",
    "# #     scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "# #     return scores[:20]\n",
    "\n",
    "# vectorizer = CountVectorizer()  # 该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "# transformer = TfidfTransformer()  # 该类会统计每个词语的tf-idf权值\n",
    "# tfidf = transformer.fit_transform(\n",
    "#     vectorizer.fit_transform(keywords_list))  # 第一个fit_transform是计算tf-idf，第二个fit_transform是将文本转为词频矩阵\n",
    "# word = vectorizer.get_feature_names()  # 获取词袋模型中的所有词语\n",
    "# weight = tfidf.toarray()  # 将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重\n",
    "\n",
    "# print(\"Calculate TF-IDF finished\")\n",
    "\n",
    "# scores = {}\n",
    "# for i in range(len(weight)):\n",
    "#     # print (u\"-------这里输出第\",i,u\"类文本的词语tf-idf权重------\")\n",
    "#     for j in range(len(word)):\n",
    "#         # print(word[j],weight[i][j])\n",
    "#         scores[word[j]] = weight[i][j]\n",
    "\n",
    "\n",
    "# # fout = 'TF-IDF/%s_%s.txt'%(big_categories,small_categories)\n",
    "# if not os.path.isdir(\"TF-IDF\"):\n",
    "#     os.mkdir(\"TF-IDF\")\n",
    "    \n",
    "# with open('TF-IDF/%s_%s.txt'%(category1,category2),'w',encoding='utf-8') as f:   \n",
    "#     f.write(\"word\" + '\\t' + \"tf-idf-weight\" + '\\n')\n",
    "#     for k, v in scores.items():\n",
    "#         if float(v) <= 0: continue\n",
    "# #         print(k,v)\n",
    "#         f.write(str(k) + '\\t' + str(v) + '\\n')\n",
    "\n",
    "# # sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "# sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "# sorted_words = [word[0] for word in sorted_words]\n",
    "# print(\"\\nTop 20 Important words\", sorted_words[:20])\n",
    "\n",
    "# # total_keywords = set(sorted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然語言處理 -- Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用total_keywords計算pmi\n",
    "from math import log\n",
    "# need pmi feature words & opinions\n",
    "def p(x): # p(x) 計算單字 x 出現的機率\n",
    "    return freq_words[x]/float(len(freq_words))\n",
    "\n",
    "def pxy(x,y): # pxy(x,y) 計算單字 x 和單字 y 出現在同一個句子的機率\n",
    "#     print((lambda s :  x in s and y in s ,_Sents))\n",
    "#     filter()函數用於過濾序列，過濾掉不符合條件的元素\n",
    "    return (len(list(filter(lambda s :  x in s and y in s ,_Sents)))+1)/ float(len(_Sents) )\n",
    "\n",
    "def pmi(x,y): # pmi(x,y) 計算單字 x 和單字 y 的 Pointwise Mutual Information\n",
    "    try:\n",
    "#         print(\"X :%s Y: %s\"%(x,y))\n",
    "#         print('pxy(x,y)',pxy(x,y))\n",
    "#         print('p(x)',p(x))\n",
    "#         print('p(y)',p(y))\n",
    "#         print('log(pxy(x,y)/(p(x)*p(y)),2)',log(pxy(x,y)/(p(x)*p(y)),2))\n",
    "        return  log(pxy(x,y)/(p(x)*p(y)),2) \n",
    "    except Exception as e :\n",
    "#         print(e)\n",
    "        return 0\n",
    "    \n",
    "def pmi_fopair(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    noun_,op_ = [],[]\n",
    "    cand_pairs = []\n",
    "    cand_pairs_score = []\n",
    "    for token in doc:\n",
    "        if token.pos in [ADJ,ADV,VERB]: op_.append(token.text)\n",
    "        if token.pos in [NOUN]: noun_.append(token.text)\n",
    "\n",
    "    for f in noun_:\n",
    "        for o in op_:\n",
    "            pair = (f,o)\n",
    "            score = pmi(f,o)\n",
    "#             if score > 0: cand_pairs.append((pair,score))\n",
    "            if score > 0: cand_pairs.append(pair)\n",
    "            cand_pairs_score.append((pair,score))\n",
    "\n",
    "#             cand_pairs.append((pair,score))\n",
    "#     print(sentence)\n",
    "#     print(cand_pairs)\n",
    "    return cand_pairs,cand_pairs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _Sents[3]\n",
    "\n",
    "# p('year')\n",
    "# p('close')\n",
    "\n",
    "# pmi_fopair(\"player close year absolutly amazing piece equipment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算FO-PAIR以及句子抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Product : B01GW45SB8 , Processing ID 1471132800  :   7%|▋         | 190/2564 [00:30<06:18,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "i = 0    \n",
    "\n",
    "if not os.path.exists('Train'):\n",
    "    os.makedirs('Train')\n",
    "    \n",
    "pattern_text = open(\"View/review_pattern.txt\",'w',encoding=\"utf-8\")\n",
    "# training_data = open(\"View/%s_%s_train.txt\"%(category1,category2),'w',encoding=\"utf-8\")\n",
    "# training_data.write(\"rev_sent | polarity,subjectivity | f1_o1 f2_o2 f3_o3 | summary\" + '\\n') \n",
    "\n",
    "topic_train_df = {}\n",
    "key_train_df = {}\n",
    "i = 0\n",
    "j = 0\n",
    "with tqdm(total=Reviews) as pbar:\n",
    "    with open(\"View/result.txt\",'w',encoding=\"utf-8\") as result:\n",
    "        for asin,DATA_DICT in PROD_DICT.items():\n",
    "            REVIEW_ITEM_LIST = DATA_DICT['REVIEW_ITEM_LIST']\n",
    "            title = DATA_DICT['title']\n",
    "#             if \"(new Date()).getTime();\" in title : continue\n",
    "            description = DATA_DICT['description']\n",
    "            feature = DATA_DICT['feature']\n",
    "            big_categories = DATA_DICT[\"category1\"]\n",
    "            small_categories = DATA_DICT[\"category2\"]\n",
    "            if \"(new Date()).getTime();\" not in title :result.write(\"title:\" + title+\"\\n\")\n",
    "            else: result.write(\"title:\" + asin+\"\\n\")\n",
    "\n",
    "            result.write('************************************************************************'+\"\\n\")      \n",
    "            for pair in REVIEW_ITEM_LIST:\n",
    "                review = pair[\"review\"]  \n",
    "                rev_dash_keywords, newreview = cleanReview(review)\n",
    "#                 f.write('newreview:\\n'+newreview+\"\\n\")\n",
    "                vote = pair[\"vote\"]  \n",
    "                overall = pair[\"overall\"] \n",
    "                summary = pair[\"summary\"]\n",
    "                lemm_review = pair[\"lemm_review\"]\n",
    "                lemm_summary = pair[\"lemm_summary\"]\n",
    "                lemm_review_len = pair['lemm_review_len'] \n",
    "                lemm_summary_len = pair['lemm_summary_len'] \n",
    "\n",
    "                review_ID = pair[\"review_ID\"]\n",
    "                \n",
    "                \n",
    "                pbar.set_description(\"Product : %s , Processing ID %s  \" % (asin,review_ID))\n",
    "                pbar.update(1)\n",
    "            \n",
    "                summ_token_set = set(nltk.word_tokenize(lemm_summary))\n",
    "                if len(summ_token_set & total_keywords) == 0: continue\n",
    "                result.write('overall:' + str(overall) + \"\\n\")\n",
    "                result.write('review:\\n' + review + \"\\n\")\n",
    "                result.write('summary:\\n' + summary + \"\\n\")\n",
    "                result.write(\"************************************************\\n\")\n",
    "     \n",
    "                extract_sents = sentence_extract_blob(review)\n",
    "                keyword_list = []\n",
    "                rev_summ_row = {}\n",
    "#                 lemm_review,_ = lemm_sent_process2(review, remove_stopwords=False, summary=False, mode=\"spacy\", withdot=True)\n",
    "#                 print('lemm_review:')\n",
    "#                 print(lemm_review)\n",
    "                extract_sents = sentence_extract_blob(lemm_review)\n",
    "                for rev_sent in extract_sents:\n",
    "                    fop_row = {} # fop train row\n",
    "                    fop_row['overall'] = overall\n",
    "                    sentiment = TextBlob(rev_sent).sentences[0].sentiment\n",
    "                    lemm_sent = rev_sent\n",
    "#                     rev_sent = rev_sent.replace(\"s.\",\"\")\n",
    "#                     rev_sent = re.sub(r'(?:^| )\\w(?:$| )', \" \", rev_sent).strip() # remove single alphbet\n",
    "#                     rev_sent = rev_sent.replace(\" ve \",\" \").replace(\" ha \",\" \")                  \n",
    "#                     lemm_sent,pos_sent = lemm_sent_process2(rev_sent, remove_stopwords=False, summary=False, mode=\"spacy\", withdot=False)\n",
    "    #                     _,pos_sent = get_pos_sequence(rev_sent,remove_stopwords=False,summary=False,mode = \"spacy\",withdot =True)\n",
    "#                     if len(pos_sent) == 0: continue\n",
    "#                     print('lemm_sent:',lemm_sent)\n",
    "                    token_set = set(nltk.word_tokenize(rev_sent))\n",
    "                    mention_features = list(token_set & total_keywords)                   \n",
    "                    POS_fops = FO_rule_POS(lemm_sent).run()\n",
    "                    DEP_fops = FOP_rule_Depend(lemm_sent).run()\n",
    "                    POS_fops = [(f,o) for f,o in POS_fops if f in total_keywords]\n",
    "                    DEP_fops = [(f,o) for f,o in DEP_fops if f in total_keywords]\n",
    "#                     PMI_fops,cand_pairs_score = pmi_fopair(lemm_sent) # 皆為空有問題                     \n",
    "\n",
    "                    result.write(\"************************************************\\n\")                     \n",
    "                    result.write('rev_sent:\\n' + rev_sent + \"\\n\") ;fop_row['rev_sent'] = rev_sent\n",
    "                    result.write('lemm_sent:\\n' + lemm_sent) ;fop_row['lemm_sent'] = lemm_sent\n",
    "                    \n",
    "#                     result.write(str(pos_sent)+\"\\n\");pattern_text.write(str(pos_sent)+\"\\n\") ; \n",
    "                    result.write('mention features:' + str(mention_features) + \"\\n\")\n",
    "                    result.write(\"polarity: %s subjectivity: %s \\n\"%(sentiment.polarity,sentiment.subjectivity))\n",
    "                    fop_row['polarity'] = sentiment.polarity;fop_row['subjectivity'] = sentiment.subjectivity\n",
    "                    fop_row['mention_features'] = \",\".join(mention_features)\n",
    "                \n",
    "                    result.write(\"POS_fops:\"+str(POS_fops)+\"\\n\") ;pattern_text.write(\"POS_fops:\"+str(POS_fops)+\"\\n\")\n",
    "                    result.write(\"DEP_fops:\"+str(DEP_fops)+\"\\n\") ;pattern_text.write(\"DEP_fops:\"+str(DEP_fops)+\"\\n\")\n",
    "#                     f.write(\"PMI_fops:\"+str(cand_pairs_score)+\"\\n\")\n",
    "#                     print(cand_pairs_score)\n",
    "                    result.write(\"************************************************\\n\") \n",
    "                    if \"not\" in token_set: continue              \n",
    "                    if overall > 3 and sentiment.polarity < 0: continue\n",
    "                    if overall < 3 and sentiment.polarity > 0: continue\n",
    "                    if (len(POS_fops) == 0) and (len(DEP_fops) == 0): continue\n",
    "                        \n",
    "#                     fop_list = set(PMI_fops + POS_fops + DEP_fops)\n",
    "                    fop_list = set(POS_fops + DEP_fops)\n",
    "                    fop_prob_list = []\n",
    "                    \n",
    "                    for fop in fop_list:\n",
    "                        p1,p2,p3 = 0,0,0; y1,y2,y3 = 0.6,0.2,0.2;\n",
    "                        feat,opinion = fop\n",
    "#                         if fop in PMI_fops: \n",
    "#                             p1 = pmi(feat,opinion)\n",
    "                        if fop in POS_fops: p2 = 1\n",
    "                        if fop in DEP_fops: p3 = 1\n",
    "#                         p = y1*p1 + y2*p2 + y3*p3\n",
    "                        p = y2*p2 + y3*p3\n",
    "                        fop_prob_list.append((fop,p))\n",
    "                    result.write(\"fop_prob_list:\" + str(fop_prob_list)+ \"\\n\")   \n",
    "                    i +=1 \n",
    "                    result.write(\"\\n\");pattern_text.write(\"\\n\")\n",
    "                    # ------------------------------------------------------\n",
    "#                     training_data.write(\"%s\"%(rev_sent) + \"|\") \n",
    "#                     training_data.write(\"%s|%s|\"%(sentiment.polarity,sentiment.subjectivity))\n",
    "#                     if len(token_set & total_keywords) == 0: continue                    \n",
    "                    \n",
    "                    fop_str = \"\"\n",
    "                    for idx,fop_score in enumerate(fop_prob_list):\n",
    "                        fop,score = fop_score\n",
    "                        feat,o = fop                        \n",
    "                        if idx == len(fop_prob_list) - 1:\n",
    "                            fop_str += \"%s %s\"%(feat,o)\n",
    "                        else:     \n",
    "                            fop_str += \"%s %s,\"%(feat,o)\n",
    "                    fop_row[\"fops\"] = fop_str \n",
    "                    keyword_list.append(fop_str)\n",
    "                    fop_row['review_ID'] = review_ID\n",
    "                    fop_row['summary'] = lemm_sent_process2(summary, remove_stopwords=False, summary=False, mode=\"spacy\", withdot=False)[0]\n",
    "                    \n",
    "                    if abs(sentiment.polarity) <= 0.5: continue\n",
    "                    if abs(sentiment.subjectivity) < 0.4: continue\n",
    "                    if len(mention_features) == 0: continue\n",
    "                    topic_train_df[i] = fop_row ;i +=1       \n",
    "                \n",
    "                    \n",
    "                total_keyword = \" \".join(keyword_list)\n",
    "                rev_summ_row['review_ID'] = review_ID\n",
    "                rev_summ_row['big_categories'] = big_categories\n",
    "                rev_summ_row['small_categories'] = small_categories\n",
    "                rev_summ_row['review'] = review\n",
    "                rev_summ_row['summary'] = summary\n",
    "                rev_summ_row['big_categories'] = big_categories\n",
    "                rev_summ_row['small_categories'] = small_categories\n",
    "                rev_summ_row['lemm_review'] = lemm_review\n",
    "                rev_summ_row['lemm_summary'] = lemm_summary\n",
    "                rev_summ_row['lemm_review_len'] = lemm_review_len\n",
    "                rev_summ_row['lemm_summary_len'] = lemm_summary_len\n",
    "                rev_summ_row['overall'] = overall\n",
    "                rev_summ_row['total_keyword'] = total_keyword\n",
    "                key_train_df[j] = rev_summ_row;\n",
    "                j += 1    \n",
    "                if j > 50: break\n",
    "#                 f.write(\"---------------------------------------------------\\n\")\n",
    "            \n",
    "#             break  \n",
    "            \n",
    "print(\"finished...\")\n",
    "# training_data.close()\n",
    "topic_train_df = pd.DataFrame.from_dict(topic_train_df, orient='index')\n",
    "key_train_df = pd.DataFrame.from_dict(key_train_df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topic-to-eaasy DataSet 建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Cell Phones & Accessories_Cell Phones_topic.xlsx Write finished\n",
      " Write finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"Train/%s_%s_topic.xlsx\"%(category1,category2)\n",
    "# df.to_csv(csv_path) #默认dt是DataFrame的一个实例，参数解释如下\n",
    "topic_train_df.to_excel(csv_path, encoding='utf8')\n",
    "print(csv_path + \" Write finished\")\n",
    "print(\" Write finished\")\n",
    "topic_train_df.head()\n",
    "len(topic_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key word Attention DataSet 建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Cell Phones & Accessories_Cell Phones_key.xlsx Write finished\n",
      " Write finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"Train/%s_%s_key.xlsx\"%(category1,category2)\n",
    "# df.to_csv(csv_path) #默认dt是DataFrame的一个实例，参数解释如下\n",
    "key_train_df.to_excel(csv_path, encoding='utf8')\n",
    "print(csv_path + \" Write finished\")\n",
    "print(\" Write finished\")\n",
    "len(key_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
